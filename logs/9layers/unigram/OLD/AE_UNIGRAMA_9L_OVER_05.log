[2017-10-20 01:51:10,231 AE_UNIGRAMA_9L_OVER_05.py:148]: >> Initializing execution of experiment AE_UNIGRAMA_9L_OVER_05
[2017-10-20 01:51:10,231 AE_UNIGRAMA_9L_OVER_05.py:149]: >> Printing header log
[2017-10-20 01:51:10,231 AE_UNIGRAMA_9L_OVER_05.py:38]: 
	=======================================
	network_name = AE_UNIGRAMA_9L_OVER_05
	layers = 96,172,156,139,123,107,91,74,58,42,9
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/9layers/unigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/9layers/unigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/9layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/9layers/unigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/9layers/unigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/9layers/unigram/', 'data_dir': '/home/dhiegorp/malware_deepnn/', 'fullds_data_dir': '/home/dhiegorp/malware_deepnn/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f48dcae9748>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f48dcae9828>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-10-20 01:51:10,231 AE_UNIGRAMA_9L_OVER_05.py:151]: >> Loading dataset... 
[2017-10-20 01:51:10,747 AE_UNIGRAMA_9L_OVER_05.py:55]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_deepnn/	
	trainx shape = (1627, 96)
	trainy shape = (1627, 9)
	valx shape = (1076, 96)
	valy shape = (1076, 9)
	=======================================
	
[2017-10-20 01:51:10,747 AE_UNIGRAMA_9L_OVER_05.py:153]: >> Executing autoencoder part ... 
[2017-10-20 01:51:10,747 AE_UNIGRAMA_9L_OVER_05.py:60]: =======================================
[2017-10-20 01:51:10,747 AE_UNIGRAMA_9L_OVER_05.py:65]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f48dcae9748>, 'discard_decoder_function': True}
[2017-10-20 01:51:10,935 AE_UNIGRAMA_9L_OVER_05.py:76]: training and evaluate autoencoder
[2017-10-20 01:52:27,207 AE_UNIGRAMA_9L_OVER_05.py:88]: trained and evaluated!
[2017-10-20 01:52:27,208 AE_UNIGRAMA_9L_OVER_05.py:91]: Training history: 
{'val_loss': [0.010190376014791457, 0.0099684487765163299, 0.009759131265063269, 0.0095616450159356943, 0.0093757852593429909, 0.0091985347421085079, 0.0090290789897501684, 0.0088675029257305488, 0.0087149444912113672, 0.0085707292390024802, 0.0084340805741767895, 0.0083032918117635756, 0.0081794368324725163, 0.0080620628300375648, 0.0079508219814433487, 0.0078448867563809168, 0.0077444867968393082, 0.0076493109199028035, 0.0075589641928672791, 0.0074731818396861225, 0.0073917115342605954, 0.0073143430054187775, 0.0072408506688298345, 0.0071709651714640926, 0.0071045109884446435, 0.0070412506117419685, 0.006981086377123696, 0.0069238011704473908, 0.0068692299606603986, 0.0068172530174172279, 0.0067677302407204666, 0.0067205457518839705, 0.0066755778540521305, 0.0066326555993311024, 0.0065917154431315381, 0.0065525766759833882, 0.0065152228577928031, 0.0064795714764956207, 0.0064455163820913292, 0.0064129769476649933, 0.0063818916297590424, 0.0063521561756397713, 0.006323757815108977, 0.0062965925481710512, 0.0062706225766319106, 0.0062457520850556711, 0.0062219179270020424, 0.0061991081043357744, 0.0061772804184501724, 0.0061563555199694456, 0.0061362928132696224, 0.0061170769349903863, 0.0060986484016627629, 0.0060809849377126057, 0.0060640282852775992, 0.0060477371009059772, 0.0060321027534186401, 0.0060170914728512994, 0.0060026672907644487, 0.0059888423351245517, 0.0059755579862729767, 0.0059627818400698083, 0.0059505098890582425, 0.0059387085675417714, 0.0059273692235682976, 0.0059164570848765649, 0.0059059706038785027, 0.0058958799349746521, 0.0058861828485302983, 0.0058768277652007719, 0.0058678459621910491, 0.0058591865768247596, 0.0058508376473193936, 0.0058428130690770077, 0.0058350878954626147, 0.0058276197963072242, 0.0058204196185759893, 0.0058134864707612413, 0.0058068001961342468, 0.0058003499009906135, 0.0057941247383651905, 0.0057881337081577252, 0.0057823692715023977, 0.00577681130109888, 0.0057714463533577419, 0.0057662439993046034, 0.0057612348202491339, 0.0057563997812042893, 0.0057517404886149566, 0.0057472393408306913, 0.0057429011052536703, 0.0057387053274089075, 0.0057346542767460242, 0.0057307405615412395, 0.0057269562534350889, 0.0057233090072759465, 0.0057197879882162385, 0.0057163857716416118, 0.0057130911401351812, 0.0057099146394536844, 0.0057068488276719384, 0.0057038795514565421], 'loss': [0.010300707085757586, 0.010074232481122384, 0.0098581971736886997, 0.0096545093970461428, 0.0094624761762194792, 0.0092810394074115277, 0.0091072416600451265, 0.0089410867790457437, 0.0087835924475515043, 0.0086348780568805874, 0.0084941809689002157, 0.008360006570010077, 0.0082322766382725756, 0.0081112885380595391, 0.0079965721506719285, 0.0078875788186402607, 0.0077839572584804907, 0.0076857438819287159, 0.0075925881173015191, 0.0075040949716335178, 0.0074200606982200218, 0.0073401959107521307, 0.0072643070764491623, 0.007192191556519666, 0.0071235847923865735, 0.0070583030796703122, 0.0069961516131070503, 0.0069369908617425893, 0.0068806298566523073, 0.0068269244340244395, 0.0067757341745134942, 0.0067269274974659718, 0.0066804080447680058, 0.0066360423396345514, 0.0065936695551929037, 0.0065532128118235018, 0.0065145328579116766, 0.0064775935851023765, 0.0064423158135180391, 0.0064085776704124828, 0.0063763392603193129, 0.0063455077895657663, 0.0063160094260807662, 0.0062878290769434004, 0.006260835109696424, 0.0062349982546515345, 0.0062102377346013324, 0.0061864971907920428, 0.0061637772022235335, 0.0061419998640772676, 0.0061211126819871176, 0.0061010652222884126, 0.0060818547797380738, 0.0060634256664551358, 0.0060457197520549892, 0.0060287257358798167, 0.0060123877606131984, 0.0059966939468836591, 0.0059816169717933554, 0.0059671079102828041, 0.0059531937910274777, 0.0059398040965373483, 0.0059269271641008018, 0.0059145429428064649, 0.0059026250442273604, 0.0058911606965938003, 0.0058801099054112379, 0.0058694879275980761, 0.0058592640721544251, 0.0058494143201737736, 0.005839915055445022, 0.0058307830351917158, 0.0058219623319763457, 0.0058134720942947717, 0.0058052800587174089, 0.005797384920705753, 0.0057897574171542976, 0.0057823899226090297, 0.0057752867067288821, 0.0057684314928344409, 0.0057618058691306139, 0.0057554151376305214, 0.005749260614034803, 0.005743311785651484, 0.005737583155525687, 0.0057320428091160308, 0.0057266710589959826, 0.0057214925583101671, 0.005716477839600183, 0.0057116537921579431, 0.0057069856164600301, 0.0057024681544562165, 0.0056981073718772174, 0.0056938743270297104, 0.0056897981485011908, 0.0056858445668264484, 0.0056820335201506937, 0.0056783426418492928, 0.005674778983241935, 0.0056713133129922613, 0.0056679740593354802, 0.005664749806557943]}
[2017-10-20 01:52:27,208 AE_UNIGRAMA_9L_OVER_05.py:95]: done!
[2017-10-20 01:52:27,208 AE_UNIGRAMA_9L_OVER_05.py:155]: >> Executing classifier part ... 
[2017-10-20 01:52:27,208 AE_UNIGRAMA_9L_OVER_05.py:100]: =======================================
[2017-10-20 01:52:27,208 AE_UNIGRAMA_9L_OVER_05.py:104]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f48dcae9828>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-10-20 01:52:27,240 AE_UNIGRAMA_9L_OVER_05.py:113]: training ... 
[2017-10-20 01:54:25,662 AE_UNIGRAMA_9L_OVER_05.py:125]: trained!
[2017-10-20 01:54:25,663 AE_UNIGRAMA_9L_OVER_05.py:128]: Training history: 
{'val_loss': [0.010190376014791457, 0.0099684487765163299, 0.009759131265063269, 0.0095616450159356943, 0.0093757852593429909, 0.0091985347421085079, 0.0090290789897501684, 0.0088675029257305488, 0.0087149444912113672, 0.0085707292390024802, 0.0084340805741767895, 0.0083032918117635756, 0.0081794368324725163, 0.0080620628300375648, 0.0079508219814433487, 0.0078448867563809168, 0.0077444867968393082, 0.0076493109199028035, 0.0075589641928672791, 0.0074731818396861225, 0.0073917115342605954, 0.0073143430054187775, 0.0072408506688298345, 0.0071709651714640926, 0.0071045109884446435, 0.0070412506117419685, 0.006981086377123696, 0.0069238011704473908, 0.0068692299606603986, 0.0068172530174172279, 0.0067677302407204666, 0.0067205457518839705, 0.0066755778540521305, 0.0066326555993311024, 0.0065917154431315381, 0.0065525766759833882, 0.0065152228577928031, 0.0064795714764956207, 0.0064455163820913292, 0.0064129769476649933, 0.0063818916297590424, 0.0063521561756397713, 0.006323757815108977, 0.0062965925481710512, 0.0062706225766319106, 0.0062457520850556711, 0.0062219179270020424, 0.0061991081043357744, 0.0061772804184501724, 0.0061563555199694456, 0.0061362928132696224, 0.0061170769349903863, 0.0060986484016627629, 0.0060809849377126057, 0.0060640282852775992, 0.0060477371009059772, 0.0060321027534186401, 0.0060170914728512994, 0.0060026672907644487, 0.0059888423351245517, 0.0059755579862729767, 0.0059627818400698083, 0.0059505098890582425, 0.0059387085675417714, 0.0059273692235682976, 0.0059164570848765649, 0.0059059706038785027, 0.0058958799349746521, 0.0058861828485302983, 0.0058768277652007719, 0.0058678459621910491, 0.0058591865768247596, 0.0058508376473193936, 0.0058428130690770077, 0.0058350878954626147, 0.0058276197963072242, 0.0058204196185759893, 0.0058134864707612413, 0.0058068001961342468, 0.0058003499009906135, 0.0057941247383651905, 0.0057881337081577252, 0.0057823692715023977, 0.00577681130109888, 0.0057714463533577419, 0.0057662439993046034, 0.0057612348202491339, 0.0057563997812042893, 0.0057517404886149566, 0.0057472393408306913, 0.0057429011052536703, 0.0057387053274089075, 0.0057346542767460242, 0.0057307405615412395, 0.0057269562534350889, 0.0057233090072759465, 0.0057197879882162385, 0.0057163857716416118, 0.0057130911401351812, 0.0057099146394536844, 0.0057068488276719384, 0.0057038795514565421], 'loss': [0.010300707085757586, 0.010074232481122384, 0.0098581971736886997, 0.0096545093970461428, 0.0094624761762194792, 0.0092810394074115277, 0.0091072416600451265, 0.0089410867790457437, 0.0087835924475515043, 0.0086348780568805874, 0.0084941809689002157, 0.008360006570010077, 0.0082322766382725756, 0.0081112885380595391, 0.0079965721506719285, 0.0078875788186402607, 0.0077839572584804907, 0.0076857438819287159, 0.0075925881173015191, 0.0075040949716335178, 0.0074200606982200218, 0.0073401959107521307, 0.0072643070764491623, 0.007192191556519666, 0.0071235847923865735, 0.0070583030796703122, 0.0069961516131070503, 0.0069369908617425893, 0.0068806298566523073, 0.0068269244340244395, 0.0067757341745134942, 0.0067269274974659718, 0.0066804080447680058, 0.0066360423396345514, 0.0065936695551929037, 0.0065532128118235018, 0.0065145328579116766, 0.0064775935851023765, 0.0064423158135180391, 0.0064085776704124828, 0.0063763392603193129, 0.0063455077895657663, 0.0063160094260807662, 0.0062878290769434004, 0.006260835109696424, 0.0062349982546515345, 0.0062102377346013324, 0.0061864971907920428, 0.0061637772022235335, 0.0061419998640772676, 0.0061211126819871176, 0.0061010652222884126, 0.0060818547797380738, 0.0060634256664551358, 0.0060457197520549892, 0.0060287257358798167, 0.0060123877606131984, 0.0059966939468836591, 0.0059816169717933554, 0.0059671079102828041, 0.0059531937910274777, 0.0059398040965373483, 0.0059269271641008018, 0.0059145429428064649, 0.0059026250442273604, 0.0058911606965938003, 0.0058801099054112379, 0.0058694879275980761, 0.0058592640721544251, 0.0058494143201737736, 0.005839915055445022, 0.0058307830351917158, 0.0058219623319763457, 0.0058134720942947717, 0.0058052800587174089, 0.005797384920705753, 0.0057897574171542976, 0.0057823899226090297, 0.0057752867067288821, 0.0057684314928344409, 0.0057618058691306139, 0.0057554151376305214, 0.005749260614034803, 0.005743311785651484, 0.005737583155525687, 0.0057320428091160308, 0.0057266710589959826, 0.0057214925583101671, 0.005716477839600183, 0.0057116537921579431, 0.0057069856164600301, 0.0057024681544562165, 0.0056981073718772174, 0.0056938743270297104, 0.0056897981485011908, 0.0056858445668264484, 0.0056820335201506937, 0.0056783426418492928, 0.005674778983241935, 0.0056713133129922613, 0.0056679740593354802, 0.005664749806557943]}
[2017-10-20 01:54:25,663 AE_UNIGRAMA_9L_OVER_05.py:132]: evaluating model ... 
[2017-10-20 01:54:25,755 AE_UNIGRAMA_9L_OVER_05.py:136]: evaluated! 
[2017-10-20 01:54:25,755 AE_UNIGRAMA_9L_OVER_05.py:138]: generating reports ... 
[2017-10-20 01:54:26,363 AE_UNIGRAMA_9L_OVER_05.py:141]: done!
[2017-10-20 01:54:26,364 AE_UNIGRAMA_9L_OVER_05.py:157]: >> experiment AE_UNIGRAMA_9L_OVER_05 finished!
