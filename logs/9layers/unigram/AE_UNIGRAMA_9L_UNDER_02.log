[2017-10-20 01:43:26,422 AE_UNIGRAMA_9L_UNDER_02.py:148]: >> Initializing execution of experiment AE_UNIGRAMA_9L_UNDER_02
[2017-10-20 01:43:26,423 AE_UNIGRAMA_9L_UNDER_02.py:149]: >> Printing header log
[2017-10-20 01:43:26,423 AE_UNIGRAMA_9L_UNDER_02.py:38]: 
	=======================================
	network_name = AE_UNIGRAMA_9L_UNDER_02
	layers = 96,76,69,63,56,49,43,36,29,22,9
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/9layers/unigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/9layers/unigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/9layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/9layers/unigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/9layers/unigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/9layers/unigram/', 'data_dir': '/home/dhiegorp/malware_deepnn/', 'fullds_data_dir': '/home/dhiegorp/malware_deepnn/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fed821ddb70>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fed821ddcf8>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-10-20 01:43:26,423 AE_UNIGRAMA_9L_UNDER_02.py:151]: >> Loading dataset... 
[2017-10-20 01:43:27,036 AE_UNIGRAMA_9L_UNDER_02.py:55]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_deepnn/	
	trainx shape = (1627, 96)
	trainy shape = (1627, 9)
	valx shape = (1076, 96)
	valy shape = (1076, 9)
	=======================================
	
[2017-10-20 01:43:27,036 AE_UNIGRAMA_9L_UNDER_02.py:153]: >> Executing autoencoder part ... 
[2017-10-20 01:43:27,037 AE_UNIGRAMA_9L_UNDER_02.py:60]: =======================================
[2017-10-20 01:43:27,037 AE_UNIGRAMA_9L_UNDER_02.py:65]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fed821ddb70>, 'discard_decoder_function': True}
[2017-10-20 01:43:27,248 AE_UNIGRAMA_9L_UNDER_02.py:76]: training and evaluate autoencoder
[2017-10-20 01:44:17,127 AE_UNIGRAMA_9L_UNDER_02.py:88]: trained and evaluated!
[2017-10-20 01:44:17,128 AE_UNIGRAMA_9L_UNDER_02.py:91]: Training history: 
{'val_loss': [0.010327834410932206, 0.010242996571177001, 0.010163031708878435, 0.010088128861746159, 0.010018726045674329, 0.0099542430813206167, 0.0098942164400917885, 0.0098382945632956732, 0.0097855980850783866, 0.0097362981852988778, 0.0096902092226320482, 0.009647194496927209, 0.0096069570270723575, 0.0095693061065939729, 0.0095339986983839029, 0.0095008665758105453, 0.0094697746036221109, 0.0094405423164312281, 0.009413045083196854, 0.0093871512585302266, 0.0093626437438686545, 0.0093395252985582038, 0.0093177276071451862, 0.0092971215862434585, 0.0092775966023479258, 0.009259127364338109, 0.009241626120162055, 0.0092250247274743586, 0.0092092536798363284, 0.009194250367443358, 0.0091799674313307687, 0.0091663104028451396, 0.0091525057510760191, 0.0091392992888352249, 0.0091266888349583603, 0.0091146340863744561, 0.0091030682752178942, 0.0090919997937783433, 0.0090813678533503556, 0.0090711642624498739, 0.0090613877366134219, 0.0090519843906939693, 0.0090429311978274113, 0.0090342258625978871, 0.0090258448803624252, 0.0090177753199321194, 0.0090099984267553428, 0.009002477290683519, 0.0089952288243961166, 0.0089882295717204821, 0.0089814740797456317, 0.0089749560750273082, 0.0089686406935568629, 0.0089625518796723133, 0.0089566502997090822, 0.0089509526863140246, 0.0089454188783649625, 0.0089400679246976027, 0.0089348800286484474, 0.008929852925961124, 0.0089249866616437877, 0.0089202637067514728, 0.0089156818974306142, 0.0089112434460051002, 0.0089069352239425741, 0.0089027567742371652, 0.008898687933235807, 0.0088947444884141143, 0.0088909128092257064, 0.0088871947306184071, 0.0088835873305243632, 0.0088800774042435745, 0.0088766681646658147, 0.0088733503712708182, 0.0088701299616727684, 0.0088669992394773047, 0.0088639471049811753, 0.0088609820093308676, 0.0088580989808564291, 0.008855302028745746, 0.0088525754486151794, 0.0088499212969911593, 0.0088473364267650598, 0.0088448258182622468, 0.0088423818322140926, 0.0088400002811311797, 0.0088376826779799165, 0.0088354336222474465, 0.0088332403973991318, 0.0088311025273871916, 0.0088290207081069289, 0.0088269925368153479, 0.0088250182506707971, 0.0088230955629128287, 0.0088212222508310609, 0.0088193979370494308, 0.0088176227496680713, 0.008815890972651402, 0.008814202645814331, 0.0088125606721827978, 0.0088107675629006659, 0.0088079807462800828], 'loss': [0.01036700639660804, 0.010280660250565905, 0.010199106420884304, 0.010122273642782719, 0.01005085309613645, 0.0099846135519248026, 0.0099229967065920261, 0.009865691067940557, 0.0098120398509196505, 0.0097615956657949084, 0.0097144772421703714, 0.0096704383657188527, 0.0096293247494326824, 0.0095908757077086976, 0.0095548705909385152, 0.0095211148523046112, 0.0094894319870031246, 0.0094596896333174613, 0.0094317228137787389, 0.0094054001002826064, 0.0093805692504951767, 0.0093571037350573937, 0.0093349620617454443, 0.0093140838800551112, 0.0092943303662534858, 0.0092756194986984564, 0.0092579083884685547, 0.0092411278784229264, 0.0092252040105587369, 0.0092100666620675304, 0.0091956632959381088, 0.0091819454198582961, 0.0091684557555887919, 0.0091552312824172196, 0.0091425788456593552, 0.0091305004905613933, 0.0091189384396338502, 0.0091078581950171048, 0.0090972438853030267, 0.0090870476346726278, 0.0090772676434466345, 0.0090678866160432466, 0.0090588660579868519, 0.0090501731222686621, 0.0090418258560540938, 0.009033782889044167, 0.0090260335645502739, 0.00901856603983563, 0.0090113423676553422, 0.0090043842449060735, 0.0089976649362761042, 0.0089911737604628835, 0.008984913771311491, 0.0089788434485496733, 0.0089729962276309718, 0.0089673266466191812, 0.0089618480617218253, 0.00895653764609094, 0.0089513898655982863, 0.0089464019205519659, 0.0089415706609408396, 0.0089368936997857203, 0.0089323547191931633, 0.0089279488158385371, 0.0089236844294391382, 0.0089195433863253782, 0.0089155262955490176, 0.0089116181878312346, 0.0089078300346900289, 0.0089041434249027274, 0.0089005690689651449, 0.008897100001706364, 0.0088937303260857147, 0.0088904493270280469, 0.0088872686062825557, 0.0088841685110539401, 0.0088811656317061716, 0.0088782298161700305, 0.0088753841603295015, 0.0088726135226082203, 0.0088699239454407882, 0.0088673085615396056, 0.0088647624538420317, 0.0088622796640587722, 0.0088598691384962312, 0.0088575243412962919, 0.0088552391910999926, 0.0088530137062246776, 0.0088508534791851314, 0.0088487491606933314, 0.0088467020020529981, 0.0088447042945234335, 0.0088427589282384297, 0.0088408691407889407, 0.0088390275136497245, 0.0088372317073522125, 0.0088354842514074428, 0.0088337815647742176, 0.0088321217607658705, 0.008830506835973145, 0.0088289241651651248, 0.0088266681635909162]}
[2017-10-20 01:44:17,128 AE_UNIGRAMA_9L_UNDER_02.py:95]: done!
[2017-10-20 01:44:17,128 AE_UNIGRAMA_9L_UNDER_02.py:155]: >> Executing classifier part ... 
[2017-10-20 01:44:17,128 AE_UNIGRAMA_9L_UNDER_02.py:100]: =======================================
[2017-10-20 01:44:17,128 AE_UNIGRAMA_9L_UNDER_02.py:104]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fed821ddcf8>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-10-20 01:44:17,166 AE_UNIGRAMA_9L_UNDER_02.py:113]: training ... 
[2017-10-20 01:45:31,410 AE_UNIGRAMA_9L_UNDER_02.py:125]: trained!
[2017-10-20 01:45:31,411 AE_UNIGRAMA_9L_UNDER_02.py:128]: Training history: 
{'val_loss': [0.010327834410932206, 0.010242996571177001, 0.010163031708878435, 0.010088128861746159, 0.010018726045674329, 0.0099542430813206167, 0.0098942164400917885, 0.0098382945632956732, 0.0097855980850783866, 0.0097362981852988778, 0.0096902092226320482, 0.009647194496927209, 0.0096069570270723575, 0.0095693061065939729, 0.0095339986983839029, 0.0095008665758105453, 0.0094697746036221109, 0.0094405423164312281, 0.009413045083196854, 0.0093871512585302266, 0.0093626437438686545, 0.0093395252985582038, 0.0093177276071451862, 0.0092971215862434585, 0.0092775966023479258, 0.009259127364338109, 0.009241626120162055, 0.0092250247274743586, 0.0092092536798363284, 0.009194250367443358, 0.0091799674313307687, 0.0091663104028451396, 0.0091525057510760191, 0.0091392992888352249, 0.0091266888349583603, 0.0091146340863744561, 0.0091030682752178942, 0.0090919997937783433, 0.0090813678533503556, 0.0090711642624498739, 0.0090613877366134219, 0.0090519843906939693, 0.0090429311978274113, 0.0090342258625978871, 0.0090258448803624252, 0.0090177753199321194, 0.0090099984267553428, 0.009002477290683519, 0.0089952288243961166, 0.0089882295717204821, 0.0089814740797456317, 0.0089749560750273082, 0.0089686406935568629, 0.0089625518796723133, 0.0089566502997090822, 0.0089509526863140246, 0.0089454188783649625, 0.0089400679246976027, 0.0089348800286484474, 0.008929852925961124, 0.0089249866616437877, 0.0089202637067514728, 0.0089156818974306142, 0.0089112434460051002, 0.0089069352239425741, 0.0089027567742371652, 0.008898687933235807, 0.0088947444884141143, 0.0088909128092257064, 0.0088871947306184071, 0.0088835873305243632, 0.0088800774042435745, 0.0088766681646658147, 0.0088733503712708182, 0.0088701299616727684, 0.0088669992394773047, 0.0088639471049811753, 0.0088609820093308676, 0.0088580989808564291, 0.008855302028745746, 0.0088525754486151794, 0.0088499212969911593, 0.0088473364267650598, 0.0088448258182622468, 0.0088423818322140926, 0.0088400002811311797, 0.0088376826779799165, 0.0088354336222474465, 0.0088332403973991318, 0.0088311025273871916, 0.0088290207081069289, 0.0088269925368153479, 0.0088250182506707971, 0.0088230955629128287, 0.0088212222508310609, 0.0088193979370494308, 0.0088176227496680713, 0.008815890972651402, 0.008814202645814331, 0.0088125606721827978, 0.0088107675629006659, 0.0088079807462800828], 'loss': [0.01036700639660804, 0.010280660250565905, 0.010199106420884304, 0.010122273642782719, 0.01005085309613645, 0.0099846135519248026, 0.0099229967065920261, 0.009865691067940557, 0.0098120398509196505, 0.0097615956657949084, 0.0097144772421703714, 0.0096704383657188527, 0.0096293247494326824, 0.0095908757077086976, 0.0095548705909385152, 0.0095211148523046112, 0.0094894319870031246, 0.0094596896333174613, 0.0094317228137787389, 0.0094054001002826064, 0.0093805692504951767, 0.0093571037350573937, 0.0093349620617454443, 0.0093140838800551112, 0.0092943303662534858, 0.0092756194986984564, 0.0092579083884685547, 0.0092411278784229264, 0.0092252040105587369, 0.0092100666620675304, 0.0091956632959381088, 0.0091819454198582961, 0.0091684557555887919, 0.0091552312824172196, 0.0091425788456593552, 0.0091305004905613933, 0.0091189384396338502, 0.0091078581950171048, 0.0090972438853030267, 0.0090870476346726278, 0.0090772676434466345, 0.0090678866160432466, 0.0090588660579868519, 0.0090501731222686621, 0.0090418258560540938, 0.009033782889044167, 0.0090260335645502739, 0.00901856603983563, 0.0090113423676553422, 0.0090043842449060735, 0.0089976649362761042, 0.0089911737604628835, 0.008984913771311491, 0.0089788434485496733, 0.0089729962276309718, 0.0089673266466191812, 0.0089618480617218253, 0.00895653764609094, 0.0089513898655982863, 0.0089464019205519659, 0.0089415706609408396, 0.0089368936997857203, 0.0089323547191931633, 0.0089279488158385371, 0.0089236844294391382, 0.0089195433863253782, 0.0089155262955490176, 0.0089116181878312346, 0.0089078300346900289, 0.0089041434249027274, 0.0089005690689651449, 0.008897100001706364, 0.0088937303260857147, 0.0088904493270280469, 0.0088872686062825557, 0.0088841685110539401, 0.0088811656317061716, 0.0088782298161700305, 0.0088753841603295015, 0.0088726135226082203, 0.0088699239454407882, 0.0088673085615396056, 0.0088647624538420317, 0.0088622796640587722, 0.0088598691384962312, 0.0088575243412962919, 0.0088552391910999926, 0.0088530137062246776, 0.0088508534791851314, 0.0088487491606933314, 0.0088467020020529981, 0.0088447042945234335, 0.0088427589282384297, 0.0088408691407889407, 0.0088390275136497245, 0.0088372317073522125, 0.0088354842514074428, 0.0088337815647742176, 0.0088321217607658705, 0.008830506835973145, 0.0088289241651651248, 0.0088266681635909162]}
[2017-10-20 01:45:31,411 AE_UNIGRAMA_9L_UNDER_02.py:132]: evaluating model ... 
[2017-10-20 01:45:31,476 AE_UNIGRAMA_9L_UNDER_02.py:136]: evaluated! 
[2017-10-20 01:45:31,477 AE_UNIGRAMA_9L_UNDER_02.py:138]: generating reports ... 
[2017-10-20 01:45:32,102 AE_UNIGRAMA_9L_UNDER_02.py:141]: done!
[2017-10-20 01:45:32,102 AE_UNIGRAMA_9L_UNDER_02.py:157]: >> experiment AE_UNIGRAMA_9L_UNDER_02 finished!
