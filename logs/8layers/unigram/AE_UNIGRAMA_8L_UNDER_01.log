[2017-10-20 01:48:20,910 AE_UNIGRAMA_8L_UNDER_01.py:148]: >> Initializing execution of experiment AE_UNIGRAMA_8L_UNDER_01
[2017-10-20 01:48:20,910 AE_UNIGRAMA_8L_UNDER_01.py:149]: >> Printing header log
[2017-10-20 01:48:20,910 AE_UNIGRAMA_8L_UNDER_01.py:38]: 
	=======================================
	network_name = AE_UNIGRAMA_8L_UNDER_01
	layers = 96,28,26,24,22,20,19,17,15,9
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/8layers/unigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/8layers/unigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/8layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/8layers/unigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/8layers/unigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/8layers/unigram/', 'data_dir': '/home/dhiegorp/malware_deepnn/', 'fullds_data_dir': '/home/dhiegorp/malware_deepnn/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f2923f357f0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f2923f358d0>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-10-20 01:48:20,910 AE_UNIGRAMA_8L_UNDER_01.py:151]: >> Loading dataset... 
[2017-10-20 01:48:21,451 AE_UNIGRAMA_8L_UNDER_01.py:55]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_deepnn/	
	trainx shape = (1627, 96)
	trainy shape = (1627, 9)
	valx shape = (1076, 96)
	valy shape = (1076, 9)
	=======================================
	
[2017-10-20 01:48:21,451 AE_UNIGRAMA_8L_UNDER_01.py:153]: >> Executing autoencoder part ... 
[2017-10-20 01:48:21,451 AE_UNIGRAMA_8L_UNDER_01.py:60]: =======================================
[2017-10-20 01:48:21,451 AE_UNIGRAMA_8L_UNDER_01.py:65]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f2923f357f0>, 'discard_decoder_function': True}
[2017-10-20 01:48:21,629 AE_UNIGRAMA_8L_UNDER_01.py:76]: training and evaluate autoencoder
[2017-10-20 01:48:58,579 AE_UNIGRAMA_8L_UNDER_01.py:88]: trained and evaluated!
[2017-10-20 01:48:58,579 AE_UNIGRAMA_8L_UNDER_01.py:91]: Training history: 
{'val_loss': [0.010280479649324178, 0.010150863124286154, 0.010026613191517977, 0.0099074579429116842, 0.0097931054313442085, 0.0096833357712094673, 0.0095778379353891954, 0.0094764448882367971, 0.009378914143128466, 0.0092851061055429802, 0.0091947932973448672, 0.0091077767686440596, 0.0090239885640521038, 0.0089432660587215069, 0.0088654213332221409, 0.0087903509553535729, 0.0087179135337305778, 0.0086480418157843415, 0.0085805706977567266, 0.0085154241366401928, 0.0084525177386835168, 0.0083916938991794796, 0.0083328844134636978, 0.0082760300231079637, 0.0082210475080352289, 0.0081678343673676369, 0.0081163421827178013, 0.0080664968810504698, 0.0080182512211145959, 0.0079714894471309226, 0.007926217467285222, 0.0078823365786954147, 0.0078398080778110876, 0.0077986042621133499, 0.0077586258453452009, 0.0077198576095572858, 0.0076822408883517341, 0.0076457699241469787, 0.0076103601500835116, 0.0075759761932143272, 0.0075426272836731931, 0.0075102269850311227, 0.0074787921632283464, 0.0074482396293473066, 0.0074185616748760628, 0.0073897208772655092, 0.0073616915974475195, 0.0073344572880011064, 0.0073079950709352927, 0.0072822645600871304, 0.0072572668737361425, 0.0072329488434035982, 0.0072093048171039849, 0.0071863025326461818, 0.0071639314183072084, 0.0071421692314034948, 0.0071209694490833794, 0.0071003564051785213, 0.007080289066100985, 0.0070607540575046519, 0.0070417386265021493, 0.0070232409312213223, 0.0070052260089817986, 0.006987697682014518, 0.0069705981581184503, 0.006953964089510831, 0.0069377604061841526, 0.0069219750355666012, 0.0069065813293687477, 0.0068916160904120335, 0.0068770159125937404, 0.006862808029309303, 0.0068489456840108761, 0.0068354479636179915, 0.0068223047724554533, 0.0068094794461884467, 0.0067969910475013428, 0.0067848207664478665, 0.0067729609983810486, 0.0067613916973614562, 0.0067501174888424716, 0.0067391380075656126, 0.0067284289335478855, 0.006717976541033354, 0.0067077921443795402, 0.0066978638354675034, 0.0066881958675678111, 0.0066787574516408502, 0.006669551360881351, 0.0066605734926229515, 0.006651830991044585, 0.0066432956931593247, 0.0066349792456926026, 0.0066268709678632175, 0.0066189541824189704, 0.0066112177636903679, 0.0066036873334347091, 0.0065963327154157109, 0.0065891706007342359, 0.0065821781858358686, 0.0065753498187350965, 0.006568698280016618], 'loss': [0.010342351087101706, 0.010209151958679229, 0.010081418966227648, 0.0099588884658923741, 0.009841343912867841, 0.0097284784074586524, 0.00962008553435173, 0.0095158697162613451, 0.0094156551450152084, 0.009319222145831212, 0.0092264207889404346, 0.0091370347796835699, 0.0090508816790089847, 0.0089678962720121866, 0.0088879094008624225, 0.0088107417725122768, 0.0087362862121687021, 0.0086644145218768807, 0.0085950566658357496, 0.0085280648957578482, 0.0084633471692678163, 0.0084008225511711439, 0.0083403465439369148, 0.0082818529131858352, 0.0082252702693504723, 0.0081705335114373341, 0.0081175334669177923, 0.0080662314993568043, 0.0080165489672350612, 0.0079684377697736633, 0.0079217890527078765, 0.0078766028320653783, 0.0078327890322958688, 0.0077903141927594555, 0.0077491373661733804, 0.0077091690471147419, 0.007670396636397599, 0.0076327642807124509, 0.0075962548916924744, 0.0075608000603459256, 0.0075263571347320051, 0.0074929294093971652, 0.0074604488645932682, 0.0074289198506900557, 0.007398259513750993, 0.0073684648216828545, 0.0073395022866559415, 0.0073113397308170206, 0.0072839597893218245, 0.0072573559494951275, 0.0072314666347153434, 0.0072063110787684778, 0.0071818336727671192, 0.0071580118863753513, 0.0071348325133589272, 0.0071122771313330401, 0.007090327711449621, 0.0070689384154082446, 0.0070481370266162876, 0.0070278628794539352, 0.0070081277116194373, 0.0069889050997683768, 0.0069702011856497175, 0.0069519688478901697, 0.006934233080794933, 0.0069169095026463106, 0.0069000572884900663, 0.0068836326604410228, 0.0068676266724789848, 0.0068520136150636221, 0.0068368184939484765, 0.0068219930824228933, 0.0068075608742007212, 0.0067934717896572456, 0.0067797405433456959, 0.0067663700511586319, 0.0067533164527828187, 0.0067405960131631205, 0.0067281981826529192, 0.00671610395586062, 0.0067043038180696684, 0.0066927979475879824, 0.0066815910717219424, 0.0066706494882716565, 0.0066599644054833301, 0.0066495549575423963, 0.0066393946306625429, 0.0066294959247153486, 0.0066198297526135572, 0.0066104025700768959, 0.0066011966960006652, 0.0065922239310489926, 0.0065834677701480529, 0.0065749236766063481, 0.0065665939346542003, 0.0065584446019627558, 0.0065504838600892118, 0.0065427327768438678, 0.0065351628298289067, 0.0065277739211610109, 0.0065205638962623231, 0.0065135179489928851]}
[2017-10-20 01:48:58,579 AE_UNIGRAMA_8L_UNDER_01.py:95]: done!
[2017-10-20 01:48:58,579 AE_UNIGRAMA_8L_UNDER_01.py:155]: >> Executing classifier part ... 
[2017-10-20 01:48:58,580 AE_UNIGRAMA_8L_UNDER_01.py:100]: =======================================
[2017-10-20 01:48:58,580 AE_UNIGRAMA_8L_UNDER_01.py:104]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f2923f358d0>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-10-20 01:48:58,614 AE_UNIGRAMA_8L_UNDER_01.py:113]: training ... 
[2017-10-20 01:49:58,906 AE_UNIGRAMA_8L_UNDER_01.py:125]: trained!
[2017-10-20 01:49:58,906 AE_UNIGRAMA_8L_UNDER_01.py:128]: Training history: 
{'val_loss': [0.010280479649324178, 0.010150863124286154, 0.010026613191517977, 0.0099074579429116842, 0.0097931054313442085, 0.0096833357712094673, 0.0095778379353891954, 0.0094764448882367971, 0.009378914143128466, 0.0092851061055429802, 0.0091947932973448672, 0.0091077767686440596, 0.0090239885640521038, 0.0089432660587215069, 0.0088654213332221409, 0.0087903509553535729, 0.0087179135337305778, 0.0086480418157843415, 0.0085805706977567266, 0.0085154241366401928, 0.0084525177386835168, 0.0083916938991794796, 0.0083328844134636978, 0.0082760300231079637, 0.0082210475080352289, 0.0081678343673676369, 0.0081163421827178013, 0.0080664968810504698, 0.0080182512211145959, 0.0079714894471309226, 0.007926217467285222, 0.0078823365786954147, 0.0078398080778110876, 0.0077986042621133499, 0.0077586258453452009, 0.0077198576095572858, 0.0076822408883517341, 0.0076457699241469787, 0.0076103601500835116, 0.0075759761932143272, 0.0075426272836731931, 0.0075102269850311227, 0.0074787921632283464, 0.0074482396293473066, 0.0074185616748760628, 0.0073897208772655092, 0.0073616915974475195, 0.0073344572880011064, 0.0073079950709352927, 0.0072822645600871304, 0.0072572668737361425, 0.0072329488434035982, 0.0072093048171039849, 0.0071863025326461818, 0.0071639314183072084, 0.0071421692314034948, 0.0071209694490833794, 0.0071003564051785213, 0.007080289066100985, 0.0070607540575046519, 0.0070417386265021493, 0.0070232409312213223, 0.0070052260089817986, 0.006987697682014518, 0.0069705981581184503, 0.006953964089510831, 0.0069377604061841526, 0.0069219750355666012, 0.0069065813293687477, 0.0068916160904120335, 0.0068770159125937404, 0.006862808029309303, 0.0068489456840108761, 0.0068354479636179915, 0.0068223047724554533, 0.0068094794461884467, 0.0067969910475013428, 0.0067848207664478665, 0.0067729609983810486, 0.0067613916973614562, 0.0067501174888424716, 0.0067391380075656126, 0.0067284289335478855, 0.006717976541033354, 0.0067077921443795402, 0.0066978638354675034, 0.0066881958675678111, 0.0066787574516408502, 0.006669551360881351, 0.0066605734926229515, 0.006651830991044585, 0.0066432956931593247, 0.0066349792456926026, 0.0066268709678632175, 0.0066189541824189704, 0.0066112177636903679, 0.0066036873334347091, 0.0065963327154157109, 0.0065891706007342359, 0.0065821781858358686, 0.0065753498187350965, 0.006568698280016618], 'loss': [0.010342351087101706, 0.010209151958679229, 0.010081418966227648, 0.0099588884658923741, 0.009841343912867841, 0.0097284784074586524, 0.00962008553435173, 0.0095158697162613451, 0.0094156551450152084, 0.009319222145831212, 0.0092264207889404346, 0.0091370347796835699, 0.0090508816790089847, 0.0089678962720121866, 0.0088879094008624225, 0.0088107417725122768, 0.0087362862121687021, 0.0086644145218768807, 0.0085950566658357496, 0.0085280648957578482, 0.0084633471692678163, 0.0084008225511711439, 0.0083403465439369148, 0.0082818529131858352, 0.0082252702693504723, 0.0081705335114373341, 0.0081175334669177923, 0.0080662314993568043, 0.0080165489672350612, 0.0079684377697736633, 0.0079217890527078765, 0.0078766028320653783, 0.0078327890322958688, 0.0077903141927594555, 0.0077491373661733804, 0.0077091690471147419, 0.007670396636397599, 0.0076327642807124509, 0.0075962548916924744, 0.0075608000603459256, 0.0075263571347320051, 0.0074929294093971652, 0.0074604488645932682, 0.0074289198506900557, 0.007398259513750993, 0.0073684648216828545, 0.0073395022866559415, 0.0073113397308170206, 0.0072839597893218245, 0.0072573559494951275, 0.0072314666347153434, 0.0072063110787684778, 0.0071818336727671192, 0.0071580118863753513, 0.0071348325133589272, 0.0071122771313330401, 0.007090327711449621, 0.0070689384154082446, 0.0070481370266162876, 0.0070278628794539352, 0.0070081277116194373, 0.0069889050997683768, 0.0069702011856497175, 0.0069519688478901697, 0.006934233080794933, 0.0069169095026463106, 0.0069000572884900663, 0.0068836326604410228, 0.0068676266724789848, 0.0068520136150636221, 0.0068368184939484765, 0.0068219930824228933, 0.0068075608742007212, 0.0067934717896572456, 0.0067797405433456959, 0.0067663700511586319, 0.0067533164527828187, 0.0067405960131631205, 0.0067281981826529192, 0.00671610395586062, 0.0067043038180696684, 0.0066927979475879824, 0.0066815910717219424, 0.0066706494882716565, 0.0066599644054833301, 0.0066495549575423963, 0.0066393946306625429, 0.0066294959247153486, 0.0066198297526135572, 0.0066104025700768959, 0.0066011966960006652, 0.0065922239310489926, 0.0065834677701480529, 0.0065749236766063481, 0.0065665939346542003, 0.0065584446019627558, 0.0065504838600892118, 0.0065427327768438678, 0.0065351628298289067, 0.0065277739211610109, 0.0065205638962623231, 0.0065135179489928851]}
[2017-10-20 01:49:58,907 AE_UNIGRAMA_8L_UNDER_01.py:132]: evaluating model ... 
[2017-10-20 01:49:58,978 AE_UNIGRAMA_8L_UNDER_01.py:136]: evaluated! 
[2017-10-20 01:49:58,979 AE_UNIGRAMA_8L_UNDER_01.py:138]: generating reports ... 
[2017-10-20 01:49:59,594 AE_UNIGRAMA_8L_UNDER_01.py:141]: done!
[2017-10-20 01:49:59,594 AE_UNIGRAMA_8L_UNDER_01.py:157]: >> experiment AE_UNIGRAMA_8L_UNDER_01 finished!
