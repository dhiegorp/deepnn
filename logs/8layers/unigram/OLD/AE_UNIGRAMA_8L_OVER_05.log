[2017-10-20 01:50:14,927 AE_UNIGRAMA_8L_OVER_05.py:148]: >> Initializing execution of experiment AE_UNIGRAMA_8L_OVER_05
[2017-10-20 01:50:14,927 AE_UNIGRAMA_8L_OVER_05.py:149]: >> Printing header log
[2017-10-20 01:50:14,927 AE_UNIGRAMA_8L_OVER_05.py:38]: 
	=======================================
	network_name = AE_UNIGRAMA_8L_OVER_05
	layers = 96,172,156,139,123,107,91,74,58,9
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/8layers/unigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/8layers/unigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/8layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/8layers/unigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/8layers/unigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/8layers/unigram/', 'data_dir': '/home/dhiegorp/malware_deepnn/', 'fullds_data_dir': '/home/dhiegorp/malware_deepnn/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f3bf52887f0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f3bf52888d0>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-10-20 01:50:14,927 AE_UNIGRAMA_8L_OVER_05.py:151]: >> Loading dataset... 
[2017-10-20 01:50:15,483 AE_UNIGRAMA_8L_OVER_05.py:55]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_deepnn/	
	trainx shape = (1627, 96)
	trainy shape = (1627, 9)
	valx shape = (1076, 96)
	valy shape = (1076, 9)
	=======================================
	
[2017-10-20 01:50:15,483 AE_UNIGRAMA_8L_OVER_05.py:153]: >> Executing autoencoder part ... 
[2017-10-20 01:50:15,483 AE_UNIGRAMA_8L_OVER_05.py:60]: =======================================
[2017-10-20 01:50:15,483 AE_UNIGRAMA_8L_OVER_05.py:65]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f3bf52887f0>, 'discard_decoder_function': True}
[2017-10-20 01:50:15,665 AE_UNIGRAMA_8L_OVER_05.py:76]: training and evaluate autoencoder
[2017-10-20 01:51:28,320 AE_UNIGRAMA_8L_OVER_05.py:88]: trained and evaluated!
[2017-10-20 01:51:28,321 AE_UNIGRAMA_8L_OVER_05.py:91]: Training history: 
{'val_loss': [0.01017300901609061, 0.009941441093512627, 0.0097227845978005677, 0.0095146250007431751, 0.0093173701731578129, 0.0091314744065796125, 0.0089549039140842217, 0.0087876171358345165, 0.0086296600286180653, 0.0084804643194803953, 0.0083385138221943687, 0.0082040493528888342, 0.0080768361144100201, 0.0079563240932653823, 0.0078421922082074508, 0.0077340558870629753, 0.0076314617626950642, 0.007534182564698187, 0.0074418151377246729, 0.007354153718342347, 0.0072708805403274013, 0.0071917182357365533, 0.0071164654277015797, 0.0070449300075718464, 0.0069768578818130231, 0.0069120452621584706, 0.006850341040654918, 0.0067915441104830637, 0.0067355611455041679, 0.0066821844936746644, 0.0066312649650810823, 0.0065827238941253557, 0.0065363894985317073, 0.0064921503969672441, 0.0064499230889194957, 0.0064095842295419769, 0.0063710400824293105, 0.0063341899176013955, 0.0062989140791493052, 0.0062652151169580823, 0.0062329875197967852, 0.0062021474045836347, 0.0061726204905534323, 0.0061443165380881621, 0.0061172064528783234, 0.0060912027719627971, 0.0060662893079858286, 0.0060423827023837421, 0.0060194682599942033, 0.0059974873109571777, 0.0059763826187492304, 0.0059561058723101162, 0.005936642336662596, 0.0059179312784958726, 0.0058999632357110768, 0.0058826980766144383, 0.0058661135498669732, 0.0058501396418227138, 0.0058348051559692421, 0.005820033407067278, 0.0058058170414768632, 0.0057921332210219261, 0.0057789742233418625, 0.005766295658279086, 0.0057541066642199752, 0.0057423478286699513, 0.0057310268030152222, 0.005720119912966698, 0.0057095887908040367, 0.0056994584368257935, 0.0056896998294279476, 0.0056802924743206088, 0.0056712005138535703, 0.0056622168533919468, 0.0056530071931396057, 0.0056441306676646369, 0.0056355773820974571, 0.0056273411391614542, 0.0056194010273757696, 0.0056117322385920466, 0.0056043363446335141, 0.0055971862575156985, 0.0055902982286444164, 0.0055836578069400171, 0.005577250646918901, 0.0055710671289536592, 0.0055650957725028125, 0.0055593368337666232, 0.0055537664566923028, 0.0055483846672460934, 0.0055431898503276941, 0.0055381721716554191, 0.00553332891862244, 0.0055286449355983818, 0.0055241292813398134, 0.0055197554841277547, 0.0055155361964467735, 0.0055114470446503078, 0.0055074961042398639, 0.005503679597992658, 0.0054999999805841748, 0.0054964364149701194], 'loss': [0.010287402380319666, 0.010050876857169069, 0.0098244688738670979, 0.0096103305411516481, 0.0094060838341566404, 0.009213450397968696, 0.0090311919645244317, 0.0088582210942241921, 0.0086945818992447254, 0.008540063217790602, 0.008393611833361592, 0.0082544368098145925, 0.0081226970020454939, 0.0079979874616112161, 0.0078798228272047421, 0.007767862660355852, 0.0076617253984346961, 0.0075609946727052046, 0.0074654091427190841, 0.0073746466610212042, 0.0072884533898338226, 0.0072065409362778643, 0.0071286486963195971, 0.0070545550626968778, 0.0069840998565131301, 0.00691701190863274, 0.0068531151026368286, 0.0067922530298538562, 0.0067342527167208732, 0.0066789779829270063, 0.0066262656166238259, 0.0065759557528855175, 0.006527979919754459, 0.0064821577468943065, 0.0064383821791471809, 0.0063965857448542891, 0.006356643798767366, 0.006318453863347848, 0.0062819252319556108, 0.0062469533448849968, 0.0062135188266767742, 0.0061815433177270437, 0.0061509099419182365, 0.0061215753108949918, 0.0060934411403599995, 0.0060664779840342896, 0.0060406128882207824, 0.0060158083690462043, 0.0059920079549906184, 0.0059691801914497616, 0.0059472701824816844, 0.0059262306779192574, 0.0059059980942466435, 0.005886573727988508, 0.0058678922723674233, 0.0058499341507195586, 0.0058326807569142273, 0.0058160884710533644, 0.0058001172728836536, 0.0057847554968313971, 0.0057699624700878176, 0.0057557205112620082, 0.005742013455009673, 0.0057288165474407831, 0.0057160918659243795, 0.0057038520451362651, 0.0056920458597589582, 0.005680668686952192, 0.0056697062915620367, 0.0056591266305394787, 0.005648933967533072, 0.0056391084353773231, 0.0056296410854761595, 0.0056204479780178644, 0.0056110541880034599, 0.0056017932319497319, 0.0055928639492298206, 0.0055842546021454744, 0.0055759574785214962, 0.0055679524335701114, 0.0055602233606197749, 0.0055527737914838156, 0.0055455417053450013, 0.0055385913102261918, 0.0055318854413766326, 0.0055254083928167269, 0.0055191521664489504, 0.0055131133103121554, 0.0055072907757382676, 0.0055016415630431172, 0.0054961932461294353, 0.0054909323840719761, 0.0054858414168322493, 0.005480919801472665, 0.0054761690242741473, 0.0054715880760653991, 0.0054671426782225384, 0.0054628488051887665, 0.0054586919358878246, 0.0054546736187078906, 0.0054507824957494401, 0.0054470406007765806]}
[2017-10-20 01:51:28,321 AE_UNIGRAMA_8L_OVER_05.py:95]: done!
[2017-10-20 01:51:28,322 AE_UNIGRAMA_8L_OVER_05.py:155]: >> Executing classifier part ... 
[2017-10-20 01:51:28,322 AE_UNIGRAMA_8L_OVER_05.py:100]: =======================================
[2017-10-20 01:51:28,322 AE_UNIGRAMA_8L_OVER_05.py:104]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f3bf52888d0>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-10-20 01:51:28,368 AE_UNIGRAMA_8L_OVER_05.py:113]: training ... 
[2017-10-20 01:53:20,116 AE_UNIGRAMA_8L_OVER_05.py:125]: trained!
[2017-10-20 01:53:20,117 AE_UNIGRAMA_8L_OVER_05.py:128]: Training history: 
{'val_loss': [0.01017300901609061, 0.009941441093512627, 0.0097227845978005677, 0.0095146250007431751, 0.0093173701731578129, 0.0091314744065796125, 0.0089549039140842217, 0.0087876171358345165, 0.0086296600286180653, 0.0084804643194803953, 0.0083385138221943687, 0.0082040493528888342, 0.0080768361144100201, 0.0079563240932653823, 0.0078421922082074508, 0.0077340558870629753, 0.0076314617626950642, 0.007534182564698187, 0.0074418151377246729, 0.007354153718342347, 0.0072708805403274013, 0.0071917182357365533, 0.0071164654277015797, 0.0070449300075718464, 0.0069768578818130231, 0.0069120452621584706, 0.006850341040654918, 0.0067915441104830637, 0.0067355611455041679, 0.0066821844936746644, 0.0066312649650810823, 0.0065827238941253557, 0.0065363894985317073, 0.0064921503969672441, 0.0064499230889194957, 0.0064095842295419769, 0.0063710400824293105, 0.0063341899176013955, 0.0062989140791493052, 0.0062652151169580823, 0.0062329875197967852, 0.0062021474045836347, 0.0061726204905534323, 0.0061443165380881621, 0.0061172064528783234, 0.0060912027719627971, 0.0060662893079858286, 0.0060423827023837421, 0.0060194682599942033, 0.0059974873109571777, 0.0059763826187492304, 0.0059561058723101162, 0.005936642336662596, 0.0059179312784958726, 0.0058999632357110768, 0.0058826980766144383, 0.0058661135498669732, 0.0058501396418227138, 0.0058348051559692421, 0.005820033407067278, 0.0058058170414768632, 0.0057921332210219261, 0.0057789742233418625, 0.005766295658279086, 0.0057541066642199752, 0.0057423478286699513, 0.0057310268030152222, 0.005720119912966698, 0.0057095887908040367, 0.0056994584368257935, 0.0056896998294279476, 0.0056802924743206088, 0.0056712005138535703, 0.0056622168533919468, 0.0056530071931396057, 0.0056441306676646369, 0.0056355773820974571, 0.0056273411391614542, 0.0056194010273757696, 0.0056117322385920466, 0.0056043363446335141, 0.0055971862575156985, 0.0055902982286444164, 0.0055836578069400171, 0.005577250646918901, 0.0055710671289536592, 0.0055650957725028125, 0.0055593368337666232, 0.0055537664566923028, 0.0055483846672460934, 0.0055431898503276941, 0.0055381721716554191, 0.00553332891862244, 0.0055286449355983818, 0.0055241292813398134, 0.0055197554841277547, 0.0055155361964467735, 0.0055114470446503078, 0.0055074961042398639, 0.005503679597992658, 0.0054999999805841748, 0.0054964364149701194], 'loss': [0.010287402380319666, 0.010050876857169069, 0.0098244688738670979, 0.0096103305411516481, 0.0094060838341566404, 0.009213450397968696, 0.0090311919645244317, 0.0088582210942241921, 0.0086945818992447254, 0.008540063217790602, 0.008393611833361592, 0.0082544368098145925, 0.0081226970020454939, 0.0079979874616112161, 0.0078798228272047421, 0.007767862660355852, 0.0076617253984346961, 0.0075609946727052046, 0.0074654091427190841, 0.0073746466610212042, 0.0072884533898338226, 0.0072065409362778643, 0.0071286486963195971, 0.0070545550626968778, 0.0069840998565131301, 0.00691701190863274, 0.0068531151026368286, 0.0067922530298538562, 0.0067342527167208732, 0.0066789779829270063, 0.0066262656166238259, 0.0065759557528855175, 0.006527979919754459, 0.0064821577468943065, 0.0064383821791471809, 0.0063965857448542891, 0.006356643798767366, 0.006318453863347848, 0.0062819252319556108, 0.0062469533448849968, 0.0062135188266767742, 0.0061815433177270437, 0.0061509099419182365, 0.0061215753108949918, 0.0060934411403599995, 0.0060664779840342896, 0.0060406128882207824, 0.0060158083690462043, 0.0059920079549906184, 0.0059691801914497616, 0.0059472701824816844, 0.0059262306779192574, 0.0059059980942466435, 0.005886573727988508, 0.0058678922723674233, 0.0058499341507195586, 0.0058326807569142273, 0.0058160884710533644, 0.0058001172728836536, 0.0057847554968313971, 0.0057699624700878176, 0.0057557205112620082, 0.005742013455009673, 0.0057288165474407831, 0.0057160918659243795, 0.0057038520451362651, 0.0056920458597589582, 0.005680668686952192, 0.0056697062915620367, 0.0056591266305394787, 0.005648933967533072, 0.0056391084353773231, 0.0056296410854761595, 0.0056204479780178644, 0.0056110541880034599, 0.0056017932319497319, 0.0055928639492298206, 0.0055842546021454744, 0.0055759574785214962, 0.0055679524335701114, 0.0055602233606197749, 0.0055527737914838156, 0.0055455417053450013, 0.0055385913102261918, 0.0055318854413766326, 0.0055254083928167269, 0.0055191521664489504, 0.0055131133103121554, 0.0055072907757382676, 0.0055016415630431172, 0.0054961932461294353, 0.0054909323840719761, 0.0054858414168322493, 0.005480919801472665, 0.0054761690242741473, 0.0054715880760653991, 0.0054671426782225384, 0.0054628488051887665, 0.0054586919358878246, 0.0054546736187078906, 0.0054507824957494401, 0.0054470406007765806]}
[2017-10-20 01:53:20,117 AE_UNIGRAMA_8L_OVER_05.py:132]: evaluating model ... 
[2017-10-20 01:53:20,207 AE_UNIGRAMA_8L_OVER_05.py:136]: evaluated! 
[2017-10-20 01:53:20,207 AE_UNIGRAMA_8L_OVER_05.py:138]: generating reports ... 
[2017-10-20 01:53:20,782 AE_UNIGRAMA_8L_OVER_05.py:141]: done!
[2017-10-20 01:53:20,782 AE_UNIGRAMA_8L_OVER_05.py:157]: >> experiment AE_UNIGRAMA_8L_OVER_05 finished!
