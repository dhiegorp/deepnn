[2017-10-20 01:53:34,980 AE_UNIGRAMA_8L_UNDER_03.py:148]: >> Initializing execution of experiment AE_UNIGRAMA_8L_UNDER_03
[2017-10-20 01:53:34,980 AE_UNIGRAMA_8L_UNDER_03.py:149]: >> Printing header log
[2017-10-20 01:53:34,980 AE_UNIGRAMA_8L_UNDER_03.py:38]: 
	=======================================
	network_name = AE_UNIGRAMA_8L_UNDER_03
	layers = 96,86,78,71,63,55,48,40,32,9
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/8layers/unigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/8layers/unigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/8layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/8layers/unigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/8layers/unigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/8layers/unigram/', 'data_dir': '/home/dhiegorp/malware_deepnn/', 'fullds_data_dir': '/home/dhiegorp/malware_deepnn/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f446aad77b8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f446aad7898>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-10-20 01:53:34,980 AE_UNIGRAMA_8L_UNDER_03.py:151]: >> Loading dataset... 
[2017-10-20 01:53:35,508 AE_UNIGRAMA_8L_UNDER_03.py:55]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_deepnn/	
	trainx shape = (1627, 96)
	trainy shape = (1627, 9)
	valx shape = (1076, 96)
	valy shape = (1076, 9)
	=======================================
	
[2017-10-20 01:53:35,509 AE_UNIGRAMA_8L_UNDER_03.py:153]: >> Executing autoencoder part ... 
[2017-10-20 01:53:35,509 AE_UNIGRAMA_8L_UNDER_03.py:60]: =======================================
[2017-10-20 01:53:35,509 AE_UNIGRAMA_8L_UNDER_03.py:65]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f446aad77b8>, 'discard_decoder_function': True}
[2017-10-20 01:53:35,685 AE_UNIGRAMA_8L_UNDER_03.py:76]: training and evaluate autoencoder
[2017-10-20 01:54:20,158 AE_UNIGRAMA_8L_UNDER_03.py:88]: trained and evaluated!
[2017-10-20 01:54:20,159 AE_UNIGRAMA_8L_UNDER_03.py:91]: Training history: 
{'val_loss': [0.010192465269072569, 0.0099916144007588404, 0.0098029085639241009, 0.0096265997453748517, 0.0094620681031984469, 0.0093082586808829505, 0.0091643627615278535, 0.0090296741014131831, 0.0089033980019473675, 0.0087848846180897666, 0.0086728343982457236, 0.0085675030129660455, 0.0084686092547656879, 0.0083756042526667886, 0.0082880595913606952, 0.0082055826629875532, 0.0081278301765068985, 0.0080545220427132008, 0.0079853487076397053, 0.0079200221969984944, 0.0078582979295555109, 0.0077998675388668325, 0.0077446616206276594, 0.0076923852537145844, 0.0076425853531572234, 0.0075945705556238006, 0.0075490128328559785, 0.0075057783444519377, 0.0074647317456988599, 0.0074256911049822008, 0.0073886220966600793, 0.007353389306776364, 0.00731985628272521, 0.0072879597101512894, 0.0072576052952405467, 0.0072287037452772873, 0.0072012087953411954, 0.0071749831809485715, 0.0071500067124801057, 0.0071261819143914598, 0.0071034510827668318, 0.0070817927175571924, 0.0070610813914233871, 0.0070413313771435322, 0.0070224478027219005, 0.0070044360466329138, 0.0069871999922322741, 0.0069707440260839285, 0.0069550011604747365, 0.0069399319682616504, 0.0069255441631498613, 0.0069117633708956957, 0.0068982499703543337, 0.0068843352788415316, 0.0068710393023064366, 0.0068583026092125582, 0.0068461248533433251, 0.0068344821976878611, 0.0068233213595159659, 0.0068126374987199844, 0.0068024169882157032, 0.0067926853303157042, 0.0067833735396372339, 0.0067744756716417781, 0.0067659673094057019, 0.0067577958522454518, 0.0067499798314029618, 0.0067424633173223539, 0.0067352553272142061, 0.0067283485559088809, 0.0067217006039940732, 0.0067153267690489502, 0.0067092113640007034, 0.0067033358212546567, 0.0066976955343993399, 0.0066922719669995695, 0.0066870498128207643, 0.0066820408518817346, 0.0066772323191775264, 0.0066725949351727739, 0.006668140251383352, 0.0066638526465175986, 0.0066597283987473825, 0.0066557591123208677, 0.0066519418495904559, 0.0066482641848434306, 0.0066447225832086069, 0.0066413112628692589, 0.0066380085489369904, 0.0066348314856535659, 0.0066317734793244019, 0.0066288272195866114, 0.0066259823805309807, 0.0066232645579485643, 0.0066206401065130437, 0.0066181091716353774, 0.0066156627205252424, 0.006613301926856812, 0.0066110249781863393, 0.0066088295548628034, 0.0066067140590967298, 0.0066046693438463269], 'loss': [0.010293059922255618, 0.010086396151550284, 0.009890855855543737, 0.0097079369661380484, 0.0095370286278834201, 0.0093774371834834738, 0.0092281086700061171, 0.0090883316302737289, 0.0089573926448629684, 0.0088345193960683872, 0.0087187741651270165, 0.0086095636949794895, 0.0085069412822344585, 0.0084104985605465091, 0.0083197446937255497, 0.0082342491148024808, 0.0081536354481047402, 0.0080775971734494464, 0.0080058706240853286, 0.0079381247334406654, 0.0078741049407749508, 0.0078135614075023081, 0.0077562222598417082, 0.0077019996827178633, 0.0076505863568536621, 0.0076011361442071277, 0.0075539360140560773, 0.0075091231730551938, 0.0074665486708411647, 0.0074260972271952145, 0.0073876055590029908, 0.0073510401329276347, 0.0073162458788949281, 0.0072831123338619585, 0.0072515958564650191, 0.0072215658749356986, 0.0071929588964862665, 0.0071657084820983263, 0.0071397199290881841, 0.0071149373071562423, 0.0070912864797055431, 0.0070686984836446157, 0.0070471682201683995, 0.0070265576875349024, 0.0070069109212169819, 0.0069880815066589519, 0.0069701139590191783, 0.0069529192980674233, 0.0069364868868634641, 0.0069207557489650157, 0.006905702724091173, 0.0068913029650479246, 0.0068774557962656465, 0.0068633553420880964, 0.0068494765655799884, 0.006836206428981406, 0.0068234955495854203, 0.0068113301779339852, 0.0067996831013195412, 0.0067885173255716209, 0.0067778288171533325, 0.0067676077691292058, 0.0067578658227452158, 0.0067485325933120954, 0.0067396006399803388, 0.0067310377084790337, 0.006722810882823135, 0.0067149391083004438, 0.0067073662860345348, 0.0067000899975632848, 0.0066931272542634186, 0.006686405500840137, 0.0066799528644372694, 0.0066737714569875999, 0.0066678202879908782, 0.0066621027135283905, 0.0066565988106239808, 0.0066513030425735279, 0.0066462103182996024, 0.006641319312084267, 0.0066366027233539437, 0.0066320559318442428, 0.0066276862991249078, 0.0066234806252582793, 0.0066194314859949451, 0.0066155257000008018, 0.0066117620883828923, 0.0066081329913421794, 0.0066046406868123979, 0.0066012510973742639, 0.0065979956481525626, 0.0065948561889468049, 0.0065918210719282189, 0.0065889011518417253, 0.0065861007203843122, 0.00658338055582448, 0.0065807718463063462, 0.0065782272220276106, 0.0065757891162657525, 0.0065734208047449512, 0.0065711452765934075, 0.0065689615260743264]}
[2017-10-20 01:54:20,159 AE_UNIGRAMA_8L_UNDER_03.py:95]: done!
[2017-10-20 01:54:20,159 AE_UNIGRAMA_8L_UNDER_03.py:155]: >> Executing classifier part ... 
[2017-10-20 01:54:20,159 AE_UNIGRAMA_8L_UNDER_03.py:100]: =======================================
[2017-10-20 01:54:20,159 AE_UNIGRAMA_8L_UNDER_03.py:104]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f446aad7898>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-10-20 01:54:20,191 AE_UNIGRAMA_8L_UNDER_03.py:113]: training ... 
[2017-10-20 01:55:34,697 AE_UNIGRAMA_8L_UNDER_03.py:125]: trained!
[2017-10-20 01:55:34,697 AE_UNIGRAMA_8L_UNDER_03.py:128]: Training history: 
{'val_loss': [0.010192465269072569, 0.0099916144007588404, 0.0098029085639241009, 0.0096265997453748517, 0.0094620681031984469, 0.0093082586808829505, 0.0091643627615278535, 0.0090296741014131831, 0.0089033980019473675, 0.0087848846180897666, 0.0086728343982457236, 0.0085675030129660455, 0.0084686092547656879, 0.0083756042526667886, 0.0082880595913606952, 0.0082055826629875532, 0.0081278301765068985, 0.0080545220427132008, 0.0079853487076397053, 0.0079200221969984944, 0.0078582979295555109, 0.0077998675388668325, 0.0077446616206276594, 0.0076923852537145844, 0.0076425853531572234, 0.0075945705556238006, 0.0075490128328559785, 0.0075057783444519377, 0.0074647317456988599, 0.0074256911049822008, 0.0073886220966600793, 0.007353389306776364, 0.00731985628272521, 0.0072879597101512894, 0.0072576052952405467, 0.0072287037452772873, 0.0072012087953411954, 0.0071749831809485715, 0.0071500067124801057, 0.0071261819143914598, 0.0071034510827668318, 0.0070817927175571924, 0.0070610813914233871, 0.0070413313771435322, 0.0070224478027219005, 0.0070044360466329138, 0.0069871999922322741, 0.0069707440260839285, 0.0069550011604747365, 0.0069399319682616504, 0.0069255441631498613, 0.0069117633708956957, 0.0068982499703543337, 0.0068843352788415316, 0.0068710393023064366, 0.0068583026092125582, 0.0068461248533433251, 0.0068344821976878611, 0.0068233213595159659, 0.0068126374987199844, 0.0068024169882157032, 0.0067926853303157042, 0.0067833735396372339, 0.0067744756716417781, 0.0067659673094057019, 0.0067577958522454518, 0.0067499798314029618, 0.0067424633173223539, 0.0067352553272142061, 0.0067283485559088809, 0.0067217006039940732, 0.0067153267690489502, 0.0067092113640007034, 0.0067033358212546567, 0.0066976955343993399, 0.0066922719669995695, 0.0066870498128207643, 0.0066820408518817346, 0.0066772323191775264, 0.0066725949351727739, 0.006668140251383352, 0.0066638526465175986, 0.0066597283987473825, 0.0066557591123208677, 0.0066519418495904559, 0.0066482641848434306, 0.0066447225832086069, 0.0066413112628692589, 0.0066380085489369904, 0.0066348314856535659, 0.0066317734793244019, 0.0066288272195866114, 0.0066259823805309807, 0.0066232645579485643, 0.0066206401065130437, 0.0066181091716353774, 0.0066156627205252424, 0.006613301926856812, 0.0066110249781863393, 0.0066088295548628034, 0.0066067140590967298, 0.0066046693438463269], 'loss': [0.010293059922255618, 0.010086396151550284, 0.009890855855543737, 0.0097079369661380484, 0.0095370286278834201, 0.0093774371834834738, 0.0092281086700061171, 0.0090883316302737289, 0.0089573926448629684, 0.0088345193960683872, 0.0087187741651270165, 0.0086095636949794895, 0.0085069412822344585, 0.0084104985605465091, 0.0083197446937255497, 0.0082342491148024808, 0.0081536354481047402, 0.0080775971734494464, 0.0080058706240853286, 0.0079381247334406654, 0.0078741049407749508, 0.0078135614075023081, 0.0077562222598417082, 0.0077019996827178633, 0.0076505863568536621, 0.0076011361442071277, 0.0075539360140560773, 0.0075091231730551938, 0.0074665486708411647, 0.0074260972271952145, 0.0073876055590029908, 0.0073510401329276347, 0.0073162458788949281, 0.0072831123338619585, 0.0072515958564650191, 0.0072215658749356986, 0.0071929588964862665, 0.0071657084820983263, 0.0071397199290881841, 0.0071149373071562423, 0.0070912864797055431, 0.0070686984836446157, 0.0070471682201683995, 0.0070265576875349024, 0.0070069109212169819, 0.0069880815066589519, 0.0069701139590191783, 0.0069529192980674233, 0.0069364868868634641, 0.0069207557489650157, 0.006905702724091173, 0.0068913029650479246, 0.0068774557962656465, 0.0068633553420880964, 0.0068494765655799884, 0.006836206428981406, 0.0068234955495854203, 0.0068113301779339852, 0.0067996831013195412, 0.0067885173255716209, 0.0067778288171533325, 0.0067676077691292058, 0.0067578658227452158, 0.0067485325933120954, 0.0067396006399803388, 0.0067310377084790337, 0.006722810882823135, 0.0067149391083004438, 0.0067073662860345348, 0.0067000899975632848, 0.0066931272542634186, 0.006686405500840137, 0.0066799528644372694, 0.0066737714569875999, 0.0066678202879908782, 0.0066621027135283905, 0.0066565988106239808, 0.0066513030425735279, 0.0066462103182996024, 0.006641319312084267, 0.0066366027233539437, 0.0066320559318442428, 0.0066276862991249078, 0.0066234806252582793, 0.0066194314859949451, 0.0066155257000008018, 0.0066117620883828923, 0.0066081329913421794, 0.0066046406868123979, 0.0066012510973742639, 0.0065979956481525626, 0.0065948561889468049, 0.0065918210719282189, 0.0065889011518417253, 0.0065861007203843122, 0.00658338055582448, 0.0065807718463063462, 0.0065782272220276106, 0.0065757891162657525, 0.0065734208047449512, 0.0065711452765934075, 0.0065689615260743264]}
[2017-10-20 01:55:34,697 AE_UNIGRAMA_8L_UNDER_03.py:132]: evaluating model ... 
[2017-10-20 01:55:34,759 AE_UNIGRAMA_8L_UNDER_03.py:136]: evaluated! 
[2017-10-20 01:55:34,760 AE_UNIGRAMA_8L_UNDER_03.py:138]: generating reports ... 
[2017-10-20 01:55:35,292 AE_UNIGRAMA_8L_UNDER_03.py:141]: done!
[2017-10-20 01:55:35,292 AE_UNIGRAMA_8L_UNDER_03.py:157]: >> experiment AE_UNIGRAMA_8L_UNDER_03 finished!
