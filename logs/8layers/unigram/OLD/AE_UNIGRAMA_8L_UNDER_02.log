[2017-10-20 01:43:33,510 AE_UNIGRAMA_8L_UNDER_02.py:148]: >> Initializing execution of experiment AE_UNIGRAMA_8L_UNDER_02
[2017-10-20 01:43:33,510 AE_UNIGRAMA_8L_UNDER_02.py:149]: >> Printing header log
[2017-10-20 01:43:33,510 AE_UNIGRAMA_8L_UNDER_02.py:38]: 
	=======================================
	network_name = AE_UNIGRAMA_8L_UNDER_02
	layers = 96,76,69,63,56,49,43,36,29,9
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/8layers/unigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/8layers/unigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/8layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/8layers/unigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/8layers/unigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/8layers/unigram/', 'data_dir': '/home/dhiegorp/malware_deepnn/', 'fullds_data_dir': '/home/dhiegorp/malware_deepnn/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f9b1c645b70>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f9b1c645cf8>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-10-20 01:43:33,510 AE_UNIGRAMA_8L_UNDER_02.py:151]: >> Loading dataset... 
[2017-10-20 01:43:34,126 AE_UNIGRAMA_8L_UNDER_02.py:55]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_deepnn/	
	trainx shape = (1627, 96)
	trainy shape = (1627, 9)
	valx shape = (1076, 96)
	valy shape = (1076, 9)
	=======================================
	
[2017-10-20 01:43:34,126 AE_UNIGRAMA_8L_UNDER_02.py:153]: >> Executing autoencoder part ... 
[2017-10-20 01:43:34,126 AE_UNIGRAMA_8L_UNDER_02.py:60]: =======================================
[2017-10-20 01:43:34,126 AE_UNIGRAMA_8L_UNDER_02.py:65]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f9b1c645b70>, 'discard_decoder_function': True}
[2017-10-20 01:43:34,334 AE_UNIGRAMA_8L_UNDER_02.py:76]: training and evaluate autoencoder
[2017-10-20 01:44:19,168 AE_UNIGRAMA_8L_UNDER_02.py:88]: trained and evaluated!
[2017-10-20 01:44:19,168 AE_UNIGRAMA_8L_UNDER_02.py:91]: Training history: 
{'val_loss': [0.010200887925396621, 0.0099849821445722568, 0.0097815909814158788, 0.0095905077169375786, 0.0094110128127010782, 0.0092426137617861916, 0.0090843946136340333, 0.0089355590355108226, 0.0087954493405705929, 0.008663486114609641, 0.0085389993766315803, 0.0084215070027511794, 0.0083105366150796629, 0.0082055510326416743, 0.0081062911538801897, 0.0080121472931456612, 0.0079229053916568867, 0.0078382470323029065, 0.0077578918108211128, 0.0076815327802555265, 0.0076089379789660858, 0.0075397759444014514, 0.0074739309121269496, 0.0074112356006822179, 0.0073513811831552962, 0.0072939091891770468, 0.0072375953259798233, 0.0071838342698921061, 0.0071323540989572673, 0.0070830843304058875, 0.0070358705464542799, 0.0069906578977422642, 0.0069472632530148811, 0.0069056306619432557, 0.0068656865552372427, 0.0068273441675621117, 0.0067904745176656095, 0.0067550517500942521, 0.0067209926084182518, 0.0066882209457650946, 0.0066567134911879955, 0.0066263997723040305, 0.0065971557137497515, 0.0065690088934060812, 0.0065418739723505587, 0.0065157518992165868, 0.006490560490845748, 0.0064662428119230224, 0.0064427640486411667, 0.006420113573882549, 0.0063982722141734949, 0.0063771742004831928, 0.0063568322649260219, 0.0063371495915002093, 0.0063181357773290916, 0.006299745011036059, 0.00628199529068953, 0.0062648159777226277, 0.0062481985638631323, 0.0062320889700827322, 0.0062165070553639118, 0.0062014249596592441, 0.0061867346772473981, 0.0061726093666150221, 0.0061589571938167938, 0.0061457404350951927, 0.0061329515411978983, 0.0061205805081971073, 0.0061085717198629359, 0.0060969638485863084, 0.006085722367919732, 0.0060748191720020155, 0.0060642534853080389, 0.0060540296147719415, 0.0060441072400119226, 0.0060344944088321639, 0.0060251775512741848, 0.0060161441221805532, 0.0060073825250272416, 0.0059988746769227726, 0.005990637391874892, 0.0059819938216815428, 0.0059732039092060133, 0.0059647098232102217, 0.0059564886320396205, 0.0059485706192576307, 0.0059409229202562995, 0.005933531968539311, 0.0059263592596307786, 0.0059194151817589004, 0.005912697804766295, 0.0059061753857866982, 0.005899861126057942, 0.0058937451185928627, 0.0058878147576461272, 0.0058820681563374281, 0.0058765031058050443, 0.0058710998249652214, 0.0058658542613530028, 0.0058607664617076243, 0.0058558368830349591, 0.0058510504424101138], 'loss': [0.010307498279095759, 0.010088439566582973, 0.0098785668389428666, 0.0096812594244651586, 0.0094958502334469876, 0.0093217861867832055, 0.0091583622355053008, 0.0090047086594190126, 0.0088600458115907117, 0.0087237982222490972, 0.0085953226823590988, 0.0084740671466007941, 0.0083595099936215958, 0.0082512483465557972, 0.0081487515364265829, 0.0080517339202464903, 0.0079596755272275509, 0.0078723228338489926, 0.0077894048063518375, 0.0077106297907969593, 0.0076357070866058634, 0.0075644024604268612, 0.0074964329046806862, 0.0074316786121881549, 0.0073699550866306784, 0.0073109257347497127, 0.0072534784798286558, 0.0071979886849342714, 0.0071449475254137676, 0.0070941335107780855, 0.007045464777658432, 0.0069987766167633209, 0.0069540417143535817, 0.0069110598893104593, 0.0068697959085046037, 0.0068301660130080924, 0.006792095250529641, 0.0067554663557080714, 0.0067202404056135945, 0.0066863494763779322, 0.0066537140552629962, 0.0066222969363040002, 0.00659204805962462, 0.00656285743786053, 0.0065347334016835719, 0.006507597921488288, 0.006481459241797753, 0.0064562250580695885, 0.0064318445010297341, 0.0064082926980116176, 0.0063855475276713182, 0.0063635917817206379, 0.0063423733324265838, 0.00632189912463927, 0.0063020657205904969, 0.0062828839091972188, 0.006264336629568633, 0.006246397074268468, 0.0062290301658648175, 0.0062122127612492901, 0.006195917883263952, 0.0061801413260743742, 0.0061648262898794295, 0.0061499331030277988, 0.0061356462360793052, 0.0061217829076769016, 0.0061083559559906601, 0.0060953601096317709, 0.006082764883670491, 0.0060705358775581001, 0.0060587124495475693, 0.0060472350757049317, 0.006036102747145099, 0.0060253028440751142, 0.0060148474522228063, 0.0060046838588449364, 0.005994836012440715, 0.0059852842822528977, 0.0059760102347345464, 0.0059670057570185113, 0.0059582579410998419, 0.005949561974147052, 0.0059403231554649213, 0.0059312797631191454, 0.0059225409707831835, 0.0059140738773548352, 0.0059059182019960349, 0.0058980319058246676, 0.0058903978254872744, 0.0058829759275649471, 0.0058757966969874313, 0.0058688388429879775, 0.0058620615167395609, 0.0058555153771529698, 0.0058491549027606289, 0.0058429924082571954, 0.005837013638167932, 0.0058312087818366588, 0.0058255737181466712, 0.0058200875372747809, 0.005814776072403406, 0.0058096176220565465]}
[2017-10-20 01:44:19,168 AE_UNIGRAMA_8L_UNDER_02.py:95]: done!
[2017-10-20 01:44:19,168 AE_UNIGRAMA_8L_UNDER_02.py:155]: >> Executing classifier part ... 
[2017-10-20 01:44:19,168 AE_UNIGRAMA_8L_UNDER_02.py:100]: =======================================
[2017-10-20 01:44:19,169 AE_UNIGRAMA_8L_UNDER_02.py:104]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f9b1c645cf8>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-10-20 01:44:19,205 AE_UNIGRAMA_8L_UNDER_02.py:113]: training ... 
[2017-10-20 01:45:29,109 AE_UNIGRAMA_8L_UNDER_02.py:125]: trained!
[2017-10-20 01:45:29,110 AE_UNIGRAMA_8L_UNDER_02.py:128]: Training history: 
{'val_loss': [0.010200887925396621, 0.0099849821445722568, 0.0097815909814158788, 0.0095905077169375786, 0.0094110128127010782, 0.0092426137617861916, 0.0090843946136340333, 0.0089355590355108226, 0.0087954493405705929, 0.008663486114609641, 0.0085389993766315803, 0.0084215070027511794, 0.0083105366150796629, 0.0082055510326416743, 0.0081062911538801897, 0.0080121472931456612, 0.0079229053916568867, 0.0078382470323029065, 0.0077578918108211128, 0.0076815327802555265, 0.0076089379789660858, 0.0075397759444014514, 0.0074739309121269496, 0.0074112356006822179, 0.0073513811831552962, 0.0072939091891770468, 0.0072375953259798233, 0.0071838342698921061, 0.0071323540989572673, 0.0070830843304058875, 0.0070358705464542799, 0.0069906578977422642, 0.0069472632530148811, 0.0069056306619432557, 0.0068656865552372427, 0.0068273441675621117, 0.0067904745176656095, 0.0067550517500942521, 0.0067209926084182518, 0.0066882209457650946, 0.0066567134911879955, 0.0066263997723040305, 0.0065971557137497515, 0.0065690088934060812, 0.0065418739723505587, 0.0065157518992165868, 0.006490560490845748, 0.0064662428119230224, 0.0064427640486411667, 0.006420113573882549, 0.0063982722141734949, 0.0063771742004831928, 0.0063568322649260219, 0.0063371495915002093, 0.0063181357773290916, 0.006299745011036059, 0.00628199529068953, 0.0062648159777226277, 0.0062481985638631323, 0.0062320889700827322, 0.0062165070553639118, 0.0062014249596592441, 0.0061867346772473981, 0.0061726093666150221, 0.0061589571938167938, 0.0061457404350951927, 0.0061329515411978983, 0.0061205805081971073, 0.0061085717198629359, 0.0060969638485863084, 0.006085722367919732, 0.0060748191720020155, 0.0060642534853080389, 0.0060540296147719415, 0.0060441072400119226, 0.0060344944088321639, 0.0060251775512741848, 0.0060161441221805532, 0.0060073825250272416, 0.0059988746769227726, 0.005990637391874892, 0.0059819938216815428, 0.0059732039092060133, 0.0059647098232102217, 0.0059564886320396205, 0.0059485706192576307, 0.0059409229202562995, 0.005933531968539311, 0.0059263592596307786, 0.0059194151817589004, 0.005912697804766295, 0.0059061753857866982, 0.005899861126057942, 0.0058937451185928627, 0.0058878147576461272, 0.0058820681563374281, 0.0058765031058050443, 0.0058710998249652214, 0.0058658542613530028, 0.0058607664617076243, 0.0058558368830349591, 0.0058510504424101138], 'loss': [0.010307498279095759, 0.010088439566582973, 0.0098785668389428666, 0.0096812594244651586, 0.0094958502334469876, 0.0093217861867832055, 0.0091583622355053008, 0.0090047086594190126, 0.0088600458115907117, 0.0087237982222490972, 0.0085953226823590988, 0.0084740671466007941, 0.0083595099936215958, 0.0082512483465557972, 0.0081487515364265829, 0.0080517339202464903, 0.0079596755272275509, 0.0078723228338489926, 0.0077894048063518375, 0.0077106297907969593, 0.0076357070866058634, 0.0075644024604268612, 0.0074964329046806862, 0.0074316786121881549, 0.0073699550866306784, 0.0073109257347497127, 0.0072534784798286558, 0.0071979886849342714, 0.0071449475254137676, 0.0070941335107780855, 0.007045464777658432, 0.0069987766167633209, 0.0069540417143535817, 0.0069110598893104593, 0.0068697959085046037, 0.0068301660130080924, 0.006792095250529641, 0.0067554663557080714, 0.0067202404056135945, 0.0066863494763779322, 0.0066537140552629962, 0.0066222969363040002, 0.00659204805962462, 0.00656285743786053, 0.0065347334016835719, 0.006507597921488288, 0.006481459241797753, 0.0064562250580695885, 0.0064318445010297341, 0.0064082926980116176, 0.0063855475276713182, 0.0063635917817206379, 0.0063423733324265838, 0.00632189912463927, 0.0063020657205904969, 0.0062828839091972188, 0.006264336629568633, 0.006246397074268468, 0.0062290301658648175, 0.0062122127612492901, 0.006195917883263952, 0.0061801413260743742, 0.0061648262898794295, 0.0061499331030277988, 0.0061356462360793052, 0.0061217829076769016, 0.0061083559559906601, 0.0060953601096317709, 0.006082764883670491, 0.0060705358775581001, 0.0060587124495475693, 0.0060472350757049317, 0.006036102747145099, 0.0060253028440751142, 0.0060148474522228063, 0.0060046838588449364, 0.005994836012440715, 0.0059852842822528977, 0.0059760102347345464, 0.0059670057570185113, 0.0059582579410998419, 0.005949561974147052, 0.0059403231554649213, 0.0059312797631191454, 0.0059225409707831835, 0.0059140738773548352, 0.0059059182019960349, 0.0058980319058246676, 0.0058903978254872744, 0.0058829759275649471, 0.0058757966969874313, 0.0058688388429879775, 0.0058620615167395609, 0.0058555153771529698, 0.0058491549027606289, 0.0058429924082571954, 0.005837013638167932, 0.0058312087818366588, 0.0058255737181466712, 0.0058200875372747809, 0.005814776072403406, 0.0058096176220565465]}
[2017-10-20 01:45:29,110 AE_UNIGRAMA_8L_UNDER_02.py:132]: evaluating model ... 
[2017-10-20 01:45:29,169 AE_UNIGRAMA_8L_UNDER_02.py:136]: evaluated! 
[2017-10-20 01:45:29,169 AE_UNIGRAMA_8L_UNDER_02.py:138]: generating reports ... 
[2017-10-20 01:45:29,749 AE_UNIGRAMA_8L_UNDER_02.py:141]: done!
[2017-10-20 01:45:29,749 AE_UNIGRAMA_8L_UNDER_02.py:157]: >> experiment AE_UNIGRAMA_8L_UNDER_02 finished!
