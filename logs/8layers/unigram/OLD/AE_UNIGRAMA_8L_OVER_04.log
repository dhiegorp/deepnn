[2017-10-20 01:45:44,927 AE_UNIGRAMA_8L_OVER_04.py:148]: >> Initializing execution of experiment AE_UNIGRAMA_8L_OVER_04
[2017-10-20 01:45:44,927 AE_UNIGRAMA_8L_OVER_04.py:149]: >> Printing header log
[2017-10-20 01:45:44,927 AE_UNIGRAMA_8L_OVER_04.py:38]: 
	=======================================
	network_name = AE_UNIGRAMA_8L_OVER_04
	layers = 96,134,122,109,97,84,72,59,47,9
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/8layers/unigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/8layers/unigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/8layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/8layers/unigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/8layers/unigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/8layers/unigram/', 'data_dir': '/home/dhiegorp/malware_deepnn/', 'fullds_data_dir': '/home/dhiegorp/malware_deepnn/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fd3cb84d7b8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fd3cb84d898>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-10-20 01:45:44,927 AE_UNIGRAMA_8L_OVER_04.py:151]: >> Loading dataset... 
[2017-10-20 01:45:45,476 AE_UNIGRAMA_8L_OVER_04.py:55]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_deepnn/	
	trainx shape = (1627, 96)
	trainy shape = (1627, 9)
	valx shape = (1076, 96)
	valy shape = (1076, 9)
	=======================================
	
[2017-10-20 01:45:45,476 AE_UNIGRAMA_8L_OVER_04.py:153]: >> Executing autoencoder part ... 
[2017-10-20 01:45:45,476 AE_UNIGRAMA_8L_OVER_04.py:60]: =======================================
[2017-10-20 01:45:45,476 AE_UNIGRAMA_8L_OVER_04.py:65]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fd3cb84d7b8>, 'discard_decoder_function': True}
[2017-10-20 01:45:45,655 AE_UNIGRAMA_8L_OVER_04.py:76]: training and evaluate autoencoder
[2017-10-20 01:46:38,663 AE_UNIGRAMA_8L_OVER_04.py:88]: trained and evaluated!
[2017-10-20 01:46:38,664 AE_UNIGRAMA_8L_OVER_04.py:91]: Training history: 
{'val_loss': [0.010304948747795089, 0.01020502791736538, 0.010112255939791415, 0.010025832459302862, 0.0099456346521091723, 0.0098707523806276821, 0.0098007222409031199, 0.0097356998261244332, 0.0096747627114828639, 0.0096176416339650468, 0.0095644369236709461, 0.0095141755250805378, 0.0094671654255305965, 0.0094232340325985705, 0.0093821827287622981, 0.009343829192913598, 0.0093079374299893589, 0.0092743486373730311, 0.0092428489259295307, 0.009213305431587767, 0.0091855774484713281, 0.0091595667445526687, 0.0091350881770585549, 0.0091121342988705542, 0.009090482503120341, 0.0090701098252627937, 0.0090508797521041667, 0.0090327455721047733, 0.0090156342058709121, 0.0089994589192273454, 0.0089841620809637483, 0.0089697002409005258, 0.0089559932429772778, 0.0089430016254292994, 0.0089306647830459263, 0.0089189937082032739, 0.0089078699532882423, 0.0088973050542369651, 0.0088872420930452504, 0.0088776592961529819, 0.008868526238047944, 0.0088598269166002483, 0.0088515321145935123, 0.0088435935555891479, 0.008836015041045097, 0.0088287688667113891, 0.0088218496419960237, 0.0088152302512171984, 0.0088088821661295065, 0.00880277911235741, 0.0087969322640407254, 0.0087913314216392854, 0.0087859561774173405, 0.0087808093063007058, 0.0087758581704532792, 0.0087710983486894555, 0.0087665159317585166, 0.0087621224988736635, 0.0087578907837488843, 0.0087538150391891102, 0.0087483214062108877, 0.0087423305594982043, 0.008736554908888158, 0.008731006090719683, 0.0087256926426934039, 0.0087205887941789011, 0.0087156866514383636, 0.008710975403859381, 0.0087064602775810823, 0.0087021148891697139, 0.0086979375058113418, 0.0086939292336679075, 0.0086900950669134419, 0.0086864105782378121, 0.0086828571152232845, 0.0086794494336200918, 0.00867616395954195, 0.0086729943347215437, 0.0086699436145201043, 0.0086670043587435356, 0.0086641642230400138, 0.0086614444668380516, 0.008658808060544353, 0.0086562675718204231, 0.0086538184375318673, 0.0086514690984386723, 0.0086491985114976813, 0.0086470022555232926, 0.0086448932824774085, 0.008642849302937218, 0.008640882069224207, 0.0086389770592843283, 0.0086371356735636083, 0.0086353545174085739, 0.0086336343351848515, 0.0086319648996549467, 0.0086303612383491039, 0.0086288094056638648, 0.0086273107553060165, 0.008625860937064354, 0.0086244523341743248, 0.008623098134025101], 'loss': [0.010352452564927318, 0.010251073093458634, 0.010155920153363541, 0.010067555317271687, 0.0099853716645282187, 0.0099090279515583087, 0.0098376097191942569, 0.0097711154367928141, 0.0097092631546014576, 0.0096510934300569565, 0.0095968986698268897, 0.0095461306139854249, 0.0094983102450377378, 0.0094536644902902291, 0.0094119880723949577, 0.0093730666188691462, 0.0093367000596429502, 0.0093026735751758557, 0.0092708386764727129, 0.0092409983873779442, 0.0092130057535465687, 0.0091867559110332501, 0.0091621206887867752, 0.0091389536579410281, 0.0091172263333053043, 0.0090967435395481328, 0.0090774720867829017, 0.0090592863632351171, 0.0090421491011189545, 0.0090259701985529253, 0.0090106835453722407, 0.0089962310881823871, 0.0089825557873394867, 0.0089696226504155669, 0.0089573340934656041, 0.0089456968521212758, 0.0089346606155013709, 0.008924157666093965, 0.0089141813867577181, 0.0089046816297502843, 0.008895631055287517, 0.0088870097529864349, 0.0088788010323100752, 0.0088709703507502905, 0.0088634860673810559, 0.0088563336302155942, 0.0088494970733969582, 0.0088429661449354047, 0.0088367170025150914, 0.0088307249131392099, 0.0088249751880133016, 0.0088194599512508836, 0.0088141732886387811, 0.0088091041485206002, 0.0088042460265211637, 0.0087995751787803625, 0.0087950840322203478, 0.0087907637870734863, 0.0087866152573168529, 0.0087826284960590036, 0.0087782665972667151, 0.0087724398820447497, 0.0087667317567570631, 0.0087612247690970566, 0.0087559417212987583, 0.0087508918435874345, 0.0087460402328321807, 0.0087413793709071857, 0.008736900864460934, 0.0087326111892477524, 0.0087284804456343509, 0.0087245261495831044, 0.0087207268114123295, 0.0087170897380259415, 0.008713599898896484, 0.0087102253096476984, 0.0087069959374580855, 0.0087038804819897855, 0.008700878111520793, 0.0086979910579052678, 0.0086951978692410541, 0.0086925208835319356, 0.0086899505156540543, 0.008687454906018072, 0.0086850597979369233, 0.008682749508900113, 0.0086805293446414744, 0.0086783842986750698, 0.0086763146205747436, 0.0086743237534291778, 0.0086723951257641659, 0.0086705423261885569, 0.0086687484529434939, 0.008667017083063255, 0.0086653416680965451, 0.0086637104677692398, 0.0086621501361098339, 0.008660646963729474, 0.0086591878474289903, 0.0086577832847650454, 0.0086564257289909775, 0.0086551108938477568]}
[2017-10-20 01:46:38,664 AE_UNIGRAMA_8L_OVER_04.py:95]: done!
[2017-10-20 01:46:38,664 AE_UNIGRAMA_8L_OVER_04.py:155]: >> Executing classifier part ... 
[2017-10-20 01:46:38,664 AE_UNIGRAMA_8L_OVER_04.py:100]: =======================================
[2017-10-20 01:46:38,664 AE_UNIGRAMA_8L_OVER_04.py:104]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fd3cb84d898>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-10-20 01:46:38,697 AE_UNIGRAMA_8L_OVER_04.py:113]: training ... 
[2017-10-20 01:48:05,150 AE_UNIGRAMA_8L_OVER_04.py:125]: trained!
[2017-10-20 01:48:05,151 AE_UNIGRAMA_8L_OVER_04.py:128]: Training history: 
{'val_loss': [0.010304948747795089, 0.01020502791736538, 0.010112255939791415, 0.010025832459302862, 0.0099456346521091723, 0.0098707523806276821, 0.0098007222409031199, 0.0097356998261244332, 0.0096747627114828639, 0.0096176416339650468, 0.0095644369236709461, 0.0095141755250805378, 0.0094671654255305965, 0.0094232340325985705, 0.0093821827287622981, 0.009343829192913598, 0.0093079374299893589, 0.0092743486373730311, 0.0092428489259295307, 0.009213305431587767, 0.0091855774484713281, 0.0091595667445526687, 0.0091350881770585549, 0.0091121342988705542, 0.009090482503120341, 0.0090701098252627937, 0.0090508797521041667, 0.0090327455721047733, 0.0090156342058709121, 0.0089994589192273454, 0.0089841620809637483, 0.0089697002409005258, 0.0089559932429772778, 0.0089430016254292994, 0.0089306647830459263, 0.0089189937082032739, 0.0089078699532882423, 0.0088973050542369651, 0.0088872420930452504, 0.0088776592961529819, 0.008868526238047944, 0.0088598269166002483, 0.0088515321145935123, 0.0088435935555891479, 0.008836015041045097, 0.0088287688667113891, 0.0088218496419960237, 0.0088152302512171984, 0.0088088821661295065, 0.00880277911235741, 0.0087969322640407254, 0.0087913314216392854, 0.0087859561774173405, 0.0087808093063007058, 0.0087758581704532792, 0.0087710983486894555, 0.0087665159317585166, 0.0087621224988736635, 0.0087578907837488843, 0.0087538150391891102, 0.0087483214062108877, 0.0087423305594982043, 0.008736554908888158, 0.008731006090719683, 0.0087256926426934039, 0.0087205887941789011, 0.0087156866514383636, 0.008710975403859381, 0.0087064602775810823, 0.0087021148891697139, 0.0086979375058113418, 0.0086939292336679075, 0.0086900950669134419, 0.0086864105782378121, 0.0086828571152232845, 0.0086794494336200918, 0.00867616395954195, 0.0086729943347215437, 0.0086699436145201043, 0.0086670043587435356, 0.0086641642230400138, 0.0086614444668380516, 0.008658808060544353, 0.0086562675718204231, 0.0086538184375318673, 0.0086514690984386723, 0.0086491985114976813, 0.0086470022555232926, 0.0086448932824774085, 0.008642849302937218, 0.008640882069224207, 0.0086389770592843283, 0.0086371356735636083, 0.0086353545174085739, 0.0086336343351848515, 0.0086319648996549467, 0.0086303612383491039, 0.0086288094056638648, 0.0086273107553060165, 0.008625860937064354, 0.0086244523341743248, 0.008623098134025101], 'loss': [0.010352452564927318, 0.010251073093458634, 0.010155920153363541, 0.010067555317271687, 0.0099853716645282187, 0.0099090279515583087, 0.0098376097191942569, 0.0097711154367928141, 0.0097092631546014576, 0.0096510934300569565, 0.0095968986698268897, 0.0095461306139854249, 0.0094983102450377378, 0.0094536644902902291, 0.0094119880723949577, 0.0093730666188691462, 0.0093367000596429502, 0.0093026735751758557, 0.0092708386764727129, 0.0092409983873779442, 0.0092130057535465687, 0.0091867559110332501, 0.0091621206887867752, 0.0091389536579410281, 0.0091172263333053043, 0.0090967435395481328, 0.0090774720867829017, 0.0090592863632351171, 0.0090421491011189545, 0.0090259701985529253, 0.0090106835453722407, 0.0089962310881823871, 0.0089825557873394867, 0.0089696226504155669, 0.0089573340934656041, 0.0089456968521212758, 0.0089346606155013709, 0.008924157666093965, 0.0089141813867577181, 0.0089046816297502843, 0.008895631055287517, 0.0088870097529864349, 0.0088788010323100752, 0.0088709703507502905, 0.0088634860673810559, 0.0088563336302155942, 0.0088494970733969582, 0.0088429661449354047, 0.0088367170025150914, 0.0088307249131392099, 0.0088249751880133016, 0.0088194599512508836, 0.0088141732886387811, 0.0088091041485206002, 0.0088042460265211637, 0.0087995751787803625, 0.0087950840322203478, 0.0087907637870734863, 0.0087866152573168529, 0.0087826284960590036, 0.0087782665972667151, 0.0087724398820447497, 0.0087667317567570631, 0.0087612247690970566, 0.0087559417212987583, 0.0087508918435874345, 0.0087460402328321807, 0.0087413793709071857, 0.008736900864460934, 0.0087326111892477524, 0.0087284804456343509, 0.0087245261495831044, 0.0087207268114123295, 0.0087170897380259415, 0.008713599898896484, 0.0087102253096476984, 0.0087069959374580855, 0.0087038804819897855, 0.008700878111520793, 0.0086979910579052678, 0.0086951978692410541, 0.0086925208835319356, 0.0086899505156540543, 0.008687454906018072, 0.0086850597979369233, 0.008682749508900113, 0.0086805293446414744, 0.0086783842986750698, 0.0086763146205747436, 0.0086743237534291778, 0.0086723951257641659, 0.0086705423261885569, 0.0086687484529434939, 0.008667017083063255, 0.0086653416680965451, 0.0086637104677692398, 0.0086621501361098339, 0.008660646963729474, 0.0086591878474289903, 0.0086577832847650454, 0.0086564257289909775, 0.0086551108938477568]}
[2017-10-20 01:48:05,151 AE_UNIGRAMA_8L_OVER_04.py:132]: evaluating model ... 
[2017-10-20 01:48:05,231 AE_UNIGRAMA_8L_OVER_04.py:136]: evaluated! 
[2017-10-20 01:48:05,231 AE_UNIGRAMA_8L_OVER_04.py:138]: generating reports ... 
[2017-10-20 01:48:05,830 AE_UNIGRAMA_8L_OVER_04.py:141]: done!
[2017-10-20 01:48:05,830 AE_UNIGRAMA_8L_OVER_04.py:157]: >> experiment AE_UNIGRAMA_8L_OVER_04 finished!
