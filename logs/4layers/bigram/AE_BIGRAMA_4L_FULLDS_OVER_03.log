[2018-06-03 00:54:28,307 AE_BIGRAMA_4L_FULLDS_OVER_03.py:145]: >> Initializing execution of experiment AE_BIGRAMA_4L_FULLDS_OVER_03
[2018-06-03 00:54:28,307 AE_BIGRAMA_4L_FULLDS_OVER_03.py:146]: >> Printing header log
[2018-06-03 00:54:28,307 AE_BIGRAMA_4L_FULLDS_OVER_03.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_4L_FULLDS_OVER_03
	layers = 9216,15667,14101,12535,10970
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiego/deepnn/logs/4layers/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/4layers/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/4layers/bigram/fullds/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/4layers/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/4layers/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/4layers/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7ff975b085c0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7ff975b08da0>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2018-06-03 00:54:28,307 AE_BIGRAMA_4L_FULLDS_OVER_03.py:148]: >> Loading dataset... 
[2018-06-03 01:16:42,023 AE_BIGRAMA_4L_FULLDS_OVER_03.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (8147, 9216)
	trainy shape = (8147, 9)
	valx shape = (2721, 9216)
	valy shape = (2721, 9)
	=======================================
	
[2018-06-03 01:16:42,023 AE_BIGRAMA_4L_FULLDS_OVER_03.py:150]: >> Executing autoencoder part ... 
[2018-06-03 01:16:42,023 AE_BIGRAMA_4L_FULLDS_OVER_03.py:57]: =======================================
[2018-06-03 01:16:42,024 AE_BIGRAMA_4L_FULLDS_OVER_03.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7ff975b085c0>, 'discard_decoder_function': True}
[2018-06-03 01:16:43,715 AE_BIGRAMA_4L_FULLDS_OVER_03.py:73]: training and evaluate autoencoder
[2018-06-12 16:07:26,805 AE_BIGRAMA_4L_FULLDS_OVER_03.py:85]: trained and evaluated!
[2018-06-12 16:07:26,813 AE_BIGRAMA_4L_FULLDS_OVER_03.py:88]: Training history: 
{'val_loss': [0.0001069265612796307, 0.00010688172034703289, 0.0001068368062029527, 0.00010679183132962835, 0.00010674678800187, 0.00010670169427720861, 0.00010665656889771949, 0.00010661139565360661, 0.00010656620327701422, 0.00010652100030068056, 0.00010647579423052737, 0.00010643057622095825, 0.00010638532006141217, 0.00010634002704075779, 0.00010629465231331882, 0.00010624923114649878, 0.00010620375515462951, 0.00010615824307978632, 0.00010611269640069193, 0.00010606717022316574, 0.00010602166554461022, 0.00010597619963907395, 0.00010593078250197391, 0.00010588541218396314, 0.00010584008909951182, 0.00010579480600475037, 0.00010574956356015711, 0.00010570438181272, 0.00010565924252842398, 0.0001056141603180123, 0.00010556912362445126, 0.00010552414103930707, 0.00010547920623589263, 0.00010543430834171851, 0.0001053894818941859, 0.00010534470052496766, 0.00010529997643018388, 0.00010525529991925373, 0.00010521069152583343, 0.000105166137238156, 0.00010512165575551273, 0.00010507720144072295, 0.00010503282789949129, 0.00010498850412142437, 0.00010494422790314506, 0.00010490002769870152, 0.00010485586787970002, 0.00010481178030151841, 0.00010476776429030836, 0.00010472379301781454, 0.00010467988091294809, 0.00010463602485782349, 0.00010459224455448252, 0.00010454852314067279, 0.00010450487660692227, 0.00010446130480081286, 0.00010441778411358686, 0.00010437431715774352, 0.00010433092980691974, 0.00010428759182618249, 0.00010424433121232566, 0.00010420113457127661, 0.00010415801353494111, 0.00010411495260213349, 0.00010407196517494836, 0.00010402903772026501, 0.0001039861838059662, 0.0001039434019533292, 0.00010390067999832194, 0.00010385804226633451, 0.00010381548114983315, 0.00010377297855117685, 0.00010373053316545297, 0.00010368816053128323, 0.0001036458571751399, 0.00010360360796217547, 0.00010356142582118636, 0.0001035193305852404, 0.0001034772873933823, 0.00010343530463395423, 0.00010339340060247325, 0.00010335157964953954, 0.00010330980442737655, 0.0001032680963012549, 0.00010322647201844488, 0.0001031849011461374, 0.00010314341269724686, 0.00010310197602771804, 0.00010306059778511825, 0.00010301930950127114, 0.00010297807360645805, 0.00010293691443676507, 0.00010289582418410833, 0.00010285479281028682, 0.00010281384066177669, 0.0001027729613049307, 0.00010273212421869771, 0.0001026913645982832, 0.00010265066889452244, 0.00010261005380387972, 0.0001025695017127083], 'val_acc': [0.0, 0.0, 0.0, 0.002205071664829107, 0.07056229327453142, 0.2965821389195149, 0.4693127526644616, 0.4924660051451672, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436], 'loss': [0.00010695565004008519, 0.00010691114934557835, 0.00010686655903720888, 0.00010682189781616154, 0.0001067771769558392, 0.00010673239685991594, 0.00010668756510531877, 0.00010664270646709867, 0.00010659780688810909, 0.00010655289778080335, 0.00010650799054718841, 0.00010646307541870846, 0.00010641814538375882, 0.0001063731696309319, 0.00010632815238273016, 0.00010628304713804142, 0.00010623788600949704, 0.00010619265946967398, 0.00010614741156995422, 0.00010610214653194717, 0.00010605690518121444, 0.00010601169038991504, 0.00010596651619376152, 0.00010592139542012334, 0.00010587632167362399, 0.00010583129270726344, 0.00010578630624724911, 0.0001057413596696994, 0.00010569647273678319, 0.00010565162758234951, 0.00010560683865828817, 0.00010556209750441177, 0.00010551741010617116, 0.00010547276878661389, 0.00010542816595988208, 0.00010538363523630947, 0.00010533915097544643, 0.000105294722269784, 0.00010525034165135145, 0.00010520602739443926, 0.00010516176685297408, 0.00010511757856559934, 0.00010507342119412305, 0.00010502934037604491, 0.00010498531273309779, 0.00010494133270851128, 0.00010489743066625778, 0.00010485356902624394, 0.00010480978074417271, 0.00010476606304344506, 0.0001047223948267311, 0.00010467878011112391, 0.00010463522560368641, 0.00010459174611890062, 0.0001045483243550448, 0.00010450497876770558, 0.00010446170965160076, 0.00010441848764926395, 0.0001043753241359096, 0.00010433223909608461, 0.00010428920411899191, 0.000104246246221324, 0.0001042033538599666, 0.00010416053615598943, 0.00010411777643015051, 0.00010407509375069228, 0.0001040324707131883, 0.00010398991923406212, 0.00010394744168088014, 0.00010390502494763058, 0.00010386269075323002, 0.00010382043463136397, 0.00010377823672876893, 0.00010373609469931249, 0.00010369402705305942, 0.00010365202652834144, 0.00010361008126053392, 0.00010356820328037518, 0.00010352641189277496, 0.00010348467253537169, 0.0001034429919786373, 0.00010340139360530435, 0.00010335987576316694, 0.00010331840257345727, 0.00010327699948103951, 0.0001032356785541615, 0.00010319441096049041, 0.00010315322781240325, 0.00010311209545491208, 0.00010307102034948164, 0.00010303003465250445, 0.00010298910189667019, 0.00010294824860568883, 0.00010290746011243728, 0.00010286673384036738, 0.00010282608148709701, 0.00010278550371261547, 0.00010274496891424248, 0.00010270451072999737, 0.0001026641179597102, 0.00010262380419523064], 'acc': [0.0, 0.0, 0.0, 0.0004909782742113662, 0.026144593101755247, 0.17478826558998176, 0.397937891237338, 0.4845955565880892, 0.4928194427652653, 0.4931876764709238, 0.4931876764892142, 0.4931876764160527, 0.4931876764709238, 0.4931876764892142, 0.4931876764745819, 0.4931876763867881, 0.49318767645263345, 0.49318767645263345, 0.4931876763867881, 0.49318767643800115, 0.4931876764709238, 0.49318767643800115, 0.49318767645263345, 0.4931876764709238, 0.4931876764709238, 0.4931876764892142, 0.49318767643800115, 0.4931876764709238, 0.4931876763867881, 0.4931876764709238, 0.4931876764709238, 0.4931876764892142, 0.49318767643800115, 0.4931876763867881, 0.4931876764709238, 0.4931876764892142, 0.4931876764892142, 0.4931876763867881, 0.4931876764892142, 0.4931876764709238, 0.4931876764343431, 0.49318767643800115, 0.4931876764709238, 0.4931876764343431, 0.49318767643800115, 0.4931876764343431, 0.4931876763867881, 0.49318767643800115, 0.4931876764343431, 0.49318767643800115, 0.4931876764892142, 0.4931876764745819, 0.49318767643800115, 0.4931876764892142, 0.4931876764709238, 0.49318767646726575, 0.4931876764709238, 0.4931876764892142, 0.49318767643800115, 0.4931876763867881, 0.49318767643800115, 0.4931876764892142, 0.49318767645263345, 0.49318767643800115, 0.4931876764709238, 0.4931876763867881, 0.4931876764892142, 0.4931876764343431, 0.49318767643800115, 0.49318767645263345, 0.49318767646726575, 0.4931876764709238, 0.49318767643800115, 0.4931876764709238, 0.49318767643800115, 0.4931876764892142, 0.4931876764745819, 0.49318767645263345, 0.4931876764745819, 0.4931876764745819, 0.4931876764892142, 0.4931876764160527, 0.4931876764745819, 0.49318767643800115, 0.49318767645263345, 0.4931876763867881, 0.49318767645263345, 0.49318767643800115, 0.4931876763867881, 0.4931876764160527, 0.49318767643800115, 0.4931876764892142, 0.49318767643800115, 0.4931876764709238, 0.49318767645263345, 0.4931876764343431, 0.49318767643800115, 0.49318767643800115, 0.49318767643800115, 0.49318767645263345, 0.4931876764892142]}
[2018-06-12 16:07:26,813 AE_BIGRAMA_4L_FULLDS_OVER_03.py:92]: done!
[2018-06-12 16:07:26,813 AE_BIGRAMA_4L_FULLDS_OVER_03.py:152]: >> Executing classifier part ... 
[2018-06-12 16:07:26,813 AE_BIGRAMA_4L_FULLDS_OVER_03.py:97]: =======================================
[2018-06-12 16:07:26,814 AE_BIGRAMA_4L_FULLDS_OVER_03.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7ff975b08da0>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2018-06-12 16:07:27,066 AE_BIGRAMA_4L_FULLDS_OVER_03.py:110]: training ... 
[2018-06-21 07:28:13,245 AE_BIGRAMA_4L_FULLDS_OVER_03.py:122]: trained!
[2018-06-21 07:28:13,246 AE_BIGRAMA_4L_FULLDS_OVER_03.py:125]: Training history: 
{'val_loss': [0.0001069265612796307, 0.00010688172034703289, 0.0001068368062029527, 0.00010679183132962835, 0.00010674678800187, 0.00010670169427720861, 0.00010665656889771949, 0.00010661139565360661, 0.00010656620327701422, 0.00010652100030068056, 0.00010647579423052737, 0.00010643057622095825, 0.00010638532006141217, 0.00010634002704075779, 0.00010629465231331882, 0.00010624923114649878, 0.00010620375515462951, 0.00010615824307978632, 0.00010611269640069193, 0.00010606717022316574, 0.00010602166554461022, 0.00010597619963907395, 0.00010593078250197391, 0.00010588541218396314, 0.00010584008909951182, 0.00010579480600475037, 0.00010574956356015711, 0.00010570438181272, 0.00010565924252842398, 0.0001056141603180123, 0.00010556912362445126, 0.00010552414103930707, 0.00010547920623589263, 0.00010543430834171851, 0.0001053894818941859, 0.00010534470052496766, 0.00010529997643018388, 0.00010525529991925373, 0.00010521069152583343, 0.000105166137238156, 0.00010512165575551273, 0.00010507720144072295, 0.00010503282789949129, 0.00010498850412142437, 0.00010494422790314506, 0.00010490002769870152, 0.00010485586787970002, 0.00010481178030151841, 0.00010476776429030836, 0.00010472379301781454, 0.00010467988091294809, 0.00010463602485782349, 0.00010459224455448252, 0.00010454852314067279, 0.00010450487660692227, 0.00010446130480081286, 0.00010441778411358686, 0.00010437431715774352, 0.00010433092980691974, 0.00010428759182618249, 0.00010424433121232566, 0.00010420113457127661, 0.00010415801353494111, 0.00010411495260213349, 0.00010407196517494836, 0.00010402903772026501, 0.0001039861838059662, 0.0001039434019533292, 0.00010390067999832194, 0.00010385804226633451, 0.00010381548114983315, 0.00010377297855117685, 0.00010373053316545297, 0.00010368816053128323, 0.0001036458571751399, 0.00010360360796217547, 0.00010356142582118636, 0.0001035193305852404, 0.0001034772873933823, 0.00010343530463395423, 0.00010339340060247325, 0.00010335157964953954, 0.00010330980442737655, 0.0001032680963012549, 0.00010322647201844488, 0.0001031849011461374, 0.00010314341269724686, 0.00010310197602771804, 0.00010306059778511825, 0.00010301930950127114, 0.00010297807360645805, 0.00010293691443676507, 0.00010289582418410833, 0.00010285479281028682, 0.00010281384066177669, 0.0001027729613049307, 0.00010273212421869771, 0.0001026913645982832, 0.00010265066889452244, 0.00010261005380387972, 0.0001025695017127083], 'val_acc': [0.0, 0.0, 0.0, 0.002205071664829107, 0.07056229327453142, 0.2965821389195149, 0.4693127526644616, 0.4924660051451672, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436], 'loss': [0.00010695565004008519, 0.00010691114934557835, 0.00010686655903720888, 0.00010682189781616154, 0.0001067771769558392, 0.00010673239685991594, 0.00010668756510531877, 0.00010664270646709867, 0.00010659780688810909, 0.00010655289778080335, 0.00010650799054718841, 0.00010646307541870846, 0.00010641814538375882, 0.0001063731696309319, 0.00010632815238273016, 0.00010628304713804142, 0.00010623788600949704, 0.00010619265946967398, 0.00010614741156995422, 0.00010610214653194717, 0.00010605690518121444, 0.00010601169038991504, 0.00010596651619376152, 0.00010592139542012334, 0.00010587632167362399, 0.00010583129270726344, 0.00010578630624724911, 0.0001057413596696994, 0.00010569647273678319, 0.00010565162758234951, 0.00010560683865828817, 0.00010556209750441177, 0.00010551741010617116, 0.00010547276878661389, 0.00010542816595988208, 0.00010538363523630947, 0.00010533915097544643, 0.000105294722269784, 0.00010525034165135145, 0.00010520602739443926, 0.00010516176685297408, 0.00010511757856559934, 0.00010507342119412305, 0.00010502934037604491, 0.00010498531273309779, 0.00010494133270851128, 0.00010489743066625778, 0.00010485356902624394, 0.00010480978074417271, 0.00010476606304344506, 0.0001047223948267311, 0.00010467878011112391, 0.00010463522560368641, 0.00010459174611890062, 0.0001045483243550448, 0.00010450497876770558, 0.00010446170965160076, 0.00010441848764926395, 0.0001043753241359096, 0.00010433223909608461, 0.00010428920411899191, 0.000104246246221324, 0.0001042033538599666, 0.00010416053615598943, 0.00010411777643015051, 0.00010407509375069228, 0.0001040324707131883, 0.00010398991923406212, 0.00010394744168088014, 0.00010390502494763058, 0.00010386269075323002, 0.00010382043463136397, 0.00010377823672876893, 0.00010373609469931249, 0.00010369402705305942, 0.00010365202652834144, 0.00010361008126053392, 0.00010356820328037518, 0.00010352641189277496, 0.00010348467253537169, 0.0001034429919786373, 0.00010340139360530435, 0.00010335987576316694, 0.00010331840257345727, 0.00010327699948103951, 0.0001032356785541615, 0.00010319441096049041, 0.00010315322781240325, 0.00010311209545491208, 0.00010307102034948164, 0.00010303003465250445, 0.00010298910189667019, 0.00010294824860568883, 0.00010290746011243728, 0.00010286673384036738, 0.00010282608148709701, 0.00010278550371261547, 0.00010274496891424248, 0.00010270451072999737, 0.0001026641179597102, 0.00010262380419523064], 'acc': [0.0, 0.0, 0.0, 0.0004909782742113662, 0.026144593101755247, 0.17478826558998176, 0.397937891237338, 0.4845955565880892, 0.4928194427652653, 0.4931876764709238, 0.4931876764892142, 0.4931876764160527, 0.4931876764709238, 0.4931876764892142, 0.4931876764745819, 0.4931876763867881, 0.49318767645263345, 0.49318767645263345, 0.4931876763867881, 0.49318767643800115, 0.4931876764709238, 0.49318767643800115, 0.49318767645263345, 0.4931876764709238, 0.4931876764709238, 0.4931876764892142, 0.49318767643800115, 0.4931876764709238, 0.4931876763867881, 0.4931876764709238, 0.4931876764709238, 0.4931876764892142, 0.49318767643800115, 0.4931876763867881, 0.4931876764709238, 0.4931876764892142, 0.4931876764892142, 0.4931876763867881, 0.4931876764892142, 0.4931876764709238, 0.4931876764343431, 0.49318767643800115, 0.4931876764709238, 0.4931876764343431, 0.49318767643800115, 0.4931876764343431, 0.4931876763867881, 0.49318767643800115, 0.4931876764343431, 0.49318767643800115, 0.4931876764892142, 0.4931876764745819, 0.49318767643800115, 0.4931876764892142, 0.4931876764709238, 0.49318767646726575, 0.4931876764709238, 0.4931876764892142, 0.49318767643800115, 0.4931876763867881, 0.49318767643800115, 0.4931876764892142, 0.49318767645263345, 0.49318767643800115, 0.4931876764709238, 0.4931876763867881, 0.4931876764892142, 0.4931876764343431, 0.49318767643800115, 0.49318767645263345, 0.49318767646726575, 0.4931876764709238, 0.49318767643800115, 0.4931876764709238, 0.49318767643800115, 0.4931876764892142, 0.4931876764745819, 0.49318767645263345, 0.4931876764745819, 0.4931876764745819, 0.4931876764892142, 0.4931876764160527, 0.4931876764745819, 0.49318767643800115, 0.49318767645263345, 0.4931876763867881, 0.49318767645263345, 0.49318767643800115, 0.4931876763867881, 0.4931876764160527, 0.49318767643800115, 0.4931876764892142, 0.49318767643800115, 0.4931876764709238, 0.49318767645263345, 0.4931876764343431, 0.49318767643800115, 0.49318767643800115, 0.49318767643800115, 0.49318767645263345, 0.4931876764892142]}
[2018-06-21 07:28:13,247 AE_BIGRAMA_4L_FULLDS_OVER_03.py:129]: evaluating model ... 
[2018-06-21 07:32:26,326 AE_BIGRAMA_4L_FULLDS_OVER_03.py:133]: evaluated! 
[2018-06-21 07:32:26,327 AE_BIGRAMA_4L_FULLDS_OVER_03.py:135]: generating reports ... 
[2018-06-21 07:32:30,687 AE_BIGRAMA_4L_FULLDS_OVER_03.py:138]: done!
[2018-06-21 07:32:30,687 AE_BIGRAMA_4L_FULLDS_OVER_03.py:154]: >> experiment AE_BIGRAMA_4L_FULLDS_OVER_03 finished!
