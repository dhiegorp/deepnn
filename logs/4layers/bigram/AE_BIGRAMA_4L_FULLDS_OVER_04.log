[2018-06-03 00:54:28,368 AE_BIGRAMA_4L_FULLDS_OVER_04.py:145]: >> Initializing execution of experiment AE_BIGRAMA_4L_FULLDS_OVER_04
[2018-06-03 00:54:28,369 AE_BIGRAMA_4L_FULLDS_OVER_04.py:146]: >> Printing header log
[2018-06-03 00:54:28,369 AE_BIGRAMA_4L_FULLDS_OVER_04.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_4L_FULLDS_OVER_04
	layers = 9216,16589,14931,13273,11615
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiego/deepnn/logs/4layers/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/4layers/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/4layers/bigram/fullds/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/4layers/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/4layers/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/4layers/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f115da9b630>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f115da9be10>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2018-06-03 00:54:28,369 AE_BIGRAMA_4L_FULLDS_OVER_04.py:148]: >> Loading dataset... 
[2018-06-03 01:10:30,606 AE_BIGRAMA_4L_FULLDS_OVER_04.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (8147, 9216)
	trainy shape = (8147, 9)
	valx shape = (2721, 9216)
	valy shape = (2721, 9)
	=======================================
	
[2018-06-03 01:10:30,607 AE_BIGRAMA_4L_FULLDS_OVER_04.py:150]: >> Executing autoencoder part ... 
[2018-06-03 01:10:30,607 AE_BIGRAMA_4L_FULLDS_OVER_04.py:57]: =======================================
[2018-06-03 01:10:30,607 AE_BIGRAMA_4L_FULLDS_OVER_04.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f115da9b630>, 'discard_decoder_function': True}
[2018-06-03 01:10:31,020 AE_BIGRAMA_4L_FULLDS_OVER_04.py:73]: training and evaluate autoencoder
[2018-06-15 12:04:07,446 AE_BIGRAMA_4L_FULLDS_OVER_04.py:85]: trained and evaluated!
[2018-06-15 12:04:07,448 AE_BIGRAMA_4L_FULLDS_OVER_04.py:88]: Training history: 
{'val_loss': [0.00010703900041763655, 0.00010702749897638344, 0.00010701585832394719, 0.00010700408844237474, 0.00010699220373918525, 0.00010698019982634255, 0.00010696806722451194, 0.00010695579033624364, 0.00010694337881200846, 0.00010693084316330564, 0.00010691821514390093, 0.00010690553464722015, 0.00010689281882430783, 0.00010688007484416151, 0.00010686731010574289, 0.00010685452129061631, 0.00010684170611518462, 0.00010682886958785234, 0.00010681600582581653, 0.00010680309921023543, 0.00010679014715267577, 0.00010677709011917196, 0.00010676383421484135, 0.00010675012276229647, 0.00010673506318524192, 0.00010671675781713311, 0.0001066901477362491, 0.00010664884664393562, 0.00010660018325050508, 0.00010655092130594321, 0.00010650168215456866, 0.00010645246257688385, 0.0001064032562141136, 0.00010635408107833085, 0.00010630493797441, 0.00010625577333408249, 0.00010620660664546993, 0.00010615742559479624, 0.00010610826074857061, 0.00010605909350911379, 0.00010600993919845339, 0.00010596081989581837, 0.00010591172508436154, 0.00010586266888280987, 0.00010581363893727724, 0.0001057646343332552, 0.00010571572012117408, 0.00010566686794324974, 0.00010561808674936468, 0.00010556935199486069, 0.000105520692326314, 0.00010547208633566996, 0.00010542354817909162, 0.00010537508479293842, 0.00010532669378665318, 0.00010527836022326447, 0.00010523012122325873, 0.00010518194761862947, 0.00010513385920233454, 0.00010508583794090895, 0.00010503789114239836, 0.0001049899881119415, 0.00010494217030725499, 0.00010489441601812203, 0.00010484674628358505, 0.000104799152886438, 0.00010475160947707178, 0.00010470414327147192, 0.00010465676520362983, 0.00010460945174500783, 0.00010456221137486415, 0.00010451503152806652, 0.00010446794452794302, 0.00010442096608425144, 0.00010437402931754455, 0.00010432717889434056, 0.00010428038968972298, 0.00010423366335622462, 0.00010418701078772691, 0.0001041404478223397, 0.0001040939611408624, 0.00010404753035403486, 0.00010400119890903065, 0.00010395490729060224, 0.00010390870629942684, 0.00010386261304911296, 0.00010381657169046886, 0.0001037706101400684, 0.00010372471930898106, 0.00010367888530844395, 0.0001036331450289794, 0.00010358747791821319, 0.00010354190434668745, 0.00010349640952717484, 0.0001034509718724626, 0.00010340559790711401, 0.00010336031356091995, 0.00010331511240290718, 0.00010326998500989508, 0.00010322493485006333, 0.00010317997086527238], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0003675119441381845, 0.0003675119441381845, 0.000735023888276369, 0.0018375597206909224, 0.00404263138552003, 0.007717750826901874, 0.015067989709665565, 0.02719588386622565, 0.03601617052554208, 0.07864755604557148, 0.08710033076074973, 0.08967291436971701, 0.09151047409040794, 0.09224549797868431, 0.09224549797868431, 0.09224549797868431, 0.09224549797868431, 0.09224549797868431, 0.09224549797868431, 0.09224549797868431, 0.09224549797868431, 0.09224549797868431, 0.09224549797868431, 0.09224549797868431, 0.09371554575523705, 0.10290334435869165, 0.18265343623667768, 0.3083425211319368, 0.43733921352443955, 0.5148842337375965, 0.5420801176038221, 0.537669974274164, 0.4954061006982727, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436], 'loss': [0.00010704577995271986, 0.00010703421025130566, 0.00010702249809937675, 0.00010701066042891797, 0.00010699870969488266, 0.00010698665826559498, 0.00010697449565624552, 0.00010696221276094697, 0.00010694979191091997, 0.00010693724086170841, 0.00010692457212988841, 0.00010691182877462539, 0.00010689904372125743, 0.00010688622252655489, 0.00010687338376309843, 0.00010686052643688524, 0.00010684764994776271, 0.00010683475273729876, 0.00010682183614333362, 0.00010680889467697135, 0.00010679590234945943, 0.0001067828455144701, 0.00010676965505263874, 0.00010675619913132961, 0.00010674197175042474, 0.00010672569000596867, 0.00010670420439249329, 0.00010667066186351352, 0.0001066248406367313, 0.00010657602517700671, 0.0001065269855361597, 0.00010647799280813672, 0.0001064290149776526, 0.00010638004763644333, 0.0001063311012594944, 0.00010628217538061582, 0.00010623321563955375, 0.00010618425830981923, 0.0001061352857110228, 0.0001060863337424732, 0.00010603738757718526, 0.00010598845576733399, 0.00010593956667816917, 0.00010589069841930204, 0.00010584187791900978, 0.00010579308468547362, 0.0001057443202780187, 0.00010569565095813997, 0.00010564704568669332, 0.0001055985126246829, 0.00010555002940124062, 0.00010550162190152121, 0.00010545326942025875, 0.00010540498480447059, 0.00010535677806205914, 0.00010530864076498808, 0.00010526056360910541, 0.00010521258312959583, 0.00010516466717006686, 0.00010511683782899585, 0.00010506907633910979, 0.00010502138713367903, 0.00010497374455171302, 0.000104926186172412, 0.00010487868938377516, 0.00010483127937703069, 0.00010478394511544312, 0.00010473665840255559, 0.00010468945105092319, 0.00010464233241560373, 0.00010459527873430382, 0.00010454829786437894, 0.00010450137804480224, 0.00010445454904564504, 0.00010440782783282935, 0.0001043611472124803, 0.00010431455482796483, 0.00010426802032602759, 0.00010422155474608336, 0.00010417516131484562, 0.00010412885781540853, 0.00010408262994592044, 0.00010403645751910435, 0.00010399038609208235, 0.0001039443554950874, 0.00010389841534788196, 0.00010385258517953994, 0.00010380680365214006, 0.0001037611036285045, 0.00010371547370662624, 0.00010366990198615733, 0.00010362442561404521, 0.00010357901863190223, 0.0001035337039732396, 0.00010348846866779435, 0.00010344329128690228, 0.00010339817666840095, 0.00010335315085284173, 0.00010330821210228263, 0.00010326334260862312, 0.00010321855235833161], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0002454891371056831, 0.0007364674113170492, 0.001841168528292623, 0.003682337056585246, 0.007241929544617651, 0.012888179698048362, 0.02135755492819443, 0.03215907696084448, 0.05867190376825825, 0.08702589910762272, 0.09193568184790735, 0.09389959494383829, 0.09512704063028123, 0.09586350804068376, 0.09586350803976924, 0.09598625259368979, 0.0959862526092366, 0.0959862526092366, 0.09598625261015112, 0.0959862526092366, 0.09598625260832208, 0.09598625261015112, 0.0959862526092366, 0.09623174174908584, 0.09930035596290687, 0.1333006014337536, 0.2345648704752156, 0.3693384068011067, 0.48705044801035907, 0.5401988462449525, 0.5521050693799459, 0.5259604762745325, 0.494046888366658, 0.4930649318913967, 0.49318767645263345, 0.49318767645263345, 0.4931876764343431, 0.49318767645263345, 0.4931876764343431, 0.4931876764709238, 0.49318767643800115, 0.4931876764709238, 0.4931876763867881, 0.49318767643800115, 0.49318767645263345, 0.4931876764892142, 0.49318767646726575, 0.4931876764892142, 0.4931876764160527, 0.4931876764709238, 0.49318767643800115, 0.4931876764892142, 0.49318767645263345, 0.4931876764892142, 0.49318767651116263, 0.4931876764892142, 0.4931876764745819, 0.4931876764892142, 0.49318767643800115, 0.4931876763867881, 0.49318767643800115, 0.49318767646726575, 0.49318767645263345, 0.4931876764709238, 0.4931876764709238, 0.4931876764745819, 0.49318767643800115, 0.4931876764709238, 0.49318767645263345, 0.49318767642336886, 0.4931876764160527, 0.49318767651116263, 0.4931876763867881, 0.4931876764160527, 0.4931876764892142, 0.4931876764892142, 0.4931876764709238, 0.4931876763867881, 0.49318767643800115, 0.4931876764892142, 0.49318767646726575, 0.4931876764709238, 0.49318767645263345, 0.4931876763867881, 0.4931876763867881, 0.4931876764709238, 0.49318767645263345, 0.4931876764892142, 0.4931876764709238]}
[2018-06-15 12:04:07,448 AE_BIGRAMA_4L_FULLDS_OVER_04.py:92]: done!
[2018-06-15 12:04:07,448 AE_BIGRAMA_4L_FULLDS_OVER_04.py:152]: >> Executing classifier part ... 
[2018-06-15 12:04:07,450 AE_BIGRAMA_4L_FULLDS_OVER_04.py:97]: =======================================
[2018-06-15 12:04:07,450 AE_BIGRAMA_4L_FULLDS_OVER_04.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f115da9be10>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2018-06-15 12:04:07,521 AE_BIGRAMA_4L_FULLDS_OVER_04.py:110]: training ... 
