[2017-10-25 00:45:48,071 AE_UNIGRAMA_03.py:148]: >> Initializing execution of experiment AE_UNIGRAMA_03
[2017-10-25 00:45:48,071 AE_UNIGRAMA_03.py:149]: >> Printing header log
[2017-10-25 00:45:48,071 AE_UNIGRAMA_03.py:38]: 
	=======================================
	network_name = AE_UNIGRAMA_03
	layers = 96,86,78,71,63,55,48,40,32,24,17,9
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/10layers/unigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/10layers/unigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/10layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/10layers/unigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/10layers/unigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/10layers/unigram/', 'data_dir': '/home/dhiegorp/malware_deepnn/', 'fullds_data_dir': '/home/dhiegorp/malware_deepnn/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7feb9dffd710>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7feb9dffd7f0>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-10-25 00:45:48,071 AE_UNIGRAMA_03.py:151]: >> Loading dataset... 
[2017-10-25 00:45:50,288 AE_UNIGRAMA_03.py:55]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_deepnn/	
	trainx shape = (8147, 96)
	trainy shape = (8147, 9)
	valx shape = (2721, 96)
	valy shape = (2721, 9)
	=======================================
	
[2017-10-25 00:45:50,288 AE_UNIGRAMA_03.py:153]: >> Executing autoencoder part ... 
[2017-10-25 00:45:50,288 AE_UNIGRAMA_03.py:60]: =======================================
[2017-10-25 00:45:50,289 AE_UNIGRAMA_03.py:65]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7feb9dffd710>, 'discard_decoder_function': True}
[2017-10-25 00:45:50,492 AE_UNIGRAMA_03.py:76]: training and evaluate autoencoder
[2017-10-25 00:48:22,399 AE_UNIGRAMA_03.py:88]: trained and evaluated!
[2017-10-25 00:48:22,399 AE_UNIGRAMA_03.py:91]: Training history: 
{'val_loss': [0.0091917822561892511, 0.0083562442409925407, 0.0077728425159102983, 0.0073540258902107603, 0.0070459674244615815, 0.0068134689354123834, 0.0066321096574707599, 0.0064767140630551867, 0.0063475345967160196, 0.0062440156463283098, 0.006159194137761421, 0.0060890038102079363, 0.0060303278160594659, 0.0059808276556719752, 0.005938753153846758, 0.0059027796834009022, 0.0058718907204619204, 0.0058452497150520326, 0.0058222695838903698, 0.0058023820554879346, 0.005785093009225457, 0.0057700619105631621, 0.005756948278184593, 0.0057454684877426571, 0.0057355164217727395, 0.0057268134270887444, 0.005719200347987295, 0.0057123924767443117, 0.005706357038660315, 0.00570111506867049, 0.0056965848259341676, 0.0056926222728241544, 0.0056891601101605876, 0.0056861538298972952, 0.005683509899489513, 0.0056812130204836253, 0.0056792069338597508, 0.005677454551861227, 0.005675907132192866, 0.0056745464021414624, 0.0056733469479128265, 0.0056723120584858061, 0.00567139026100341, 0.0056705789846593696, 0.0056698662918560731, 0.0056692397332134513, 0.0056686878396189251, 0.0056682053371717644, 0.0056677785127374758, 0.005667401176751706, 0.0056670757019307051, 0.0056667748530789281, 0.0056665159529331729, 0.0056662887036362276, 0.00566608777404234, 0.0056659034512374371, 0.0056657448931412687, 0.0056655984964887095, 0.00566547559031745, 0.0056653604769989581, 0.0056652682079519094, 0.0056651808928666813, 0.0056650989128633356, 0.0056650299676110401, 0.0056649685612455616, 0.0056649181904193891, 0.0056648680981171949, 0.0056648229630284069, 0.0056647815311533624, 0.0056647507714101472, 0.005664712528912751, 0.0056646873029404122, 0.0056646647436105505, 0.0056646383962690155, 0.0056646155491738267, 0.005664597320271459, 0.0056645810322234324, 0.0056645586522441876, 0.0056645453807263636, 0.0056645349838669269, 0.0056645228243913832, 0.0056645114529119429, 0.0056645119349167262, 0.0056644976597713314, 0.0056644882305153191, 0.0056644788395937909, 0.0056644728548793299, 0.0056644665317432613, 0.0056644710521318101, 0.005664461038617204, 0.005664450583571499, 0.0056644405229089017, 0.0056644357308418174, 0.0056644344287529146, 0.0056644290256444383, 0.0056644273881288091, 0.0056644218807128891, 0.0056644170424390624, 0.0056644119476322438, 0.0056644044261157395, 0.0056644013283814647, 0.0056643976338956615], 'loss': [0.0097692212618259149, 0.0087532861446553648, 0.0080545697271150792, 0.0075602784594634102, 0.0072016763370321788, 0.0069347738343328363, 0.0067306620618016377, 0.0065649691373004648, 0.0064220841623873811, 0.0063068088491237961, 0.0062135587591228419, 0.0061367521833775194, 0.0060728918735865689, 0.0060193999836202364, 0.0059740842076305839, 0.0059354701300627314, 0.0059024096155436039, 0.0058739969304636916, 0.0058494738268373208, 0.0058283175578873825, 0.0058099875984295273, 0.0057940462090777366, 0.005780183030442144, 0.0057680726246122967, 0.0057575310343457864, 0.0057483716646436391, 0.0057403626778004457, 0.005733322660820871, 0.0057270097736270608, 0.005721505882639844, 0.0057167402809622739, 0.0057126140723566679, 0.005709003116908056, 0.0057058654564596384, 0.0057031307184852217, 0.0057007370051853903, 0.0056986574672745084, 0.0056968381293108859, 0.0056952536706833829, 0.0056938527787989733, 0.005692630991608552, 0.005691568455948641, 0.0056906283925224213, 0.0056898163828065541, 0.0056890880800939099, 0.0056884520759981393, 0.0056879023779094458, 0.0056874112956997727, 0.0056869868508949229, 0.0056866146033080836, 0.0056862821737909052, 0.0056859925257827853, 0.005685734828071911, 0.0056855095863869044, 0.0056853159674301211, 0.0056851396890549412, 0.0056849847610240135, 0.0056848510066066747, 0.0056847332486005099, 0.0056846232420514628, 0.0056845375284236147, 0.005684453823992474, 0.0056843830171272453, 0.0056843024779582585, 0.0056842715349457625, 0.0056842141301176024, 0.005684170018613148, 0.0056841299681986814, 0.0056840901090328541, 0.0056840619227214615, 0.0056840348011951581, 0.005684010518676604, 0.0056839927719345602, 0.005683971666795528, 0.0056839508884253169, 0.0056839364471525135, 0.0056839235669624497, 0.00568390474811917, 0.0056839003349967946, 0.0056838862119763592, 0.0056838779531330572, 0.0056838661465874784, 0.0056838629481168789, 0.005683843576047184, 0.0056838526003994616, 0.0056838438623485693, 0.0056838395795450521, 0.0056838335065148033, 0.0056838260292988605, 0.0056838300794146285, 0.0056838184682330211, 0.0056838179470719051, 0.0056838134106610306, 0.0056838085910924279, 0.0056838019355711792, 0.005683795547889477, 0.0056838000731546596, 0.0056837993744626777, 0.0056837944413794909, 0.0056837909129964139, 0.0056837904259011036, 0.0056837905083792235]}
[2017-10-25 00:48:22,399 AE_UNIGRAMA_03.py:95]: done!
[2017-10-25 00:48:22,399 AE_UNIGRAMA_03.py:155]: >> Executing classifier part ... 
[2017-10-25 00:48:22,400 AE_UNIGRAMA_03.py:100]: =======================================
[2017-10-25 00:48:22,400 AE_UNIGRAMA_03.py:104]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7feb9dffd7f0>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-10-25 00:48:22,430 AE_UNIGRAMA_03.py:113]: training ... 
[2017-10-25 00:55:10,392 AE_UNIGRAMA_03.py:125]: trained!
[2017-10-25 00:55:10,393 AE_UNIGRAMA_03.py:128]: Training history: 
{'val_loss': [0.0091917822561892511, 0.0083562442409925407, 0.0077728425159102983, 0.0073540258902107603, 0.0070459674244615815, 0.0068134689354123834, 0.0066321096574707599, 0.0064767140630551867, 0.0063475345967160196, 0.0062440156463283098, 0.006159194137761421, 0.0060890038102079363, 0.0060303278160594659, 0.0059808276556719752, 0.005938753153846758, 0.0059027796834009022, 0.0058718907204619204, 0.0058452497150520326, 0.0058222695838903698, 0.0058023820554879346, 0.005785093009225457, 0.0057700619105631621, 0.005756948278184593, 0.0057454684877426571, 0.0057355164217727395, 0.0057268134270887444, 0.005719200347987295, 0.0057123924767443117, 0.005706357038660315, 0.00570111506867049, 0.0056965848259341676, 0.0056926222728241544, 0.0056891601101605876, 0.0056861538298972952, 0.005683509899489513, 0.0056812130204836253, 0.0056792069338597508, 0.005677454551861227, 0.005675907132192866, 0.0056745464021414624, 0.0056733469479128265, 0.0056723120584858061, 0.00567139026100341, 0.0056705789846593696, 0.0056698662918560731, 0.0056692397332134513, 0.0056686878396189251, 0.0056682053371717644, 0.0056677785127374758, 0.005667401176751706, 0.0056670757019307051, 0.0056667748530789281, 0.0056665159529331729, 0.0056662887036362276, 0.00566608777404234, 0.0056659034512374371, 0.0056657448931412687, 0.0056655984964887095, 0.00566547559031745, 0.0056653604769989581, 0.0056652682079519094, 0.0056651808928666813, 0.0056650989128633356, 0.0056650299676110401, 0.0056649685612455616, 0.0056649181904193891, 0.0056648680981171949, 0.0056648229630284069, 0.0056647815311533624, 0.0056647507714101472, 0.005664712528912751, 0.0056646873029404122, 0.0056646647436105505, 0.0056646383962690155, 0.0056646155491738267, 0.005664597320271459, 0.0056645810322234324, 0.0056645586522441876, 0.0056645453807263636, 0.0056645349838669269, 0.0056645228243913832, 0.0056645114529119429, 0.0056645119349167262, 0.0056644976597713314, 0.0056644882305153191, 0.0056644788395937909, 0.0056644728548793299, 0.0056644665317432613, 0.0056644710521318101, 0.005664461038617204, 0.005664450583571499, 0.0056644405229089017, 0.0056644357308418174, 0.0056644344287529146, 0.0056644290256444383, 0.0056644273881288091, 0.0056644218807128891, 0.0056644170424390624, 0.0056644119476322438, 0.0056644044261157395, 0.0056644013283814647, 0.0056643976338956615], 'loss': [0.0097692212618259149, 0.0087532861446553648, 0.0080545697271150792, 0.0075602784594634102, 0.0072016763370321788, 0.0069347738343328363, 0.0067306620618016377, 0.0065649691373004648, 0.0064220841623873811, 0.0063068088491237961, 0.0062135587591228419, 0.0061367521833775194, 0.0060728918735865689, 0.0060193999836202364, 0.0059740842076305839, 0.0059354701300627314, 0.0059024096155436039, 0.0058739969304636916, 0.0058494738268373208, 0.0058283175578873825, 0.0058099875984295273, 0.0057940462090777366, 0.005780183030442144, 0.0057680726246122967, 0.0057575310343457864, 0.0057483716646436391, 0.0057403626778004457, 0.005733322660820871, 0.0057270097736270608, 0.005721505882639844, 0.0057167402809622739, 0.0057126140723566679, 0.005709003116908056, 0.0057058654564596384, 0.0057031307184852217, 0.0057007370051853903, 0.0056986574672745084, 0.0056968381293108859, 0.0056952536706833829, 0.0056938527787989733, 0.005692630991608552, 0.005691568455948641, 0.0056906283925224213, 0.0056898163828065541, 0.0056890880800939099, 0.0056884520759981393, 0.0056879023779094458, 0.0056874112956997727, 0.0056869868508949229, 0.0056866146033080836, 0.0056862821737909052, 0.0056859925257827853, 0.005685734828071911, 0.0056855095863869044, 0.0056853159674301211, 0.0056851396890549412, 0.0056849847610240135, 0.0056848510066066747, 0.0056847332486005099, 0.0056846232420514628, 0.0056845375284236147, 0.005684453823992474, 0.0056843830171272453, 0.0056843024779582585, 0.0056842715349457625, 0.0056842141301176024, 0.005684170018613148, 0.0056841299681986814, 0.0056840901090328541, 0.0056840619227214615, 0.0056840348011951581, 0.005684010518676604, 0.0056839927719345602, 0.005683971666795528, 0.0056839508884253169, 0.0056839364471525135, 0.0056839235669624497, 0.00568390474811917, 0.0056839003349967946, 0.0056838862119763592, 0.0056838779531330572, 0.0056838661465874784, 0.0056838629481168789, 0.005683843576047184, 0.0056838526003994616, 0.0056838438623485693, 0.0056838395795450521, 0.0056838335065148033, 0.0056838260292988605, 0.0056838300794146285, 0.0056838184682330211, 0.0056838179470719051, 0.0056838134106610306, 0.0056838085910924279, 0.0056838019355711792, 0.005683795547889477, 0.0056838000731546596, 0.0056837993744626777, 0.0056837944413794909, 0.0056837909129964139, 0.0056837904259011036, 0.0056837905083792235]}
[2017-10-25 00:55:10,393 AE_UNIGRAMA_03.py:132]: evaluating model ... 
[2017-10-25 00:55:10,521 AE_UNIGRAMA_03.py:136]: evaluated! 
[2017-10-25 00:55:10,522 AE_UNIGRAMA_03.py:138]: generating reports ... 
[2017-10-25 00:55:11,328 AE_UNIGRAMA_03.py:141]: done!
[2017-10-25 00:55:11,328 AE_UNIGRAMA_03.py:157]: >> experiment AE_UNIGRAMA_03 finished!
