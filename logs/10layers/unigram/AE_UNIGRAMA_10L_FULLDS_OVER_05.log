[2018-07-21 00:52:16,557 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:145]: >> Initializing execution of experiment AE_UNIGRAMA_10L_FULLDS_OVER_05
[2018-07-21 00:52:16,557 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:146]: >> Printing header log
[2018-07-21 00:52:16,557 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:35]: 
	=======================================
	network_name = AE_UNIGRAMA_10L_FULLDS_OVER_05
	layers = 96,192,174,155,137,119,100,82,64,46,27
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiego/deepnn/logs/10layers/unigram/', 'reports_dir': '/home/dhiego/deepnn/reports/10layers/unigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/10layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/10layers/unigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/10layers/unigram/', 'executed_path': '/home/dhiego/deepnn/executed/10layers/unigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f7e8b1ff6a0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f7e8b1ffe80>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2018-07-21 00:52:16,557 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:148]: >> Loading dataset... 
[2018-07-21 00:52:18,496 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (8147, 96)
	trainy shape = (8147, 9)
	valx shape = (2721, 96)
	valy shape = (2721, 9)
	=======================================
	
[2018-07-21 00:52:18,496 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:150]: >> Executing autoencoder part ... 
[2018-07-21 00:52:18,496 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:57]: =======================================
[2018-07-21 00:52:18,496 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f7e8b1ff6a0>, 'discard_decoder_function': True}
[2018-07-21 00:52:18,877 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:73]: training and evaluate autoencoder
[2018-07-21 01:16:32,147 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:145]: >> Initializing execution of experiment AE_UNIGRAMA_10L_FULLDS_OVER_05
[2018-07-21 01:16:32,147 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:146]: >> Printing header log
[2018-07-21 01:16:32,147 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:35]: 
	=======================================
	network_name = AE_UNIGRAMA_10L_FULLDS_OVER_05
	layers = 96,192,174,155,137,119,100,82,64,46,27
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiego/deepnn/logs/10layers/unigram/', 'reports_dir': '/home/dhiego/deepnn/reports/10layers/unigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/10layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/10layers/unigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/10layers/unigram/', 'executed_path': '/home/dhiego/deepnn/executed/10layers/unigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f95f659b5f8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f95f659bdd8>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2018-07-21 01:16:32,147 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:148]: >> Loading dataset... 
[2018-07-21 01:16:34,077 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (8147, 96)
	trainy shape = (8147, 9)
	valx shape = (2721, 96)
	valy shape = (2721, 9)
	=======================================
	
[2018-07-21 01:16:34,077 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:150]: >> Executing autoencoder part ... 
[2018-07-21 01:16:34,078 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:57]: =======================================
[2018-07-21 01:16:34,078 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f95f659b5f8>, 'discard_decoder_function': True}
[2018-07-21 01:16:34,455 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:73]: training and evaluate autoencoder
[2018-07-21 01:18:50,118 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:145]: >> Initializing execution of experiment AE_UNIGRAMA_10L_FULLDS_OVER_05
[2018-07-21 01:18:50,118 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:146]: >> Printing header log
[2018-07-21 01:18:50,118 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:35]: 
	=======================================
	network_name = AE_UNIGRAMA_10L_FULLDS_OVER_05
	layers = 96,192,174,155,137,119,100,82,64,46,27
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiego/deepnn/logs/10layers/unigram/', 'reports_dir': '/home/dhiego/deepnn/reports/10layers/unigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/10layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/10layers/unigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/10layers/unigram/', 'executed_path': '/home/dhiego/deepnn/executed/10layers/unigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f8c166a26a0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f8c166a2e80>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2018-07-21 01:18:50,118 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:148]: >> Loading dataset... 
[2018-07-21 01:18:52,123 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (8147, 96)
	trainy shape = (8147, 9)
	valx shape = (2721, 96)
	valy shape = (2721, 9)
	=======================================
	
[2018-07-21 01:18:52,123 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:150]: >> Executing autoencoder part ... 
[2018-07-21 01:18:52,123 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:57]: =======================================
[2018-07-21 01:18:52,123 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f8c166a26a0>, 'discard_decoder_function': True}
[2018-07-21 01:18:52,496 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:73]: training and evaluate autoencoder
[2018-07-21 01:23:02,109 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:85]: trained and evaluated!
[2018-07-21 01:23:02,109 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:88]: Training history: 
{'val_loss': [0.009543489002496365, 0.008849118344721242, 0.008287635789817909, 0.007824033714566641, 0.007449821696864172, 0.007146264192638306, 0.006899249985182944, 0.006697627493802928, 0.006532480825903835, 0.0063968028788321685, 0.006282921072379034, 0.006187686354889836, 0.006109052840332771, 0.006039702304930196, 0.005981434390423919, 0.005933288877740234, 0.005893389443164908, 0.005860269115108261, 0.005832654718708046, 0.005809646068316792, 0.005790529288324154, 0.005774486340970508, 0.005761071358482569, 0.005749819787165772, 0.005740343516734276, 0.005732349796495847, 0.005725608003291191, 0.005719944694120874, 0.0057151311734168955, 0.0057110613711193366, 0.005707592277358758, 0.005704643233473529, 0.005702132194086318, 0.005700004178820502, 0.005698175950934895, 0.00569661283671845, 0.005695270900147358, 0.0056941168043401005, 0.005693127209158428, 0.005689426666849218, 0.005685532464807013, 0.005682426218223409, 0.005679932575589351, 0.005677918852787795, 0.005676269686555997, 0.005674911920068178, 0.005673780633706171, 0.005672844532755358, 0.005672008707538899, 0.005671315121737222, 0.005670743506676964, 0.005670247429116564, 0.005669833161279105, 0.0056694670805713734, 0.005669155320767361, 0.005668889337299047, 0.005668651029444993, 0.005668442711136203, 0.005668254535630186, 0.005668086613480855, 0.005667935547294648, 0.0056677226830124635, 0.005667466077117064, 0.005667278983704513, 0.005667143180204479, 0.00566703177711631, 0.005666927627203264, 0.005666834589412921, 0.005666736485053214, 0.005666631540812029, 0.005666519449842366, 0.005666391629241722, 0.005666251037525166, 0.005666133074961994, 0.0056660237096767575, 0.005665885188450126, 0.005665779904674948, 0.005665676227439769, 0.005665574841288927, 0.005665482071754398, 0.005665397939201375, 0.005665315979392086, 0.005665237979415102, 0.005665158047140581, 0.0056650824135478116, 0.005665004865797931, 0.005664932724408113, 0.00566486345297044, 0.0056647920837466365, 0.005664725125640992, 0.005664661341596317, 0.005664598522160185, 0.005664536776260713, 0.0056644520526892206, 0.005664385887628195, 0.005664321265957951, 0.005664260158139373, 0.005664196778574835, 0.005664132550603155, 0.005664067577932801, 0.005664001730243644], 'val_acc': [0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607], 'loss': [0.009965005235402873, 0.009189407379134741, 0.008567807113387562, 0.008058083164733719, 0.00764319283441459, 0.007307713020435631, 0.007035347306866547, 0.006813525000756978, 0.006632359269219933, 0.006483930555185182, 0.006361292465655671, 0.006257938295858864, 0.006172545383063265, 0.006100402307199714, 0.006037387748963228, 0.0059852937424590405, 0.005942315654663388, 0.005906707318964945, 0.005877163713861715, 0.005852554509709097, 0.005832136141855786, 0.00581515321355554, 0.005800936876416547, 0.005789059494822445, 0.0057791003637195065, 0.005770743775214738, 0.005763718092052066, 0.005757799828364463, 0.005752836365279712, 0.0057486277725787615, 0.005745077252157542, 0.005742056522362014, 0.005739501410128387, 0.0057373415024058756, 0.005735506912048874, 0.005733936268534926, 0.005732596430349539, 0.005731451696298657, 0.0057304789393575984, 0.005728571366382015, 0.005724376490655375, 0.005720911369667809, 0.005718146337188686, 0.0057159143801135195, 0.005714103868108861, 0.005712619019672046, 0.005711383910693537, 0.005710356729225145, 0.005709488556482656, 0.005708732630458948, 0.005708108992651114, 0.0057075830522046265, 0.005707133302621235, 0.0057067521916373395, 0.005706414820977781, 0.0057061280824524776, 0.005705872933275479, 0.0057056525794050034, 0.005705460381495345, 0.005705285323887782, 0.0057051305202885, 0.005704965235795074, 0.005704717179164757, 0.00570450207743004, 0.005704340391051711, 0.0057042241411433825, 0.005704116747430279, 0.005704020144404393, 0.005703925721590137, 0.005703828498578, 0.005703715457714389, 0.0057036042064487775, 0.005703467960140541, 0.005703337588697869, 0.005703226949530788, 0.005703105884454975, 0.005702985747672395, 0.005702877045626547, 0.005702776330869224, 0.005702680082905069, 0.005702595713161864, 0.0057025165141622895, 0.005702433682924654, 0.00570235912304779, 0.005702284896398532, 0.005702206505775979, 0.005702138232297914, 0.005702063867613551, 0.005702001868817021, 0.005701931757557274, 0.005701864196431809, 0.005701805258185089, 0.005701741373080249, 0.005701672048877815, 0.005701593549770527, 0.005701531619162768, 0.005701466683847975, 0.005701407230041562, 0.0057013412669796705, 0.005701278914378873, 0.0057012146647241755], 'acc': [0.5868417822438192, 0.5938382226513312, 0.593838222687912, 0.593838222687912, 0.5938382226366989, 0.5938382226001182, 0.5938382226513312, 0.5938382226842539, 0.5938382227025443, 0.5938382227244927, 0.5938382226842539, 0.5938382226513312, 0.5938382226659635, 0.5938382226001182, 0.593838222687912, 0.5938382226659635, 0.5938382227025443, 0.5938382226513312, 0.5938382226732797, 0.5938382226842539, 0.5938382227025443, 0.5938382226732797, 0.5938382227244927, 0.5938382226842539, 0.5938382226513312, 0.5938382226001182, 0.5938382226732797, 0.5938382226659635, 0.5938382226732797, 0.5938382226476732, 0.5938382226513312, 0.593838222687912, 0.5938382226659635, 0.5938382226476732, 0.5938382226513312, 0.5938382226001182, 0.5938382226513312, 0.5938382226001182, 0.593838222687912, 0.593838222687912, 0.593838222687912, 0.5938382226001182, 0.5938382226001182, 0.5938382226659635, 0.5938382226513312, 0.5938382226001182, 0.5938382226513312, 0.5938382226513312, 0.5938382226513312, 0.5938382226842539, 0.5938382226476732, 0.5938382227244927, 0.5938382226001182, 0.5938382226366989, 0.5938382226001182, 0.5938382226001182, 0.5938382226001182, 0.5938382226293828, 0.5938382226842539, 0.5938382226513312, 0.5938382226001182, 0.5938382226001182, 0.5938382226220666, 0.5938382227244927, 0.5938382226001182, 0.5938382226513312, 0.5938382226513312, 0.5938382227025443, 0.5938382227244927, 0.5938382226366989, 0.5938382226842539, 0.5938382226001182, 0.5938382226842539, 0.5938382226366989, 0.5938382226513312, 0.5938382226659635, 0.5938382226001182, 0.5938382226842539, 0.5938382226001182, 0.593838222687912, 0.593838222687912, 0.5938382226001182, 0.5938382226001182, 0.5938382226366989, 0.593838222687912, 0.5938382227025443, 0.5938382226659635, 0.5938382226842539, 0.593838222687912, 0.5938382226842539, 0.5938382226513312, 0.5938382226513312, 0.5938382226513312, 0.5938382226001182, 0.593838222687912, 0.593838222687912, 0.5938382227025443, 0.5938382227025443, 0.5938382226001182, 0.5938382226001182, 0.5938382227025443]}
[2018-07-21 01:23:02,110 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:92]: done!
[2018-07-21 01:23:02,110 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:152]: >> Executing classifier part ... 
[2018-07-21 01:23:02,110 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:97]: =======================================
[2018-07-21 01:23:02,110 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f8c166a2e80>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2018-07-21 01:23:02,156 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:110]: training ... 
[2018-07-21 01:30:46,257 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:122]: trained!
[2018-07-21 01:30:46,258 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:125]: Training history: 
{'val_loss': [0.009543489002496365, 0.008849118344721242, 0.008287635789817909, 0.007824033714566641, 0.007449821696864172, 0.007146264192638306, 0.006899249985182944, 0.006697627493802928, 0.006532480825903835, 0.0063968028788321685, 0.006282921072379034, 0.006187686354889836, 0.006109052840332771, 0.006039702304930196, 0.005981434390423919, 0.005933288877740234, 0.005893389443164908, 0.005860269115108261, 0.005832654718708046, 0.005809646068316792, 0.005790529288324154, 0.005774486340970508, 0.005761071358482569, 0.005749819787165772, 0.005740343516734276, 0.005732349796495847, 0.005725608003291191, 0.005719944694120874, 0.0057151311734168955, 0.0057110613711193366, 0.005707592277358758, 0.005704643233473529, 0.005702132194086318, 0.005700004178820502, 0.005698175950934895, 0.00569661283671845, 0.005695270900147358, 0.0056941168043401005, 0.005693127209158428, 0.005689426666849218, 0.005685532464807013, 0.005682426218223409, 0.005679932575589351, 0.005677918852787795, 0.005676269686555997, 0.005674911920068178, 0.005673780633706171, 0.005672844532755358, 0.005672008707538899, 0.005671315121737222, 0.005670743506676964, 0.005670247429116564, 0.005669833161279105, 0.0056694670805713734, 0.005669155320767361, 0.005668889337299047, 0.005668651029444993, 0.005668442711136203, 0.005668254535630186, 0.005668086613480855, 0.005667935547294648, 0.0056677226830124635, 0.005667466077117064, 0.005667278983704513, 0.005667143180204479, 0.00566703177711631, 0.005666927627203264, 0.005666834589412921, 0.005666736485053214, 0.005666631540812029, 0.005666519449842366, 0.005666391629241722, 0.005666251037525166, 0.005666133074961994, 0.0056660237096767575, 0.005665885188450126, 0.005665779904674948, 0.005665676227439769, 0.005665574841288927, 0.005665482071754398, 0.005665397939201375, 0.005665315979392086, 0.005665237979415102, 0.005665158047140581, 0.0056650824135478116, 0.005665004865797931, 0.005664932724408113, 0.00566486345297044, 0.0056647920837466365, 0.005664725125640992, 0.005664661341596317, 0.005664598522160185, 0.005664536776260713, 0.0056644520526892206, 0.005664385887628195, 0.005664321265957951, 0.005664260158139373, 0.005664196778574835, 0.005664132550603155, 0.005664067577932801, 0.005664001730243644], 'val_acc': [0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607], 'loss': [0.009965005235402873, 0.009189407379134741, 0.008567807113387562, 0.008058083164733719, 0.00764319283441459, 0.007307713020435631, 0.007035347306866547, 0.006813525000756978, 0.006632359269219933, 0.006483930555185182, 0.006361292465655671, 0.006257938295858864, 0.006172545383063265, 0.006100402307199714, 0.006037387748963228, 0.0059852937424590405, 0.005942315654663388, 0.005906707318964945, 0.005877163713861715, 0.005852554509709097, 0.005832136141855786, 0.00581515321355554, 0.005800936876416547, 0.005789059494822445, 0.0057791003637195065, 0.005770743775214738, 0.005763718092052066, 0.005757799828364463, 0.005752836365279712, 0.0057486277725787615, 0.005745077252157542, 0.005742056522362014, 0.005739501410128387, 0.0057373415024058756, 0.005735506912048874, 0.005733936268534926, 0.005732596430349539, 0.005731451696298657, 0.0057304789393575984, 0.005728571366382015, 0.005724376490655375, 0.005720911369667809, 0.005718146337188686, 0.0057159143801135195, 0.005714103868108861, 0.005712619019672046, 0.005711383910693537, 0.005710356729225145, 0.005709488556482656, 0.005708732630458948, 0.005708108992651114, 0.0057075830522046265, 0.005707133302621235, 0.0057067521916373395, 0.005706414820977781, 0.0057061280824524776, 0.005705872933275479, 0.0057056525794050034, 0.005705460381495345, 0.005705285323887782, 0.0057051305202885, 0.005704965235795074, 0.005704717179164757, 0.00570450207743004, 0.005704340391051711, 0.0057042241411433825, 0.005704116747430279, 0.005704020144404393, 0.005703925721590137, 0.005703828498578, 0.005703715457714389, 0.0057036042064487775, 0.005703467960140541, 0.005703337588697869, 0.005703226949530788, 0.005703105884454975, 0.005702985747672395, 0.005702877045626547, 0.005702776330869224, 0.005702680082905069, 0.005702595713161864, 0.0057025165141622895, 0.005702433682924654, 0.00570235912304779, 0.005702284896398532, 0.005702206505775979, 0.005702138232297914, 0.005702063867613551, 0.005702001868817021, 0.005701931757557274, 0.005701864196431809, 0.005701805258185089, 0.005701741373080249, 0.005701672048877815, 0.005701593549770527, 0.005701531619162768, 0.005701466683847975, 0.005701407230041562, 0.0057013412669796705, 0.005701278914378873, 0.0057012146647241755], 'acc': [0.5868417822438192, 0.5938382226513312, 0.593838222687912, 0.593838222687912, 0.5938382226366989, 0.5938382226001182, 0.5938382226513312, 0.5938382226842539, 0.5938382227025443, 0.5938382227244927, 0.5938382226842539, 0.5938382226513312, 0.5938382226659635, 0.5938382226001182, 0.593838222687912, 0.5938382226659635, 0.5938382227025443, 0.5938382226513312, 0.5938382226732797, 0.5938382226842539, 0.5938382227025443, 0.5938382226732797, 0.5938382227244927, 0.5938382226842539, 0.5938382226513312, 0.5938382226001182, 0.5938382226732797, 0.5938382226659635, 0.5938382226732797, 0.5938382226476732, 0.5938382226513312, 0.593838222687912, 0.5938382226659635, 0.5938382226476732, 0.5938382226513312, 0.5938382226001182, 0.5938382226513312, 0.5938382226001182, 0.593838222687912, 0.593838222687912, 0.593838222687912, 0.5938382226001182, 0.5938382226001182, 0.5938382226659635, 0.5938382226513312, 0.5938382226001182, 0.5938382226513312, 0.5938382226513312, 0.5938382226513312, 0.5938382226842539, 0.5938382226476732, 0.5938382227244927, 0.5938382226001182, 0.5938382226366989, 0.5938382226001182, 0.5938382226001182, 0.5938382226001182, 0.5938382226293828, 0.5938382226842539, 0.5938382226513312, 0.5938382226001182, 0.5938382226001182, 0.5938382226220666, 0.5938382227244927, 0.5938382226001182, 0.5938382226513312, 0.5938382226513312, 0.5938382227025443, 0.5938382227244927, 0.5938382226366989, 0.5938382226842539, 0.5938382226001182, 0.5938382226842539, 0.5938382226366989, 0.5938382226513312, 0.5938382226659635, 0.5938382226001182, 0.5938382226842539, 0.5938382226001182, 0.593838222687912, 0.593838222687912, 0.5938382226001182, 0.5938382226001182, 0.5938382226366989, 0.593838222687912, 0.5938382227025443, 0.5938382226659635, 0.5938382226842539, 0.593838222687912, 0.5938382226842539, 0.5938382226513312, 0.5938382226513312, 0.5938382226513312, 0.5938382226001182, 0.593838222687912, 0.593838222687912, 0.5938382227025443, 0.5938382227025443, 0.5938382226001182, 0.5938382226001182, 0.5938382227025443]}
[2018-07-21 01:30:46,258 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:129]: evaluating model ... 
[2018-07-21 01:30:46,494 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:133]: evaluated! 
[2018-07-21 01:30:46,494 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:135]: generating reports ... 
[2018-07-21 01:30:47,497 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:138]: done!
[2018-07-21 01:30:47,497 AE_UNIGRAMA_10L_FULLDS_OVER_05.py:154]: >> experiment AE_UNIGRAMA_10L_FULLDS_OVER_05 finished!
