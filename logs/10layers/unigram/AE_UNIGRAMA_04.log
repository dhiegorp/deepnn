[2017-10-25 00:55:25,887 AE_UNIGRAMA_04.py:148]: >> Initializing execution of experiment AE_UNIGRAMA_04
[2017-10-25 00:55:25,887 AE_UNIGRAMA_04.py:149]: >> Printing header log
[2017-10-25 00:55:25,887 AE_UNIGRAMA_04.py:38]: 
	=======================================
	network_name = AE_UNIGRAMA_04
	layers = 96,134,122,109,97,84,72,59,47,34,22,9
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/10layers/unigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/10layers/unigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/10layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/10layers/unigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/10layers/unigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/10layers/unigram/', 'data_dir': '/home/dhiegorp/malware_deepnn/', 'fullds_data_dir': '/home/dhiegorp/malware_deepnn/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fd0ab507710>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fd0ab5077f0>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-10-25 00:55:25,887 AE_UNIGRAMA_04.py:151]: >> Loading dataset... 
[2017-10-25 00:55:28,131 AE_UNIGRAMA_04.py:55]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_deepnn/	
	trainx shape = (8147, 96)
	trainy shape = (8147, 9)
	valx shape = (2721, 96)
	valy shape = (2721, 9)
	=======================================
	
[2017-10-25 00:55:28,131 AE_UNIGRAMA_04.py:153]: >> Executing autoencoder part ... 
[2017-10-25 00:55:28,131 AE_UNIGRAMA_04.py:60]: =======================================
[2017-10-25 00:55:28,132 AE_UNIGRAMA_04.py:65]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fd0ab507710>, 'discard_decoder_function': True}
[2017-10-25 00:55:28,352 AE_UNIGRAMA_04.py:76]: training and evaluate autoencoder
[2017-10-25 00:58:50,378 AE_UNIGRAMA_04.py:88]: trained and evaluated!
[2017-10-25 00:58:50,379 AE_UNIGRAMA_04.py:91]: Training history: 
{'val_loss': [0.0093085481879362636, 0.0085048587439675531, 0.0079255083090502831, 0.00749847917752159, 0.0071769089495180632, 0.0069298705581379849, 0.0067371964031602474, 0.0065845963274748587, 0.0064585465721879124, 0.0063552967563972494, 0.0062709708266146412, 0.0062018462794927696, 0.0061447427064544547, 0.0060972675076319269, 0.0060575777955369685, 0.0060242401254224738, 0.0059961500341974222, 0.0059676372618282687, 0.0059416447049920685, 0.0059201164688444047, 0.0059021733857305206, 0.0058871686689792495, 0.0058745640871320053, 0.005863923376829297, 0.0058549574090462917, 0.0058473331118033336, 0.0058408716510576072, 0.0058353631348184521, 0.0058306794397454901, 0.0058266554792431816, 0.0058232213205184537, 0.005820278236019545, 0.0058177283018497486, 0.0058155389858094188, 0.0058136591588539574, 0.0058120259008336457, 0.0058106158118837407, 0.0058093943807869079, 0.0058083433188287183, 0.0058074229072063383, 0.0058066369863846523, 0.0058059381405442673, 0.0058053426427588017, 0.0058048029465464623, 0.0058043368115544714, 0.0058028357173975052, 0.0057937042808720828, 0.0057868456240546253, 0.0057815565842825704, 0.0057774477473842454, 0.0057741843056322538, 0.0057715776365986921, 0.005769461318398718, 0.0057677365788996208, 0.00576631419416428, 0.0057651398423477287, 0.0057641545308733996, 0.0057633367173876154, 0.0057626479674638608, 0.0057620590165748959, 0.0057615628728703988, 0.0057611466211378737, 0.005760790603231941, 0.0057604883196607963, 0.0057602313148471824, 0.0057599978498876292, 0.0057598070850926469, 0.0057596375420278859, 0.0057594936791306329, 0.0057593631837585474, 0.0057592473280517189, 0.005759151338506182, 0.0057590645541995217, 0.0057589851771760285, 0.0057589162800985418, 0.0057588616715150637, 0.0057587998237896218, 0.0057587573580814878, 0.0057587154146806858, 0.0057586722229277113, 0.0057586312621047409, 0.005758594936930473, 0.0057585673183045295, 0.0057585388928575142, 0.0057585152148110644, 0.0057584930313816134, 0.0057584706164050389, 0.0057584535745881264, 0.0057584307177381807, 0.0057584111965872358, 0.0057583912067801224, 0.0057583719868286873, 0.0057583586780887638, 0.0057583435957800242, 0.0057583258215006669, 0.0057583108474355011, 0.0057582961333260485, 0.0057582854350959663, 0.0057582752211810043, 0.0057582643139081945, 0.0057582483268029732, 0.0057582350291013277], 'loss': [0.0098450889165498465, 0.0088950314563500735, 0.008215092755401443, 0.007720859194311815, 0.0073531579413845987, 0.0070740663291246515, 0.0068583237234746746, 0.0066888676735668423, 0.0065527158551904868, 0.0064398347101424303, 0.0063479653709606152, 0.0062729016675030702, 0.006211206249273583, 0.0061601399446558564, 0.0061176483234960137, 0.0060820771186123621, 0.0060522035449035432, 0.0060253158961384445, 0.0059976618732066811, 0.0059743837504980897, 0.0059550486412322289, 0.0059389513708642833, 0.0059254808074527101, 0.0059141796735567237, 0.0059046546078768448, 0.0058966211753878964, 0.0058898225704916082, 0.0058840518734770289, 0.0058791555968510257, 0.0058749970039520816, 0.0058714294431970477, 0.0058684053805538872, 0.0058658046869562675, 0.005863566391869174, 0.0058616503887495789, 0.0058600133885879256, 0.0058585962743491638, 0.0058573845633785385, 0.00585633345828287, 0.005855431134797787, 0.0058546452476684521, 0.0058539803990938994, 0.0058533962121766281, 0.0058528949330307429, 0.0058524580682100067, 0.0058520121621755614, 0.0058459750567776076, 0.0058379867122128719, 0.0058319361034377898, 0.0058272590656558241, 0.0058236058263104245, 0.0058207074857560513, 0.0058183785879119595, 0.0058164896438456367, 0.0058149489322870448, 0.0058136825952521164, 0.0058126400514766788, 0.0058117676203368789, 0.0058110321703207695, 0.0058104256250314495, 0.0058099101906855175, 0.0058094740478198221, 0.0058091041449435917, 0.0058087926644114453, 0.0058085211633042838, 0.0058083088917750781, 0.0058081051538447686, 0.0058079341685871987, 0.0058077839335212459, 0.0058076615713155387, 0.0058075569061249426, 0.0058074597979991677, 0.0058073731329865005, 0.0058073015642703691, 0.0058072356020086808, 0.0058071799676307941, 0.0058071326133587601, 0.005807083370377805, 0.005807043862101219, 0.0058070066317182471, 0.0058069740375429595, 0.0058069412301705921, 0.005806914782015545, 0.0058068941373936355, 0.005806862432587395, 0.0058068499191191699, 0.0058068297126658407, 0.0058068067069291501, 0.0058067904603972204, 0.0058067750793425483, 0.0058067548934087236, 0.0058067441669092581, 0.0058067331725130886, 0.0058067145704678034, 0.0058067071864184123, 0.0058066927335998159, 0.0058066740821705425, 0.0058066711072996668, 0.0058066538231324105, 0.0058066478857938149, 0.0058066323587020012, 0.005806621555554471]}
[2017-10-25 00:58:50,379 AE_UNIGRAMA_04.py:95]: done!
[2017-10-25 00:58:50,379 AE_UNIGRAMA_04.py:155]: >> Executing classifier part ... 
[2017-10-25 00:58:50,379 AE_UNIGRAMA_04.py:100]: =======================================
[2017-10-25 00:58:50,379 AE_UNIGRAMA_04.py:104]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fd0ab5077f0>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-10-25 00:58:50,426 AE_UNIGRAMA_04.py:113]: training ... 
[2017-10-25 01:08:20,405 AE_UNIGRAMA_04.py:125]: trained!
[2017-10-25 01:08:20,405 AE_UNIGRAMA_04.py:128]: Training history: 
{'val_loss': [0.0093085481879362636, 0.0085048587439675531, 0.0079255083090502831, 0.00749847917752159, 0.0071769089495180632, 0.0069298705581379849, 0.0067371964031602474, 0.0065845963274748587, 0.0064585465721879124, 0.0063552967563972494, 0.0062709708266146412, 0.0062018462794927696, 0.0061447427064544547, 0.0060972675076319269, 0.0060575777955369685, 0.0060242401254224738, 0.0059961500341974222, 0.0059676372618282687, 0.0059416447049920685, 0.0059201164688444047, 0.0059021733857305206, 0.0058871686689792495, 0.0058745640871320053, 0.005863923376829297, 0.0058549574090462917, 0.0058473331118033336, 0.0058408716510576072, 0.0058353631348184521, 0.0058306794397454901, 0.0058266554792431816, 0.0058232213205184537, 0.005820278236019545, 0.0058177283018497486, 0.0058155389858094188, 0.0058136591588539574, 0.0058120259008336457, 0.0058106158118837407, 0.0058093943807869079, 0.0058083433188287183, 0.0058074229072063383, 0.0058066369863846523, 0.0058059381405442673, 0.0058053426427588017, 0.0058048029465464623, 0.0058043368115544714, 0.0058028357173975052, 0.0057937042808720828, 0.0057868456240546253, 0.0057815565842825704, 0.0057774477473842454, 0.0057741843056322538, 0.0057715776365986921, 0.005769461318398718, 0.0057677365788996208, 0.00576631419416428, 0.0057651398423477287, 0.0057641545308733996, 0.0057633367173876154, 0.0057626479674638608, 0.0057620590165748959, 0.0057615628728703988, 0.0057611466211378737, 0.005760790603231941, 0.0057604883196607963, 0.0057602313148471824, 0.0057599978498876292, 0.0057598070850926469, 0.0057596375420278859, 0.0057594936791306329, 0.0057593631837585474, 0.0057592473280517189, 0.005759151338506182, 0.0057590645541995217, 0.0057589851771760285, 0.0057589162800985418, 0.0057588616715150637, 0.0057587998237896218, 0.0057587573580814878, 0.0057587154146806858, 0.0057586722229277113, 0.0057586312621047409, 0.005758594936930473, 0.0057585673183045295, 0.0057585388928575142, 0.0057585152148110644, 0.0057584930313816134, 0.0057584706164050389, 0.0057584535745881264, 0.0057584307177381807, 0.0057584111965872358, 0.0057583912067801224, 0.0057583719868286873, 0.0057583586780887638, 0.0057583435957800242, 0.0057583258215006669, 0.0057583108474355011, 0.0057582961333260485, 0.0057582854350959663, 0.0057582752211810043, 0.0057582643139081945, 0.0057582483268029732, 0.0057582350291013277], 'loss': [0.0098450889165498465, 0.0088950314563500735, 0.008215092755401443, 0.007720859194311815, 0.0073531579413845987, 0.0070740663291246515, 0.0068583237234746746, 0.0066888676735668423, 0.0065527158551904868, 0.0064398347101424303, 0.0063479653709606152, 0.0062729016675030702, 0.006211206249273583, 0.0061601399446558564, 0.0061176483234960137, 0.0060820771186123621, 0.0060522035449035432, 0.0060253158961384445, 0.0059976618732066811, 0.0059743837504980897, 0.0059550486412322289, 0.0059389513708642833, 0.0059254808074527101, 0.0059141796735567237, 0.0059046546078768448, 0.0058966211753878964, 0.0058898225704916082, 0.0058840518734770289, 0.0058791555968510257, 0.0058749970039520816, 0.0058714294431970477, 0.0058684053805538872, 0.0058658046869562675, 0.005863566391869174, 0.0058616503887495789, 0.0058600133885879256, 0.0058585962743491638, 0.0058573845633785385, 0.00585633345828287, 0.005855431134797787, 0.0058546452476684521, 0.0058539803990938994, 0.0058533962121766281, 0.0058528949330307429, 0.0058524580682100067, 0.0058520121621755614, 0.0058459750567776076, 0.0058379867122128719, 0.0058319361034377898, 0.0058272590656558241, 0.0058236058263104245, 0.0058207074857560513, 0.0058183785879119595, 0.0058164896438456367, 0.0058149489322870448, 0.0058136825952521164, 0.0058126400514766788, 0.0058117676203368789, 0.0058110321703207695, 0.0058104256250314495, 0.0058099101906855175, 0.0058094740478198221, 0.0058091041449435917, 0.0058087926644114453, 0.0058085211633042838, 0.0058083088917750781, 0.0058081051538447686, 0.0058079341685871987, 0.0058077839335212459, 0.0058076615713155387, 0.0058075569061249426, 0.0058074597979991677, 0.0058073731329865005, 0.0058073015642703691, 0.0058072356020086808, 0.0058071799676307941, 0.0058071326133587601, 0.005807083370377805, 0.005807043862101219, 0.0058070066317182471, 0.0058069740375429595, 0.0058069412301705921, 0.005806914782015545, 0.0058068941373936355, 0.005806862432587395, 0.0058068499191191699, 0.0058068297126658407, 0.0058068067069291501, 0.0058067904603972204, 0.0058067750793425483, 0.0058067548934087236, 0.0058067441669092581, 0.0058067331725130886, 0.0058067145704678034, 0.0058067071864184123, 0.0058066927335998159, 0.0058066740821705425, 0.0058066711072996668, 0.0058066538231324105, 0.0058066478857938149, 0.0058066323587020012, 0.005806621555554471]}
[2017-10-25 01:08:20,405 AE_UNIGRAMA_04.py:132]: evaluating model ... 
[2017-10-25 01:08:20,584 AE_UNIGRAMA_04.py:136]: evaluated! 
[2017-10-25 01:08:20,584 AE_UNIGRAMA_04.py:138]: generating reports ... 
[2017-10-25 01:08:21,423 AE_UNIGRAMA_04.py:141]: done!
[2017-10-25 01:08:21,423 AE_UNIGRAMA_04.py:157]: >> experiment AE_UNIGRAMA_04 finished!
