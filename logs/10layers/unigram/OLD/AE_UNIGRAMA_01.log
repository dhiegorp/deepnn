[2017-10-25 00:37:57,656 AE_UNIGRAMA_01.py:148]: >> Initializing execution of experiment AE_UNIGRAMA_01
[2017-10-25 00:37:57,656 AE_UNIGRAMA_01.py:149]: >> Printing header log
[2017-10-25 00:37:57,656 AE_UNIGRAMA_01.py:38]: 
	=======================================
	network_name = AE_UNIGRAMA_01
	layers = 96,28,26,24,22,20,19,17,15,13,11,9
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/10layers/unigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/10layers/unigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/10layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/10layers/unigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/10layers/unigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/10layers/unigram/', 'data_dir': '/home/dhiegorp/malware_deepnn/', 'fullds_data_dir': '/home/dhiegorp/malware_deepnn/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f10ac8d0710>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f10ac8d07f0>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-10-25 00:37:57,656 AE_UNIGRAMA_01.py:151]: >> Loading dataset... 
[2017-10-25 00:37:59,888 AE_UNIGRAMA_01.py:55]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_deepnn/	
	trainx shape = (8147, 96)
	trainy shape = (8147, 9)
	valx shape = (2721, 96)
	valy shape = (2721, 9)
	=======================================
	
[2017-10-25 00:37:59,888 AE_UNIGRAMA_01.py:153]: >> Executing autoencoder part ... 
[2017-10-25 00:37:59,889 AE_UNIGRAMA_01.py:60]: =======================================
[2017-10-25 00:37:59,889 AE_UNIGRAMA_01.py:65]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f10ac8d0710>, 'discard_decoder_function': True}
[2017-10-25 00:38:00,154 AE_UNIGRAMA_01.py:76]: training and evaluate autoencoder
[2017-10-25 00:39:57,934 AE_UNIGRAMA_01.py:88]: trained and evaluated!
[2017-10-25 00:39:57,934 AE_UNIGRAMA_01.py:91]: Training history: 
{'val_loss': [0.0094980492935307498, 0.0088086284417077179, 0.0082858094961979021, 0.007877398410812704, 0.007544919788202176, 0.0072720297023541715, 0.0070438260586986384, 0.0068503144939554301, 0.0066844574376004268, 0.0065410217167231168, 0.006416522254308262, 0.0063076883674938423, 0.0062123569064335173, 0.0061286416447946298, 0.0060523428128623578, 0.0059851885850937302, 0.0059262008960018235, 0.0058742263194348819, 0.0058283252146633093, 0.0057877390083262356, 0.005748230757212998, 0.0057116619080923928, 0.0056791685663474447, 0.005650603325451556, 0.0056254221769053182, 0.0056031946289992777, 0.0055835324505399478, 0.0055661489722414444, 0.0055507633800678932, 0.005537158790007179, 0.0055250987456409821, 0.0055143826899386721, 0.0055049189309240793, 0.0054964998151521031, 0.0054890119054966371, 0.0054823837847302048, 0.0054764959071097142, 0.0054712655938831825, 0.0054666198134108784, 0.0054624955997641788, 0.0054588319080170191, 0.0054555658027885682, 0.0054526602109542981, 0.0054500774907900408, 0.0054477768010113801, 0.0054457362763308476, 0.0054439289348343035, 0.0054423133793873295, 0.0054408750657330366, 0.0054395955179341185, 0.0054384612339524463, 0.0054374468489387477, 0.0054365393337038942, 0.0054357408698906973, 0.0054350190602403129, 0.0054343769988931577, 0.0054338116390279427, 0.0054333040860229309, 0.0054328530288901408, 0.0054324516674516691, 0.0054320927937123176, 0.005431781393294084, 0.0054314965958091287, 0.0054312416735505329, 0.0054310128714503206, 0.0054308106378850333, 0.0054306295296147836, 0.0054304641829779629, 0.0054303159398526139, 0.0054301871461721902, 0.0054300767966833285, 0.0054299705858639781, 0.0054298772021220401, 0.0054297927011985941, 0.0054297228099915891, 0.0054296559426736382, 0.0054295953798379221, 0.0054295414685484236, 0.0054294976482981766, 0.0054294532763051913, 0.0054294179267777446, 0.0054293854086112552, 0.0054293513637397525, 0.0054293239239510179, 0.0054293053219719861, 0.0054292820143495924, 0.0054292579294271011, 0.0054292413127977907, 0.0054292272238484563, 0.0054292138679605424, 0.005429201008196138, 0.0054291927358200614, 0.0054291814303991494, 0.0054291687184107552, 0.0054291598291746597, 0.0054291530630528346, 0.0054291439616935616, 0.0054291379879318107, 0.0054291312539835697, 0.0054291278132925806, 0.0054291219972327316, 0.0054291140496729434], 'loss': [0.009944155053670559, 0.0091437206031898428, 0.0085466528762686584, 0.0080864667844215213, 0.0077203829474751752, 0.0074211502181599515, 0.0071731556741477555, 0.0069643088603398877, 0.0067862242501289865, 0.0066328946580122448, 0.0064999708550068456, 0.0063842698798288675, 0.0062830011015459555, 0.0061941534757241016, 0.0061146387049263685, 0.0060434085498716553, 0.0059807332140799719, 0.0059256134830242401, 0.0058770107633622972, 0.005834074414010423, 0.0057943720507480194, 0.0057564855458214197, 0.005721963319092499, 0.0056915702945876849, 0.005664851176341564, 0.0056412911783445202, 0.0056204817683529829, 0.0056020905158780969, 0.0055858142769650747, 0.0055714295360250448, 0.005558698442280457, 0.0055474146392493181, 0.0055373994740096797, 0.0055285502650754556, 0.0055206833619108131, 0.005513700696088605, 0.0055075131882106922, 0.0055020181875416654, 0.0054971431505460421, 0.0054928185491981219, 0.0054889729290965609, 0.0054855640209768653, 0.0054825312434887141, 0.0054798294945900185, 0.0054774368719104185, 0.0054753048208154523, 0.0054734112135631559, 0.0054717381605751585, 0.0054702383290134472, 0.0054689158643894805, 0.0054677369710223961, 0.0054666917952632885, 0.0054657570397584438, 0.0054649256607164371, 0.0054641893662569921, 0.0054635333357536823, 0.0054629477011539406, 0.0054624295318838724, 0.0054619672180193325, 0.0054615598109190571, 0.0054611978347445132, 0.0054608673863026048, 0.0054605849330385866, 0.0054603246159758267, 0.0054600987517324871, 0.0054598946343342395, 0.0054597135625685436, 0.0054595518496120266, 0.0054594120655469007, 0.0054592808782967473, 0.0054591707368562507, 0.0054590682960599492, 0.0054589796597459272, 0.0054589005760328161, 0.0054588280654771495, 0.0054587639671180733, 0.0054587094118717814, 0.0054586624195775224, 0.0054586158804270819, 0.0054585775047814103, 0.0054585390630046346, 0.0054585115639537449, 0.0054584823808745617, 0.0054584567208056604, 0.0054584356895711391, 0.0054584179594618963, 0.0054583995227157935, 0.0054583834707476334, 0.0054583705688377596, 0.005458358107497078, 0.0054583478240245713, 0.0054583357683299135, 0.0054583289429940476, 0.0054583207164446718, 0.0054583105396278633, 0.0054583050963577795, 0.0054583008724263769, 0.005458294582369504, 0.0054582929585279463, 0.0054582860789497136, 0.0054582824306504242, 0.0054582788740315949]}
[2017-10-25 00:39:57,934 AE_UNIGRAMA_01.py:95]: done!
[2017-10-25 00:39:57,934 AE_UNIGRAMA_01.py:155]: >> Executing classifier part ... 
[2017-10-25 00:39:57,935 AE_UNIGRAMA_01.py:100]: =======================================
[2017-10-25 00:39:57,935 AE_UNIGRAMA_01.py:104]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f10ac8d07f0>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-10-25 00:39:57,966 AE_UNIGRAMA_01.py:113]: training ... 
[2017-10-25 00:45:33,062 AE_UNIGRAMA_01.py:125]: trained!
[2017-10-25 00:45:33,063 AE_UNIGRAMA_01.py:128]: Training history: 
{'val_loss': [0.0094980492935307498, 0.0088086284417077179, 0.0082858094961979021, 0.007877398410812704, 0.007544919788202176, 0.0072720297023541715, 0.0070438260586986384, 0.0068503144939554301, 0.0066844574376004268, 0.0065410217167231168, 0.006416522254308262, 0.0063076883674938423, 0.0062123569064335173, 0.0061286416447946298, 0.0060523428128623578, 0.0059851885850937302, 0.0059262008960018235, 0.0058742263194348819, 0.0058283252146633093, 0.0057877390083262356, 0.005748230757212998, 0.0057116619080923928, 0.0056791685663474447, 0.005650603325451556, 0.0056254221769053182, 0.0056031946289992777, 0.0055835324505399478, 0.0055661489722414444, 0.0055507633800678932, 0.005537158790007179, 0.0055250987456409821, 0.0055143826899386721, 0.0055049189309240793, 0.0054964998151521031, 0.0054890119054966371, 0.0054823837847302048, 0.0054764959071097142, 0.0054712655938831825, 0.0054666198134108784, 0.0054624955997641788, 0.0054588319080170191, 0.0054555658027885682, 0.0054526602109542981, 0.0054500774907900408, 0.0054477768010113801, 0.0054457362763308476, 0.0054439289348343035, 0.0054423133793873295, 0.0054408750657330366, 0.0054395955179341185, 0.0054384612339524463, 0.0054374468489387477, 0.0054365393337038942, 0.0054357408698906973, 0.0054350190602403129, 0.0054343769988931577, 0.0054338116390279427, 0.0054333040860229309, 0.0054328530288901408, 0.0054324516674516691, 0.0054320927937123176, 0.005431781393294084, 0.0054314965958091287, 0.0054312416735505329, 0.0054310128714503206, 0.0054308106378850333, 0.0054306295296147836, 0.0054304641829779629, 0.0054303159398526139, 0.0054301871461721902, 0.0054300767966833285, 0.0054299705858639781, 0.0054298772021220401, 0.0054297927011985941, 0.0054297228099915891, 0.0054296559426736382, 0.0054295953798379221, 0.0054295414685484236, 0.0054294976482981766, 0.0054294532763051913, 0.0054294179267777446, 0.0054293854086112552, 0.0054293513637397525, 0.0054293239239510179, 0.0054293053219719861, 0.0054292820143495924, 0.0054292579294271011, 0.0054292413127977907, 0.0054292272238484563, 0.0054292138679605424, 0.005429201008196138, 0.0054291927358200614, 0.0054291814303991494, 0.0054291687184107552, 0.0054291598291746597, 0.0054291530630528346, 0.0054291439616935616, 0.0054291379879318107, 0.0054291312539835697, 0.0054291278132925806, 0.0054291219972327316, 0.0054291140496729434], 'loss': [0.009944155053670559, 0.0091437206031898428, 0.0085466528762686584, 0.0080864667844215213, 0.0077203829474751752, 0.0074211502181599515, 0.0071731556741477555, 0.0069643088603398877, 0.0067862242501289865, 0.0066328946580122448, 0.0064999708550068456, 0.0063842698798288675, 0.0062830011015459555, 0.0061941534757241016, 0.0061146387049263685, 0.0060434085498716553, 0.0059807332140799719, 0.0059256134830242401, 0.0058770107633622972, 0.005834074414010423, 0.0057943720507480194, 0.0057564855458214197, 0.005721963319092499, 0.0056915702945876849, 0.005664851176341564, 0.0056412911783445202, 0.0056204817683529829, 0.0056020905158780969, 0.0055858142769650747, 0.0055714295360250448, 0.005558698442280457, 0.0055474146392493181, 0.0055373994740096797, 0.0055285502650754556, 0.0055206833619108131, 0.005513700696088605, 0.0055075131882106922, 0.0055020181875416654, 0.0054971431505460421, 0.0054928185491981219, 0.0054889729290965609, 0.0054855640209768653, 0.0054825312434887141, 0.0054798294945900185, 0.0054774368719104185, 0.0054753048208154523, 0.0054734112135631559, 0.0054717381605751585, 0.0054702383290134472, 0.0054689158643894805, 0.0054677369710223961, 0.0054666917952632885, 0.0054657570397584438, 0.0054649256607164371, 0.0054641893662569921, 0.0054635333357536823, 0.0054629477011539406, 0.0054624295318838724, 0.0054619672180193325, 0.0054615598109190571, 0.0054611978347445132, 0.0054608673863026048, 0.0054605849330385866, 0.0054603246159758267, 0.0054600987517324871, 0.0054598946343342395, 0.0054597135625685436, 0.0054595518496120266, 0.0054594120655469007, 0.0054592808782967473, 0.0054591707368562507, 0.0054590682960599492, 0.0054589796597459272, 0.0054589005760328161, 0.0054588280654771495, 0.0054587639671180733, 0.0054587094118717814, 0.0054586624195775224, 0.0054586158804270819, 0.0054585775047814103, 0.0054585390630046346, 0.0054585115639537449, 0.0054584823808745617, 0.0054584567208056604, 0.0054584356895711391, 0.0054584179594618963, 0.0054583995227157935, 0.0054583834707476334, 0.0054583705688377596, 0.005458358107497078, 0.0054583478240245713, 0.0054583357683299135, 0.0054583289429940476, 0.0054583207164446718, 0.0054583105396278633, 0.0054583050963577795, 0.0054583008724263769, 0.005458294582369504, 0.0054582929585279463, 0.0054582860789497136, 0.0054582824306504242, 0.0054582788740315949]}
[2017-10-25 00:45:33,063 AE_UNIGRAMA_01.py:132]: evaluating model ... 
[2017-10-25 00:45:33,163 AE_UNIGRAMA_01.py:136]: evaluated! 
[2017-10-25 00:45:33,163 AE_UNIGRAMA_01.py:138]: generating reports ... 
[2017-10-25 00:45:33,964 AE_UNIGRAMA_01.py:141]: done!
[2017-10-25 00:45:33,964 AE_UNIGRAMA_01.py:157]: >> experiment AE_UNIGRAMA_01 finished!
