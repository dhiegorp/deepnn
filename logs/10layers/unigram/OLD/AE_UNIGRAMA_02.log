[2017-10-25 00:25:19,143 AE_UNIGRAMA_02.py:149]: >> Initializing execution of experiment AE_UNIGRAMA_02
[2017-10-25 00:25:19,143 AE_UNIGRAMA_02.py:150]: >> Printing header log
[2017-10-25 00:25:19,143 AE_UNIGRAMA_02.py:39]: 
	=======================================
	network_name = AE_UNIGRAMA_02
	layers = 96,76,69,63,56,49,43,36,29,22,16,9
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/10layers/unigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/10layers/unigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/10layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/10layers/unigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/10layers/unigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/10layers/unigram/', 'data_dir': '/home/dhiegorp/malware_deepnn/', 'fullds_data_dir': '/home/dhiegorp/malware_deepnn/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fec178ad748>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fec178ad828>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-10-25 00:25:19,144 AE_UNIGRAMA_02.py:152]: >> Loading dataset... 
[2017-10-25 00:25:21,474 AE_UNIGRAMA_02.py:56]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_deepnn/	
	trainx shape = (8147, 96)
	trainy shape = (8147, 9)
	valx shape = (2721, 96)
	valy shape = (2721, 9)
	=======================================
	
[2017-10-25 00:25:21,475 AE_UNIGRAMA_02.py:154]: >> Executing autoencoder part ... 
[2017-10-25 00:25:21,475 AE_UNIGRAMA_02.py:61]: =======================================
[2017-10-25 00:25:21,475 AE_UNIGRAMA_02.py:66]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fec178ad748>, 'discard_decoder_function': True}
[2017-10-25 00:25:21,680 AE_UNIGRAMA_02.py:77]: training and evaluate autoencoder
[2017-10-25 00:27:52,215 AE_UNIGRAMA_02.py:89]: trained and evaluated!
[2017-10-25 00:27:52,216 AE_UNIGRAMA_02.py:92]: Training history: 
{'val_loss': [0.0094069723571756445, 0.0086037688274230861, 0.0079919854463110594, 0.0075166534773802784, 0.0071430267895839202, 0.0068449413584246105, 0.0066041377450393231, 0.0064075575771480415, 0.0062454039210748666, 0.0061109146147416008, 0.005998433848760149, 0.005903970833678854, 0.0058245800451972125, 0.0057577310648200177, 0.0057013353499050295, 0.0056534420608350157, 0.0056127531827728306, 0.0055779669296409894, 0.0055481556145654349, 0.00552260529245457, 0.0055007149605884823, 0.0054818136989420027, 0.0054654949686111102, 0.0054514301972972343, 0.0054392356963575608, 0.0054286722027419767, 0.0054195174537167825, 0.005411590344348881, 0.0054047392651805628, 0.0053987308099519536, 0.0053934959499819159, 0.0053889588761776245, 0.0053850066396642238, 0.0053815377905426795, 0.0053784061482197408, 0.0053757445152386209, 0.0053734537255113414, 0.0053714244753616568, 0.0053696497744794708, 0.0053681101114685945, 0.0053667368049685766, 0.0053655007527747527, 0.0053644279164023556, 0.0053634866134279662, 0.0053626392708385545, 0.0053618893380530481, 0.0053612283590663522, 0.0053606089843743071, 0.0053600705182025787, 0.0053595683076382997, 0.0053591179107485261, 0.005358690723591306, 0.0053583431902701961, 0.0053580270108767878, 0.0053577662138586668, 0.0053575266746804804, 0.0053573150097199797, 0.0053571190562500363, 0.0053569469292015278, 0.0053567958997240107, 0.0053566462125523764, 0.0053565094185166829, 0.0053563930777246211, 0.0053562731737937016, 0.0053561785231313252, 0.0053560796294919938, 0.0053559930204286847, 0.0053559214815234535, 0.0053558368633717891, 0.0053557815030730576, 0.0053556910911948035, 0.0053556295699114442, 0.0053555754559968206, 0.0053555084312336713, 0.0053554558477034876, 0.0053553960510440242, 0.0053553468011592088, 0.0053552875601786138, 0.0053552462163530849, 0.0053552038693278261, 0.0053551491878403761, 0.0053551115768351336, 0.0053550714760565582, 0.0053550333374387657, 0.0053549851047012716, 0.0053549356699894839, 0.005354896580967443, 0.0053548527555831137, 0.0053548207503970394, 0.005354780968787263, 0.0053547460115892908, 0.0053547008893357088, 0.0053546576938177408, 0.0053546158022711714, 0.0053545828929731608, 0.0053545466985468618, 0.0053544780392629646, 0.0053543954548925347, 0.0053542784097043473, 0.0053541610551021178, 0.0053540099916540877, 0.0053539065932851591], 'loss': [0.0099045160401768449, 0.0089923440576709888, 0.0082926656306658748, 0.007754777391305475, 0.0073347648392392915, 0.0070024526743642408, 0.0067358050461434633, 0.0065192963748624336, 0.0063418842834625467, 0.0061950486382451016, 0.0060729240957815376, 0.0059705877379273419, 0.0058846251663093561, 0.0058123305016641374, 0.0057514510386682547, 0.0056999485992440249, 0.0056562250280233562, 0.0056189986233208186, 0.0055871573403135635, 0.0055598348740484175, 0.0055364368606198268, 0.0055163650333582333, 0.0054990275501994555, 0.0054840764408167287, 0.0054711619699201229, 0.0054599608117363576, 0.0054502659830526186, 0.0054418704122907585, 0.0054346045782798502, 0.0054282939699513318, 0.0054227960064716389, 0.005418004580511975, 0.0054138523127645437, 0.005410225588962759, 0.0054069785450380449, 0.0054041698726029668, 0.005401739828920487, 0.0053996307394367036, 0.0053977775334309577, 0.0053961730009552799, 0.0053947526364041621, 0.0053934803609446474, 0.0053923632416312201, 0.0053913829088777145, 0.005390513580184093, 0.005389744630276634, 0.0053890580421731352, 0.0053884449975737233, 0.0053878874559473502, 0.005387383360256571, 0.0053869206363448881, 0.0053864958399649091, 0.0053861274895406512, 0.0053858090926780684, 0.0053855343147946773, 0.0053852916167843436, 0.0053850741048366456, 0.0053848781051600951, 0.0053847068527488676, 0.0053845442377343615, 0.0053844095958341691, 0.0053842787462698985, 0.0053841615880734243, 0.0053840516964678963, 0.0053839512060533433, 0.0053838585983292855, 0.0053837746020524912, 0.0053836939423385615, 0.0053836165446542522, 0.0053835397759870625, 0.0053834819488229194, 0.0053834149401808234, 0.0053833572201867934, 0.0053832999048099546, 0.0053832460461408207, 0.0053831895554880176, 0.0053831439251401381, 0.0053830945890497887, 0.0053830442402447365, 0.0053830043997122193, 0.0053829600277415218, 0.0053829110090461802, 0.0053828701002419732, 0.0053828279432346004, 0.0053827886183189328, 0.0053827468891918103, 0.005382710851569333, 0.0053826682433043361, 0.0053826352018152843, 0.0053825928315536752, 0.0053825540306569938, 0.0053825152194148249, 0.0053824814614006839, 0.0053824404451977342, 0.0053823984203382556, 0.0053823632378475466, 0.0053823156891260595, 0.0053822394870029567, 0.005382134336660402, 0.0053820216664624892, 0.0053818927017204362, 0.00538175767195017]}
[2017-10-25 00:27:52,216 AE_UNIGRAMA_02.py:96]: done!
[2017-10-25 00:27:52,217 AE_UNIGRAMA_02.py:156]: >> Executing classifier part ... 
[2017-10-25 00:27:52,217 AE_UNIGRAMA_02.py:101]: =======================================
[2017-10-25 00:27:52,217 AE_UNIGRAMA_02.py:105]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fec178ad828>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-10-25 00:27:52,255 AE_UNIGRAMA_02.py:114]: training ... 
[2017-10-25 00:37:42,542 AE_UNIGRAMA_02.py:126]: trained!
[2017-10-25 00:37:42,542 AE_UNIGRAMA_02.py:129]: Training history: 
{'val_loss': [0.0094069723571756445, 0.0086037688274230861, 0.0079919854463110594, 0.0075166534773802784, 0.0071430267895839202, 0.0068449413584246105, 0.0066041377450393231, 0.0064075575771480415, 0.0062454039210748666, 0.0061109146147416008, 0.005998433848760149, 0.005903970833678854, 0.0058245800451972125, 0.0057577310648200177, 0.0057013353499050295, 0.0056534420608350157, 0.0056127531827728306, 0.0055779669296409894, 0.0055481556145654349, 0.00552260529245457, 0.0055007149605884823, 0.0054818136989420027, 0.0054654949686111102, 0.0054514301972972343, 0.0054392356963575608, 0.0054286722027419767, 0.0054195174537167825, 0.005411590344348881, 0.0054047392651805628, 0.0053987308099519536, 0.0053934959499819159, 0.0053889588761776245, 0.0053850066396642238, 0.0053815377905426795, 0.0053784061482197408, 0.0053757445152386209, 0.0053734537255113414, 0.0053714244753616568, 0.0053696497744794708, 0.0053681101114685945, 0.0053667368049685766, 0.0053655007527747527, 0.0053644279164023556, 0.0053634866134279662, 0.0053626392708385545, 0.0053618893380530481, 0.0053612283590663522, 0.0053606089843743071, 0.0053600705182025787, 0.0053595683076382997, 0.0053591179107485261, 0.005358690723591306, 0.0053583431902701961, 0.0053580270108767878, 0.0053577662138586668, 0.0053575266746804804, 0.0053573150097199797, 0.0053571190562500363, 0.0053569469292015278, 0.0053567958997240107, 0.0053566462125523764, 0.0053565094185166829, 0.0053563930777246211, 0.0053562731737937016, 0.0053561785231313252, 0.0053560796294919938, 0.0053559930204286847, 0.0053559214815234535, 0.0053558368633717891, 0.0053557815030730576, 0.0053556910911948035, 0.0053556295699114442, 0.0053555754559968206, 0.0053555084312336713, 0.0053554558477034876, 0.0053553960510440242, 0.0053553468011592088, 0.0053552875601786138, 0.0053552462163530849, 0.0053552038693278261, 0.0053551491878403761, 0.0053551115768351336, 0.0053550714760565582, 0.0053550333374387657, 0.0053549851047012716, 0.0053549356699894839, 0.005354896580967443, 0.0053548527555831137, 0.0053548207503970394, 0.005354780968787263, 0.0053547460115892908, 0.0053547008893357088, 0.0053546576938177408, 0.0053546158022711714, 0.0053545828929731608, 0.0053545466985468618, 0.0053544780392629646, 0.0053543954548925347, 0.0053542784097043473, 0.0053541610551021178, 0.0053540099916540877, 0.0053539065932851591], 'loss': [0.0099045160401768449, 0.0089923440576709888, 0.0082926656306658748, 0.007754777391305475, 0.0073347648392392915, 0.0070024526743642408, 0.0067358050461434633, 0.0065192963748624336, 0.0063418842834625467, 0.0061950486382451016, 0.0060729240957815376, 0.0059705877379273419, 0.0058846251663093561, 0.0058123305016641374, 0.0057514510386682547, 0.0056999485992440249, 0.0056562250280233562, 0.0056189986233208186, 0.0055871573403135635, 0.0055598348740484175, 0.0055364368606198268, 0.0055163650333582333, 0.0054990275501994555, 0.0054840764408167287, 0.0054711619699201229, 0.0054599608117363576, 0.0054502659830526186, 0.0054418704122907585, 0.0054346045782798502, 0.0054282939699513318, 0.0054227960064716389, 0.005418004580511975, 0.0054138523127645437, 0.005410225588962759, 0.0054069785450380449, 0.0054041698726029668, 0.005401739828920487, 0.0053996307394367036, 0.0053977775334309577, 0.0053961730009552799, 0.0053947526364041621, 0.0053934803609446474, 0.0053923632416312201, 0.0053913829088777145, 0.005390513580184093, 0.005389744630276634, 0.0053890580421731352, 0.0053884449975737233, 0.0053878874559473502, 0.005387383360256571, 0.0053869206363448881, 0.0053864958399649091, 0.0053861274895406512, 0.0053858090926780684, 0.0053855343147946773, 0.0053852916167843436, 0.0053850741048366456, 0.0053848781051600951, 0.0053847068527488676, 0.0053845442377343615, 0.0053844095958341691, 0.0053842787462698985, 0.0053841615880734243, 0.0053840516964678963, 0.0053839512060533433, 0.0053838585983292855, 0.0053837746020524912, 0.0053836939423385615, 0.0053836165446542522, 0.0053835397759870625, 0.0053834819488229194, 0.0053834149401808234, 0.0053833572201867934, 0.0053832999048099546, 0.0053832460461408207, 0.0053831895554880176, 0.0053831439251401381, 0.0053830945890497887, 0.0053830442402447365, 0.0053830043997122193, 0.0053829600277415218, 0.0053829110090461802, 0.0053828701002419732, 0.0053828279432346004, 0.0053827886183189328, 0.0053827468891918103, 0.005382710851569333, 0.0053826682433043361, 0.0053826352018152843, 0.0053825928315536752, 0.0053825540306569938, 0.0053825152194148249, 0.0053824814614006839, 0.0053824404451977342, 0.0053823984203382556, 0.0053823632378475466, 0.0053823156891260595, 0.0053822394870029567, 0.005382134336660402, 0.0053820216664624892, 0.0053818927017204362, 0.00538175767195017]}
[2017-10-25 00:37:42,543 AE_UNIGRAMA_02.py:133]: evaluating model ... 
[2017-10-25 00:37:42,674 AE_UNIGRAMA_02.py:137]: evaluated! 
[2017-10-25 00:37:42,674 AE_UNIGRAMA_02.py:139]: generating reports ... 
[2017-10-25 00:37:43,545 AE_UNIGRAMA_02.py:142]: done!
[2017-10-25 00:37:43,546 AE_UNIGRAMA_02.py:158]: >> experiment AE_UNIGRAMA_02 finished!
