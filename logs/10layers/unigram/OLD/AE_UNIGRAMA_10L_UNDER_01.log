[2017-10-20 01:41:10,492 AE_UNIGRAMA_10L_UNDER_01.py:148]: >> Initializing execution of experiment AE_UNIGRAMA_10L_UNDER_01
[2017-10-20 01:41:10,492 AE_UNIGRAMA_10L_UNDER_01.py:149]: >> Printing header log
[2017-10-20 01:41:10,492 AE_UNIGRAMA_10L_UNDER_01.py:38]: 
	=======================================
	network_name = AE_UNIGRAMA_10L_UNDER_01
	layers = 96,28,26,24,22,20,19,17,15,13,11,9
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/10layers/unigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/10layers/unigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/10layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/10layers/unigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/10layers/unigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/10layers/unigram/', 'data_dir': '/home/dhiegorp/malware_deepnn/', 'fullds_data_dir': '/home/dhiegorp/malware_deepnn/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f181dfbb7b8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f181dfbb898>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-10-20 01:41:10,492 AE_UNIGRAMA_10L_UNDER_01.py:151]: >> Loading dataset... 
[2017-10-20 01:41:11,078 AE_UNIGRAMA_10L_UNDER_01.py:55]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_deepnn/	
	trainx shape = (1627, 96)
	trainy shape = (1627, 9)
	valx shape = (1076, 96)
	valy shape = (1076, 9)
	=======================================
	
[2017-10-20 01:41:11,078 AE_UNIGRAMA_10L_UNDER_01.py:153]: >> Executing autoencoder part ... 
[2017-10-20 01:41:11,078 AE_UNIGRAMA_10L_UNDER_01.py:60]: =======================================
[2017-10-20 01:41:11,078 AE_UNIGRAMA_10L_UNDER_01.py:65]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f181dfbb7b8>, 'discard_decoder_function': True}
[2017-10-20 01:41:11,319 AE_UNIGRAMA_10L_UNDER_01.py:76]: training and evaluate autoencoder
[2017-10-21 21:29:58,029 AE_UNIGRAMA_10L_UNDER_01.py:148]: >> Initializing execution of experiment AE_UNIGRAMA_10L_UNDER_01
[2017-10-21 21:29:58,034 AE_UNIGRAMA_10L_UNDER_01.py:149]: >> Printing header log
[2017-10-21 21:29:58,034 AE_UNIGRAMA_10L_UNDER_01.py:38]: 
	=======================================
	network_name = AE_UNIGRAMA_10L_UNDER_01
	layers = 96,28,26,24,22,20,19,17,15,13,11,9
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/10layers/unigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/10layers/unigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/10layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/10layers/unigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/10layers/unigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/10layers/unigram/', 'data_dir': '/home/dhiegorp/malware_deepnn/', 'fullds_data_dir': '/home/dhiegorp/malware_deepnn/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f8f7e4b3780>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f8f7e4b3860>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-10-21 21:29:58,034 AE_UNIGRAMA_10L_UNDER_01.py:151]: >> Loading dataset... 
[2017-10-21 21:29:58,582 AE_UNIGRAMA_10L_UNDER_01.py:55]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_deepnn/	
	trainx shape = (1627, 96)
	trainy shape = (1627, 9)
	valx shape = (1076, 96)
	valy shape = (1076, 9)
	=======================================
	
[2017-10-21 21:29:58,582 AE_UNIGRAMA_10L_UNDER_01.py:153]: >> Executing autoencoder part ... 
[2017-10-21 21:29:58,582 AE_UNIGRAMA_10L_UNDER_01.py:60]: =======================================
[2017-10-21 21:29:58,582 AE_UNIGRAMA_10L_UNDER_01.py:65]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f8f7e4b3780>, 'discard_decoder_function': True}
[2017-10-21 21:29:58,786 AE_UNIGRAMA_10L_UNDER_01.py:76]: training and evaluate autoencoder
[2017-10-21 21:30:43,074 AE_UNIGRAMA_10L_UNDER_01.py:88]: trained and evaluated!
[2017-10-21 21:30:43,075 AE_UNIGRAMA_10L_UNDER_01.py:91]: Training history: 
{'val_loss': [0.01015796376302229, 0.0099195512219377169, 0.0097007797823242536, 0.0094997024883734255, 0.0093144397164753815, 0.0091434482374266619, 0.008985255211314748, 0.0088386674442754356, 0.0087024770109294519, 0.0085758421964749528, 0.0084578083918107928, 0.0083476018340614203, 0.0082444091029742171, 0.0081476537635390648, 0.0080568415656353462, 0.0079713544417103419, 0.0078908230825483136, 0.0078148060274002284, 0.0077428733445288524, 0.0076747191031238402, 0.0076100634026676972, 0.0075486442281909594, 0.0074901829290179517, 0.0074344429544079705, 0.0073812380052244355, 0.0073303898664876877, 0.0072817479876531319, 0.0072351253027144865, 0.0071904341773334265, 0.0071475291590043606, 0.0071062767050717179, 0.0070665811593592391, 0.0070283522909043222, 0.0069915176971666656, 0.0069560110759419357, 0.0069217276828688985, 0.0068886274399173521, 0.006856641204176007, 0.0068256880925296626, 0.0067957273708677208, 0.0067667278087244383, 0.006738631740267835, 0.0067114346577581644, 0.0066850496175591611, 0.0066594614224944641, 0.0066346619659030746, 0.006610548485513624, 0.0065871622728453918, 0.0065644509193650185, 0.0065423852563264422, 0.0065209203240458183, 0.0065000571161647046, 0.0064797984474238186, 0.00646009054173435, 0.0064409258706169719, 0.0064222836108760537, 0.006404159564635678, 0.0063865199030747419, 0.0063693377130481389, 0.0063526399180874726, 0.0063363866496235688, 0.0063205491837986559, 0.0063051244998731581, 0.0062901159596099728, 0.0062754780753449882, 0.0062612393303154573, 0.0062473577907703626, 0.0062338301607279527, 0.0062206353486747544, 0.0062077983895309791, 0.0061952968726980201, 0.0061831080396296144, 0.0061712620111673963, 0.0061597035484575431, 0.0061484428094942334, 0.0061374614707654738, 0.0061267612391189569, 0.006116339902230798, 0.0061061680680452451, 0.0060962583267278828, 0.0060865990990655126, 0.0060771978927644651, 0.0060680262972929661, 0.0060590930961654087, 0.0060503729913369874, 0.0060418668864329513, 0.0060335693475842252, 0.0060254749980474713, 0.0060175990142260563, 0.0060099188539590757, 0.0060024302103123921, 0.0059951205917921427, 0.005988000703414687, 0.0059810629890033745, 0.0059742962830738062, 0.0059676991869110378, 0.0059612594029024857, 0.0059549822472035885, 0.005948865176853637, 0.0059429030816960512, 0.0059370977153177596, 0.0059314412913080931], 'loss': [0.010282332836647449, 0.010031893986180544, 0.0098023648724674291, 0.0095914711216355877, 0.0093973713503954746, 0.0092183140100202607, 0.0090528404432879545, 0.0088995425751834615, 0.0087572921069765941, 0.0086249897757930925, 0.0085017838765487543, 0.0083868098322167021, 0.0082793189418750007, 0.0081785303862798638, 0.0080839137637732442, 0.0079949942584016161, 0.0079111817573925877, 0.0078321226953214571, 0.0077574023597409515, 0.0076866075296364795, 0.0076194486003220265, 0.0075556639877858338, 0.0074949986435880528, 0.0074371908666576317, 0.0073820131896126647, 0.0073292996409820245, 0.0072788554223708821, 0.0072305576767619029, 0.0071842187632633154, 0.007139757611876389, 0.0070970312934929818, 0.0070559215144046479, 0.007016321753804843, 0.0069781629306841795, 0.0069413662466810723, 0.0069058674849001959, 0.0068715627808364245, 0.0068384234566807051, 0.0068063856670363921, 0.0067753522263607436, 0.006745290404990471, 0.0067161855988488398, 0.006687969632204061, 0.0066606422016750868, 0.0066341140823233842, 0.0066083759454679441, 0.0065834056162438516, 0.0065591220999252197, 0.0065355630286413431, 0.0065126716418016633, 0.0064904181997634623, 0.0064687606241478459, 0.0064477060429783926, 0.006427243673634734, 0.0064073321870319229, 0.0063879660799522377, 0.0063691165495134965, 0.0063507776457264551, 0.0063329275672283358, 0.0063155374410158666, 0.0062986212366414572, 0.0062821491623724357, 0.0062660978109299334, 0.0062504541552462455, 0.0062352263193506553, 0.0062203728081572617, 0.006205910760405592, 0.006191809214342479, 0.0061780642964216047, 0.0061646584895357591, 0.0061515981861525832, 0.0061388772860233518, 0.0061264736543523435, 0.0061144073792580413, 0.0061026326845472732, 0.0060911564412283511, 0.0060799591235825637, 0.0060690485402373866, 0.0060584079208036799, 0.0060480270808369162, 0.0060379142571325385, 0.0060280392807350253, 0.0060184303600714301, 0.0060090509749326192, 0.0059999112118798012, 0.0059909927289527693, 0.0059822831845532621, 0.0059737806075630959, 0.0059654945516231864, 0.0059574140432120722, 0.0059495408645503052, 0.0059418559764736843, 0.0059343432033344065, 0.0059270258136002027, 0.0059198951989769492, 0.0059129367822915151, 0.0059061416315479226, 0.0058995132118729119, 0.0058930481342712136, 0.0058867446219602387, 0.0058805991880614, 0.0058746009449157964]}
[2017-10-21 21:30:43,075 AE_UNIGRAMA_10L_UNDER_01.py:95]: done!
[2017-10-21 21:30:43,075 AE_UNIGRAMA_10L_UNDER_01.py:155]: >> Executing classifier part ... 
[2017-10-21 21:30:43,075 AE_UNIGRAMA_10L_UNDER_01.py:100]: =======================================
[2017-10-21 21:30:43,075 AE_UNIGRAMA_10L_UNDER_01.py:104]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f8f7e4b3860>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-10-21 21:30:43,106 AE_UNIGRAMA_10L_UNDER_01.py:113]: training ... 
[2017-10-21 21:31:55,422 AE_UNIGRAMA_10L_UNDER_01.py:125]: trained!
[2017-10-21 21:31:55,424 AE_UNIGRAMA_10L_UNDER_01.py:128]: Training history: 
{'val_loss': [0.01015796376302229, 0.0099195512219377169, 0.0097007797823242536, 0.0094997024883734255, 0.0093144397164753815, 0.0091434482374266619, 0.008985255211314748, 0.0088386674442754356, 0.0087024770109294519, 0.0085758421964749528, 0.0084578083918107928, 0.0083476018340614203, 0.0082444091029742171, 0.0081476537635390648, 0.0080568415656353462, 0.0079713544417103419, 0.0078908230825483136, 0.0078148060274002284, 0.0077428733445288524, 0.0076747191031238402, 0.0076100634026676972, 0.0075486442281909594, 0.0074901829290179517, 0.0074344429544079705, 0.0073812380052244355, 0.0073303898664876877, 0.0072817479876531319, 0.0072351253027144865, 0.0071904341773334265, 0.0071475291590043606, 0.0071062767050717179, 0.0070665811593592391, 0.0070283522909043222, 0.0069915176971666656, 0.0069560110759419357, 0.0069217276828688985, 0.0068886274399173521, 0.006856641204176007, 0.0068256880925296626, 0.0067957273708677208, 0.0067667278087244383, 0.006738631740267835, 0.0067114346577581644, 0.0066850496175591611, 0.0066594614224944641, 0.0066346619659030746, 0.006610548485513624, 0.0065871622728453918, 0.0065644509193650185, 0.0065423852563264422, 0.0065209203240458183, 0.0065000571161647046, 0.0064797984474238186, 0.00646009054173435, 0.0064409258706169719, 0.0064222836108760537, 0.006404159564635678, 0.0063865199030747419, 0.0063693377130481389, 0.0063526399180874726, 0.0063363866496235688, 0.0063205491837986559, 0.0063051244998731581, 0.0062901159596099728, 0.0062754780753449882, 0.0062612393303154573, 0.0062473577907703626, 0.0062338301607279527, 0.0062206353486747544, 0.0062077983895309791, 0.0061952968726980201, 0.0061831080396296144, 0.0061712620111673963, 0.0061597035484575431, 0.0061484428094942334, 0.0061374614707654738, 0.0061267612391189569, 0.006116339902230798, 0.0061061680680452451, 0.0060962583267278828, 0.0060865990990655126, 0.0060771978927644651, 0.0060680262972929661, 0.0060590930961654087, 0.0060503729913369874, 0.0060418668864329513, 0.0060335693475842252, 0.0060254749980474713, 0.0060175990142260563, 0.0060099188539590757, 0.0060024302103123921, 0.0059951205917921427, 0.005988000703414687, 0.0059810629890033745, 0.0059742962830738062, 0.0059676991869110378, 0.0059612594029024857, 0.0059549822472035885, 0.005948865176853637, 0.0059429030816960512, 0.0059370977153177596, 0.0059314412913080931], 'loss': [0.010282332836647449, 0.010031893986180544, 0.0098023648724674291, 0.0095914711216355877, 0.0093973713503954746, 0.0092183140100202607, 0.0090528404432879545, 0.0088995425751834615, 0.0087572921069765941, 0.0086249897757930925, 0.0085017838765487543, 0.0083868098322167021, 0.0082793189418750007, 0.0081785303862798638, 0.0080839137637732442, 0.0079949942584016161, 0.0079111817573925877, 0.0078321226953214571, 0.0077574023597409515, 0.0076866075296364795, 0.0076194486003220265, 0.0075556639877858338, 0.0074949986435880528, 0.0074371908666576317, 0.0073820131896126647, 0.0073292996409820245, 0.0072788554223708821, 0.0072305576767619029, 0.0071842187632633154, 0.007139757611876389, 0.0070970312934929818, 0.0070559215144046479, 0.007016321753804843, 0.0069781629306841795, 0.0069413662466810723, 0.0069058674849001959, 0.0068715627808364245, 0.0068384234566807051, 0.0068063856670363921, 0.0067753522263607436, 0.006745290404990471, 0.0067161855988488398, 0.006687969632204061, 0.0066606422016750868, 0.0066341140823233842, 0.0066083759454679441, 0.0065834056162438516, 0.0065591220999252197, 0.0065355630286413431, 0.0065126716418016633, 0.0064904181997634623, 0.0064687606241478459, 0.0064477060429783926, 0.006427243673634734, 0.0064073321870319229, 0.0063879660799522377, 0.0063691165495134965, 0.0063507776457264551, 0.0063329275672283358, 0.0063155374410158666, 0.0062986212366414572, 0.0062821491623724357, 0.0062660978109299334, 0.0062504541552462455, 0.0062352263193506553, 0.0062203728081572617, 0.006205910760405592, 0.006191809214342479, 0.0061780642964216047, 0.0061646584895357591, 0.0061515981861525832, 0.0061388772860233518, 0.0061264736543523435, 0.0061144073792580413, 0.0061026326845472732, 0.0060911564412283511, 0.0060799591235825637, 0.0060690485402373866, 0.0060584079208036799, 0.0060480270808369162, 0.0060379142571325385, 0.0060280392807350253, 0.0060184303600714301, 0.0060090509749326192, 0.0059999112118798012, 0.0059909927289527693, 0.0059822831845532621, 0.0059737806075630959, 0.0059654945516231864, 0.0059574140432120722, 0.0059495408645503052, 0.0059418559764736843, 0.0059343432033344065, 0.0059270258136002027, 0.0059198951989769492, 0.0059129367822915151, 0.0059061416315479226, 0.0058995132118729119, 0.0058930481342712136, 0.0058867446219602387, 0.0058805991880614, 0.0058746009449157964]}
[2017-10-21 21:31:55,424 AE_UNIGRAMA_10L_UNDER_01.py:132]: evaluating model ... 
[2017-10-21 21:31:55,511 AE_UNIGRAMA_10L_UNDER_01.py:136]: evaluated! 
[2017-10-21 21:31:55,511 AE_UNIGRAMA_10L_UNDER_01.py:138]: generating reports ... 
[2017-10-21 21:31:56,080 AE_UNIGRAMA_10L_UNDER_01.py:141]: done!
[2017-10-21 21:31:56,080 AE_UNIGRAMA_10L_UNDER_01.py:157]: >> experiment AE_UNIGRAMA_10L_UNDER_01 finished!
