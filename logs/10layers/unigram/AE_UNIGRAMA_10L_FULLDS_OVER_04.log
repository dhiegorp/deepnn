[2018-07-21 00:52:16,388 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:145]: >> Initializing execution of experiment AE_UNIGRAMA_10L_FULLDS_OVER_04
[2018-07-21 00:52:16,389 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:146]: >> Printing header log
[2018-07-21 00:52:16,389 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:35]: 
	=======================================
	network_name = AE_UNIGRAMA_10L_FULLDS_OVER_04
	layers = 96,163,148,132,117,101,86,71,55,40,24
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiego/deepnn/logs/10layers/unigram/', 'reports_dir': '/home/dhiego/deepnn/reports/10layers/unigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/10layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/10layers/unigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/10layers/unigram/', 'executed_path': '/home/dhiego/deepnn/executed/10layers/unigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f281a9f60f0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f281a9f6668>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2018-07-21 00:52:16,389 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:148]: >> Loading dataset... 
[2018-07-21 00:52:18,376 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (8147, 96)
	trainy shape = (8147, 9)
	valx shape = (2721, 96)
	valy shape = (2721, 9)
	=======================================
	
[2018-07-21 00:52:18,376 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:150]: >> Executing autoencoder part ... 
[2018-07-21 00:52:18,376 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:57]: =======================================
[2018-07-21 00:52:18,376 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f281a9f60f0>, 'discard_decoder_function': True}
[2018-07-21 00:52:18,756 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:73]: training and evaluate autoencoder
[2018-07-21 01:16:32,083 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:145]: >> Initializing execution of experiment AE_UNIGRAMA_10L_FULLDS_OVER_04
[2018-07-21 01:16:32,083 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:146]: >> Printing header log
[2018-07-21 01:16:32,084 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:35]: 
	=======================================
	network_name = AE_UNIGRAMA_10L_FULLDS_OVER_04
	layers = 96,163,148,132,117,101,86,71,55,40,24
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiego/deepnn/logs/10layers/unigram/', 'reports_dir': '/home/dhiego/deepnn/reports/10layers/unigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/10layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/10layers/unigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/10layers/unigram/', 'executed_path': '/home/dhiego/deepnn/executed/10layers/unigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7ff7995e6668>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7ff7995e6e48>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2018-07-21 01:16:32,084 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:148]: >> Loading dataset... 
[2018-07-21 01:16:34,038 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (8147, 96)
	trainy shape = (8147, 9)
	valx shape = (2721, 96)
	valy shape = (2721, 9)
	=======================================
	
[2018-07-21 01:16:34,039 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:150]: >> Executing autoencoder part ... 
[2018-07-21 01:16:34,039 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:57]: =======================================
[2018-07-21 01:16:34,039 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7ff7995e6668>, 'discard_decoder_function': True}
[2018-07-21 01:16:34,465 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:73]: training and evaluate autoencoder
[2018-07-21 01:18:50,153 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:145]: >> Initializing execution of experiment AE_UNIGRAMA_10L_FULLDS_OVER_04
[2018-07-21 01:18:50,153 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:146]: >> Printing header log
[2018-07-21 01:18:50,153 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:35]: 
	=======================================
	network_name = AE_UNIGRAMA_10L_FULLDS_OVER_04
	layers = 96,163,148,132,117,101,86,71,55,40,24
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiego/deepnn/logs/10layers/unigram/', 'reports_dir': '/home/dhiego/deepnn/reports/10layers/unigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/10layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/10layers/unigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/10layers/unigram/', 'executed_path': '/home/dhiego/deepnn/executed/10layers/unigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f5b6fcf0668>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f5b6fcf0e48>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2018-07-21 01:18:50,153 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:148]: >> Loading dataset... 
[2018-07-21 01:18:52,070 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (8147, 96)
	trainy shape = (8147, 9)
	valx shape = (2721, 96)
	valy shape = (2721, 9)
	=======================================
	
[2018-07-21 01:18:52,070 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:150]: >> Executing autoencoder part ... 
[2018-07-21 01:18:52,070 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:57]: =======================================
[2018-07-21 01:18:52,070 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f5b6fcf0668>, 'discard_decoder_function': True}
[2018-07-21 01:18:52,467 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:73]: training and evaluate autoencoder
[2018-07-21 01:22:07,836 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:85]: trained and evaluated!
[2018-07-21 01:22:07,837 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:88]: Training history: 
{'val_loss': [0.009350637328571058, 0.008517289967869998, 0.007887842205689534, 0.007406926003155716, 0.007035256804811444, 0.006745848831409525, 0.006518714747212633, 0.006339354632584565, 0.006196699321519163, 0.006082591230548762, 0.0059910696047778865, 0.005917049915992198, 0.005856944407921357, 0.005807732603451438, 0.005767288926861515, 0.005733824411365356, 0.005706072329738284, 0.005682907193109875, 0.005663661081299394, 0.005647498470803299, 0.005633892237921737, 0.0056224687163400075, 0.0056128018667961814, 0.005604626563641571, 0.005597646441361985, 0.005591656630126316, 0.005586601398099393, 0.005582268027833792, 0.005578573654638175, 0.005575366564918527, 0.005572635791589975, 0.005570273118032686, 0.005568237167477432, 0.005566473630914456, 0.005564939694031416, 0.005563622610013725, 0.005562486955574284, 0.005561466416079128, 0.005560570220212777, 0.005559778386981626, 0.00555908294247093, 0.0055584380031282335, 0.00555787374520827, 0.005557384546633942, 0.005556934912839972, 0.005556557799844521, 0.0055561878470117305, 0.005555875877052606, 0.005555598794553499, 0.005555336634350892, 0.00555509391486527, 0.005554875808256962, 0.0055546809486033055, 0.00555449951012598, 0.005554325506313595, 0.005554185383424343, 0.005554025398994474, 0.005553878035765307, 0.005553742567863136, 0.005553610082007246, 0.0055534906315869515, 0.005553380563531381, 0.005553260655150924, 0.005553144377593645, 0.005553040202951435, 0.0055529212763786966, 0.005552806482229003, 0.005552680925159786, 0.005552528145473528, 0.00555238080021365, 0.005552261790554342, 0.00555214604163643, 0.005552044102288194, 0.0055519271240142155, 0.005551830788003674, 0.005551734640071691, 0.005551623127028587, 0.005551502404040364, 0.005551390857026748, 0.0055512792350555255, 0.0055511702619286275, 0.005551058641155358, 0.005550958304068718, 0.0055508583172976435, 0.005550753946790184, 0.005550639457861698, 0.0055505225053437, 0.005550400612126928, 0.005550278793953329, 0.005550156532366134, 0.005550041889586308, 0.0055499097909258105, 0.005549771643802661, 0.005549645448395847, 0.005549515865949208, 0.00554938378132187, 0.005549226012248621, 0.005549069725641709, 0.0055489086268592225, 0.005548743046022669, 0.005548568971188809], 'val_acc': [0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607], 'loss': [0.00987945006831308, 0.00892040494562377, 0.008196738558522218, 0.007647477457423517, 0.007225775559654232, 0.006898782281514851, 0.006643373381261414, 0.006442345245781889, 0.00628318549135552, 0.006156257392990179, 0.006054608349138383, 0.005972838478179574, 0.0059065830183233844, 0.0058525706849657565, 0.0058082955754741004, 0.005771740559638059, 0.005741504396513018, 0.005716358951579977, 0.005695394017120106, 0.00567789963342874, 0.005663197321436154, 0.005650837221245184, 0.005640434632387606, 0.005631640089356389, 0.005624177436660635, 0.0056177875383379705, 0.005612343364195493, 0.0056077225363663705, 0.005603780933510961, 0.005600406947301417, 0.0055974913937979, 0.005595002983899896, 0.005592861639127013, 0.005591007059914881, 0.00558940956437174, 0.005588023789474843, 0.005586825803480563, 0.00558578634574713, 0.005584864230371296, 0.005584057855802883, 0.00558334905376407, 0.005582704688627577, 0.005582130568384817, 0.00558162133127215, 0.005581181722896511, 0.0055807860089415445, 0.0055804412672644825, 0.005580130710224348, 0.00557985259606388, 0.0055796012871207, 0.005579371747085364, 0.0055791624384239815, 0.0055789759522526855, 0.005578797974016614, 0.005578638794904522, 0.005578485413724142, 0.005578346492678505, 0.005578209757903181, 0.005578088881675398, 0.005577973047872896, 0.005577861970937335, 0.005577752637130813, 0.005577649430368277, 0.005577547844475115, 0.005577441387285908, 0.005577346328309859, 0.005577239590192062, 0.00557712275939314, 0.00557699164816232, 0.0055768531268183385, 0.0055767271639254505, 0.005576620208895344, 0.005576516163519526, 0.005576415682535944, 0.005576316780811151, 0.005576217267673718, 0.005576123818821452, 0.005576012940393521, 0.005575909323812471, 0.005575798796959668, 0.005575694014253471, 0.005575589656912598, 0.00557549381716655, 0.005575393685243171, 0.005575288030772402, 0.005575189363907342, 0.005575066311697576, 0.005574947574413783, 0.00557483549945297, 0.00557471736060709, 0.0055746021151828, 0.005574481602075939, 0.005574351831528948, 0.0055742234712263345, 0.005574096591357378, 0.005573967426335817, 0.005573819394515923, 0.0055736738788216625, 0.005573524475910206, 0.005573365562008421, 0.005573200611828591], 'acc': [0.5482999877035947, 0.5938382226513312, 0.5938382227025443, 0.5938382227025443, 0.593838222687912, 0.593838222687912, 0.5938382226366989, 0.593838222687912, 0.593838222687912, 0.5938382226001182, 0.5938382226659635, 0.593838222687912, 0.593838222687912, 0.5938382226366989, 0.5938382226001182, 0.5938382226513312, 0.5938382226001182, 0.5938382227244927, 0.593838222687912, 0.5938382226513312, 0.5938382227244927, 0.5938382226513312, 0.5938382226513312, 0.5938382226001182, 0.5938382226001182, 0.5938382226366989, 0.593838222687912, 0.593838222687912, 0.5938382227025443, 0.5938382226732797, 0.5938382226842539, 0.5938382226001182, 0.593838222687912, 0.5938382226513312, 0.593838222687912, 0.5938382226001182, 0.5938382226513312, 0.5938382226513312, 0.5938382226513312, 0.5938382226842539, 0.5938382226513312, 0.5938382226842539, 0.593838222687912, 0.5938382226513312, 0.5938382227025443, 0.5938382226513312, 0.5938382226842539, 0.5938382226293828, 0.5938382227025443, 0.593838222687912, 0.5938382227025443, 0.5938382227025443, 0.5938382227025443, 0.5938382226513312, 0.5938382227025443, 0.5938382226659635, 0.5938382227025443, 0.5938382227025443, 0.5938382227025443, 0.5938382226513312, 0.5938382227244927, 0.5938382226842539, 0.5938382226001182, 0.5938382226001182, 0.5938382226513312, 0.5938382226513312, 0.5938382226001182, 0.5938382226001182, 0.5938382226001182, 0.5938382227244927, 0.5938382226513312, 0.593838222687912, 0.5938382226842539, 0.593838222687912, 0.593838222687912, 0.5938382226001182, 0.5938382226513312, 0.5938382226659635, 0.5938382226513312, 0.5938382227244927, 0.5938382226513312, 0.5938382227025443, 0.5938382226513312, 0.5938382226659635, 0.5938382226001182, 0.5938382226001182, 0.5938382226842539, 0.5938382226513312, 0.5938382227244927, 0.5938382226366989, 0.5938382227025443, 0.5938382226513312, 0.5938382226842539, 0.593838222687912, 0.5938382227025443, 0.5938382226513312, 0.5938382227025443, 0.5938382226001182, 0.5938382226659635, 0.593838222687912, 0.5938382227025443]}
[2018-07-21 01:22:07,837 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:92]: done!
[2018-07-21 01:22:07,837 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:152]: >> Executing classifier part ... 
[2018-07-21 01:22:07,837 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:97]: =======================================
[2018-07-21 01:22:07,837 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f5b6fcf0e48>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2018-07-21 01:22:07,880 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:110]: training ... 
[2018-07-21 01:28:09,519 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:122]: trained!
[2018-07-21 01:28:09,520 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:125]: Training history: 
{'val_loss': [0.009350637328571058, 0.008517289967869998, 0.007887842205689534, 0.007406926003155716, 0.007035256804811444, 0.006745848831409525, 0.006518714747212633, 0.006339354632584565, 0.006196699321519163, 0.006082591230548762, 0.0059910696047778865, 0.005917049915992198, 0.005856944407921357, 0.005807732603451438, 0.005767288926861515, 0.005733824411365356, 0.005706072329738284, 0.005682907193109875, 0.005663661081299394, 0.005647498470803299, 0.005633892237921737, 0.0056224687163400075, 0.0056128018667961814, 0.005604626563641571, 0.005597646441361985, 0.005591656630126316, 0.005586601398099393, 0.005582268027833792, 0.005578573654638175, 0.005575366564918527, 0.005572635791589975, 0.005570273118032686, 0.005568237167477432, 0.005566473630914456, 0.005564939694031416, 0.005563622610013725, 0.005562486955574284, 0.005561466416079128, 0.005560570220212777, 0.005559778386981626, 0.00555908294247093, 0.0055584380031282335, 0.00555787374520827, 0.005557384546633942, 0.005556934912839972, 0.005556557799844521, 0.0055561878470117305, 0.005555875877052606, 0.005555598794553499, 0.005555336634350892, 0.00555509391486527, 0.005554875808256962, 0.0055546809486033055, 0.00555449951012598, 0.005554325506313595, 0.005554185383424343, 0.005554025398994474, 0.005553878035765307, 0.005553742567863136, 0.005553610082007246, 0.0055534906315869515, 0.005553380563531381, 0.005553260655150924, 0.005553144377593645, 0.005553040202951435, 0.0055529212763786966, 0.005552806482229003, 0.005552680925159786, 0.005552528145473528, 0.00555238080021365, 0.005552261790554342, 0.00555214604163643, 0.005552044102288194, 0.0055519271240142155, 0.005551830788003674, 0.005551734640071691, 0.005551623127028587, 0.005551502404040364, 0.005551390857026748, 0.0055512792350555255, 0.0055511702619286275, 0.005551058641155358, 0.005550958304068718, 0.0055508583172976435, 0.005550753946790184, 0.005550639457861698, 0.0055505225053437, 0.005550400612126928, 0.005550278793953329, 0.005550156532366134, 0.005550041889586308, 0.0055499097909258105, 0.005549771643802661, 0.005549645448395847, 0.005549515865949208, 0.00554938378132187, 0.005549226012248621, 0.005549069725641709, 0.0055489086268592225, 0.005548743046022669, 0.005548568971188809], 'val_acc': [0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607, 0.6030871003307607], 'loss': [0.00987945006831308, 0.00892040494562377, 0.008196738558522218, 0.007647477457423517, 0.007225775559654232, 0.006898782281514851, 0.006643373381261414, 0.006442345245781889, 0.00628318549135552, 0.006156257392990179, 0.006054608349138383, 0.005972838478179574, 0.0059065830183233844, 0.0058525706849657565, 0.0058082955754741004, 0.005771740559638059, 0.005741504396513018, 0.005716358951579977, 0.005695394017120106, 0.00567789963342874, 0.005663197321436154, 0.005650837221245184, 0.005640434632387606, 0.005631640089356389, 0.005624177436660635, 0.0056177875383379705, 0.005612343364195493, 0.0056077225363663705, 0.005603780933510961, 0.005600406947301417, 0.0055974913937979, 0.005595002983899896, 0.005592861639127013, 0.005591007059914881, 0.00558940956437174, 0.005588023789474843, 0.005586825803480563, 0.00558578634574713, 0.005584864230371296, 0.005584057855802883, 0.00558334905376407, 0.005582704688627577, 0.005582130568384817, 0.00558162133127215, 0.005581181722896511, 0.0055807860089415445, 0.0055804412672644825, 0.005580130710224348, 0.00557985259606388, 0.0055796012871207, 0.005579371747085364, 0.0055791624384239815, 0.0055789759522526855, 0.005578797974016614, 0.005578638794904522, 0.005578485413724142, 0.005578346492678505, 0.005578209757903181, 0.005578088881675398, 0.005577973047872896, 0.005577861970937335, 0.005577752637130813, 0.005577649430368277, 0.005577547844475115, 0.005577441387285908, 0.005577346328309859, 0.005577239590192062, 0.00557712275939314, 0.00557699164816232, 0.0055768531268183385, 0.0055767271639254505, 0.005576620208895344, 0.005576516163519526, 0.005576415682535944, 0.005576316780811151, 0.005576217267673718, 0.005576123818821452, 0.005576012940393521, 0.005575909323812471, 0.005575798796959668, 0.005575694014253471, 0.005575589656912598, 0.00557549381716655, 0.005575393685243171, 0.005575288030772402, 0.005575189363907342, 0.005575066311697576, 0.005574947574413783, 0.00557483549945297, 0.00557471736060709, 0.0055746021151828, 0.005574481602075939, 0.005574351831528948, 0.0055742234712263345, 0.005574096591357378, 0.005573967426335817, 0.005573819394515923, 0.0055736738788216625, 0.005573524475910206, 0.005573365562008421, 0.005573200611828591], 'acc': [0.5482999877035947, 0.5938382226513312, 0.5938382227025443, 0.5938382227025443, 0.593838222687912, 0.593838222687912, 0.5938382226366989, 0.593838222687912, 0.593838222687912, 0.5938382226001182, 0.5938382226659635, 0.593838222687912, 0.593838222687912, 0.5938382226366989, 0.5938382226001182, 0.5938382226513312, 0.5938382226001182, 0.5938382227244927, 0.593838222687912, 0.5938382226513312, 0.5938382227244927, 0.5938382226513312, 0.5938382226513312, 0.5938382226001182, 0.5938382226001182, 0.5938382226366989, 0.593838222687912, 0.593838222687912, 0.5938382227025443, 0.5938382226732797, 0.5938382226842539, 0.5938382226001182, 0.593838222687912, 0.5938382226513312, 0.593838222687912, 0.5938382226001182, 0.5938382226513312, 0.5938382226513312, 0.5938382226513312, 0.5938382226842539, 0.5938382226513312, 0.5938382226842539, 0.593838222687912, 0.5938382226513312, 0.5938382227025443, 0.5938382226513312, 0.5938382226842539, 0.5938382226293828, 0.5938382227025443, 0.593838222687912, 0.5938382227025443, 0.5938382227025443, 0.5938382227025443, 0.5938382226513312, 0.5938382227025443, 0.5938382226659635, 0.5938382227025443, 0.5938382227025443, 0.5938382227025443, 0.5938382226513312, 0.5938382227244927, 0.5938382226842539, 0.5938382226001182, 0.5938382226001182, 0.5938382226513312, 0.5938382226513312, 0.5938382226001182, 0.5938382226001182, 0.5938382226001182, 0.5938382227244927, 0.5938382226513312, 0.593838222687912, 0.5938382226842539, 0.593838222687912, 0.593838222687912, 0.5938382226001182, 0.5938382226513312, 0.5938382226659635, 0.5938382226513312, 0.5938382227244927, 0.5938382226513312, 0.5938382227025443, 0.5938382226513312, 0.5938382226659635, 0.5938382226001182, 0.5938382226001182, 0.5938382226842539, 0.5938382226513312, 0.5938382227244927, 0.5938382226366989, 0.5938382227025443, 0.5938382226513312, 0.5938382226842539, 0.593838222687912, 0.5938382227025443, 0.5938382226513312, 0.5938382227025443, 0.5938382226001182, 0.5938382226659635, 0.593838222687912, 0.5938382227025443]}
[2018-07-21 01:28:09,520 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:129]: evaluating model ... 
[2018-07-21 01:28:09,688 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:133]: evaluated! 
[2018-07-21 01:28:09,688 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:135]: generating reports ... 
[2018-07-21 01:28:10,691 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:138]: done!
[2018-07-21 01:28:10,691 AE_UNIGRAMA_10L_FULLDS_OVER_04.py:154]: >> experiment AE_UNIGRAMA_10L_FULLDS_OVER_04 finished!
