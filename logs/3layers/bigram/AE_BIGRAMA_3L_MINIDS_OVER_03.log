[2018-06-03 00:54:28,237 AE_BIGRAMA_3L_MINIDS_OVER_03.py:145]: >> Initializing execution of experiment AE_BIGRAMA_3L_MINIDS_OVER_03
[2018-06-03 00:54:28,237 AE_BIGRAMA_3L_MINIDS_OVER_03.py:146]: >> Printing header log
[2018-06-03 00:54:28,237 AE_BIGRAMA_3L_MINIDS_OVER_03.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_3L_MINIDS_OVER_03
	layers = 9216,15667,14101,12535
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiego/deepnn/logs/3layers/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/3layers/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/3layers/bigram/fullds/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/3layers/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/3layers/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/3layers/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7ff3d5563630>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7ff3d5563e10>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2018-06-03 00:54:28,237 AE_BIGRAMA_3L_MINIDS_OVER_03.py:148]: >> Loading dataset... 
[2018-06-03 00:55:00,404 AE_BIGRAMA_3L_MINIDS_OVER_03.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-06-03 00:55:00,405 AE_BIGRAMA_3L_MINIDS_OVER_03.py:150]: >> Executing autoencoder part ... 
[2018-06-03 00:55:00,405 AE_BIGRAMA_3L_MINIDS_OVER_03.py:57]: =======================================
[2018-06-03 00:55:00,406 AE_BIGRAMA_3L_MINIDS_OVER_03.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7ff3d5563630>, 'discard_decoder_function': True}
[2018-06-03 00:55:00,603 AE_BIGRAMA_3L_MINIDS_OVER_03.py:73]: training and evaluate autoencoder
[2018-06-25 04:45:41,509 AE_BIGRAMA_3L_MINIDS_OVER_03.py:85]: trained and evaluated!
[2018-06-25 04:45:41,509 AE_BIGRAMA_3L_MINIDS_OVER_03.py:88]: Training history: 
{'val_loss': [0.00010635939659336262, 0.00010635561415014894, 0.00010635163961807884, 0.0001063474711915707, 0.00010634313815322543, 0.00010633867375434809, 0.00010633398478402223, 0.00010632913495603885, 0.00010632413964465727, 0.00010631904756482714, 0.0001063138148462782, 0.00010630844089906796, 0.0001063029705945812, 0.00010629739915964672, 0.00010629173102777186, 0.00010628604793280972, 0.00010628028456991222, 0.00010627454442929714, 0.00010626874615252934, 0.00010626294068918913, 0.00010625714781128914, 0.00010625133736025317, 0.00010624555846220294, 0.0001062397456335199, 0.00010623393076685364, 0.0001062280840075378, 0.00010622224699121188, 0.00010621642837036602, 0.00010621060247356255, 0.00010620478000915186, 0.0001061989765122867, 0.00010619311563010471, 0.00010618729243273515, 0.00010618144495833747, 0.00010617561766712443, 0.00010616980142392566, 0.00010616396358525563, 0.00010615816049956252, 0.00010615232487764617, 0.00010614654075949858, 0.00010614072124480044, 0.00010613490425076573, 0.00010612907034504577, 0.00010612326767052472, 0.00010611742722180601, 0.00010611157148821321, 0.00010610577240697833, 0.0001060999689994984, 0.00010609415062893118, 0.00010608828778027417, 0.00010608248338955671, 0.0001060766937116479, 0.0001060708855488738, 0.00010606508401848367, 0.00010605927692833231, 0.00010605346844377139, 0.00010604765759944042, 0.0001060418492757729, 0.0001060360444738834, 0.00010603022569214413, 0.0001060243840992945, 0.00010601857472088128, 0.0001060127557782486, 0.00010600693216970699, 0.00010600111968068781, 0.00010599529623303961, 0.00010598947010383454, 0.00010598366562373185, 0.00010597783531129808, 0.00010597201964016477, 0.00010596624165384388, 0.00010596043520726616, 0.00010595461512050254, 0.00010594882299343848, 0.00010594299816925775, 0.0001059371735774786, 0.00010593135307954295, 0.00010592552432241216, 0.00010591972368587432, 0.00010591393785153032, 0.00010590814499150737, 0.00010590233863431489, 0.00010589652124698518, 0.00010589074268859882, 0.00010588497655475924, 0.0001058791838556297, 0.00010587337832078131, 0.00010586757923954645, 0.00010586177500972241, 0.00010585596816984969, 0.00010585017784836723, 0.00010584436707554445, 0.00010583859874282824, 0.00010583280750961645, 0.00010582700009767827, 0.00010582117813382486, 0.00010581540971172341, 0.00010580963385277096, 0.00010580384270894439, 0.00010579804927685597, 0.00010579225339561229], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.004914004914004914, 0.004914004914004914, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.012285012285012284, 0.014742014742014743, 0.0171990171990172, 0.022113022113022112, 0.02702702702702703, 0.02702702702702703, 0.036855036855036855, 0.036855036855036855, 0.036855036855036855, 0.03931203931203931, 0.04176904176904177, 0.056511056511056514, 0.06633906633906633, 0.07862407862407862, 0.10073710078286598, 0.11302211306787827, 0.12285012289588809, 0.14004914014067052, 0.14987714996868035, 0.16707616716769755], 'loss': [0.00010656336713859096, 0.00010655956349547207, 0.00010655559248162786, 0.00010655145028132812, 0.00010654713208343476, 0.00010654261952030232, 0.00010653788464940957, 0.00010653298885424254, 0.00010652795616679154, 0.00010652285458289499, 0.0001065176619665776, 0.00010651230176623322, 0.00010650682465286307, 0.00010650129118044663, 0.0001064957063971239, 0.00010649003477631355, 0.0001064843264439125, 0.00010647854449872855, 0.0001064727806367878, 0.00010646693467739697, 0.00010646108374096671, 0.00010645524771195451, 0.00010644938009059215, 0.00010644354465408463, 0.00010643767295628999, 0.00010643178310415159, 0.0001064258722773471, 0.00010641997668976328, 0.00010641410876029847, 0.00010640823519018901, 0.00010640236811393095, 0.00010639651682199789, 0.00010639060670619904, 0.00010638473424999838, 0.00010637884219374252, 0.00010637296890803531, 0.00010636709920105642, 0.00010636121273804487, 0.00010635536670755346, 0.00010634948531638206, 0.00010634365623152484, 0.00010633778747255347, 0.00010633192018299372, 0.00010632603791491539, 0.00010632018240434891, 0.00010631428996889006, 0.00010630838511453286, 0.00010630253856263733, 0.00010629668539838944, 0.00010629081533590774, 0.00010628490659471978, 0.00010627905314606963, 0.00010627321569504616, 0.00010626735800406243, 0.00010626150581152224, 0.00010625565276577529, 0.00010624979599909887, 0.00010624393411318193, 0.00010623807936102145, 0.00010623222259434504, 0.00010622635350357104, 0.00010622046789376624, 0.0001062146073113596, 0.00010620873739107903, 0.0001062028654799827, 0.0001061970013899483, 0.00010619112834124295, 0.00010618525263811659, 0.000106179398881364, 0.00010617351779829505, 0.00010616765095903887, 0.00010616182538180942, 0.0001061559709377514, 0.00010615010559160705, 0.00010614426532026125, 0.00010613839205825422, 0.00010613251848814475, 0.00010612665041647882, 0.00010612077255663538, 0.00010611492827995786, 0.00010610909364925674, 0.0001061032570988404, 0.00010609740410049383, 0.00010609153617102901, 0.0001060857080341793, 0.00010607989748286884, 0.00010607405742482473, 0.00010606820290966615, 0.00010606235678437399, 0.00010605650620344657, 0.00010605064827546094, 0.00010604480916542435, 0.00010603895258834943, 0.00010603313592239055, 0.00010602729512964062, 0.00010602144061448204, 0.00010601556875078609, 0.00010600975175302457, 0.00010600392380577636, 0.00010599808353443056, 0.00010599224323938458], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0024429967426710096, 0.0024429967426710096, 0.003257328990228013, 0.004071661237785016, 0.005700325732899023, 0.006514657980456026, 0.006514657980456026, 0.006514657980456026, 0.008143322475570033, 0.008143322475570033, 0.008143322475570033, 0.008957654723127036, 0.011400651465798045, 0.013029315960912053, 0.014657980480295051, 0.021172638436482084, 0.023615635227691077, 0.02768729641693811, 0.03175895765472313, 0.03908794790700515, 0.04560260591173017, 0.05863192182410423, 0.06840390884332626, 0.07491856682378228, 0.08876221503225135, 0.1017915309688944, 0.11889250816759146, 0.13599348534201955, 0.1522801303174286]}
[2018-06-25 04:45:41,510 AE_BIGRAMA_3L_MINIDS_OVER_03.py:92]: done!
[2018-06-25 04:45:41,510 AE_BIGRAMA_3L_MINIDS_OVER_03.py:152]: >> Executing classifier part ... 
[2018-06-25 04:45:41,510 AE_BIGRAMA_3L_MINIDS_OVER_03.py:97]: =======================================
[2018-06-25 04:45:41,510 AE_BIGRAMA_3L_MINIDS_OVER_03.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7ff3d5563e10>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2018-06-25 04:45:41,549 AE_BIGRAMA_3L_MINIDS_OVER_03.py:110]: training ... 
[2018-06-25 10:37:36,580 AE_BIGRAMA_3L_MINIDS_OVER_03.py:122]: trained!
[2018-06-25 10:37:36,581 AE_BIGRAMA_3L_MINIDS_OVER_03.py:125]: Training history: 
{'val_loss': [0.00010635939659336262, 0.00010635561415014894, 0.00010635163961807884, 0.0001063474711915707, 0.00010634313815322543, 0.00010633867375434809, 0.00010633398478402223, 0.00010632913495603885, 0.00010632413964465727, 0.00010631904756482714, 0.0001063138148462782, 0.00010630844089906796, 0.0001063029705945812, 0.00010629739915964672, 0.00010629173102777186, 0.00010628604793280972, 0.00010628028456991222, 0.00010627454442929714, 0.00010626874615252934, 0.00010626294068918913, 0.00010625714781128914, 0.00010625133736025317, 0.00010624555846220294, 0.0001062397456335199, 0.00010623393076685364, 0.0001062280840075378, 0.00010622224699121188, 0.00010621642837036602, 0.00010621060247356255, 0.00010620478000915186, 0.0001061989765122867, 0.00010619311563010471, 0.00010618729243273515, 0.00010618144495833747, 0.00010617561766712443, 0.00010616980142392566, 0.00010616396358525563, 0.00010615816049956252, 0.00010615232487764617, 0.00010614654075949858, 0.00010614072124480044, 0.00010613490425076573, 0.00010612907034504577, 0.00010612326767052472, 0.00010611742722180601, 0.00010611157148821321, 0.00010610577240697833, 0.0001060999689994984, 0.00010609415062893118, 0.00010608828778027417, 0.00010608248338955671, 0.0001060766937116479, 0.0001060708855488738, 0.00010606508401848367, 0.00010605927692833231, 0.00010605346844377139, 0.00010604765759944042, 0.0001060418492757729, 0.0001060360444738834, 0.00010603022569214413, 0.0001060243840992945, 0.00010601857472088128, 0.0001060127557782486, 0.00010600693216970699, 0.00010600111968068781, 0.00010599529623303961, 0.00010598947010383454, 0.00010598366562373185, 0.00010597783531129808, 0.00010597201964016477, 0.00010596624165384388, 0.00010596043520726616, 0.00010595461512050254, 0.00010594882299343848, 0.00010594299816925775, 0.0001059371735774786, 0.00010593135307954295, 0.00010592552432241216, 0.00010591972368587432, 0.00010591393785153032, 0.00010590814499150737, 0.00010590233863431489, 0.00010589652124698518, 0.00010589074268859882, 0.00010588497655475924, 0.0001058791838556297, 0.00010587337832078131, 0.00010586757923954645, 0.00010586177500972241, 0.00010585596816984969, 0.00010585017784836723, 0.00010584436707554445, 0.00010583859874282824, 0.00010583280750961645, 0.00010582700009767827, 0.00010582117813382486, 0.00010581540971172341, 0.00010580963385277096, 0.00010580384270894439, 0.00010579804927685597, 0.00010579225339561229], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.004914004914004914, 0.004914004914004914, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.012285012285012284, 0.014742014742014743, 0.0171990171990172, 0.022113022113022112, 0.02702702702702703, 0.02702702702702703, 0.036855036855036855, 0.036855036855036855, 0.036855036855036855, 0.03931203931203931, 0.04176904176904177, 0.056511056511056514, 0.06633906633906633, 0.07862407862407862, 0.10073710078286598, 0.11302211306787827, 0.12285012289588809, 0.14004914014067052, 0.14987714996868035, 0.16707616716769755], 'loss': [0.00010656336713859096, 0.00010655956349547207, 0.00010655559248162786, 0.00010655145028132812, 0.00010654713208343476, 0.00010654261952030232, 0.00010653788464940957, 0.00010653298885424254, 0.00010652795616679154, 0.00010652285458289499, 0.0001065176619665776, 0.00010651230176623322, 0.00010650682465286307, 0.00010650129118044663, 0.0001064957063971239, 0.00010649003477631355, 0.0001064843264439125, 0.00010647854449872855, 0.0001064727806367878, 0.00010646693467739697, 0.00010646108374096671, 0.00010645524771195451, 0.00010644938009059215, 0.00010644354465408463, 0.00010643767295628999, 0.00010643178310415159, 0.0001064258722773471, 0.00010641997668976328, 0.00010641410876029847, 0.00010640823519018901, 0.00010640236811393095, 0.00010639651682199789, 0.00010639060670619904, 0.00010638473424999838, 0.00010637884219374252, 0.00010637296890803531, 0.00010636709920105642, 0.00010636121273804487, 0.00010635536670755346, 0.00010634948531638206, 0.00010634365623152484, 0.00010633778747255347, 0.00010633192018299372, 0.00010632603791491539, 0.00010632018240434891, 0.00010631428996889006, 0.00010630838511453286, 0.00010630253856263733, 0.00010629668539838944, 0.00010629081533590774, 0.00010628490659471978, 0.00010627905314606963, 0.00010627321569504616, 0.00010626735800406243, 0.00010626150581152224, 0.00010625565276577529, 0.00010624979599909887, 0.00010624393411318193, 0.00010623807936102145, 0.00010623222259434504, 0.00010622635350357104, 0.00010622046789376624, 0.0001062146073113596, 0.00010620873739107903, 0.0001062028654799827, 0.0001061970013899483, 0.00010619112834124295, 0.00010618525263811659, 0.000106179398881364, 0.00010617351779829505, 0.00010616765095903887, 0.00010616182538180942, 0.0001061559709377514, 0.00010615010559160705, 0.00010614426532026125, 0.00010613839205825422, 0.00010613251848814475, 0.00010612665041647882, 0.00010612077255663538, 0.00010611492827995786, 0.00010610909364925674, 0.0001061032570988404, 0.00010609740410049383, 0.00010609153617102901, 0.0001060857080341793, 0.00010607989748286884, 0.00010607405742482473, 0.00010606820290966615, 0.00010606235678437399, 0.00010605650620344657, 0.00010605064827546094, 0.00010604480916542435, 0.00010603895258834943, 0.00010603313592239055, 0.00010602729512964062, 0.00010602144061448204, 0.00010601556875078609, 0.00010600975175302457, 0.00010600392380577636, 0.00010599808353443056, 0.00010599224323938458], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0024429967426710096, 0.0024429967426710096, 0.003257328990228013, 0.004071661237785016, 0.005700325732899023, 0.006514657980456026, 0.006514657980456026, 0.006514657980456026, 0.008143322475570033, 0.008143322475570033, 0.008143322475570033, 0.008957654723127036, 0.011400651465798045, 0.013029315960912053, 0.014657980480295051, 0.021172638436482084, 0.023615635227691077, 0.02768729641693811, 0.03175895765472313, 0.03908794790700515, 0.04560260591173017, 0.05863192182410423, 0.06840390884332626, 0.07491856682378228, 0.08876221503225135, 0.1017915309688944, 0.11889250816759146, 0.13599348534201955, 0.1522801303174286]}
[2018-06-25 10:37:36,581 AE_BIGRAMA_3L_MINIDS_OVER_03.py:129]: evaluating model ... 
[2018-06-25 10:37:39,468 AE_BIGRAMA_3L_MINIDS_OVER_03.py:133]: evaluated! 
[2018-06-25 10:37:39,468 AE_BIGRAMA_3L_MINIDS_OVER_03.py:135]: generating reports ... 
[2018-06-25 10:37:40,006 AE_BIGRAMA_3L_MINIDS_OVER_03.py:138]: done!
[2018-06-25 10:37:40,006 AE_BIGRAMA_3L_MINIDS_OVER_03.py:154]: >> experiment AE_BIGRAMA_3L_MINIDS_OVER_03 finished!
