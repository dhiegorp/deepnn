[2018-06-03 00:54:28,270 AE_BIGRAMA_3L_FULLDS_OVER_05.py:145]: >> Initializing execution of experiment AE_BIGRAMA_3L_FULLDS_OVER_05
[2018-06-03 00:54:28,270 AE_BIGRAMA_3L_FULLDS_OVER_05.py:146]: >> Printing header log
[2018-06-03 00:54:28,271 AE_BIGRAMA_3L_FULLDS_OVER_05.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_3L_FULLDS_OVER_05
	layers = 9216,18432,16590,14747
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiego/deepnn/logs/3layers/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/3layers/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/3layers/bigram/fullds/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/3layers/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/3layers/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/3layers/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f2eb9463630>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f2eb9463e10>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2018-06-03 00:54:28,271 AE_BIGRAMA_3L_FULLDS_OVER_05.py:148]: >> Loading dataset... 
[2018-06-03 01:16:39,575 AE_BIGRAMA_3L_FULLDS_OVER_05.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (8147, 9216)
	trainy shape = (8147, 9)
	valx shape = (2721, 9216)
	valy shape = (2721, 9)
	=======================================
	
[2018-06-03 01:16:39,576 AE_BIGRAMA_3L_FULLDS_OVER_05.py:150]: >> Executing autoencoder part ... 
[2018-06-03 01:16:39,576 AE_BIGRAMA_3L_FULLDS_OVER_05.py:57]: =======================================
[2018-06-03 01:16:39,577 AE_BIGRAMA_3L_FULLDS_OVER_05.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f2eb9463630>, 'discard_decoder_function': True}
[2018-06-03 01:16:40,879 AE_BIGRAMA_3L_FULLDS_OVER_05.py:73]: training and evaluate autoencoder
[2018-06-12 05:44:49,160 AE_BIGRAMA_3L_FULLDS_OVER_05.py:85]: trained and evaluated!
[2018-06-12 05:44:49,161 AE_BIGRAMA_3L_FULLDS_OVER_05.py:88]: Training history: 
{'val_loss': [0.00010603884797655723, 0.00010599263819861347, 0.00010594637897303715, 0.00010590006987466205, 0.0001058537701352916, 0.00010580744704119354, 0.00010576112447654775, 0.00010571481247955522, 0.00010566850611935748, 0.00010562223530198539, 0.00010557600047667119, 0.00010552978727600572, 0.00010548360050516939, 0.00010543745654509432, 0.00010539138037362692, 0.0001053453490237698, 0.00010529937100152114, 0.00010525343408402092, 0.0001052075640058579, 0.00010516174998011071, 0.00010511600752133508, 0.00010507033111841426, 0.00010502472507114238, 0.00010497914967059974, 0.00010493366814087372, 0.00010488820588267526, 0.00010484283291472917, 0.00010479756824918487, 0.00010475234521516725, 0.000104707199449092, 0.00010466210770586561, 0.00010461708600008199, 0.00010457213306961254, 0.00010452725295754727, 0.00010448244441780154, 0.00010443771130895927, 0.00010439305212288374, 0.00010434848441171964, 0.00010430397405521003, 0.0001042595347255237, 0.00010421516755376321, 0.00010417086926962495, 0.00010412664929094157, 0.00010408250054230559, 0.00010403845197512936, 0.00010399448217066225, 0.00010395055669578916, 0.00010390672966160099, 0.00010386295970664516, 0.00010381926780578809, 0.00010377565353118959, 0.00010373208753835908, 0.00010368861806895451, 0.00010364522388070933, 0.00010360189392732406, 0.00010355863962411036, 0.00010351547681185211, 0.00010347240353585435, 0.00010342936268479668, 0.00010338641639444799, 0.00010334354337440969, 0.0001033007263131972, 0.00010325799944337549, 0.00010321534191308223, 0.00010317274184975152, 0.00010313023322122222, 0.00010308779703673691, 0.00010304544385860085, 0.00010300315652507348, 0.00010296091966599518, 0.00010291876146266594, 0.0001028766772944115, 0.0001028346473656, 0.00010279270115442207, 0.00010275083733189907, 0.00010270903976310697, 0.00010266731716261613, 0.00010262564325301546, 0.00010258402721147754, 0.00010254251404679273, 0.0001025010570093958, 0.00010245966817507673, 0.00010241833260151613, 0.00010237708619787781, 0.00010233588104071, 0.00010229475053631126, 0.00010225369509112981, 0.00010221270744525203, 0.00010217177528757582, 0.00010213091113510138, 0.00010209012022084933, 0.00010204939623952452, 0.0001020087467264627, 0.00010196814684821354, 0.00010192764529834123, 0.00010188718631054805, 0.00010184680723795857, 0.000101806490410772, 0.00010176623552415221, 0.00010172608019594988, 0.00010168597313614552], 'val_acc': [0.0, 0.0, 0.0, 0.0003675119441381845, 0.0003675119441381845, 0.0018375597206909224, 0.009187798603454611, 0.030871003307607496, 0.06651966188901139, 0.1183388460124954, 0.17897831679529586, 0.26350606394707826, 0.340683572216097, 0.3950753399485483, 0.4498346196251378, 0.47739801543550164, 0.48915839764792357, 0.49209849320102905, 0.4924660051451672, 0.4924660051451672, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436], 'loss': [0.00010608437605261584, 0.00010603844745924171, 0.00010599240908348269, 0.00010594634444925972, 0.00010590025397007082, 0.00010585417300758799, 0.00010580806273854456, 0.00010576194607233846, 0.00010571583915504034, 0.00010566973796687785, 0.00010562367624678889, 0.00010557765540941891, 0.00010553165838435504, 0.00010548569539205377, 0.00010543978620361483, 0.00010539394179057859, 0.00010534814428590097, 0.0001053023977102474, 0.00010525669183869597, 0.00010521105539819559, 0.0001051654764339201, 0.00010511996742136377, 0.00010507452446935841, 0.00010502915550045466, 0.00010498381543622357, 0.00010493857037919108, 0.00010489334375631116, 0.00010484821023032258, 0.00010480318638580037, 0.00010475820371335646, 0.00010471329542679517, 0.00010466844682952159, 0.00010462366660320803, 0.00010457895455405518, 0.00010453431656748842, 0.0001044897538464923, 0.000104445269572233, 0.00010440085816829229, 0.00010435653843230807, 0.00010431227341649065, 0.00010426808331168939, 0.00010422396561190983, 0.00010417991591871187, 0.00010413594151073239, 0.00010409203636233177, 0.00010404823760706121, 0.00010400451273843915, 0.00010396083625355095, 0.00010391725717022027, 0.00010387373915777873, 0.00010383029419784496, 0.0001037869310024561, 0.00010374361501192986, 0.00010370039077063077, 0.00010365724392669446, 0.00010361416255565969, 0.00010357115205979323, 0.00010352823670151565, 0.00010348540691232204, 0.00010344261440939702, 0.00010339991206422298, 0.00010335728794072852, 0.00010331471921257259, 0.00010327223885331981, 0.00010322982894144796, 0.0001031874748660983, 0.00010314521113247494, 0.00010310302080948617, 0.00010306091162141789, 0.00010301887069981876, 0.00010297687601408579, 0.00010293496301341324, 0.00010289312232130927, 0.00010285133416488796, 0.00010280963247152796, 0.00010276801264095216, 0.00010272645704546305, 0.00010268497958234511, 0.00010264355100487438, 0.00010260217759768494, 0.00010256090391317908, 0.00010251968655907097, 0.00010247854377303469, 0.00010243745336549823, 0.0001023964492772365, 0.00010235548974852174, 0.00010231460385817807, 0.00010227379313427266, 0.00010223304925683239, 0.0001021923617937398, 0.00010215174171832141, 0.00010211119248502722, 0.00010207071197992676, 0.000102030305014065, 0.0001019899494362726, 0.00010194968994363153, 0.00010190947148106837, 0.00010186933810443054, 0.000101829264687685, 0.00010178925301789336, 0.00010174933999551182], 'acc': [0.0004909782742113662, 0.0004909782742113662, 0.0004909782742113662, 0.0004909782742113662, 0.0004909782742113662, 0.0009819565484227323, 0.004173315330796613, 0.014606603657788143, 0.035473180311771206, 0.07413771940774533, 0.1232355468124206, 0.196023075982546, 0.27457959987465497, 0.35387259110857755, 0.41340370688962835, 0.4604148766307344, 0.4826316435753794, 0.49061004053131413, 0.49294218731552775, 0.4930649318182353, 0.49306493190237094, 0.49306493186944833, 0.49306493186579026, 0.49318767642336886, 0.49318767645263345, 0.4931876764599496, 0.4931876764892142, 0.49318767646726575, 0.4931876763867881, 0.4931876764709238, 0.49318767643800115, 0.49318767643800115, 0.4931876763867881, 0.49318767645263345, 0.49318767643800115, 0.4931876764709238, 0.4931876764160527, 0.4931876764160527, 0.49318767645263345, 0.49318767643800115, 0.4931876764709238, 0.4931876763867881, 0.49318767643800115, 0.49318767643800115, 0.49318767645263345, 0.4931876764160527, 0.49318767643800115, 0.4931876764709238, 0.4931876764892142, 0.4931876764709238, 0.4931876764709238, 0.49318767642336886, 0.4931876764709238, 0.4931876764709238, 0.4931876763867881, 0.49318767643800115, 0.4931876764160527, 0.49318767646726575, 0.4931876764745819, 0.49318767643800115, 0.49318767646726575, 0.4931876764709238, 0.4931876764160527, 0.49318767645263345, 0.4931876763867881, 0.4931876764343431, 0.4931876764343431, 0.4931876764343431, 0.49318767645263345, 0.4931876764343431, 0.49318767643800115, 0.4931876764709238, 0.4931876764745819, 0.4931876764892142, 0.4931876764709238, 0.49318767645263345, 0.49318767645263345, 0.4931876764745819, 0.4931876763867881, 0.49318767645263345, 0.4931876764709238, 0.49318767645263345, 0.4931876763867881, 0.4931876764160527, 0.4931876764892142, 0.4931876764709238, 0.49318767643800115, 0.4931876764892142, 0.4931876764709238, 0.49318767643800115, 0.49318767645263345, 0.49318767645263345, 0.4931876764343431, 0.49318767643800115, 0.4931876764160527, 0.49318767643800115, 0.4931876764343431, 0.4931876764709238, 0.49318767645263345, 0.4931876764160527, 0.4931876764709238]}
[2018-06-12 05:44:49,161 AE_BIGRAMA_3L_FULLDS_OVER_05.py:92]: done!
[2018-06-12 05:44:49,161 AE_BIGRAMA_3L_FULLDS_OVER_05.py:152]: >> Executing classifier part ... 
[2018-06-12 05:44:49,161 AE_BIGRAMA_3L_FULLDS_OVER_05.py:97]: =======================================
[2018-06-12 05:44:49,162 AE_BIGRAMA_3L_FULLDS_OVER_05.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f2eb9463e10>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2018-06-12 05:44:50,064 AE_BIGRAMA_3L_FULLDS_OVER_05.py:110]: training ... 
