[2018-06-03 00:54:28,470 AE_BIGRAMA_3L_FULLDS_OVER_01.py:145]: >> Initializing execution of experiment AE_BIGRAMA_3L_FULLDS_OVER_01
[2018-06-03 00:54:28,470 AE_BIGRAMA_3L_FULLDS_OVER_01.py:146]: >> Printing header log
[2018-06-03 00:54:28,470 AE_BIGRAMA_3L_FULLDS_OVER_01.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_3L_FULLDS_OVER_01
	layers = 9216,9216,8295,7375
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiego/deepnn/logs/3layers/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/3layers/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/3layers/bigram/fullds/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/3layers/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/3layers/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/3layers/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f7773736630>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f7773736e10>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2018-06-03 00:54:28,470 AE_BIGRAMA_3L_FULLDS_OVER_01.py:148]: >> Loading dataset... 
[2018-06-03 01:14:26,001 AE_BIGRAMA_3L_FULLDS_OVER_01.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (8147, 9216)
	trainy shape = (8147, 9)
	valx shape = (2721, 9216)
	valy shape = (2721, 9)
	=======================================
	
[2018-06-03 01:14:26,002 AE_BIGRAMA_3L_FULLDS_OVER_01.py:150]: >> Executing autoencoder part ... 
[2018-06-03 01:14:26,003 AE_BIGRAMA_3L_FULLDS_OVER_01.py:57]: =======================================
[2018-06-03 01:14:26,003 AE_BIGRAMA_3L_FULLDS_OVER_01.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f7773736630>, 'discard_decoder_function': True}
[2018-06-03 01:14:27,461 AE_BIGRAMA_3L_FULLDS_OVER_01.py:73]: training and evaluate autoencoder
[2018-06-06 15:16:59,149 AE_BIGRAMA_3L_FULLDS_OVER_01.py:85]: trained and evaluated!
[2018-06-06 15:16:59,150 AE_BIGRAMA_3L_FULLDS_OVER_01.py:88]: Training history: 
{'val_loss': [0.00010633416961410823, 0.00010632835922314204, 0.00010632249509009715, 0.00010631658338656863, 0.00010631062222738555, 0.00010630459816232122, 0.00010629850852807031, 0.00010629233728597288, 0.00010628608245726792, 0.00010627975479144078, 0.00010627332070036078, 0.0001062667594150596, 0.00010626005723128041, 0.00010625319947142996, 0.00010624609529433511, 0.00010623867346460047, 0.00010623075827627276, 0.00010622191114500212, 0.000106211778626641, 0.0001061997334165218, 0.00010618459716247917, 0.00010616475006949054, 0.00010613872433009477, 0.00010610649496644549, 0.00010607018189470772, 0.00010603205220540557, 0.00010599330424688632, 0.0001059543661240886, 0.000105915375903723, 0.00010587640045721475, 0.00010583742652151724, 0.00010579844811221551, 0.00010575949482783026, 0.00010572056078555857, 0.0001056816505552687, 0.00010564274991929561, 0.00010560387994876975, 0.00010556503429293797, 0.00010552619451723509, 0.00010548737520031639, 0.0001054485627502331, 0.00010540978857580481, 0.00010537105711587376, 0.00010533234307973536, 0.00010529367195329626, 0.00010525504063471493, 0.00010521643694926334, 0.00010517785084804449, 0.00010513931889802653, 0.00010510083118936055, 0.00010506238791457464, 0.00010502398256514956, 0.00010498558344614758, 0.0001049472473426609, 0.00010490893633735069, 0.0001048706829246811, 0.00010483245645257488, 0.00010479426398574354, 0.00010475613964177006, 0.00010471804603544184, 0.00010468001603825721, 0.00010464201211793353, 0.00010460406178459649, 0.00010456617445875273, 0.00010452829963653919, 0.00010449050233095015, 0.00010445274680930585, 0.00010441504878136413, 0.00010437739417652998, 0.00010433978276483928, 0.00010430222966776954, 0.00010426472663067867, 0.0001042272815472186, 0.00010418988720025971, 0.00010415255124279384, 0.00010411527467489747, 0.00010407802805581596, 0.00010404085454125678, 0.00010400373435698009, 0.00010396667209692016, 0.00010392964921687781, 0.00010389268736021975, 0.00010385577753160555, 0.00010381891674417572, 0.00010378209860171905, 0.00010374532797091799, 0.00010370862731873169, 0.00010367198281789928, 0.00010363538873268792, 0.00010359885371883989, 0.00010356236703756588, 0.00010352592821556764, 0.00010348954865478683, 0.00010345321108117489, 0.0001034169463125971, 0.00010338073375122126, 0.00010334457885201004, 0.0001033084802271568, 0.0001032724286379869, 0.0001032364378342152, 0.0001032005019838448], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0003675119441381845, 0.001470047776552738, 0.010290334435869165, 0.03491363469312753, 0.08122013965453877, 0.17273061374494672, 0.2723263506063947, 0.3432561558250643, 0.39911797133406834, 0.4450569643513414, 0.4671076809996325, 0.4832782065417126, 0.4895259095920617, 0.4906284454244763, 0.4928335170893054, 0.4928335170893054, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436], 'loss': [0.0001063563942832607, 0.00010635066393570092, 0.00010634487493572426, 0.00010633903769937266, 0.00010633315048513176, 0.00010632720389596881, 0.00010632119652974147, 0.00010631512308152913, 0.00010630896885563, 0.00010630273069052574, 0.0001062964108269648, 0.00010628997539919135, 0.00010628339914988891, 0.00010627668044114093, 0.00010626977203414758, 0.00010626258357200014, 0.00010625500841329108, 0.00010624674229045844, 0.00010623740282946853, 0.00010622652699470182, 0.00010621326782521769, 0.00010619614397998562, 0.00010617376752119437, 0.00010614527126936833, 0.00010611134615826622, 0.00010607443274965985, 0.00010603624960830493, 0.00010599769926016661, 0.0001059590206794153, 0.00010592031908656139, 0.00010588163376302378, 0.00010584295755698356, 0.00010580428324249584, 0.00010576563035042069, 0.00010572699857889565, 0.00010568839208344185, 0.00010564979669170488, 0.00010561123500318239, 0.00010557269406011461, 0.00010553416237029303, 0.00010549565555376167, 0.00010545715610606871, 0.0001054186927202258, 0.00010538026789061736, 0.0001053418650683917, 0.00010530350585368517, 0.0001052651857569631, 0.00010522689493554368, 0.00010518861986455859, 0.00010515040376482147, 0.00010511222954001997, 0.00010507409844850987, 0.00010503600670093455, 0.00010499792648039803, 0.00010495991015200382, 0.0001049219178368597, 0.00010488398718739882, 0.00010484609034296403, 0.00010480822280598292, 0.00010477042253253731, 0.00010473265366975882, 0.00010469494707103006, 0.00010465726628958161, 0.00010461963870023275, 0.0001045820763069293, 0.00010454452748404145, 0.00010450705352126397, 0.0001044696207466403, 0.00010443224571884606, 0.00010439491579270095, 0.00010435763081102216, 0.00010432039864634756, 0.00010428321627737306, 0.00010424608884379868, 0.0001042090109219236, 0.00010417199328234421, 0.00010413503339316657, 0.00010409810524956269, 0.00010406124929038868, 0.0001040244435859601, 0.00010398769757975007, 0.0001039509918632511, 0.00010391434262986729, 0.00010387774581695828, 0.00010384119839696834, 0.00010380469626438908, 0.00010376824173052259, 0.00010373185677327986, 0.00010369552681038083, 0.00010365924749875663, 0.00010362303011091701, 0.00010358686046468367, 0.00010355073944242385, 0.00010351467825413139, 0.00010347865732250588, 0.00010344271046507656, 0.00010340681610849962, 0.00010337097858869922, 0.0001033352010055709, 0.00010329947069160725, 0.0001032637995292946], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00012274456855284154, 0.00159567939118694, 0.006750951270406285, 0.01988462010556033, 0.05253467532598388, 0.12102614459675982, 0.21431201670057748, 0.29851479069124603, 0.36958389589065743, 0.4218730821088002, 0.4598011538062605, 0.4791947956741902, 0.48741868176723063, 0.49097827418210155, 0.4926966981784221, 0.49294218730089545, 0.4931876764343431, 0.4931876764709238, 0.4931876764709238, 0.4931876764160527, 0.4931876764745819, 0.49318767645263345, 0.49318767642336886, 0.4931876764343431, 0.4931876764892142, 0.49318767651116263, 0.49318767645263345, 0.4931876764892142, 0.4931876764343431, 0.49318767643800115, 0.49318767645263345, 0.49318767645263345, 0.4931876764745819, 0.4931876764160527, 0.49318767645263345, 0.4931876764892142, 0.4931876763867881, 0.4931876764892142, 0.4931876764709238, 0.4931876764709238, 0.49318767645263345, 0.4931876764709238, 0.4931876764892142, 0.49318767645263345, 0.4931876764745819, 0.49318767645263345, 0.49318767645263345, 0.49318767646726575, 0.4931876764892142, 0.4931876764892142, 0.4931876763867881, 0.49318767645263345, 0.49318767643800115, 0.49318767643800115, 0.4931876764709238, 0.4931876764892142, 0.4931876764709238, 0.49318767645263345, 0.4931876764709238, 0.4931876764709238, 0.49318767643800115, 0.49318767643800115, 0.4931876764709238, 0.4931876764709238, 0.4931876764709238, 0.4931876764343431, 0.4931876764892142, 0.4931876764709238, 0.49318767645263345]}
[2018-06-06 15:16:59,150 AE_BIGRAMA_3L_FULLDS_OVER_01.py:92]: done!
[2018-06-06 15:16:59,151 AE_BIGRAMA_3L_FULLDS_OVER_01.py:152]: >> Executing classifier part ... 
[2018-06-06 15:16:59,151 AE_BIGRAMA_3L_FULLDS_OVER_01.py:97]: =======================================
[2018-06-06 15:16:59,151 AE_BIGRAMA_3L_FULLDS_OVER_01.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f7773736e10>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2018-06-06 15:16:59,852 AE_BIGRAMA_3L_FULLDS_OVER_01.py:110]: training ... 
[2018-06-10 14:15:56,302 AE_BIGRAMA_3L_FULLDS_OVER_01.py:122]: trained!
[2018-06-10 14:15:56,304 AE_BIGRAMA_3L_FULLDS_OVER_01.py:125]: Training history: 
{'val_loss': [0.00010633416961410823, 0.00010632835922314204, 0.00010632249509009715, 0.00010631658338656863, 0.00010631062222738555, 0.00010630459816232122, 0.00010629850852807031, 0.00010629233728597288, 0.00010628608245726792, 0.00010627975479144078, 0.00010627332070036078, 0.0001062667594150596, 0.00010626005723128041, 0.00010625319947142996, 0.00010624609529433511, 0.00010623867346460047, 0.00010623075827627276, 0.00010622191114500212, 0.000106211778626641, 0.0001061997334165218, 0.00010618459716247917, 0.00010616475006949054, 0.00010613872433009477, 0.00010610649496644549, 0.00010607018189470772, 0.00010603205220540557, 0.00010599330424688632, 0.0001059543661240886, 0.000105915375903723, 0.00010587640045721475, 0.00010583742652151724, 0.00010579844811221551, 0.00010575949482783026, 0.00010572056078555857, 0.0001056816505552687, 0.00010564274991929561, 0.00010560387994876975, 0.00010556503429293797, 0.00010552619451723509, 0.00010548737520031639, 0.0001054485627502331, 0.00010540978857580481, 0.00010537105711587376, 0.00010533234307973536, 0.00010529367195329626, 0.00010525504063471493, 0.00010521643694926334, 0.00010517785084804449, 0.00010513931889802653, 0.00010510083118936055, 0.00010506238791457464, 0.00010502398256514956, 0.00010498558344614758, 0.0001049472473426609, 0.00010490893633735069, 0.0001048706829246811, 0.00010483245645257488, 0.00010479426398574354, 0.00010475613964177006, 0.00010471804603544184, 0.00010468001603825721, 0.00010464201211793353, 0.00010460406178459649, 0.00010456617445875273, 0.00010452829963653919, 0.00010449050233095015, 0.00010445274680930585, 0.00010441504878136413, 0.00010437739417652998, 0.00010433978276483928, 0.00010430222966776954, 0.00010426472663067867, 0.0001042272815472186, 0.00010418988720025971, 0.00010415255124279384, 0.00010411527467489747, 0.00010407802805581596, 0.00010404085454125678, 0.00010400373435698009, 0.00010396667209692016, 0.00010392964921687781, 0.00010389268736021975, 0.00010385577753160555, 0.00010381891674417572, 0.00010378209860171905, 0.00010374532797091799, 0.00010370862731873169, 0.00010367198281789928, 0.00010363538873268792, 0.00010359885371883989, 0.00010356236703756588, 0.00010352592821556764, 0.00010348954865478683, 0.00010345321108117489, 0.0001034169463125971, 0.00010338073375122126, 0.00010334457885201004, 0.0001033084802271568, 0.0001032724286379869, 0.0001032364378342152, 0.0001032005019838448], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0003675119441381845, 0.001470047776552738, 0.010290334435869165, 0.03491363469312753, 0.08122013965453877, 0.17273061374494672, 0.2723263506063947, 0.3432561558250643, 0.39911797133406834, 0.4450569643513414, 0.4671076809996325, 0.4832782065417126, 0.4895259095920617, 0.4906284454244763, 0.4928335170893054, 0.4928335170893054, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436, 0.4932010290334436], 'loss': [0.0001063563942832607, 0.00010635066393570092, 0.00010634487493572426, 0.00010633903769937266, 0.00010633315048513176, 0.00010632720389596881, 0.00010632119652974147, 0.00010631512308152913, 0.00010630896885563, 0.00010630273069052574, 0.0001062964108269648, 0.00010628997539919135, 0.00010628339914988891, 0.00010627668044114093, 0.00010626977203414758, 0.00010626258357200014, 0.00010625500841329108, 0.00010624674229045844, 0.00010623740282946853, 0.00010622652699470182, 0.00010621326782521769, 0.00010619614397998562, 0.00010617376752119437, 0.00010614527126936833, 0.00010611134615826622, 0.00010607443274965985, 0.00010603624960830493, 0.00010599769926016661, 0.0001059590206794153, 0.00010592031908656139, 0.00010588163376302378, 0.00010584295755698356, 0.00010580428324249584, 0.00010576563035042069, 0.00010572699857889565, 0.00010568839208344185, 0.00010564979669170488, 0.00010561123500318239, 0.00010557269406011461, 0.00010553416237029303, 0.00010549565555376167, 0.00010545715610606871, 0.0001054186927202258, 0.00010538026789061736, 0.0001053418650683917, 0.00010530350585368517, 0.0001052651857569631, 0.00010522689493554368, 0.00010518861986455859, 0.00010515040376482147, 0.00010511222954001997, 0.00010507409844850987, 0.00010503600670093455, 0.00010499792648039803, 0.00010495991015200382, 0.0001049219178368597, 0.00010488398718739882, 0.00010484609034296403, 0.00010480822280598292, 0.00010477042253253731, 0.00010473265366975882, 0.00010469494707103006, 0.00010465726628958161, 0.00010461963870023275, 0.0001045820763069293, 0.00010454452748404145, 0.00010450705352126397, 0.0001044696207466403, 0.00010443224571884606, 0.00010439491579270095, 0.00010435763081102216, 0.00010432039864634756, 0.00010428321627737306, 0.00010424608884379868, 0.0001042090109219236, 0.00010417199328234421, 0.00010413503339316657, 0.00010409810524956269, 0.00010406124929038868, 0.0001040244435859601, 0.00010398769757975007, 0.0001039509918632511, 0.00010391434262986729, 0.00010387774581695828, 0.00010384119839696834, 0.00010380469626438908, 0.00010376824173052259, 0.00010373185677327986, 0.00010369552681038083, 0.00010365924749875663, 0.00010362303011091701, 0.00010358686046468367, 0.00010355073944242385, 0.00010351467825413139, 0.00010347865732250588, 0.00010344271046507656, 0.00010340681610849962, 0.00010337097858869922, 0.0001033352010055709, 0.00010329947069160725, 0.0001032637995292946], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00012274456855284154, 0.00159567939118694, 0.006750951270406285, 0.01988462010556033, 0.05253467532598388, 0.12102614459675982, 0.21431201670057748, 0.29851479069124603, 0.36958389589065743, 0.4218730821088002, 0.4598011538062605, 0.4791947956741902, 0.48741868176723063, 0.49097827418210155, 0.4926966981784221, 0.49294218730089545, 0.4931876764343431, 0.4931876764709238, 0.4931876764709238, 0.4931876764160527, 0.4931876764745819, 0.49318767645263345, 0.49318767642336886, 0.4931876764343431, 0.4931876764892142, 0.49318767651116263, 0.49318767645263345, 0.4931876764892142, 0.4931876764343431, 0.49318767643800115, 0.49318767645263345, 0.49318767645263345, 0.4931876764745819, 0.4931876764160527, 0.49318767645263345, 0.4931876764892142, 0.4931876763867881, 0.4931876764892142, 0.4931876764709238, 0.4931876764709238, 0.49318767645263345, 0.4931876764709238, 0.4931876764892142, 0.49318767645263345, 0.4931876764745819, 0.49318767645263345, 0.49318767645263345, 0.49318767646726575, 0.4931876764892142, 0.4931876764892142, 0.4931876763867881, 0.49318767645263345, 0.49318767643800115, 0.49318767643800115, 0.4931876764709238, 0.4931876764892142, 0.4931876764709238, 0.49318767645263345, 0.4931876764709238, 0.4931876764709238, 0.49318767643800115, 0.49318767643800115, 0.4931876764709238, 0.4931876764709238, 0.4931876764709238, 0.4931876764343431, 0.4931876764892142, 0.4931876764709238, 0.49318767645263345]}
[2018-06-10 14:15:56,304 AE_BIGRAMA_3L_FULLDS_OVER_01.py:129]: evaluating model ... 
[2018-06-10 14:18:25,310 AE_BIGRAMA_3L_FULLDS_OVER_01.py:133]: evaluated! 
[2018-06-10 14:18:25,311 AE_BIGRAMA_3L_FULLDS_OVER_01.py:135]: generating reports ... 
[2018-06-10 14:18:31,051 AE_BIGRAMA_3L_FULLDS_OVER_01.py:138]: done!
[2018-06-10 14:18:31,052 AE_BIGRAMA_3L_FULLDS_OVER_01.py:154]: >> experiment AE_BIGRAMA_3L_FULLDS_OVER_01 finished!
