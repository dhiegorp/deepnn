[2018-06-03 00:54:28,369 AE_BIGRAMA_3L_MINIDS_OVER_04.py:145]: >> Initializing execution of experiment AE_BIGRAMA_3L_MINIDS_OVER_04
[2018-06-03 00:54:28,370 AE_BIGRAMA_3L_MINIDS_OVER_04.py:146]: >> Printing header log
[2018-06-03 00:54:28,370 AE_BIGRAMA_3L_MINIDS_OVER_04.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_3L_MINIDS_OVER_04
	layers = 9216,16589,14931,13273
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiego/deepnn/logs/3layers/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/3layers/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/3layers/bigram/fullds/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/3layers/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/3layers/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/3layers/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fabee342630>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fabee342e10>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2018-06-03 00:54:28,370 AE_BIGRAMA_3L_MINIDS_OVER_04.py:148]: >> Loading dataset... 
[2018-06-03 00:54:53,239 AE_BIGRAMA_3L_MINIDS_OVER_04.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-06-03 00:54:53,240 AE_BIGRAMA_3L_MINIDS_OVER_04.py:150]: >> Executing autoencoder part ... 
[2018-06-03 00:54:53,240 AE_BIGRAMA_3L_MINIDS_OVER_04.py:57]: =======================================
[2018-06-03 00:54:53,241 AE_BIGRAMA_3L_MINIDS_OVER_04.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fabee342630>, 'discard_decoder_function': True}
[2018-06-03 00:54:53,410 AE_BIGRAMA_3L_MINIDS_OVER_04.py:73]: training and evaluate autoencoder
[2018-06-24 16:15:47,591 AE_BIGRAMA_3L_MINIDS_OVER_04.py:85]: trained and evaluated!
[2018-06-24 16:15:47,592 AE_BIGRAMA_3L_MINIDS_OVER_04.py:88]: Training history: 
{'val_loss': [0.00010636531872230329, 0.00010636459799132742, 0.00010636387268382786, 0.0001063631490210165, 0.00010636242101408303, 0.00010636169284625615, 0.0001063609685219941, 0.00010636024101561791, 0.00010635951399192196, 0.00010635878852352898, 0.00010635806330541465, 0.00010635733374317819, 0.00010635659887145916, 0.00010635587143659116, 0.0001063551417134613, 0.00010635440954117616, 0.00010635367613537489, 0.00010635294142454927, 0.00010635220655283023, 0.00010635147454143852, 0.00010635074351328432, 0.00010635000292091065, 0.00010634926754863434, 0.00010634853349925941, 0.00010634779192364823, 0.00010634705590779826, 0.00010634631874781738, 0.00010634557708282097, 0.00010634484033401213, 0.00010634409466455748, 0.00010634335766547, 0.00010634261567868676, 0.00010634187639133743, 0.0001063411335822101, 0.00010634038619655905, 0.00010633964773155383, 0.00010633890345650874, 0.00010633815819822614, 0.00010633741891087681, 0.00010633667120343893, 0.00010633592145801784, 0.00010633518061536554, 0.00010633443732355797, 0.00010633369101052967, 0.0001063329415868954, 0.00010633219330739206, 0.00010633144731615059, 0.00010633069665900018, 0.00010632995190127485, 0.00010632920362177152, 0.00010632845207076882, 0.00010632769521028352, 0.00010632694921904204, 0.00010632619521888408, 0.00010632544693938074, 0.00010632469040068228, 0.00010632393868878617, 0.00010632318493890683, 0.00010632243322701072, 0.0001063216815151146, 0.00010632092751495664, 0.0001063201702432993, 0.00010631941395487947, 0.0001063186627256636, 0.00010631790414898191, 0.00010631714621587389, 0.0001063163866559547, 0.00010631562341336408, 0.00010631486303110078, 0.00010631410223766544, 0.00010631334194478737, 0.00010631257412567305, 0.00010631181799814663, 0.00010631105123377801, 0.00010631028766940058, 0.00010630952328267903, 0.00010630875415854037, 0.00010630799091594976, 0.00010630722154153246, 0.00010630645356152472, 0.00010630568320386991, 0.00010630491268532168, 0.00010630414356118302, 0.00010630337720798645, 0.00010630260751178232, 0.00010630183895970912, 0.00010630106713613656, 0.00010630029636730969, 0.00010629951850129566, 0.00010629874438946124, 0.00010629796528993105, 0.00010629718349096697, 0.00010629640782382957, 0.00010629562659693094, 0.00010629485118007218, 0.00010629407240232882, 0.0001062932874212506, 0.00010629250260106578, 0.00010629172121327376, 0.00010629093744783464, 0.00010629015091145342], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.0001065553321587665, 0.00010655462496886667, 0.00010655391981718297, 0.00010655321265098332, 0.00010655250866060882, 0.00010655179865038666, 0.00010655108939857049, 0.00010655038133176371, 0.00010654967065793629, 0.00010654896045811262, 0.00010654825480872498, 0.00010654754641011558, 0.00010654683327146864, 0.00010654612181553501, 0.00010654541140240966, 0.0001065447017713905, 0.00010654398863274355, 0.00010654327475939079, 0.00010654255932182566, 0.00010654184118243912, 0.00010654112733278654, 0.00010654041412303904, 0.0001065396952015463, 0.00010653897699105921, 0.00010653825674235596, 0.0001065375359722486, 0.00010653681861496825, 0.00010653609637544924, 0.00010653537214511447, 0.00010653465424272981, 0.00010653392825858116, 0.00010653320663526703, 0.00010653248614956191, 0.00010653176402854384, 0.00010653103913460382, 0.00010653030931102476, 0.00010652958678710351, 0.0001065288608977556, 0.00010652813157188047, 0.00010652740764964815, 0.00010652667713876364, 0.00010652594586947315, 0.000106525221591738, 0.00010652449276356682, 0.0001065237655470084, 0.00010652303074638993, 0.00010652230075690956, 0.00010652157209463969, 0.00010652084241326176, 0.00010652011130987256, 0.000106519381675895, 0.00010651864374685178, 0.00010651790750052186, 0.00010651717656303398, 0.00010651644010340237, 0.00010651570942661655, 0.00010651497246928101, 0.0001065142373605601, 0.00010651350094832887, 0.00010651276003306197, 0.00010651202663075458, 0.00010651128701899802, 0.00010651054868705159, 0.00010650980727408075, 0.00010650906837332982, 0.00010650832475624153, 0.00010650758353287221, 0.00010650684055568899, 0.00010650609430787988, 0.00010650534926878033, 0.00010650460449038286, 0.00010650386134729833, 0.00010650310931664343, 0.0001065023649885495, 0.00010650161819563608, 0.00010650086926970576, 0.0001065001221449897, 0.00010649937013803499, 0.00010649862277631706, 0.00010649787136186703, 0.000106497119189011, 0.00010649636647105065, 0.00010649561169117398, 0.00010649486203053784, 0.00010649410957327955, 0.00010649335695011996, 0.00010649260226504404, 0.0001064918470348638, 0.00010649109291859238, 0.00010649033107605979, 0.00010648957338106003, 0.00010648881379004525, 0.00010648804931679182, 0.00010648729121888887, 0.00010648652996886097, 0.00010648576893213476, 0.00010648500972032298, 0.0001064842459106748, 0.0001064834812004195, 0.00010648271651386438, 0.00010648195230131301], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-06-24 16:15:47,592 AE_BIGRAMA_3L_MINIDS_OVER_04.py:92]: done!
[2018-06-24 16:15:47,592 AE_BIGRAMA_3L_MINIDS_OVER_04.py:152]: >> Executing classifier part ... 
[2018-06-24 16:15:47,592 AE_BIGRAMA_3L_MINIDS_OVER_04.py:97]: =======================================
[2018-06-24 16:15:47,593 AE_BIGRAMA_3L_MINIDS_OVER_04.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fabee342e10>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2018-06-24 16:15:47,632 AE_BIGRAMA_3L_MINIDS_OVER_04.py:110]: training ... 
[2018-06-24 22:37:11,472 AE_BIGRAMA_3L_MINIDS_OVER_04.py:122]: trained!
[2018-06-24 22:37:11,474 AE_BIGRAMA_3L_MINIDS_OVER_04.py:125]: Training history: 
{'val_loss': [0.00010636531872230329, 0.00010636459799132742, 0.00010636387268382786, 0.0001063631490210165, 0.00010636242101408303, 0.00010636169284625615, 0.0001063609685219941, 0.00010636024101561791, 0.00010635951399192196, 0.00010635878852352898, 0.00010635806330541465, 0.00010635733374317819, 0.00010635659887145916, 0.00010635587143659116, 0.0001063551417134613, 0.00010635440954117616, 0.00010635367613537489, 0.00010635294142454927, 0.00010635220655283023, 0.00010635147454143852, 0.00010635074351328432, 0.00010635000292091065, 0.00010634926754863434, 0.00010634853349925941, 0.00010634779192364823, 0.00010634705590779826, 0.00010634631874781738, 0.00010634557708282097, 0.00010634484033401213, 0.00010634409466455748, 0.00010634335766547, 0.00010634261567868676, 0.00010634187639133743, 0.0001063411335822101, 0.00010634038619655905, 0.00010633964773155383, 0.00010633890345650874, 0.00010633815819822614, 0.00010633741891087681, 0.00010633667120343893, 0.00010633592145801784, 0.00010633518061536554, 0.00010633443732355797, 0.00010633369101052967, 0.0001063329415868954, 0.00010633219330739206, 0.00010633144731615059, 0.00010633069665900018, 0.00010632995190127485, 0.00010632920362177152, 0.00010632845207076882, 0.00010632769521028352, 0.00010632694921904204, 0.00010632619521888408, 0.00010632544693938074, 0.00010632469040068228, 0.00010632393868878617, 0.00010632318493890683, 0.00010632243322701072, 0.0001063216815151146, 0.00010632092751495664, 0.0001063201702432993, 0.00010631941395487947, 0.0001063186627256636, 0.00010631790414898191, 0.00010631714621587389, 0.0001063163866559547, 0.00010631562341336408, 0.00010631486303110078, 0.00010631410223766544, 0.00010631334194478737, 0.00010631257412567305, 0.00010631181799814663, 0.00010631105123377801, 0.00010631028766940058, 0.00010630952328267903, 0.00010630875415854037, 0.00010630799091594976, 0.00010630722154153246, 0.00010630645356152472, 0.00010630568320386991, 0.00010630491268532168, 0.00010630414356118302, 0.00010630337720798645, 0.00010630260751178232, 0.00010630183895970912, 0.00010630106713613656, 0.00010630029636730969, 0.00010629951850129566, 0.00010629874438946124, 0.00010629796528993105, 0.00010629718349096697, 0.00010629640782382957, 0.00010629562659693094, 0.00010629485118007218, 0.00010629407240232882, 0.0001062932874212506, 0.00010629250260106578, 0.00010629172121327376, 0.00010629093744783464, 0.00010629015091145342], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.0001065553321587665, 0.00010655462496886667, 0.00010655391981718297, 0.00010655321265098332, 0.00010655250866060882, 0.00010655179865038666, 0.00010655108939857049, 0.00010655038133176371, 0.00010654967065793629, 0.00010654896045811262, 0.00010654825480872498, 0.00010654754641011558, 0.00010654683327146864, 0.00010654612181553501, 0.00010654541140240966, 0.0001065447017713905, 0.00010654398863274355, 0.00010654327475939079, 0.00010654255932182566, 0.00010654184118243912, 0.00010654112733278654, 0.00010654041412303904, 0.0001065396952015463, 0.00010653897699105921, 0.00010653825674235596, 0.0001065375359722486, 0.00010653681861496825, 0.00010653609637544924, 0.00010653537214511447, 0.00010653465424272981, 0.00010653392825858116, 0.00010653320663526703, 0.00010653248614956191, 0.00010653176402854384, 0.00010653103913460382, 0.00010653030931102476, 0.00010652958678710351, 0.0001065288608977556, 0.00010652813157188047, 0.00010652740764964815, 0.00010652667713876364, 0.00010652594586947315, 0.000106525221591738, 0.00010652449276356682, 0.0001065237655470084, 0.00010652303074638993, 0.00010652230075690956, 0.00010652157209463969, 0.00010652084241326176, 0.00010652011130987256, 0.000106519381675895, 0.00010651864374685178, 0.00010651790750052186, 0.00010651717656303398, 0.00010651644010340237, 0.00010651570942661655, 0.00010651497246928101, 0.0001065142373605601, 0.00010651350094832887, 0.00010651276003306197, 0.00010651202663075458, 0.00010651128701899802, 0.00010651054868705159, 0.00010650980727408075, 0.00010650906837332982, 0.00010650832475624153, 0.00010650758353287221, 0.00010650684055568899, 0.00010650609430787988, 0.00010650534926878033, 0.00010650460449038286, 0.00010650386134729833, 0.00010650310931664343, 0.0001065023649885495, 0.00010650161819563608, 0.00010650086926970576, 0.0001065001221449897, 0.00010649937013803499, 0.00010649862277631706, 0.00010649787136186703, 0.000106497119189011, 0.00010649636647105065, 0.00010649561169117398, 0.00010649486203053784, 0.00010649410957327955, 0.00010649335695011996, 0.00010649260226504404, 0.0001064918470348638, 0.00010649109291859238, 0.00010649033107605979, 0.00010648957338106003, 0.00010648881379004525, 0.00010648804931679182, 0.00010648729121888887, 0.00010648652996886097, 0.00010648576893213476, 0.00010648500972032298, 0.0001064842459106748, 0.0001064834812004195, 0.00010648271651386438, 0.00010648195230131301], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-06-24 22:37:11,474 AE_BIGRAMA_3L_MINIDS_OVER_04.py:129]: evaluating model ... 
[2018-06-24 22:37:14,536 AE_BIGRAMA_3L_MINIDS_OVER_04.py:133]: evaluated! 
[2018-06-24 22:37:14,538 AE_BIGRAMA_3L_MINIDS_OVER_04.py:135]: generating reports ... 
[2018-06-24 22:37:15,082 AE_BIGRAMA_3L_MINIDS_OVER_04.py:138]: done!
[2018-06-24 22:37:15,083 AE_BIGRAMA_3L_MINIDS_OVER_04.py:154]: >> experiment AE_BIGRAMA_3L_MINIDS_OVER_04 finished!
