[2017-10-20 01:42:15,978 AE_UNIGRAMA_2L_OVER_05.py:148]: >> Initializing execution of experiment AE_UNIGRAMA_2L_OVER_05
[2017-10-20 01:42:15,978 AE_UNIGRAMA_2L_OVER_05.py:149]: >> Printing header log
[2017-10-20 01:42:15,979 AE_UNIGRAMA_2L_OVER_05.py:38]: 
	=======================================
	network_name = AE_UNIGRAMA_2L_OVER_05
	layers = 96,172,156,9
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/2layers/unigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/2layers/unigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/2layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/2layers/unigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/2layers/unigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/2layers/unigram/', 'data_dir': '/home/dhiegorp/malware_deepnn/', 'fullds_data_dir': '/home/dhiegorp/malware_deepnn/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fa7b63c07b8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fa7b63c0898>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-10-20 01:42:15,979 AE_UNIGRAMA_2L_OVER_05.py:151]: >> Loading dataset... 
[2017-10-20 01:42:16,569 AE_UNIGRAMA_2L_OVER_05.py:55]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_deepnn/	
	trainx shape = (1627, 96)
	trainy shape = (1627, 9)
	valx shape = (1076, 96)
	valy shape = (1076, 9)
	=======================================
	
[2017-10-20 01:42:16,569 AE_UNIGRAMA_2L_OVER_05.py:153]: >> Executing autoencoder part ... 
[2017-10-20 01:42:16,569 AE_UNIGRAMA_2L_OVER_05.py:60]: =======================================
[2017-10-20 01:42:16,569 AE_UNIGRAMA_2L_OVER_05.py:65]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fa7b63c07b8>, 'discard_decoder_function': True}
[2017-10-20 01:42:16,643 AE_UNIGRAMA_2L_OVER_05.py:76]: training and evaluate autoencoder
[2017-10-20 01:42:53,575 AE_UNIGRAMA_2L_OVER_05.py:88]: trained and evaluated!
[2017-10-20 01:42:53,575 AE_UNIGRAMA_2L_OVER_05.py:91]: Training history: 
{'val_loss': [0.010047848509506886, 0.0099960049392731655, 0.0099442409434560061, 0.0098641713714178614, 0.0097172264646752625, 0.0095699141667567218, 0.0094252962265925343, 0.0092828303063115219, 0.0091437806884263079, 0.0090074373450698027, 0.0088744700183489515, 0.0087452784499160423, 0.0086209965371232492, 0.0085016434118982583, 0.0083871686114448379, 0.0082768367492631906, 0.008169410011097512, 0.0080636221392696456, 0.0079613341240136158, 0.0078633623336161387, 0.0077695425540956643, 0.007680354894356989, 0.0075955201895928299, 0.0075148866705762631, 0.0074380845710856763, 0.0073648448860390483, 0.0072949605519609826, 0.0072281847579827099, 0.007164397036317445, 0.0071034016672757256, 0.0070451319418487496, 0.0069893577766213496, 0.0069359290867102193, 0.0068847030318137884, 0.006835610328803054, 0.0067884896528737472, 0.0067432696262866151, 0.0066997891336984128, 0.0066579631824032525, 0.0066177338936459617, 0.0065790125130776138, 0.006541675401567637, 0.0065056831042756601, 0.0064709823280849864, 0.0064375529630061193, 0.006405321534958471, 0.0063742063558733155, 0.0063443026076102126, 0.00631557255948731, 0.0062880034690037312, 0.0062614946027172097, 0.0062360098900543495, 0.0062114965416435637, 0.0061879983157971975, 0.0061653752735085429, 0.0061436104276615666, 0.006122647550954141, 0.0061024778927624449, 0.0060830847550612842, 0.0060644138147396666, 0.0060464455070991944, 0.0060291552749985209, 0.0060125224493083902, 0.0059964953849029803, 0.0059810910117725899, 0.005966266710657391, 0.0059519997282043713, 0.0059382662776040545, 0.0059250114696813563, 0.0059122728620098425, 0.0058999757393225199, 0.0058881228609654543, 0.0058767057377083374, 0.0058656549513977033, 0.0058549231655729526, 0.0058445085400930128, 0.005834427736630338, 0.0058246532050013318, 0.0058151808061868947, 0.0058060053711736287, 0.0057970707834487066, 0.0057883619947088913, 0.0057798805837017452, 0.0057716260466821575, 0.0057635690885350166, 0.0057557281452927005, 0.0057480717476000353, 0.0057406006052009902, 0.0057333042381199758, 0.0057261648889090936, 0.0057191169183687428, 0.005712191350965806, 0.005705435483345977, 0.0056988146003737325, 0.0056922783812016356, 0.0056857794408987665, 0.0056791945177106168, 0.0056723383615368142, 0.0056652719566951679, 0.0056582235291489438, 0.0056513543188793508, 0.0056447095383083288], 'loss': [0.010071448879299276, 0.010020161510663268, 0.0099684762205761411, 0.0099101360834626673, 0.009790177476496768, 0.0096405960657229145, 0.0094934354874533829, 0.0093486762795077557, 0.0092066874860407526, 0.009067897582409494, 0.0089318915828375568, 0.0087994996912791161, 0.0086713790614215153, 0.0085482886782317861, 0.0084302267151270538, 0.0083167736321806754, 0.0082068815758679636, 0.0080991391917964242, 0.0079936756329987597, 0.007892332033476876, 0.0077952023720930857, 0.0077025445593725921, 0.0076144631624624785, 0.0075306857904028912, 0.0074510066684154589, 0.0073750548610794593, 0.0073025718436661715, 0.0072333463576416809, 0.0071671872068317889, 0.0071039472963631192, 0.0070434366521719199, 0.0069855751584787162, 0.006930110093146636, 0.0068769544141496445, 0.0068259661082469142, 0.0067770473021365922, 0.0067300791437173817, 0.0066849249798619061, 0.006641509163650766, 0.0065997232938873381, 0.0065595296228140795, 0.0065208138052555006, 0.0064834512031185492, 0.0064474308966889693, 0.0064126682878754108, 0.0063791498055277458, 0.0063467930779708224, 0.0063156051273310591, 0.00628562983023558, 0.0062568067039518215, 0.0062291369654384763, 0.0062025070517279833, 0.0061769046103461577, 0.0061522658900589611, 0.0061286227189727956, 0.0061058302778583514, 0.0060838811811043687, 0.0060627349874447772, 0.0060423769136362998, 0.0060227967677929867, 0.006003947430258799, 0.0059857958628376463, 0.0059683274188077218, 0.005951506332405594, 0.0059352996195285836, 0.0059197103683667892, 0.0059046827822800017, 0.0058902132965409099, 0.0058762783355800268, 0.005862820185159952, 0.0058498758228510599, 0.0058373896912834768, 0.0058253411485252021, 0.0058137059203717117, 0.0058024571538795779, 0.0057915096410470612, 0.0057809161312520948, 0.0057706536227733023, 0.0057606943064445392, 0.0057510415011399829, 0.0057416823311962446, 0.0057325835085571836, 0.005723688891128887, 0.0057150398549345024, 0.0057066122001837244, 0.0056983796477812374, 0.0056903702421566057, 0.0056825534377038739, 0.0056749232366369706, 0.0056674638828898066, 0.0056601279250906517, 0.0056528897369858598, 0.0056458161828997098, 0.0056389033847065451, 0.0056321041094117468, 0.0056253820360073229, 0.0056186224269258811, 0.0056117094126452716, 0.0056045539805936939, 0.0055972877320316613, 0.0055901436561623559, 0.0055832177690804449]}
[2017-10-20 01:42:53,576 AE_UNIGRAMA_2L_OVER_05.py:95]: done!
[2017-10-20 01:42:53,576 AE_UNIGRAMA_2L_OVER_05.py:155]: >> Executing classifier part ... 
[2017-10-20 01:42:53,576 AE_UNIGRAMA_2L_OVER_05.py:100]: =======================================
[2017-10-20 01:42:53,576 AE_UNIGRAMA_2L_OVER_05.py:104]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fa7b63c0898>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-10-20 01:42:53,608 AE_UNIGRAMA_2L_OVER_05.py:113]: training ... 
[2017-10-20 01:43:55,292 AE_UNIGRAMA_2L_OVER_05.py:125]: trained!
[2017-10-20 01:43:55,292 AE_UNIGRAMA_2L_OVER_05.py:128]: Training history: 
{'val_loss': [0.010047848509506886, 0.0099960049392731655, 0.0099442409434560061, 0.0098641713714178614, 0.0097172264646752625, 0.0095699141667567218, 0.0094252962265925343, 0.0092828303063115219, 0.0091437806884263079, 0.0090074373450698027, 0.0088744700183489515, 0.0087452784499160423, 0.0086209965371232492, 0.0085016434118982583, 0.0083871686114448379, 0.0082768367492631906, 0.008169410011097512, 0.0080636221392696456, 0.0079613341240136158, 0.0078633623336161387, 0.0077695425540956643, 0.007680354894356989, 0.0075955201895928299, 0.0075148866705762631, 0.0074380845710856763, 0.0073648448860390483, 0.0072949605519609826, 0.0072281847579827099, 0.007164397036317445, 0.0071034016672757256, 0.0070451319418487496, 0.0069893577766213496, 0.0069359290867102193, 0.0068847030318137884, 0.006835610328803054, 0.0067884896528737472, 0.0067432696262866151, 0.0066997891336984128, 0.0066579631824032525, 0.0066177338936459617, 0.0065790125130776138, 0.006541675401567637, 0.0065056831042756601, 0.0064709823280849864, 0.0064375529630061193, 0.006405321534958471, 0.0063742063558733155, 0.0063443026076102126, 0.00631557255948731, 0.0062880034690037312, 0.0062614946027172097, 0.0062360098900543495, 0.0062114965416435637, 0.0061879983157971975, 0.0061653752735085429, 0.0061436104276615666, 0.006122647550954141, 0.0061024778927624449, 0.0060830847550612842, 0.0060644138147396666, 0.0060464455070991944, 0.0060291552749985209, 0.0060125224493083902, 0.0059964953849029803, 0.0059810910117725899, 0.005966266710657391, 0.0059519997282043713, 0.0059382662776040545, 0.0059250114696813563, 0.0059122728620098425, 0.0058999757393225199, 0.0058881228609654543, 0.0058767057377083374, 0.0058656549513977033, 0.0058549231655729526, 0.0058445085400930128, 0.005834427736630338, 0.0058246532050013318, 0.0058151808061868947, 0.0058060053711736287, 0.0057970707834487066, 0.0057883619947088913, 0.0057798805837017452, 0.0057716260466821575, 0.0057635690885350166, 0.0057557281452927005, 0.0057480717476000353, 0.0057406006052009902, 0.0057333042381199758, 0.0057261648889090936, 0.0057191169183687428, 0.005712191350965806, 0.005705435483345977, 0.0056988146003737325, 0.0056922783812016356, 0.0056857794408987665, 0.0056791945177106168, 0.0056723383615368142, 0.0056652719566951679, 0.0056582235291489438, 0.0056513543188793508, 0.0056447095383083288], 'loss': [0.010071448879299276, 0.010020161510663268, 0.0099684762205761411, 0.0099101360834626673, 0.009790177476496768, 0.0096405960657229145, 0.0094934354874533829, 0.0093486762795077557, 0.0092066874860407526, 0.009067897582409494, 0.0089318915828375568, 0.0087994996912791161, 0.0086713790614215153, 0.0085482886782317861, 0.0084302267151270538, 0.0083167736321806754, 0.0082068815758679636, 0.0080991391917964242, 0.0079936756329987597, 0.007892332033476876, 0.0077952023720930857, 0.0077025445593725921, 0.0076144631624624785, 0.0075306857904028912, 0.0074510066684154589, 0.0073750548610794593, 0.0073025718436661715, 0.0072333463576416809, 0.0071671872068317889, 0.0071039472963631192, 0.0070434366521719199, 0.0069855751584787162, 0.006930110093146636, 0.0068769544141496445, 0.0068259661082469142, 0.0067770473021365922, 0.0067300791437173817, 0.0066849249798619061, 0.006641509163650766, 0.0065997232938873381, 0.0065595296228140795, 0.0065208138052555006, 0.0064834512031185492, 0.0064474308966889693, 0.0064126682878754108, 0.0063791498055277458, 0.0063467930779708224, 0.0063156051273310591, 0.00628562983023558, 0.0062568067039518215, 0.0062291369654384763, 0.0062025070517279833, 0.0061769046103461577, 0.0061522658900589611, 0.0061286227189727956, 0.0061058302778583514, 0.0060838811811043687, 0.0060627349874447772, 0.0060423769136362998, 0.0060227967677929867, 0.006003947430258799, 0.0059857958628376463, 0.0059683274188077218, 0.005951506332405594, 0.0059352996195285836, 0.0059197103683667892, 0.0059046827822800017, 0.0058902132965409099, 0.0058762783355800268, 0.005862820185159952, 0.0058498758228510599, 0.0058373896912834768, 0.0058253411485252021, 0.0058137059203717117, 0.0058024571538795779, 0.0057915096410470612, 0.0057809161312520948, 0.0057706536227733023, 0.0057606943064445392, 0.0057510415011399829, 0.0057416823311962446, 0.0057325835085571836, 0.005723688891128887, 0.0057150398549345024, 0.0057066122001837244, 0.0056983796477812374, 0.0056903702421566057, 0.0056825534377038739, 0.0056749232366369706, 0.0056674638828898066, 0.0056601279250906517, 0.0056528897369858598, 0.0056458161828997098, 0.0056389033847065451, 0.0056321041094117468, 0.0056253820360073229, 0.0056186224269258811, 0.0056117094126452716, 0.0056045539805936939, 0.0055972877320316613, 0.0055901436561623559, 0.0055832177690804449]}
[2017-10-20 01:43:55,292 AE_UNIGRAMA_2L_OVER_05.py:132]: evaluating model ... 
[2017-10-20 01:43:55,345 AE_UNIGRAMA_2L_OVER_05.py:136]: evaluated! 
[2017-10-20 01:43:55,345 AE_UNIGRAMA_2L_OVER_05.py:138]: generating reports ... 
[2017-10-20 01:43:55,983 AE_UNIGRAMA_2L_OVER_05.py:141]: done!
[2017-10-20 01:43:55,983 AE_UNIGRAMA_2L_OVER_05.py:157]: >> experiment AE_UNIGRAMA_2L_OVER_05 finished!
