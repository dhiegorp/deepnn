[2017-10-20 01:39:04,245 AE_UNIGRAMA_2L_UNDER_02.py:148]: >> Initializing execution of experiment AE_UNIGRAMA_2L_UNDER_02
[2017-10-20 01:39:04,245 AE_UNIGRAMA_2L_UNDER_02.py:149]: >> Printing header log
[2017-10-20 01:39:04,245 AE_UNIGRAMA_2L_UNDER_02.py:38]: 
	=======================================
	network_name = AE_UNIGRAMA_2L_UNDER_02
	layers = 96,76,69,9
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/2layers/unigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/2layers/unigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/2layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/2layers/unigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/2layers/unigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/2layers/unigram/', 'data_dir': '/home/dhiegorp/malware_deepnn/', 'fullds_data_dir': '/home/dhiegorp/malware_deepnn/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f2e152957b8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f2e15295898>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-10-20 01:39:04,245 AE_UNIGRAMA_2L_UNDER_02.py:151]: >> Loading dataset... 
[2017-10-20 01:39:04,851 AE_UNIGRAMA_2L_UNDER_02.py:55]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_deepnn/	
	trainx shape = (1627, 96)
	trainy shape = (1627, 9)
	valx shape = (1076, 96)
	valy shape = (1076, 9)
	=======================================
	
[2017-10-20 01:39:04,851 AE_UNIGRAMA_2L_UNDER_02.py:153]: >> Executing autoencoder part ... 
[2017-10-20 01:39:04,851 AE_UNIGRAMA_2L_UNDER_02.py:60]: =======================================
[2017-10-20 01:39:04,851 AE_UNIGRAMA_2L_UNDER_02.py:65]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f2e152957b8>, 'discard_decoder_function': True}
[2017-10-20 01:39:04,927 AE_UNIGRAMA_2L_UNDER_02.py:76]: training and evaluate autoencoder
[2017-10-20 01:39:30,525 AE_UNIGRAMA_2L_UNDER_02.py:88]: trained and evaluated!
[2017-10-20 01:39:30,526 AE_UNIGRAMA_2L_UNDER_02.py:91]: Training history: 
{'val_loss': [0.0099635653848652497, 0.0098954424518247069, 0.0097791461938763636, 0.0096532812612096612, 0.0095277395078628476, 0.0094046688545149047, 0.0092832926854657417, 0.0091655631082264016, 0.009052226236817118, 0.0089423795434954434, 0.0088360259953373863, 0.0087335549717332799, 0.0086351326774099495, 0.0085405583690754978, 0.0084496680058566611, 0.0083625795506633346, 0.0082791872066359101, 0.0081992002262917373, 0.0081223429485352289, 0.008048669905938402, 0.0079779788370496936, 0.0079100841611319094, 0.0078446510521397277, 0.0077819120723989594, 0.007721773895770865, 0.0076642422573329344, 0.0076090952011039934, 0.0075562519322540682, 0.0075055903142325058, 0.0074569162632231378, 0.007410146922676315, 0.0073649439524485277, 0.0073209753897054931, 0.0072780736517784319, 0.0072363656183850149, 0.0071961483416116368, 0.0071573648982608627, 0.0071198824121440213, 0.0070838684607366425, 0.0070491994135635713, 0.0070158250657599208, 0.0069836815057467798, 0.0069527037867576878, 0.0069228743383016947, 0.0068941100233937281, 0.0068663304245798785, 0.0068395124734500524, 0.0068136398753213621, 0.0067886671321431939, 0.006764536945073135, 0.0067412093053242977, 0.006718669706422379, 0.0066969443569617646, 0.0066759523582475113, 0.0066556507465343048, 0.0066359856193642852, 0.0066169652028431683, 0.0065985727487443552, 0.006580762196415308, 0.0065635259775617977, 0.0065468511644570801, 0.0065307360969927009, 0.0065151681798098259, 0.006500130250953055, 0.0064855651673774292, 0.0064714762752661028, 0.0064578699224998957, 0.0064447080391047172, 0.006431962086448661, 0.0064196399063370484, 0.0064077128216513471, 0.0063961542948914284, 0.0063849709630289485, 0.006374146354810567, 0.0063636525928780271, 0.0063534848163506802, 0.006343636782943759, 0.0063340903197494801, 0.0063248351770262739, 0.0063158541391551718, 0.0063071510768374327, 0.0062987208106731613, 0.0062905274830121535, 0.0062825744279199815, 0.0062748597083149343, 0.0062673648050726571, 0.0062600818348418378, 0.0062530076089678645, 0.0062461360582742546, 0.0062394555187247503, 0.0062329663174940089, 0.0062266558609542794, 0.0062205311028653808, 0.0062145646505722546, 0.0062087702990741532, 0.0062031338794579297, 0.0061976628803880017, 0.0061923274597270785, 0.0061871304408712899, 0.006182077983013427, 0.0061771504487496106, 0.0061723644305089814], 'loss': [0.009991458247941773, 0.0099404991793857807, 0.0098471914079743209, 0.0097234841017138397, 0.0095961937530867226, 0.0094706978678666676, 0.009347008512033116, 0.0092259386414690631, 0.0091090744919960463, 0.0089962692323387519, 0.0088866806635432768, 0.0087808617130242465, 0.0086790334493993399, 0.0085812044051174156, 0.0084871885795478226, 0.0083969142144865328, 0.0083104469575853383, 0.008227586649477335, 0.0081479804471281687, 0.0080715037267552105, 0.0079981628347465285, 0.007927699966474076, 0.0078598720086801438, 0.0077946730929449176, 0.007732168065250085, 0.007672264126198711, 0.0076149265312194167, 0.0075599533332867822, 0.0075072359348651307, 0.0074566325410907446, 0.0074080067418272975, 0.0073611725956893035, 0.0073156977119868752, 0.0072713825003356746, 0.0072282106620182307, 0.0071863897568955511, 0.0071460976750208106, 0.0071071934460970841, 0.0070696544341775046, 0.0070335755557435452, 0.0069988347042166501, 0.0069653770720933062, 0.0069331464402895074, 0.0069020959885233409, 0.00687216774268006, 0.006843283644247143, 0.0068153988844162679, 0.0067884788230821698, 0.0067624985383504107, 0.0067373961110077868, 0.0067131536616070837, 0.0066897028435500177, 0.0066670531603910104, 0.0066451925701921386, 0.0066240682809303965, 0.0066036229470049813, 0.006583812104202647, 0.0065646542933985325, 0.0065461032003274197, 0.0065281387749365595, 0.0065107519198014429, 0.0064939359797149916, 0.0064776792066754485, 0.0064619753172606249, 0.0064467812107547378, 0.0064320717766519217, 0.006417840024594907, 0.0064040920783015173, 0.0063907825018805304, 0.0063778888625931392, 0.0063654189318597095, 0.0063533358440174053, 0.0063416274738416177, 0.0063302971573790381, 0.0063193202008434285, 0.0063086780295870427, 0.0062983570570044889, 0.006288361393622756, 0.0062786667715339075, 0.0062692576244281128, 0.0062601300077793345, 0.0062512839928534989, 0.0062426926076444254, 0.0062343604798579264, 0.0062262494836549821, 0.0062183841588416382, 0.0062107411425013481, 0.0062033061522557718, 0.0061960859231641671, 0.0061890691136408963, 0.006182245100769956, 0.0061756130571224681, 0.0061691606425312157, 0.0061628957271585086, 0.0061567814264517472, 0.0061508465023652941, 0.0061450690127217702, 0.0061394613586508136, 0.0061339838830977407, 0.0061286479599897527, 0.0061234625628230647, 0.0061183959785471058]}
[2017-10-20 01:39:30,526 AE_UNIGRAMA_2L_UNDER_02.py:95]: done!
[2017-10-20 01:39:30,526 AE_UNIGRAMA_2L_UNDER_02.py:155]: >> Executing classifier part ... 
[2017-10-20 01:39:30,526 AE_UNIGRAMA_2L_UNDER_02.py:100]: =======================================
[2017-10-20 01:39:30,526 AE_UNIGRAMA_2L_UNDER_02.py:104]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f2e15295898>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-10-20 01:39:30,559 AE_UNIGRAMA_2L_UNDER_02.py:113]: training ... 
[2017-10-20 01:40:15,558 AE_UNIGRAMA_2L_UNDER_02.py:125]: trained!
[2017-10-20 01:40:15,559 AE_UNIGRAMA_2L_UNDER_02.py:128]: Training history: 
{'val_loss': [0.0099635653848652497, 0.0098954424518247069, 0.0097791461938763636, 0.0096532812612096612, 0.0095277395078628476, 0.0094046688545149047, 0.0092832926854657417, 0.0091655631082264016, 0.009052226236817118, 0.0089423795434954434, 0.0088360259953373863, 0.0087335549717332799, 0.0086351326774099495, 0.0085405583690754978, 0.0084496680058566611, 0.0083625795506633346, 0.0082791872066359101, 0.0081992002262917373, 0.0081223429485352289, 0.008048669905938402, 0.0079779788370496936, 0.0079100841611319094, 0.0078446510521397277, 0.0077819120723989594, 0.007721773895770865, 0.0076642422573329344, 0.0076090952011039934, 0.0075562519322540682, 0.0075055903142325058, 0.0074569162632231378, 0.007410146922676315, 0.0073649439524485277, 0.0073209753897054931, 0.0072780736517784319, 0.0072363656183850149, 0.0071961483416116368, 0.0071573648982608627, 0.0071198824121440213, 0.0070838684607366425, 0.0070491994135635713, 0.0070158250657599208, 0.0069836815057467798, 0.0069527037867576878, 0.0069228743383016947, 0.0068941100233937281, 0.0068663304245798785, 0.0068395124734500524, 0.0068136398753213621, 0.0067886671321431939, 0.006764536945073135, 0.0067412093053242977, 0.006718669706422379, 0.0066969443569617646, 0.0066759523582475113, 0.0066556507465343048, 0.0066359856193642852, 0.0066169652028431683, 0.0065985727487443552, 0.006580762196415308, 0.0065635259775617977, 0.0065468511644570801, 0.0065307360969927009, 0.0065151681798098259, 0.006500130250953055, 0.0064855651673774292, 0.0064714762752661028, 0.0064578699224998957, 0.0064447080391047172, 0.006431962086448661, 0.0064196399063370484, 0.0064077128216513471, 0.0063961542948914284, 0.0063849709630289485, 0.006374146354810567, 0.0063636525928780271, 0.0063534848163506802, 0.006343636782943759, 0.0063340903197494801, 0.0063248351770262739, 0.0063158541391551718, 0.0063071510768374327, 0.0062987208106731613, 0.0062905274830121535, 0.0062825744279199815, 0.0062748597083149343, 0.0062673648050726571, 0.0062600818348418378, 0.0062530076089678645, 0.0062461360582742546, 0.0062394555187247503, 0.0062329663174940089, 0.0062266558609542794, 0.0062205311028653808, 0.0062145646505722546, 0.0062087702990741532, 0.0062031338794579297, 0.0061976628803880017, 0.0061923274597270785, 0.0061871304408712899, 0.006182077983013427, 0.0061771504487496106, 0.0061723644305089814], 'loss': [0.009991458247941773, 0.0099404991793857807, 0.0098471914079743209, 0.0097234841017138397, 0.0095961937530867226, 0.0094706978678666676, 0.009347008512033116, 0.0092259386414690631, 0.0091090744919960463, 0.0089962692323387519, 0.0088866806635432768, 0.0087808617130242465, 0.0086790334493993399, 0.0085812044051174156, 0.0084871885795478226, 0.0083969142144865328, 0.0083104469575853383, 0.008227586649477335, 0.0081479804471281687, 0.0080715037267552105, 0.0079981628347465285, 0.007927699966474076, 0.0078598720086801438, 0.0077946730929449176, 0.007732168065250085, 0.007672264126198711, 0.0076149265312194167, 0.0075599533332867822, 0.0075072359348651307, 0.0074566325410907446, 0.0074080067418272975, 0.0073611725956893035, 0.0073156977119868752, 0.0072713825003356746, 0.0072282106620182307, 0.0071863897568955511, 0.0071460976750208106, 0.0071071934460970841, 0.0070696544341775046, 0.0070335755557435452, 0.0069988347042166501, 0.0069653770720933062, 0.0069331464402895074, 0.0069020959885233409, 0.00687216774268006, 0.006843283644247143, 0.0068153988844162679, 0.0067884788230821698, 0.0067624985383504107, 0.0067373961110077868, 0.0067131536616070837, 0.0066897028435500177, 0.0066670531603910104, 0.0066451925701921386, 0.0066240682809303965, 0.0066036229470049813, 0.006583812104202647, 0.0065646542933985325, 0.0065461032003274197, 0.0065281387749365595, 0.0065107519198014429, 0.0064939359797149916, 0.0064776792066754485, 0.0064619753172606249, 0.0064467812107547378, 0.0064320717766519217, 0.006417840024594907, 0.0064040920783015173, 0.0063907825018805304, 0.0063778888625931392, 0.0063654189318597095, 0.0063533358440174053, 0.0063416274738416177, 0.0063302971573790381, 0.0063193202008434285, 0.0063086780295870427, 0.0062983570570044889, 0.006288361393622756, 0.0062786667715339075, 0.0062692576244281128, 0.0062601300077793345, 0.0062512839928534989, 0.0062426926076444254, 0.0062343604798579264, 0.0062262494836549821, 0.0062183841588416382, 0.0062107411425013481, 0.0062033061522557718, 0.0061960859231641671, 0.0061890691136408963, 0.006182245100769956, 0.0061756130571224681, 0.0061691606425312157, 0.0061628957271585086, 0.0061567814264517472, 0.0061508465023652941, 0.0061450690127217702, 0.0061394613586508136, 0.0061339838830977407, 0.0061286479599897527, 0.0061234625628230647, 0.0061183959785471058]}
[2017-10-20 01:40:15,559 AE_UNIGRAMA_2L_UNDER_02.py:132]: evaluating model ... 
[2017-10-20 01:40:15,599 AE_UNIGRAMA_2L_UNDER_02.py:136]: evaluated! 
[2017-10-20 01:40:15,599 AE_UNIGRAMA_2L_UNDER_02.py:138]: generating reports ... 
[2017-10-20 01:40:16,247 AE_UNIGRAMA_2L_UNDER_02.py:141]: done!
[2017-10-20 01:40:16,247 AE_UNIGRAMA_2L_UNDER_02.py:157]: >> experiment AE_UNIGRAMA_2L_UNDER_02 finished!
