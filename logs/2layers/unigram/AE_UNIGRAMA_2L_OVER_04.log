[2017-10-20 01:40:32,605 AE_UNIGRAMA_2L_OVER_04.py:148]: >> Initializing execution of experiment AE_UNIGRAMA_2L_OVER_04
[2017-10-20 01:40:32,606 AE_UNIGRAMA_2L_OVER_04.py:149]: >> Printing header log
[2017-10-20 01:40:32,606 AE_UNIGRAMA_2L_OVER_04.py:38]: 
	=======================================
	network_name = AE_UNIGRAMA_2L_OVER_04
	layers = 96,134,122,9
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/2layers/unigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/2layers/unigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/2layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/2layers/unigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/2layers/unigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/2layers/unigram/', 'data_dir': '/home/dhiegorp/malware_deepnn/', 'fullds_data_dir': '/home/dhiegorp/malware_deepnn/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f22fbe2bb38>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f22fbe2bc18>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-10-20 01:40:32,606 AE_UNIGRAMA_2L_OVER_04.py:151]: >> Loading dataset... 
[2017-10-20 01:40:33,179 AE_UNIGRAMA_2L_OVER_04.py:55]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_deepnn/	
	trainx shape = (1627, 96)
	trainy shape = (1627, 9)
	valx shape = (1076, 96)
	valy shape = (1076, 9)
	=======================================
	
[2017-10-20 01:40:33,179 AE_UNIGRAMA_2L_OVER_04.py:153]: >> Executing autoencoder part ... 
[2017-10-20 01:40:33,179 AE_UNIGRAMA_2L_OVER_04.py:60]: =======================================
[2017-10-20 01:40:33,179 AE_UNIGRAMA_2L_OVER_04.py:65]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f22fbe2bb38>, 'discard_decoder_function': True}
[2017-10-20 01:40:33,252 AE_UNIGRAMA_2L_OVER_04.py:76]: training and evaluate autoencoder
[2017-10-20 01:41:05,122 AE_UNIGRAMA_2L_OVER_04.py:88]: trained and evaluated!
[2017-10-20 01:41:05,123 AE_UNIGRAMA_2L_OVER_04.py:91]: Training history: 
{'val_loss': [0.0098706254991789283, 0.0097186553627806524, 0.0095698614068135453, 0.0094245543121793014, 0.0092841592239956867, 0.0091484424945479439, 0.0090180097189368376, 0.0088928786996184221, 0.0087726294793548636, 0.0086573389147626429, 0.0085467106236844261, 0.0084403370295869379, 0.0083382564621066954, 0.0082404355615460518, 0.0081466707837210498, 0.0080568412782755922, 0.0079707305144421665, 0.0078881412734135604, 0.0078093422417525463, 0.0077341507590587034, 0.0076624508445119989, 0.0075939698075550193, 0.0075285115095762517, 0.0074660580598344146, 0.0074063598361821864, 0.0073493571733868916, 0.0072948363606149822, 0.0072426582257294524, 0.0071927413974497397, 0.0071450247218119165, 0.0070993780386963074, 0.0070555545389652252, 0.0070133828402230286, 0.0069726880321272239, 0.0069335447347352499, 0.0068959447704117095, 0.0068600711492357641, 0.0068259567998654336, 0.0067934982433610468, 0.0067626271781175773, 0.0067333209252185763, 0.0067054172614277735, 0.0066788740227601345, 0.0066536447019437429, 0.0066296227594844475, 0.0066067259672156721, 0.0065849555074670059, 0.0065642289685770927, 0.0065445012783417024, 0.0065257144835691025, 0.0065078425176137, 0.0064907966157400692, 0.0064745499530883309, 0.0064590367346614268, 0.0064442334056149849, 0.0064300682991189143, 0.0064165300713872825, 0.0064035512309618377, 0.0063911341355774257, 0.0063792370470953921, 0.0063678527140096659, 0.0063569421609898265, 0.0063464488560631598, 0.0063363875134339123, 0.0063267634890724293, 0.0063175569032232334, 0.0063087420510370496, 0.0063003008357737367, 0.0062921972907248706, 0.0062843986984245396, 0.0062769271994224272, 0.0062697353585862096, 0.0062628324856794674, 0.006256191179391731, 0.0062498001513317169, 0.006243619292309736, 0.0062376518429128874, 0.006231856532394886, 0.0062262195674875637, 0.0062207612953387673, 0.0062154821781476195, 0.006210375871495465, 0.0062054396973971323, 0.0062006623380329307, 0.006196040957889149, 0.0061915608254507126, 0.0061872269989417163, 0.0061830177809696881, 0.0061789226500130493, 0.006174916297640499, 0.0061710125257756184, 0.006167187622045496, 0.006163459054807194, 0.0061598041347577671, 0.0061562404670098235, 0.0061527748097108198, 0.0061494032350337636, 0.0061461141551100414, 0.0061429094412402147, 0.0061397861782807621, 0.0061367433604725451, 0.0061337831049298932], 'loss': [0.0099408725985241728, 0.0097860377419284311, 0.0096335815513058903, 0.0094845460512582042, 0.0093397154898074188, 0.0091996949970877904, 0.0090647961488697672, 0.0089352624679754755, 0.0088107779361055755, 0.0086913235347028817, 0.0085766893744551112, 0.0084665780846938515, 0.0083607863480272149, 0.0082593607431771052, 0.0081621756383344381, 0.0080690536282832966, 0.0079798080564030642, 0.0078942324412664931, 0.0078122942378148026, 0.0077341068398663068, 0.0076594427144973465, 0.0075881779477224628, 0.0075200894674642286, 0.0074550283253627938, 0.0073928988636565779, 0.0073334621245495987, 0.007276675085747213, 0.0072223056292650186, 0.0071702647033206474, 0.0071204470922184744, 0.0070727850893225015, 0.0070271264986646233, 0.0069831985984279213, 0.006940876677249957, 0.006900084555863455, 0.0068608444786820042, 0.0068232254880768398, 0.0067873754053185999, 0.0067532521815078716, 0.0067207694638087732, 0.0066898674727031149, 0.0066604836553119957, 0.0066324863491262773, 0.0066058273766631719, 0.0065804633037739538, 0.0065562868838272325, 0.0065332364895425111, 0.0065112972964624831, 0.0064904018249145184, 0.0064704882598653228, 0.0064515044189160705, 0.0064334238183323452, 0.006416183922693967, 0.0063997073446474305, 0.0063839761809902137, 0.0063689559836629791, 0.0063545586221886328, 0.0063407768566360592, 0.0063275497108491825, 0.006314929367226824, 0.0063028135242486705, 0.0062912244134489276, 0.0062801013952527556, 0.0062694153905867361, 0.0062591775512802466, 0.0062493707577697356, 0.00623997248286758, 0.0062309889813751147, 0.0062223539507954163, 0.006214072859142555, 0.0062060988240273691, 0.0061984437059315715, 0.0061910803270650442, 0.0061839999995676924, 0.0061771757210952304, 0.0061706064017655552, 0.0061642343619725017, 0.0061580806025535728, 0.0061520928146057722, 0.0061462817432561493, 0.0061406689903801322, 0.0061352354524166, 0.0061299791739888362, 0.0061249010895677121, 0.0061199817086381638, 0.0061152218766602076, 0.0061106026128561822, 0.0061061248657211763, 0.0061017780850361521, 0.0060975462242333564, 0.00609339629176869, 0.0060893577170348196, 0.0060854020949794273, 0.0060815509089876081, 0.0060777739737899522, 0.0060740878279466866, 0.0060705075291750728, 0.0060670363462627958, 0.0060636513928305205, 0.0060603565063622918, 0.0060571439769725784, 0.0060540165886118066]}
[2017-10-20 01:41:05,123 AE_UNIGRAMA_2L_OVER_04.py:95]: done!
[2017-10-20 01:41:05,123 AE_UNIGRAMA_2L_OVER_04.py:155]: >> Executing classifier part ... 
[2017-10-20 01:41:05,123 AE_UNIGRAMA_2L_OVER_04.py:100]: =======================================
[2017-10-20 01:41:05,123 AE_UNIGRAMA_2L_OVER_04.py:104]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f22fbe2bc18>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-10-20 01:41:05,156 AE_UNIGRAMA_2L_OVER_04.py:113]: training ... 
[2017-10-20 01:41:59,617 AE_UNIGRAMA_2L_OVER_04.py:125]: trained!
[2017-10-20 01:41:59,618 AE_UNIGRAMA_2L_OVER_04.py:128]: Training history: 
{'val_loss': [0.0098706254991789283, 0.0097186553627806524, 0.0095698614068135453, 0.0094245543121793014, 0.0092841592239956867, 0.0091484424945479439, 0.0090180097189368376, 0.0088928786996184221, 0.0087726294793548636, 0.0086573389147626429, 0.0085467106236844261, 0.0084403370295869379, 0.0083382564621066954, 0.0082404355615460518, 0.0081466707837210498, 0.0080568412782755922, 0.0079707305144421665, 0.0078881412734135604, 0.0078093422417525463, 0.0077341507590587034, 0.0076624508445119989, 0.0075939698075550193, 0.0075285115095762517, 0.0074660580598344146, 0.0074063598361821864, 0.0073493571733868916, 0.0072948363606149822, 0.0072426582257294524, 0.0071927413974497397, 0.0071450247218119165, 0.0070993780386963074, 0.0070555545389652252, 0.0070133828402230286, 0.0069726880321272239, 0.0069335447347352499, 0.0068959447704117095, 0.0068600711492357641, 0.0068259567998654336, 0.0067934982433610468, 0.0067626271781175773, 0.0067333209252185763, 0.0067054172614277735, 0.0066788740227601345, 0.0066536447019437429, 0.0066296227594844475, 0.0066067259672156721, 0.0065849555074670059, 0.0065642289685770927, 0.0065445012783417024, 0.0065257144835691025, 0.0065078425176137, 0.0064907966157400692, 0.0064745499530883309, 0.0064590367346614268, 0.0064442334056149849, 0.0064300682991189143, 0.0064165300713872825, 0.0064035512309618377, 0.0063911341355774257, 0.0063792370470953921, 0.0063678527140096659, 0.0063569421609898265, 0.0063464488560631598, 0.0063363875134339123, 0.0063267634890724293, 0.0063175569032232334, 0.0063087420510370496, 0.0063003008357737367, 0.0062921972907248706, 0.0062843986984245396, 0.0062769271994224272, 0.0062697353585862096, 0.0062628324856794674, 0.006256191179391731, 0.0062498001513317169, 0.006243619292309736, 0.0062376518429128874, 0.006231856532394886, 0.0062262195674875637, 0.0062207612953387673, 0.0062154821781476195, 0.006210375871495465, 0.0062054396973971323, 0.0062006623380329307, 0.006196040957889149, 0.0061915608254507126, 0.0061872269989417163, 0.0061830177809696881, 0.0061789226500130493, 0.006174916297640499, 0.0061710125257756184, 0.006167187622045496, 0.006163459054807194, 0.0061598041347577671, 0.0061562404670098235, 0.0061527748097108198, 0.0061494032350337636, 0.0061461141551100414, 0.0061429094412402147, 0.0061397861782807621, 0.0061367433604725451, 0.0061337831049298932], 'loss': [0.0099408725985241728, 0.0097860377419284311, 0.0096335815513058903, 0.0094845460512582042, 0.0093397154898074188, 0.0091996949970877904, 0.0090647961488697672, 0.0089352624679754755, 0.0088107779361055755, 0.0086913235347028817, 0.0085766893744551112, 0.0084665780846938515, 0.0083607863480272149, 0.0082593607431771052, 0.0081621756383344381, 0.0080690536282832966, 0.0079798080564030642, 0.0078942324412664931, 0.0078122942378148026, 0.0077341068398663068, 0.0076594427144973465, 0.0075881779477224628, 0.0075200894674642286, 0.0074550283253627938, 0.0073928988636565779, 0.0073334621245495987, 0.007276675085747213, 0.0072223056292650186, 0.0071702647033206474, 0.0071204470922184744, 0.0070727850893225015, 0.0070271264986646233, 0.0069831985984279213, 0.006940876677249957, 0.006900084555863455, 0.0068608444786820042, 0.0068232254880768398, 0.0067873754053185999, 0.0067532521815078716, 0.0067207694638087732, 0.0066898674727031149, 0.0066604836553119957, 0.0066324863491262773, 0.0066058273766631719, 0.0065804633037739538, 0.0065562868838272325, 0.0065332364895425111, 0.0065112972964624831, 0.0064904018249145184, 0.0064704882598653228, 0.0064515044189160705, 0.0064334238183323452, 0.006416183922693967, 0.0063997073446474305, 0.0063839761809902137, 0.0063689559836629791, 0.0063545586221886328, 0.0063407768566360592, 0.0063275497108491825, 0.006314929367226824, 0.0063028135242486705, 0.0062912244134489276, 0.0062801013952527556, 0.0062694153905867361, 0.0062591775512802466, 0.0062493707577697356, 0.00623997248286758, 0.0062309889813751147, 0.0062223539507954163, 0.006214072859142555, 0.0062060988240273691, 0.0061984437059315715, 0.0061910803270650442, 0.0061839999995676924, 0.0061771757210952304, 0.0061706064017655552, 0.0061642343619725017, 0.0061580806025535728, 0.0061520928146057722, 0.0061462817432561493, 0.0061406689903801322, 0.0061352354524166, 0.0061299791739888362, 0.0061249010895677121, 0.0061199817086381638, 0.0061152218766602076, 0.0061106026128561822, 0.0061061248657211763, 0.0061017780850361521, 0.0060975462242333564, 0.00609339629176869, 0.0060893577170348196, 0.0060854020949794273, 0.0060815509089876081, 0.0060777739737899522, 0.0060740878279466866, 0.0060705075291750728, 0.0060670363462627958, 0.0060636513928305205, 0.0060603565063622918, 0.0060571439769725784, 0.0060540165886118066]}
[2017-10-20 01:41:59,618 AE_UNIGRAMA_2L_OVER_04.py:132]: evaluating model ... 
[2017-10-20 01:41:59,660 AE_UNIGRAMA_2L_OVER_04.py:136]: evaluated! 
[2017-10-20 01:41:59,660 AE_UNIGRAMA_2L_OVER_04.py:138]: generating reports ... 
[2017-10-20 01:42:00,310 AE_UNIGRAMA_2L_OVER_04.py:141]: done!
[2017-10-20 01:42:00,310 AE_UNIGRAMA_2L_OVER_04.py:157]: >> experiment AE_UNIGRAMA_2L_OVER_04 finished!
[2017-10-20 01:42:00,310 AE_UNIGRAMA_2L_OVER_04.py:309]: The experiment AE_UNIGRAMA_2L_OVER_04 was already executed!
