[2018-05-28 01:22:51,100 AE_BIGRAMA_2L_MINIDS_OVER_03.py:145]: >> Initializing execution of experiment AE_BIGRAMA_2L_MINIDS_OVER_03
[2018-05-28 01:22:51,100 AE_BIGRAMA_2L_MINIDS_OVER_03.py:146]: >> Printing header log
[2018-05-28 01:22:51,100 AE_BIGRAMA_2L_MINIDS_OVER_03.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_2L_MINIDS_OVER_03
	layers = 9216,15667,14101
	using GLOBAL obj = 
		{'store_history': True, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'batch': 32, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/2layers/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/2layers/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/2layers/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/2layers/bigram/', 'numpy_seed': 666, 'mlp_configs': {'optimizer': <keras.optimizers.SGD object at 0x7f314c380908>, 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid', 'classifier_dim': 9}, 'epochs': 200, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/2layers/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/2layers/bigram/fullds/', 'data_dir': '/home/dhiego/malware_dataset/', 'shuffle_batches': True, 'autoencoder_configs': {'loss_function': 'mse', 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f314c380898>}}
	=======================================
	
[2018-05-28 01:22:51,100 AE_BIGRAMA_2L_MINIDS_OVER_03.py:148]: >> Loading dataset... 
[2018-05-28 01:23:09,225 AE_BIGRAMA_2L_MINIDS_OVER_03.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-05-28 01:23:09,225 AE_BIGRAMA_2L_MINIDS_OVER_03.py:150]: >> Executing autoencoder part ... 
[2018-05-28 01:23:09,225 AE_BIGRAMA_2L_MINIDS_OVER_03.py:57]: =======================================
[2018-05-28 01:23:09,225 AE_BIGRAMA_2L_MINIDS_OVER_03.py:62]: setting configurations for autoencoder: 
	 {'loss_function': 'mse', 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f314c380898>}
[2018-05-28 01:23:09,297 AE_BIGRAMA_2L_MINIDS_OVER_03.py:73]: training and evaluate autoencoder
[2018-05-28 01:27:03,963 AE_BIGRAMA_2L_MINIDS_OVER_03.py:145]: >> Initializing execution of experiment AE_BIGRAMA_2L_MINIDS_OVER_03
[2018-05-28 01:27:03,963 AE_BIGRAMA_2L_MINIDS_OVER_03.py:146]: >> Printing header log
[2018-05-28 01:27:03,963 AE_BIGRAMA_2L_MINIDS_OVER_03.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_2L_MINIDS_OVER_03
	layers = 9216,15667,14101
	using GLOBAL obj = 
		{'shuffle_batches': True, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'batch': 32, 'store_history': True, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/2layers/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/2layers/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/2layers/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'data_dir': '/home/dhiego/malware_dataset/', 'mlp_configs': {'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7fab7c48e908>, 'activation': 'sigmoid'}, 'log_dir': '/home/dhiego/deepnn/logs/2layers/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/2layers/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/2layers/bigram/fullds/', 'numpy_seed': 666, 'autoencoder_configs': {'discard_decoder_function': True, 'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fab7c48e898>}, 'epochs': 200}
	=======================================
	
[2018-05-28 01:27:03,963 AE_BIGRAMA_2L_MINIDS_OVER_03.py:148]: >> Loading dataset... 
[2018-05-28 01:27:22,058 AE_BIGRAMA_2L_MINIDS_OVER_03.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-05-28 01:27:22,059 AE_BIGRAMA_2L_MINIDS_OVER_03.py:150]: >> Executing autoencoder part ... 
[2018-05-28 01:27:22,059 AE_BIGRAMA_2L_MINIDS_OVER_03.py:57]: =======================================
[2018-05-28 01:27:22,059 AE_BIGRAMA_2L_MINIDS_OVER_03.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fab7c48e898>}
[2018-05-28 01:27:22,132 AE_BIGRAMA_2L_MINIDS_OVER_03.py:73]: training and evaluate autoencoder
[2018-05-28 17:15:10,592 AE_BIGRAMA_2L_MINIDS_OVER_03.py:85]: trained and evaluated!
[2018-05-28 17:15:10,593 AE_BIGRAMA_2L_MINIDS_OVER_03.py:88]: Training history: 
{'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00010751866254783372, 0.0001075143331397516, 0.0001075098868186437, 0.00010750530516946421, 0.00010750064621027256, 0.00010749596739032364, 0.00010749116762831703, 0.00010748625358400551, 0.00010748132347096674, 0.00010747636612641234, 0.0001074712456771664, 0.00010746601030571046, 0.00010746068354633088, 0.00010745530796456469, 0.00010744984703842271, 0.00010744429496135896, 0.00010743873307241752, 0.00010743311226480954, 0.00010742742587878226, 0.00010742174318998427, 0.00010741599997090697, 0.00010741018394485317, 0.00010740435938673179, 0.00010739856232082812, 0.0001073926867125025, 0.00010738677346827886, 0.00010738082164014972, 0.00010737485526010534, 0.00010736884091088111, 0.00010736281755558557, 0.00010735680467577299, 0.00010735076212332545, 0.0001073446886184328, 0.00010733858292868531, 0.00010733247060288527, 0.00010732635901179103, 0.000107320255905364, 0.00010731408665171317, 0.00010730793849122939, 0.00010730177366951365, 0.00010729564202806066, 0.00010728947230040608, 0.00010728327159660623, 0.0001072770939056886, 0.00010727090813300697, 0.00010726473684114, 0.00010725856360585766, 0.00010725235221327316, 0.00010724611946681957, 0.00010723992819569441, 0.00010723370495301608, 0.00010722749626225298, 0.0001072213090438599, 0.00010721510272311556, 0.00010720891266069997, 0.00010720272942393843, 0.0001071965392193217, 0.00010719032827704077, 0.00010718409325536917, 0.00010717790707978435, 0.00010717172995767122, 0.00010716555485007404, 0.00010715935137335223, 0.0001071531684920935, 0.00010714698101299835, 0.0001071407780102803, 0.00010713456228056146, 0.0001071283488971612, 0.00010712216935762892, 0.00010711599292282124, 0.00010710977482308363, 0.00010710355058499743, 0.00010709732971233786, 0.00010709109049573304, 0.00010708490650056548, 0.0001070787106553041, 0.00010707250732078341, 0.00010706628170808631, 0.00010706004483780008, 0.0001070538324498077, 0.00010704759304360138, 0.00010704142665767327, 0.00010703524410821715, 0.00010702901076925888, 0.00010702280397451079, 0.00010701660023708692, 0.00010701038865490091, 0.0001070041953692598, 0.0001069979651350261, 0.00010699173480599167, 0.00010698553322528486, 0.00010697935629277323, 0.00010697316002090846, 0.00010696696858388198, 0.00010696071083373041, 0.00010695450951372568, 0.00010694827752567809, 0.00010694209184779722, 0.00010693587405616205, 0.00010692965844494415, 0.00010692340387061773], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.0001071178088777273, 0.00010711347433771019, 0.0001071089929556394, 0.00010710443770561565, 0.00010709984793501657, 0.00010709515597922407, 0.00010709036518124571, 0.00010708556900227658, 0.00010708073955412534, 0.00010707577806611429, 0.00010707071830798284, 0.00010706556912886862, 0.00010706038201466336, 0.00010705508438456135, 0.00010704970394798351, 0.00010704429326344428, 0.00010703884832648542, 0.00010703333154167975, 0.00010702780324405663, 0.00010702222090412425, 0.00010701657120348355, 0.00010701090124815002, 0.00010700524829388699, 0.0001069995276571288, 0.00010699375439034798, 0.00010698793190806024, 0.00010698208514874439, 0.00010697621706211298, 0.00010697030467616224, 0.00010696439523992406, 0.00010695844640267706, 0.00010695244876109521, 0.00010694641581234803, 0.00010694036725693994, 0.00010693431829035978, 0.00010692826598077207, 0.00010692214361104213, 0.00010691605222223244, 0.00010690994227704929, 0.0001069038624189341, 0.00010689777144129647, 0.00010689164907156652, 0.00010688554910177489, 0.00010687943924597696, 0.00010687334614097088, 0.00010686725344713686, 0.0001068611211020154, 0.00010685496848432407, 0.00010684885993355048, 0.00010684271459181677, 0.00010683658846790722, 0.00010683047991713363, 0.00010682434741111875, 0.00010681823494527215, 0.0001068121241062367, 0.00010680600738565321, 0.00010679987259137648, 0.00010679371679157101, 0.00010678761102961656, 0.00010678150959390717, 0.00010677541470119651, 0.00010676929478062184, 0.00010676319146782266, 0.00010675708447235204, 0.00010675095761548362, 0.00010674481776200295, 0.00010673867660349793, 0.00010673257198567441, 0.00010672646867287522, 0.00010672032252667444, 0.00010671416354475483, 0.00010670801437733333, 0.00010670185237419299, 0.00010669574350163258, 0.00010668962073860763, 0.00010668349527614878, 0.00010667734528638317, 0.00010667118541061127, 0.00010666504907890862, 0.00010665888600314553, 0.0001066527939707622, 0.00010664668543786565, 0.00010664053201570727, 0.00010663440353202768, 0.00010662827839135566, 0.00010662214565293918, 0.00010661603425971531, 0.00010660988696938362, 0.0001066037393572651, 0.00010659761487804377, 0.00010659151950265287, 0.00010658540582116717, 0.0001065792973597788, 0.00010657312734772198, 0.00010656700941149937, 0.00010656085909994694, 0.0001065547587368603, 0.00010654862142192012, 0.00010654248918406093, 0.0001065363200658564, 0.0001065302040067236]}
[2018-05-28 17:15:10,594 AE_BIGRAMA_2L_MINIDS_OVER_03.py:92]: done!
[2018-05-28 17:15:10,594 AE_BIGRAMA_2L_MINIDS_OVER_03.py:152]: >> Executing classifier part ... 
[2018-05-28 17:15:10,594 AE_BIGRAMA_2L_MINIDS_OVER_03.py:97]: =======================================
[2018-05-28 17:15:10,594 AE_BIGRAMA_2L_MINIDS_OVER_03.py:101]: setting configurations for classifier: 
	 {'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7fab7c48e908>, 'activation': 'sigmoid'}
[2018-05-28 17:15:10,964 AE_BIGRAMA_2L_MINIDS_OVER_03.py:110]: training ... 
[2018-05-29 07:28:00,183 AE_BIGRAMA_2L_MINIDS_OVER_03.py:122]: trained!
[2018-05-29 07:28:00,185 AE_BIGRAMA_2L_MINIDS_OVER_03.py:125]: Training history: 
{'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00010751866254783372, 0.0001075143331397516, 0.0001075098868186437, 0.00010750530516946421, 0.00010750064621027256, 0.00010749596739032364, 0.00010749116762831703, 0.00010748625358400551, 0.00010748132347096674, 0.00010747636612641234, 0.0001074712456771664, 0.00010746601030571046, 0.00010746068354633088, 0.00010745530796456469, 0.00010744984703842271, 0.00010744429496135896, 0.00010743873307241752, 0.00010743311226480954, 0.00010742742587878226, 0.00010742174318998427, 0.00010741599997090697, 0.00010741018394485317, 0.00010740435938673179, 0.00010739856232082812, 0.0001073926867125025, 0.00010738677346827886, 0.00010738082164014972, 0.00010737485526010534, 0.00010736884091088111, 0.00010736281755558557, 0.00010735680467577299, 0.00010735076212332545, 0.0001073446886184328, 0.00010733858292868531, 0.00010733247060288527, 0.00010732635901179103, 0.000107320255905364, 0.00010731408665171317, 0.00010730793849122939, 0.00010730177366951365, 0.00010729564202806066, 0.00010728947230040608, 0.00010728327159660623, 0.0001072770939056886, 0.00010727090813300697, 0.00010726473684114, 0.00010725856360585766, 0.00010725235221327316, 0.00010724611946681957, 0.00010723992819569441, 0.00010723370495301608, 0.00010722749626225298, 0.0001072213090438599, 0.00010721510272311556, 0.00010720891266069997, 0.00010720272942393843, 0.0001071965392193217, 0.00010719032827704077, 0.00010718409325536917, 0.00010717790707978435, 0.00010717172995767122, 0.00010716555485007404, 0.00010715935137335223, 0.0001071531684920935, 0.00010714698101299835, 0.0001071407780102803, 0.00010713456228056146, 0.0001071283488971612, 0.00010712216935762892, 0.00010711599292282124, 0.00010710977482308363, 0.00010710355058499743, 0.00010709732971233786, 0.00010709109049573304, 0.00010708490650056548, 0.0001070787106553041, 0.00010707250732078341, 0.00010706628170808631, 0.00010706004483780008, 0.0001070538324498077, 0.00010704759304360138, 0.00010704142665767327, 0.00010703524410821715, 0.00010702901076925888, 0.00010702280397451079, 0.00010701660023708692, 0.00010701038865490091, 0.0001070041953692598, 0.0001069979651350261, 0.00010699173480599167, 0.00010698553322528486, 0.00010697935629277323, 0.00010697316002090846, 0.00010696696858388198, 0.00010696071083373041, 0.00010695450951372568, 0.00010694827752567809, 0.00010694209184779722, 0.00010693587405616205, 0.00010692965844494415, 0.00010692340387061773], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.0001071178088777273, 0.00010711347433771019, 0.0001071089929556394, 0.00010710443770561565, 0.00010709984793501657, 0.00010709515597922407, 0.00010709036518124571, 0.00010708556900227658, 0.00010708073955412534, 0.00010707577806611429, 0.00010707071830798284, 0.00010706556912886862, 0.00010706038201466336, 0.00010705508438456135, 0.00010704970394798351, 0.00010704429326344428, 0.00010703884832648542, 0.00010703333154167975, 0.00010702780324405663, 0.00010702222090412425, 0.00010701657120348355, 0.00010701090124815002, 0.00010700524829388699, 0.0001069995276571288, 0.00010699375439034798, 0.00010698793190806024, 0.00010698208514874439, 0.00010697621706211298, 0.00010697030467616224, 0.00010696439523992406, 0.00010695844640267706, 0.00010695244876109521, 0.00010694641581234803, 0.00010694036725693994, 0.00010693431829035978, 0.00010692826598077207, 0.00010692214361104213, 0.00010691605222223244, 0.00010690994227704929, 0.0001069038624189341, 0.00010689777144129647, 0.00010689164907156652, 0.00010688554910177489, 0.00010687943924597696, 0.00010687334614097088, 0.00010686725344713686, 0.0001068611211020154, 0.00010685496848432407, 0.00010684885993355048, 0.00010684271459181677, 0.00010683658846790722, 0.00010683047991713363, 0.00010682434741111875, 0.00010681823494527215, 0.0001068121241062367, 0.00010680600738565321, 0.00010679987259137648, 0.00010679371679157101, 0.00010678761102961656, 0.00010678150959390717, 0.00010677541470119651, 0.00010676929478062184, 0.00010676319146782266, 0.00010675708447235204, 0.00010675095761548362, 0.00010674481776200295, 0.00010673867660349793, 0.00010673257198567441, 0.00010672646867287522, 0.00010672032252667444, 0.00010671416354475483, 0.00010670801437733333, 0.00010670185237419299, 0.00010669574350163258, 0.00010668962073860763, 0.00010668349527614878, 0.00010667734528638317, 0.00010667118541061127, 0.00010666504907890862, 0.00010665888600314553, 0.0001066527939707622, 0.00010664668543786565, 0.00010664053201570727, 0.00010663440353202768, 0.00010662827839135566, 0.00010662214565293918, 0.00010661603425971531, 0.00010660988696938362, 0.0001066037393572651, 0.00010659761487804377, 0.00010659151950265287, 0.00010658540582116717, 0.0001065792973597788, 0.00010657312734772198, 0.00010656700941149937, 0.00010656085909994694, 0.0001065547587368603, 0.00010654862142192012, 0.00010654248918406093, 0.0001065363200658564, 0.0001065302040067236]}
[2018-05-29 07:28:00,186 AE_BIGRAMA_2L_MINIDS_OVER_03.py:129]: evaluating model ... 
[2018-05-29 07:28:20,942 AE_BIGRAMA_2L_MINIDS_OVER_03.py:133]: evaluated! 
[2018-05-29 07:28:20,943 AE_BIGRAMA_2L_MINIDS_OVER_03.py:135]: generating reports ... 
[2018-05-29 07:28:24,004 AE_BIGRAMA_2L_MINIDS_OVER_03.py:138]: done!
[2018-05-29 07:28:24,004 AE_BIGRAMA_2L_MINIDS_OVER_03.py:154]: >> experiment AE_BIGRAMA_2L_MINIDS_OVER_03 finished!
