[2018-05-28 01:22:51,146 AE_BIGRAMA_2L_FULLDS_OVER_03.py:145]: >> Initializing execution of experiment AE_BIGRAMA_2L_FULLDS_OVER_03
[2018-05-28 01:22:51,146 AE_BIGRAMA_2L_FULLDS_OVER_03.py:146]: >> Printing header log
[2018-05-28 01:22:51,146 AE_BIGRAMA_2L_FULLDS_OVER_03.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_2L_FULLDS_OVER_03
	layers = 9216,15667,14101
	using GLOBAL obj = 
		{'autoencoder_configs': {'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f2c754eb898>, 'output_layer_activation': 'relu'}, 'mlp_configs': {'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7f2c754eb908>}, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/2layers/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/2layers/bigram/fullds/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/2layers/bigram/', 'store_history': True, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiego/deepnn/logs/2layers/bigram/', 'numpy_seed': 666, 'batch': 32, 'shuffle_batches': True, 'data_dir': '/home/dhiego/malware_dataset/', 'executed_path': '/home/dhiego/deepnn/executed/2layers/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/2layers/bigram/'}
	=======================================
	
[2018-05-28 01:22:51,146 AE_BIGRAMA_2L_FULLDS_OVER_03.py:148]: >> Loading dataset... 
[2018-05-28 01:24:55,522 AE_BIGRAMA_2L_FULLDS_OVER_03.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (8147, 9216)
	trainy shape = (8147, 9)
	valx shape = (2721, 9216)
	valy shape = (2721, 9)
	=======================================
	
[2018-05-28 01:24:55,523 AE_BIGRAMA_2L_FULLDS_OVER_03.py:150]: >> Executing autoencoder part ... 
[2018-05-28 01:24:55,523 AE_BIGRAMA_2L_FULLDS_OVER_03.py:57]: =======================================
[2018-05-28 01:24:55,523 AE_BIGRAMA_2L_FULLDS_OVER_03.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f2c754eb898>, 'output_layer_activation': 'relu'}
[2018-05-28 01:24:55,709 AE_BIGRAMA_2L_FULLDS_OVER_03.py:73]: training and evaluate autoencoder
[2018-05-28 01:27:03,969 AE_BIGRAMA_2L_FULLDS_OVER_03.py:145]: >> Initializing execution of experiment AE_BIGRAMA_2L_FULLDS_OVER_03
[2018-05-28 01:27:03,969 AE_BIGRAMA_2L_FULLDS_OVER_03.py:146]: >> Printing header log
[2018-05-28 01:27:03,969 AE_BIGRAMA_2L_FULLDS_OVER_03.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_2L_FULLDS_OVER_03
	layers = 9216,15667,14101
	using GLOBAL obj = 
		{'fullds_data_dir': '/home/dhiego/malware_dataset/', 'autoencoder_configs': {'loss_function': 'mse', 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fbdeea44898>, 'hidden_layer_activation': 'relu'}, 'shuffle_batches': True, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/2layers/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/2layers/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'batch': 32, 'executed_path': '/home/dhiego/deepnn/executed/2layers/bigram/', 'mlp_configs': {'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'optimizer': <keras.optimizers.SGD object at 0x7fbdeea44908>, 'activation': 'sigmoid'}, 'numpy_seed': 666, 'reports_dir': '/home/dhiego/deepnn/reports/2layers/bigram/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/2layers/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/2layers/bigram/fullds/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'data_dir': '/home/dhiego/malware_dataset/', 'store_history': True, 'epochs': 1000}
	=======================================
	
[2018-05-28 01:27:03,969 AE_BIGRAMA_2L_FULLDS_OVER_03.py:148]: >> Loading dataset... 
[2018-05-28 01:33:46,082 AE_BIGRAMA_2L_FULLDS_OVER_03.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (8147, 9216)
	trainy shape = (8147, 9)
	valx shape = (2721, 9216)
	valy shape = (2721, 9)
	=======================================
	
[2018-05-28 01:33:46,083 AE_BIGRAMA_2L_FULLDS_OVER_03.py:150]: >> Executing autoencoder part ... 
[2018-05-28 01:33:46,083 AE_BIGRAMA_2L_FULLDS_OVER_03.py:57]: =======================================
[2018-05-28 01:33:46,084 AE_BIGRAMA_2L_FULLDS_OVER_03.py:62]: setting configurations for autoencoder: 
	 {'loss_function': 'mse', 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fbdeea44898>, 'hidden_layer_activation': 'relu'}
[2018-05-28 01:33:46,438 AE_BIGRAMA_2L_FULLDS_OVER_03.py:73]: training and evaluate autoencoder
[2018-05-30 17:58:22,396 AE_BIGRAMA_2L_FULLDS_OVER_03.py:85]: trained and evaluated!
[2018-05-30 17:58:22,471 AE_BIGRAMA_2L_FULLDS_OVER_03.py:88]: Training history: 
{'loss': [0.00010729524839834909, 0.00010728829457355478, 0.00010728129247755524, 0.00010727424648646343, 0.00010726715735225631, 0.00010726003150156835, 0.00010725285464415801, 0.00010724563979494247, 0.00010723837593326175, 0.0001072310709352261, 0.00010722371941821655, 0.00010721631802423626, 0.00010720887046494345, 0.00010720136784164638, 0.00010719381514579308, 0.00010718622258355071, 0.00010717856789747286, 0.00010717086539923508, 0.00010716311214255237, 0.00010715530378256968, 0.0001071474300622141, 0.00010713949599079534, 0.00010713149592312768, 0.00010712343430141231, 0.0001071153217560314, 0.0001071071444575749, 0.00010709891029287008, 0.0001070906014779297, 0.00010708204684154174, 0.00010707295364496921, 0.00010706362111877871, 0.000107054166683905, 0.00010704460029287926, 0.00010703492096241571, 0.00010702511875516557, 0.0001070151672260103, 0.0001070050803704736, 0.00010699485437687181, 0.0001069845084956365, 0.00010697401361400639, 0.00010696335273301533, 0.00010695252815056917, 0.00010694155407117335, 0.00010693043244621701, 0.00010691919545531289, 0.00010690784914285535, 0.00010689640663271788, 0.00010688487715314031, 0.00010687327675106097, 0.00010686162109653658, 0.00010684991922758006, 0.00010683817964903296, 0.00010682641611885863, 0.00010681462571399219, 0.00010680281918716835, 0.00010679100087163205, 0.00010677916341283425, 0.00010676732268713415, 0.00010675546629494965, 0.0001067436043611772, 0.00010673173195241925, 0.00010671985798969466, 0.00010670798321158412, 0.00010669609850416259, 0.00010668422909438165, 0.00010667235513701556, 0.00010666048882534398, 0.0001066486137819874, 0.00010663674356485829, 0.00010662488260186845, 0.00010661301289022502, 0.00010660114373854543, 0.00010658928340696618, 0.00010657741675592257, 0.00010656555664761439, 0.00010655370358216883, 0.00010654185433287239, 0.00010653001533261111, 0.00010651818260001531, 0.00010650636345391063, 0.00010649454993512999, 0.00010648274130687884, 0.00010647094067976976, 0.00010645916078561218, 0.00010644738676437682, 0.00010643562257867863, 0.00010642387565451334, 0.00010641214434949901, 0.00010640043097225848, 0.00010638873178969946, 0.000106377046367783, 0.00010636537899420672, 0.00010635372389875309, 0.00010634208178785178, 0.0001063304635848166, 0.00010631885713923595, 0.0001063072636219433, 0.00010629569059825558, 0.00010628413396266438, 0.00010627259986762727, 0.0001062610748132823], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.00010724017221186853, 0.00010723319198810525, 0.00010722617344055492, 0.00010721910917560389, 0.00010721200567770538, 0.00010720485391140888, 0.00010719766031036162, 0.0001071904223556543, 0.00010718314486583736, 0.00010717582813237693, 0.00010716846459319723, 0.00010716105210909723, 0.00010715358768252143, 0.00010714606909404873, 0.00010713850791140886, 0.00010713089239038801, 0.00010712323220552297, 0.00010711552871253242, 0.00010710777381454033, 0.00010709996079712937, 0.0001070920996316505, 0.00010708418242177779, 0.00010707620745080238, 0.00010706817710928148, 0.00010706008943449809, 0.00010705194632499316, 0.00010704374861505512, 0.00010703544804965598, 0.00010702675285246682, 0.00010701761698253277, 0.00010700838475228684, 0.00010699904734287267, 0.00010698958624217906, 0.00010698000901495577, 0.00010697031069825634, 0.0001069604907866945, 0.00010695049323052841, 0.00010694035084242838, 0.00010693008616422561, 0.00010691967355224737, 0.00010690908260042452, 0.00010689836066799462, 0.00010688748603939288, 0.00010687647310342042, 0.00010686536080690661, 0.0001068541635440006, 0.00010684288226664686, 0.00010683153028335, 0.00010682011166394003, 0.00010680862218082086, 0.000106797084990844, 0.00010678551636263352, 0.00010677392124576587, 0.00010676231097800671, 0.00010675069242084343, 0.00010673906194833024, 0.00010672741736243802, 0.00010671575900811298, 0.00010670409347676836, 0.0001066924205999421, 0.00010668074832476613, 0.00010666907406280718, 0.00010665737944367611, 0.00010664569681744101, 0.00010663401139152651, 0.00010662232884818545, 0.00010661063519169487, 0.00010659894734848317, 0.00010658727186450561, 0.00010657558608027493, 0.00010656390330429576, 0.00010655222704485781, 0.00010654054068839085, 0.00010652886093670717, 0.00010651718811068694, 0.00010650551672595342, 0.00010649385127215483, 0.00010648218938814803, 0.00010647053935264111, 0.00010645889788998246, 0.0001064472584542168, 0.00010643563073859897, 0.00010642402006706561, 0.00010641240957736433, 0.00010640081398452446, 0.00010638923473250671, 0.00010637766667920651, 0.00010636611909003848, 0.00010635458341622037, 0.00010634306347087807, 0.00010633156116324854, 0.00010632006680809895, 0.00010630858828838524, 0.00010629713159654435, 0.00010628568681470539, 0.00010627425502851288, 0.00010626284189615364, 0.00010625144478373635, 0.00010624006810336323, 0.00010622870134086102, 0.00010621735705333998], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-05-30 17:58:22,472 AE_BIGRAMA_2L_FULLDS_OVER_03.py:92]: done!
[2018-05-30 17:58:22,533 AE_BIGRAMA_2L_FULLDS_OVER_03.py:152]: >> Executing classifier part ... 
[2018-05-30 17:58:22,534 AE_BIGRAMA_2L_FULLDS_OVER_03.py:97]: =======================================
[2018-05-30 17:58:22,534 AE_BIGRAMA_2L_FULLDS_OVER_03.py:101]: setting configurations for classifier: 
	 {'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'optimizer': <keras.optimizers.SGD object at 0x7fbdeea44908>, 'activation': 'sigmoid'}
[2018-05-30 17:58:22,714 AE_BIGRAMA_2L_FULLDS_OVER_03.py:110]: training ... 
[2018-06-02 17:52:03,381 AE_BIGRAMA_2L_FULLDS_OVER_03.py:122]: trained!
[2018-06-02 17:52:03,383 AE_BIGRAMA_2L_FULLDS_OVER_03.py:125]: Training history: 
{'loss': [0.00010729524839834909, 0.00010728829457355478, 0.00010728129247755524, 0.00010727424648646343, 0.00010726715735225631, 0.00010726003150156835, 0.00010725285464415801, 0.00010724563979494247, 0.00010723837593326175, 0.0001072310709352261, 0.00010722371941821655, 0.00010721631802423626, 0.00010720887046494345, 0.00010720136784164638, 0.00010719381514579308, 0.00010718622258355071, 0.00010717856789747286, 0.00010717086539923508, 0.00010716311214255237, 0.00010715530378256968, 0.0001071474300622141, 0.00010713949599079534, 0.00010713149592312768, 0.00010712343430141231, 0.0001071153217560314, 0.0001071071444575749, 0.00010709891029287008, 0.0001070906014779297, 0.00010708204684154174, 0.00010707295364496921, 0.00010706362111877871, 0.000107054166683905, 0.00010704460029287926, 0.00010703492096241571, 0.00010702511875516557, 0.0001070151672260103, 0.0001070050803704736, 0.00010699485437687181, 0.0001069845084956365, 0.00010697401361400639, 0.00010696335273301533, 0.00010695252815056917, 0.00010694155407117335, 0.00010693043244621701, 0.00010691919545531289, 0.00010690784914285535, 0.00010689640663271788, 0.00010688487715314031, 0.00010687327675106097, 0.00010686162109653658, 0.00010684991922758006, 0.00010683817964903296, 0.00010682641611885863, 0.00010681462571399219, 0.00010680281918716835, 0.00010679100087163205, 0.00010677916341283425, 0.00010676732268713415, 0.00010675546629494965, 0.0001067436043611772, 0.00010673173195241925, 0.00010671985798969466, 0.00010670798321158412, 0.00010669609850416259, 0.00010668422909438165, 0.00010667235513701556, 0.00010666048882534398, 0.0001066486137819874, 0.00010663674356485829, 0.00010662488260186845, 0.00010661301289022502, 0.00010660114373854543, 0.00010658928340696618, 0.00010657741675592257, 0.00010656555664761439, 0.00010655370358216883, 0.00010654185433287239, 0.00010653001533261111, 0.00010651818260001531, 0.00010650636345391063, 0.00010649454993512999, 0.00010648274130687884, 0.00010647094067976976, 0.00010645916078561218, 0.00010644738676437682, 0.00010643562257867863, 0.00010642387565451334, 0.00010641214434949901, 0.00010640043097225848, 0.00010638873178969946, 0.000106377046367783, 0.00010636537899420672, 0.00010635372389875309, 0.00010634208178785178, 0.0001063304635848166, 0.00010631885713923595, 0.0001063072636219433, 0.00010629569059825558, 0.00010628413396266438, 0.00010627259986762727, 0.0001062610748132823], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.00010724017221186853, 0.00010723319198810525, 0.00010722617344055492, 0.00010721910917560389, 0.00010721200567770538, 0.00010720485391140888, 0.00010719766031036162, 0.0001071904223556543, 0.00010718314486583736, 0.00010717582813237693, 0.00010716846459319723, 0.00010716105210909723, 0.00010715358768252143, 0.00010714606909404873, 0.00010713850791140886, 0.00010713089239038801, 0.00010712323220552297, 0.00010711552871253242, 0.00010710777381454033, 0.00010709996079712937, 0.0001070920996316505, 0.00010708418242177779, 0.00010707620745080238, 0.00010706817710928148, 0.00010706008943449809, 0.00010705194632499316, 0.00010704374861505512, 0.00010703544804965598, 0.00010702675285246682, 0.00010701761698253277, 0.00010700838475228684, 0.00010699904734287267, 0.00010698958624217906, 0.00010698000901495577, 0.00010697031069825634, 0.0001069604907866945, 0.00010695049323052841, 0.00010694035084242838, 0.00010693008616422561, 0.00010691967355224737, 0.00010690908260042452, 0.00010689836066799462, 0.00010688748603939288, 0.00010687647310342042, 0.00010686536080690661, 0.0001068541635440006, 0.00010684288226664686, 0.00010683153028335, 0.00010682011166394003, 0.00010680862218082086, 0.000106797084990844, 0.00010678551636263352, 0.00010677392124576587, 0.00010676231097800671, 0.00010675069242084343, 0.00010673906194833024, 0.00010672741736243802, 0.00010671575900811298, 0.00010670409347676836, 0.0001066924205999421, 0.00010668074832476613, 0.00010666907406280718, 0.00010665737944367611, 0.00010664569681744101, 0.00010663401139152651, 0.00010662232884818545, 0.00010661063519169487, 0.00010659894734848317, 0.00010658727186450561, 0.00010657558608027493, 0.00010656390330429576, 0.00010655222704485781, 0.00010654054068839085, 0.00010652886093670717, 0.00010651718811068694, 0.00010650551672595342, 0.00010649385127215483, 0.00010648218938814803, 0.00010647053935264111, 0.00010645889788998246, 0.0001064472584542168, 0.00010643563073859897, 0.00010642402006706561, 0.00010641240957736433, 0.00010640081398452446, 0.00010638923473250671, 0.00010637766667920651, 0.00010636611909003848, 0.00010635458341622037, 0.00010634306347087807, 0.00010633156116324854, 0.00010632006680809895, 0.00010630858828838524, 0.00010629713159654435, 0.00010628568681470539, 0.00010627425502851288, 0.00010626284189615364, 0.00010625144478373635, 0.00010624006810336323, 0.00010622870134086102, 0.00010621735705333998], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-06-02 17:52:03,383 AE_BIGRAMA_2L_FULLDS_OVER_03.py:129]: evaluating model ... 
[2018-06-02 17:53:03,499 AE_BIGRAMA_2L_FULLDS_OVER_03.py:133]: evaluated! 
[2018-06-02 17:53:03,500 AE_BIGRAMA_2L_FULLDS_OVER_03.py:135]: generating reports ... 
[2018-06-02 17:53:05,538 AE_BIGRAMA_2L_FULLDS_OVER_03.py:138]: done!
[2018-06-02 17:53:05,538 AE_BIGRAMA_2L_FULLDS_OVER_03.py:154]: >> experiment AE_BIGRAMA_2L_FULLDS_OVER_03 finished!
