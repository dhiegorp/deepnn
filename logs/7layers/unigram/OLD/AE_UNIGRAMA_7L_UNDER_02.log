[2017-10-20 01:45:52,450 AE_UNIGRAMA_7L_UNDER_02.py:148]: >> Initializing execution of experiment AE_UNIGRAMA_7L_UNDER_02
[2017-10-20 01:45:52,450 AE_UNIGRAMA_7L_UNDER_02.py:149]: >> Printing header log
[2017-10-20 01:45:52,450 AE_UNIGRAMA_7L_UNDER_02.py:38]: 
	=======================================
	network_name = AE_UNIGRAMA_7L_UNDER_02
	layers = 96,76,69,63,56,49,43,36,9
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/7layers/unigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/7layers/unigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/7layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/7layers/unigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/7layers/unigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/7layers/unigram/', 'data_dir': '/home/dhiegorp/malware_deepnn/', 'fullds_data_dir': '/home/dhiegorp/malware_deepnn/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f64610dc780>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f64610dc860>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-10-20 01:45:52,450 AE_UNIGRAMA_7L_UNDER_02.py:151]: >> Loading dataset... 
[2017-10-20 01:45:53,041 AE_UNIGRAMA_7L_UNDER_02.py:55]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_deepnn/	
	trainx shape = (1627, 96)
	trainy shape = (1627, 9)
	valx shape = (1076, 96)
	valy shape = (1076, 9)
	=======================================
	
[2017-10-20 01:45:53,042 AE_UNIGRAMA_7L_UNDER_02.py:153]: >> Executing autoencoder part ... 
[2017-10-20 01:45:53,042 AE_UNIGRAMA_7L_UNDER_02.py:60]: =======================================
[2017-10-20 01:45:53,042 AE_UNIGRAMA_7L_UNDER_02.py:65]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f64610dc780>, 'discard_decoder_function': True}
[2017-10-20 01:45:53,219 AE_UNIGRAMA_7L_UNDER_02.py:76]: training and evaluate autoencoder
[2017-10-20 01:46:33,677 AE_UNIGRAMA_7L_UNDER_02.py:88]: trained and evaluated!
[2017-10-20 01:46:33,678 AE_UNIGRAMA_7L_UNDER_02.py:91]: Training history: 
{'val_loss': [0.010156806876889836, 0.0099119747396849353, 0.0096816840100354858, 0.0094656732598091143, 0.0092627657958233665, 0.0090738161844391811, 0.0088986298103979535, 0.0087358572876918717, 0.0085844519170742047, 0.0084435071269929624, 0.008312102382981865, 0.0081894810148774467, 0.0080749670071285015, 0.0079679205497816818, 0.0078676616025181514, 0.0077736665508588675, 0.0076856119990985853, 0.0076028775704200602, 0.0075244765908744696, 0.00745038790836044, 0.0073806388774492037, 0.0073148476403054252, 0.0072527575171569911, 0.0071941024180654034, 0.0071386853458020542, 0.0070862830633844588, 0.0070366675525792913, 0.0069896258348010505, 0.0069450040185152374, 0.0069026979011895485, 0.0068625687001806213, 0.006824444138122979, 0.0067876764576619, 0.0067512120144462501, 0.0067165354076343621, 0.0066835168908353634, 0.0066520554674823935, 0.0066220257656309459, 0.0065933251684206122, 0.0065659239289236554, 0.006539698672865182, 0.0065146245938279149, 0.0064906237896213302, 0.0064676396261388264, 0.0064456081779528281, 0.0064244899175174841, 0.0064042715812759774, 0.006384869597741441, 0.0063662530826647046, 0.0063484040760611732, 0.0063312313602510214, 0.0063146986223092533, 0.0062988280841464444, 0.0062835421447804867, 0.0062688060457987858, 0.0062545564641905979, 0.0062409421276702756, 0.0062278842511950374, 0.0062153109238521093, 0.0062032014193865006, 0.006191512417449827, 0.0061802279903487647, 0.0061690647563582019, 0.0061575068700989383, 0.0061463779911786649, 0.0061356497679224244, 0.0061253466674552528, 0.0061152768338646147, 0.0061054482439802929, 0.0060963214264289373, 0.0060875338941923304, 0.0060790494312861149, 0.0060708710549877039, 0.0060629664252072465, 0.0060553244231995821, 0.0060479456531126264, 0.0060408296631337538, 0.0060339415182799208, 0.0060272674824086931, 0.0060208196523270424, 0.0060145789238046097, 0.0060085375138928896, 0.0060026940602296791, 0.0059970335335536515, 0.005991551484981877, 0.0059862423542762335, 0.0059811028939252891, 0.0059761332735751641, 0.0059713148321527089, 0.0059666474034739689, 0.0059621182034920806, 0.0059577327422437616, 0.005953483226394011, 0.0059493673328296403, 0.0059453688430354057, 0.0059414763786037172, 0.0059377052319204938, 0.0059340572794795475, 0.0059305082757344476, 0.0059270707641115418, 0.0059237301152296447, 0.0059204877208793695], 'loss': [0.010278753613447513, 0.010028267683393764, 0.009789038801562332, 0.0095648542573170646, 0.0093543236095231588, 0.0091569200140946688, 0.0089737391809130932, 0.0088037053110523317, 0.0086455505001275679, 0.0084983396762323728, 0.0083611793587122802, 0.0082331748396506244, 0.0081136551706810708, 0.0080019210619232586, 0.0078973831696471306, 0.0077993919069466048, 0.0077074551668656916, 0.0076212531909319106, 0.0075399062499523018, 0.0074627844670577953, 0.0073900647314303148, 0.0073215450367992759, 0.0072568604383748151, 0.0071957786086678834, 0.0071380222665595912, 0.0070834210441294955, 0.0070317348859718814, 0.0069827716468450441, 0.0069363043507871336, 0.006892211653679133, 0.0068503750694804601, 0.0068106539195406891, 0.0067728021778578435, 0.0067354587556996918, 0.0066993282887496206, 0.0066649303196276268, 0.0066321516305336429, 0.0066008910307226807, 0.006571029291729218, 0.0065424810535747837, 0.0065152026474196521, 0.0064890818949731571, 0.0064640852940951706, 0.0064401581188516157, 0.0064172215505601, 0.0063952270196321447, 0.006374143153319127, 0.006353924354880627, 0.0063345105036378786, 0.0063158913725012821, 0.0062980155243590411, 0.0062807962217110028, 0.0062642427860829606, 0.0062483173414959485, 0.0062329819970881562, 0.0062181569507646648, 0.0062039060027011712, 0.0061902956929476846, 0.0061771883546261975, 0.0061645468951105047, 0.0061523722992441778, 0.0061406049603789804, 0.006129213720708902, 0.0061175146566036474, 0.0061059149466432087, 0.0060947314079349479, 0.0060839560730056525, 0.0060735685838421904, 0.0060633571495627729, 0.0060536792904216418, 0.0060445128799231232, 0.0060356758948765833, 0.0060271416074117231, 0.006018908812304399, 0.0060109408668482236, 0.0060032491000356599, 0.00599581399702392, 0.0059886318840402433, 0.0059816821804012559, 0.0059749589326832109, 0.005968445090298812, 0.0059621412190823337, 0.0059560468622449538, 0.0059501418595436385, 0.0059444166496098409, 0.005938876552774028, 0.0059334978331899305, 0.0059283039671126766, 0.0059232793181113437, 0.005918401365294897, 0.0059136673882512055, 0.0059090818008883054, 0.0059046387050206957, 0.0059003323467119789, 0.0058961487100439598, 0.0058920897151897049, 0.0058881477012053436, 0.005884325026162999, 0.0058806214027093025, 0.0058770138986715525, 0.0058735289657288123, 0.0058701384999201422]}
[2017-10-20 01:46:33,678 AE_UNIGRAMA_7L_UNDER_02.py:95]: done!
[2017-10-20 01:46:33,678 AE_UNIGRAMA_7L_UNDER_02.py:155]: >> Executing classifier part ... 
[2017-10-20 01:46:33,678 AE_UNIGRAMA_7L_UNDER_02.py:100]: =======================================
[2017-10-20 01:46:33,678 AE_UNIGRAMA_7L_UNDER_02.py:104]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f64610dc860>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-10-20 01:46:33,713 AE_UNIGRAMA_7L_UNDER_02.py:113]: training ... 
[2017-10-20 01:47:43,931 AE_UNIGRAMA_7L_UNDER_02.py:125]: trained!
[2017-10-20 01:47:43,931 AE_UNIGRAMA_7L_UNDER_02.py:128]: Training history: 
{'val_loss': [0.010156806876889836, 0.0099119747396849353, 0.0096816840100354858, 0.0094656732598091143, 0.0092627657958233665, 0.0090738161844391811, 0.0088986298103979535, 0.0087358572876918717, 0.0085844519170742047, 0.0084435071269929624, 0.008312102382981865, 0.0081894810148774467, 0.0080749670071285015, 0.0079679205497816818, 0.0078676616025181514, 0.0077736665508588675, 0.0076856119990985853, 0.0076028775704200602, 0.0075244765908744696, 0.00745038790836044, 0.0073806388774492037, 0.0073148476403054252, 0.0072527575171569911, 0.0071941024180654034, 0.0071386853458020542, 0.0070862830633844588, 0.0070366675525792913, 0.0069896258348010505, 0.0069450040185152374, 0.0069026979011895485, 0.0068625687001806213, 0.006824444138122979, 0.0067876764576619, 0.0067512120144462501, 0.0067165354076343621, 0.0066835168908353634, 0.0066520554674823935, 0.0066220257656309459, 0.0065933251684206122, 0.0065659239289236554, 0.006539698672865182, 0.0065146245938279149, 0.0064906237896213302, 0.0064676396261388264, 0.0064456081779528281, 0.0064244899175174841, 0.0064042715812759774, 0.006384869597741441, 0.0063662530826647046, 0.0063484040760611732, 0.0063312313602510214, 0.0063146986223092533, 0.0062988280841464444, 0.0062835421447804867, 0.0062688060457987858, 0.0062545564641905979, 0.0062409421276702756, 0.0062278842511950374, 0.0062153109238521093, 0.0062032014193865006, 0.006191512417449827, 0.0061802279903487647, 0.0061690647563582019, 0.0061575068700989383, 0.0061463779911786649, 0.0061356497679224244, 0.0061253466674552528, 0.0061152768338646147, 0.0061054482439802929, 0.0060963214264289373, 0.0060875338941923304, 0.0060790494312861149, 0.0060708710549877039, 0.0060629664252072465, 0.0060553244231995821, 0.0060479456531126264, 0.0060408296631337538, 0.0060339415182799208, 0.0060272674824086931, 0.0060208196523270424, 0.0060145789238046097, 0.0060085375138928896, 0.0060026940602296791, 0.0059970335335536515, 0.005991551484981877, 0.0059862423542762335, 0.0059811028939252891, 0.0059761332735751641, 0.0059713148321527089, 0.0059666474034739689, 0.0059621182034920806, 0.0059577327422437616, 0.005953483226394011, 0.0059493673328296403, 0.0059453688430354057, 0.0059414763786037172, 0.0059377052319204938, 0.0059340572794795475, 0.0059305082757344476, 0.0059270707641115418, 0.0059237301152296447, 0.0059204877208793695], 'loss': [0.010278753613447513, 0.010028267683393764, 0.009789038801562332, 0.0095648542573170646, 0.0093543236095231588, 0.0091569200140946688, 0.0089737391809130932, 0.0088037053110523317, 0.0086455505001275679, 0.0084983396762323728, 0.0083611793587122802, 0.0082331748396506244, 0.0081136551706810708, 0.0080019210619232586, 0.0078973831696471306, 0.0077993919069466048, 0.0077074551668656916, 0.0076212531909319106, 0.0075399062499523018, 0.0074627844670577953, 0.0073900647314303148, 0.0073215450367992759, 0.0072568604383748151, 0.0071957786086678834, 0.0071380222665595912, 0.0070834210441294955, 0.0070317348859718814, 0.0069827716468450441, 0.0069363043507871336, 0.006892211653679133, 0.0068503750694804601, 0.0068106539195406891, 0.0067728021778578435, 0.0067354587556996918, 0.0066993282887496206, 0.0066649303196276268, 0.0066321516305336429, 0.0066008910307226807, 0.006571029291729218, 0.0065424810535747837, 0.0065152026474196521, 0.0064890818949731571, 0.0064640852940951706, 0.0064401581188516157, 0.0064172215505601, 0.0063952270196321447, 0.006374143153319127, 0.006353924354880627, 0.0063345105036378786, 0.0063158913725012821, 0.0062980155243590411, 0.0062807962217110028, 0.0062642427860829606, 0.0062483173414959485, 0.0062329819970881562, 0.0062181569507646648, 0.0062039060027011712, 0.0061902956929476846, 0.0061771883546261975, 0.0061645468951105047, 0.0061523722992441778, 0.0061406049603789804, 0.006129213720708902, 0.0061175146566036474, 0.0061059149466432087, 0.0060947314079349479, 0.0060839560730056525, 0.0060735685838421904, 0.0060633571495627729, 0.0060536792904216418, 0.0060445128799231232, 0.0060356758948765833, 0.0060271416074117231, 0.006018908812304399, 0.0060109408668482236, 0.0060032491000356599, 0.00599581399702392, 0.0059886318840402433, 0.0059816821804012559, 0.0059749589326832109, 0.005968445090298812, 0.0059621412190823337, 0.0059560468622449538, 0.0059501418595436385, 0.0059444166496098409, 0.005938876552774028, 0.0059334978331899305, 0.0059283039671126766, 0.0059232793181113437, 0.005918401365294897, 0.0059136673882512055, 0.0059090818008883054, 0.0059046387050206957, 0.0059003323467119789, 0.0058961487100439598, 0.0058920897151897049, 0.0058881477012053436, 0.005884325026162999, 0.0058806214027093025, 0.0058770138986715525, 0.0058735289657288123, 0.0058701384999201422]}
[2017-10-20 01:47:43,931 AE_UNIGRAMA_7L_UNDER_02.py:132]: evaluating model ... 
[2017-10-20 01:47:43,996 AE_UNIGRAMA_7L_UNDER_02.py:136]: evaluated! 
[2017-10-20 01:47:43,996 AE_UNIGRAMA_7L_UNDER_02.py:138]: generating reports ... 
[2017-10-20 01:47:44,608 AE_UNIGRAMA_7L_UNDER_02.py:141]: done!
[2017-10-20 01:47:44,608 AE_UNIGRAMA_7L_UNDER_02.py:157]: >> experiment AE_UNIGRAMA_7L_UNDER_02 finished!
