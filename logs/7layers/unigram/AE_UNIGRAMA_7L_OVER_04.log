[2017-10-20 01:43:14,108 AE_UNIGRAMA_7L_OVER_04.py:148]: >> Initializing execution of experiment AE_UNIGRAMA_7L_OVER_04
[2017-10-20 01:43:14,108 AE_UNIGRAMA_7L_OVER_04.py:149]: >> Printing header log
[2017-10-20 01:43:14,108 AE_UNIGRAMA_7L_OVER_04.py:38]: 
	=======================================
	network_name = AE_UNIGRAMA_7L_OVER_04
	layers = 96,134,122,109,97,84,72,59,9
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/7layers/unigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/7layers/unigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/7layers/unigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/7layers/unigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/7layers/unigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/7layers/unigram/', 'data_dir': '/home/dhiegorp/malware_deepnn/', 'fullds_data_dir': '/home/dhiegorp/malware_deepnn/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f8c5220cb70>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f8c5220ccf8>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-10-20 01:43:14,108 AE_UNIGRAMA_7L_OVER_04.py:151]: >> Loading dataset... 
[2017-10-20 01:43:14,727 AE_UNIGRAMA_7L_OVER_04.py:55]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_deepnn/	
	trainx shape = (1627, 96)
	trainy shape = (1627, 9)
	valx shape = (1076, 96)
	valy shape = (1076, 9)
	=======================================
	
[2017-10-20 01:43:14,727 AE_UNIGRAMA_7L_OVER_04.py:153]: >> Executing autoencoder part ... 
[2017-10-20 01:43:14,727 AE_UNIGRAMA_7L_OVER_04.py:60]: =======================================
[2017-10-20 01:43:14,727 AE_UNIGRAMA_7L_OVER_04.py:65]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f8c5220cb70>, 'discard_decoder_function': True}
[2017-10-20 01:43:14,900 AE_UNIGRAMA_7L_OVER_04.py:76]: training and evaluate autoencoder
[2017-10-20 01:44:10,063 AE_UNIGRAMA_7L_OVER_04.py:88]: trained and evaluated!
[2017-10-20 01:44:10,064 AE_UNIGRAMA_7L_OVER_04.py:91]: Training history: 
{'val_loss': [0.010185005780323287, 0.009981761416454963, 0.0097921313060149832, 0.0096145842854473678, 0.00944740818224875, 0.0092902688316474618, 0.0091425507194379894, 0.0090036322691626706, 0.0088728647956063753, 0.0087469744610298959, 0.0086270241122208113, 0.0085137300643391318, 0.0084066733901413185, 0.0083053849367459472, 0.0082095396016000823, 0.0081187480174309701, 0.0080322764665026868, 0.0079501379128947142, 0.0078721746740011041, 0.0077981788242672016, 0.0077278641031771119, 0.007660979048442442, 0.0075973805220153692, 0.0075368120345428976, 0.0074791230545223422, 0.0074241252193663647, 0.0073716763552402918, 0.007321670280984122, 0.0072739123588351729, 0.0072282841758020081, 0.0071840606141683116, 0.007141557634638477, 0.0071006009691017488, 0.0070608313538161795, 0.0070229081738200109, 0.0069862196401980284, 0.0069505216252598841, 0.0069163356315635393, 0.0068836217974641506, 0.0068522798153658341, 0.0068222741322889651, 0.0067934948798670644, 0.0067658690012105116, 0.0067393555804450053, 0.0067139486245617105, 0.0066895446598142054, 0.0066661270470341118, 0.0066436343414355828, 0.0066220293680143618, 0.0066013071931843199, 0.0065813489054021559, 0.0065621836595652714, 0.0065437665877372134, 0.0065261238915880378, 0.0065091249037907912, 0.0064928047964868717, 0.0064770636082969631, 0.0064619008666354488, 0.0064473217509022213, 0.0064332845021856317, 0.0064197290674904463, 0.0064066821210723383, 0.0063941028491908952, 0.0063819607986365778, 0.0063702525124369278, 0.0063590076509655409, 0.0063481864391317158, 0.0063377471561076254, 0.0063276703583706713, 0.0063179330554094899, 0.0063084436885209334, 0.0062993460656818843, 0.0062905891961158433, 0.0062821334587984815, 0.0062739743719562946, 0.0062660758632848033, 0.0062584621575574447, 0.0062510763921646822, 0.006243947760087739, 0.0062370416500560633, 0.0062303671225573051, 0.0062239041299209496, 0.0062176424362540908, 0.0062115915988651569, 0.0062057389341100885, 0.0062000895884982048, 0.0061946140713020331, 0.0061892973567224127, 0.0061841652119276024, 0.0061791854422387138, 0.0061743697428514963, 0.006169698311909202, 0.0061651822525772464, 0.0061607948334742209, 0.0061565521813679806, 0.0061524378301984319, 0.0061484509351971426, 0.0061445784665034829, 0.0061408340840922414, 0.0061372120823143363, 0.0061336826069148939, 0.0061302693010235141], 'loss': [0.010289107924458911, 0.010076922296415316, 0.0098797724145500664, 0.0096953730908873343, 0.0095222679584458254, 0.0093592524444930519, 0.0092060382803644449, 0.0090619255282035176, 0.0089263107430718764, 0.0087976132793730951, 0.008673429978552338, 0.0085559639731707122, 0.0084449818052154558, 0.0083399883236165932, 0.0082406470759301808, 0.0081465720229386626, 0.0080572116276284786, 0.0079721274085058965, 0.0078913399396714941, 0.0078146310057204856, 0.0077417677665643338, 0.0076724937558861036, 0.0076065627899829166, 0.0075438430778379997, 0.0074840860556382926, 0.0074271190125602812, 0.0073727982650501232, 0.0073209633050714271, 0.007271516828877324, 0.0072242750685499466, 0.0071788818928050753, 0.0071349112693616492, 0.0070927514815983519, 0.0070517523845024617, 0.0070123733605184325, 0.0069747291884276865, 0.0069379682930846964, 0.0069025923665693078, 0.0068687040169311767, 0.0068362573609351378, 0.0068051751766562201, 0.0067754037138452435, 0.0067468315329157688, 0.0067193804952168871, 0.0066930534233371761, 0.0066677978309243463, 0.0066435450648507713, 0.0066202700938903522, 0.0065978867126046037, 0.0065764050207203269, 0.0065557857884921836, 0.0065359204152361993, 0.0065168435304679235, 0.0064985453933660573, 0.0064810006120817462, 0.0064640796884147941, 0.0064478314667138752, 0.0064321661141335269, 0.0064170567390921659, 0.0064025355214464858, 0.0063885387935353408, 0.0063750342229528887, 0.0063620291652584926, 0.006349467269712218, 0.0063373583569875432, 0.0063256883639418683, 0.0063144565335536158, 0.0063036464629254059, 0.0062932015010845497, 0.0062831171290844805, 0.0062733294202453275, 0.0062638699207580853, 0.0062547630676799346, 0.0062460008368683722, 0.0062375249196308736, 0.0062293428707279417, 0.0062214233772285377, 0.0062137867258798699, 0.0062063774829541497, 0.0061992278108493493, 0.0061922842234867657, 0.0061855799771533947, 0.0061790780121370112, 0.0061727956844094198, 0.0061667114871164263, 0.0061608321209722632, 0.0061551431808151312, 0.0061496444451196218, 0.0061442917436086552, 0.0061391178972460443, 0.0061341054558973779, 0.0061292483579520806, 0.0061245531089301666, 0.0061199863935856823, 0.0061155689543098231, 0.0061112992762008102, 0.0061071424595622393, 0.0061031118271152301, 0.0060992043270181682, 0.0060954117903070319, 0.0060917527504155402, 0.0060881991935384438]}
[2017-10-20 01:44:10,064 AE_UNIGRAMA_7L_OVER_04.py:95]: done!
[2017-10-20 01:44:10,064 AE_UNIGRAMA_7L_OVER_04.py:155]: >> Executing classifier part ... 
[2017-10-20 01:44:10,064 AE_UNIGRAMA_7L_OVER_04.py:100]: =======================================
[2017-10-20 01:44:10,064 AE_UNIGRAMA_7L_OVER_04.py:104]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f8c5220ccf8>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-10-20 01:44:10,100 AE_UNIGRAMA_7L_OVER_04.py:113]: training ... 
[2017-10-20 01:45:35,711 AE_UNIGRAMA_7L_OVER_04.py:125]: trained!
[2017-10-20 01:45:35,712 AE_UNIGRAMA_7L_OVER_04.py:128]: Training history: 
{'val_loss': [0.010185005780323287, 0.009981761416454963, 0.0097921313060149832, 0.0096145842854473678, 0.00944740818224875, 0.0092902688316474618, 0.0091425507194379894, 0.0090036322691626706, 0.0088728647956063753, 0.0087469744610298959, 0.0086270241122208113, 0.0085137300643391318, 0.0084066733901413185, 0.0083053849367459472, 0.0082095396016000823, 0.0081187480174309701, 0.0080322764665026868, 0.0079501379128947142, 0.0078721746740011041, 0.0077981788242672016, 0.0077278641031771119, 0.007660979048442442, 0.0075973805220153692, 0.0075368120345428976, 0.0074791230545223422, 0.0074241252193663647, 0.0073716763552402918, 0.007321670280984122, 0.0072739123588351729, 0.0072282841758020081, 0.0071840606141683116, 0.007141557634638477, 0.0071006009691017488, 0.0070608313538161795, 0.0070229081738200109, 0.0069862196401980284, 0.0069505216252598841, 0.0069163356315635393, 0.0068836217974641506, 0.0068522798153658341, 0.0068222741322889651, 0.0067934948798670644, 0.0067658690012105116, 0.0067393555804450053, 0.0067139486245617105, 0.0066895446598142054, 0.0066661270470341118, 0.0066436343414355828, 0.0066220293680143618, 0.0066013071931843199, 0.0065813489054021559, 0.0065621836595652714, 0.0065437665877372134, 0.0065261238915880378, 0.0065091249037907912, 0.0064928047964868717, 0.0064770636082969631, 0.0064619008666354488, 0.0064473217509022213, 0.0064332845021856317, 0.0064197290674904463, 0.0064066821210723383, 0.0063941028491908952, 0.0063819607986365778, 0.0063702525124369278, 0.0063590076509655409, 0.0063481864391317158, 0.0063377471561076254, 0.0063276703583706713, 0.0063179330554094899, 0.0063084436885209334, 0.0062993460656818843, 0.0062905891961158433, 0.0062821334587984815, 0.0062739743719562946, 0.0062660758632848033, 0.0062584621575574447, 0.0062510763921646822, 0.006243947760087739, 0.0062370416500560633, 0.0062303671225573051, 0.0062239041299209496, 0.0062176424362540908, 0.0062115915988651569, 0.0062057389341100885, 0.0062000895884982048, 0.0061946140713020331, 0.0061892973567224127, 0.0061841652119276024, 0.0061791854422387138, 0.0061743697428514963, 0.006169698311909202, 0.0061651822525772464, 0.0061607948334742209, 0.0061565521813679806, 0.0061524378301984319, 0.0061484509351971426, 0.0061445784665034829, 0.0061408340840922414, 0.0061372120823143363, 0.0061336826069148939, 0.0061302693010235141], 'loss': [0.010289107924458911, 0.010076922296415316, 0.0098797724145500664, 0.0096953730908873343, 0.0095222679584458254, 0.0093592524444930519, 0.0092060382803644449, 0.0090619255282035176, 0.0089263107430718764, 0.0087976132793730951, 0.008673429978552338, 0.0085559639731707122, 0.0084449818052154558, 0.0083399883236165932, 0.0082406470759301808, 0.0081465720229386626, 0.0080572116276284786, 0.0079721274085058965, 0.0078913399396714941, 0.0078146310057204856, 0.0077417677665643338, 0.0076724937558861036, 0.0076065627899829166, 0.0075438430778379997, 0.0074840860556382926, 0.0074271190125602812, 0.0073727982650501232, 0.0073209633050714271, 0.007271516828877324, 0.0072242750685499466, 0.0071788818928050753, 0.0071349112693616492, 0.0070927514815983519, 0.0070517523845024617, 0.0070123733605184325, 0.0069747291884276865, 0.0069379682930846964, 0.0069025923665693078, 0.0068687040169311767, 0.0068362573609351378, 0.0068051751766562201, 0.0067754037138452435, 0.0067468315329157688, 0.0067193804952168871, 0.0066930534233371761, 0.0066677978309243463, 0.0066435450648507713, 0.0066202700938903522, 0.0065978867126046037, 0.0065764050207203269, 0.0065557857884921836, 0.0065359204152361993, 0.0065168435304679235, 0.0064985453933660573, 0.0064810006120817462, 0.0064640796884147941, 0.0064478314667138752, 0.0064321661141335269, 0.0064170567390921659, 0.0064025355214464858, 0.0063885387935353408, 0.0063750342229528887, 0.0063620291652584926, 0.006349467269712218, 0.0063373583569875432, 0.0063256883639418683, 0.0063144565335536158, 0.0063036464629254059, 0.0062932015010845497, 0.0062831171290844805, 0.0062733294202453275, 0.0062638699207580853, 0.0062547630676799346, 0.0062460008368683722, 0.0062375249196308736, 0.0062293428707279417, 0.0062214233772285377, 0.0062137867258798699, 0.0062063774829541497, 0.0061992278108493493, 0.0061922842234867657, 0.0061855799771533947, 0.0061790780121370112, 0.0061727956844094198, 0.0061667114871164263, 0.0061608321209722632, 0.0061551431808151312, 0.0061496444451196218, 0.0061442917436086552, 0.0061391178972460443, 0.0061341054558973779, 0.0061292483579520806, 0.0061245531089301666, 0.0061199863935856823, 0.0061155689543098231, 0.0061112992762008102, 0.0061071424595622393, 0.0061031118271152301, 0.0060992043270181682, 0.0060954117903070319, 0.0060917527504155402, 0.0060881991935384438]}
[2017-10-20 01:45:35,712 AE_UNIGRAMA_7L_OVER_04.py:132]: evaluating model ... 
[2017-10-20 01:45:35,792 AE_UNIGRAMA_7L_OVER_04.py:136]: evaluated! 
[2017-10-20 01:45:35,792 AE_UNIGRAMA_7L_OVER_04.py:138]: generating reports ... 
[2017-10-20 01:45:36,389 AE_UNIGRAMA_7L_OVER_04.py:141]: done!
[2017-10-20 01:45:36,389 AE_UNIGRAMA_7L_OVER_04.py:157]: >> experiment AE_UNIGRAMA_7L_OVER_04 finished!
