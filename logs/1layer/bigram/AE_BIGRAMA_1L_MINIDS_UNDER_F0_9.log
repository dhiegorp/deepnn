[2017-12-14 09:31:58,047 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_9
[2017-12-14 09:31:58,047 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:146]: >> Printing header log
[2017-12-14 09:31:58,047 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_9
	layers = 9216,8294
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fc408207eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fc4081ea400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:58,047 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:148]: >> Loading dataset... 
[2017-12-14 09:32:22,531 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:22,531 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:22,531 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:57]: =======================================
[2017-12-14 09:32:22,531 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fc408207eb8>, 'discard_decoder_function': True}
[2017-12-14 09:32:22,578 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:55,120 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_9
[2017-12-14 10:18:55,121 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:146]: >> Printing header log
[2017-12-14 10:18:55,121 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_9
	layers = 9216,8294
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7ff8086c6eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7ff8086a9400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:55,121 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:148]: >> Loading dataset... 
[2017-12-14 10:19:17,166 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:17,166 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:17,166 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:57]: =======================================
[2017-12-14 10:19:17,166 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7ff8086c6eb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:17,217 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:73]: training and evaluate autoencoder
[2017-12-14 21:06:12,241 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:85]: trained and evaluated!
[2017-12-14 21:06:12,244 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:88]: Training history: 
{'val_loss': [0.00011974980944755581, 0.00011974610496613858, 0.00011974241437518589, 0.00011973871650827559, 0.00011973500916653105, 0.00011973131857557835, 0.00011972761981481577, 0.00011972391794344721, 0.00011972020707992466, 0.00011971652229901184, 0.00011971284061082792, 0.00011970915811817695, 0.00011970547807468125, 0.00011970177505918176, 0.00011969806584034742, 0.00011969437083376444, 0.0001196906770606976, 0.0001196869917792275, 0.00011968330887540447, 0.00011967961011464188, 0.00011967590979857632, 0.00011967222247912299, 0.0001196685352490549, 0.00011966482177548366, 0.000119661124158852, 0.00011965741885509068, 0.00011965375450764113, 0.00011965007634123522, 0.00011964639956923889, 0.00011964271682630927, 0.00011963901593817824, 0.00011963531644445677, 0.00011963161923899716, 0.00011962793805137052, 0.00011962425915200574, 0.00011962057551522384, 0.00011961688444159092, 0.00011961318053223916, 0.00011960948822509008, 0.00011960580230004632, 0.00011960212291800131, 0.00011959843600972004, 0.0001195947357830397, 0.00011959104985799595, 0.00011958734936315992, 0.00011958366865821353, 0.00011957995026845471, 0.00011957626629200895, 0.00011957256236478014, 0.00011956888754138178, 0.0001195651944118886, 0.00011956152057172776, 0.00011955782204336676, 0.00011955412335411235, 0.0001195504343005856, 0.00011954673487837232, 0.00011954305032986109, 0.00011953935866628566, 0.00011953566848650502, 0.00011953197984415031, 0.00011952826399293199, 0.00011952457944442077, 0.00011952087977192885, 0.00011951720224909658, 0.00011951351843354423, 0.00011950981729513455, 0.00011950612759803415, 0.00011950242490432149, 0.00011949872605417368, 0.00011949503978946605, 0.00011949134755382515, 0.00011948764730926777, 0.00011948395532390551, 0.00011948027420778707, 0.00011947658238331821, 0.0001194728854102602, 0.00011946920070085556, 0.00011946550798253442, 0.00011946180103408489, 0.0001194581060275019, 0.00011945443031025126, 0.00011945072507799811, 0.00011944702279545751, 0.00011944333309835711, 0.00011943963923590505, 0.00011943593082153777, 0.00011943221595355696, 0.00011942850483975577, 0.00011942478759412788, 0.00011942107959093265, 0.00011941736062910838, 0.0001194136377522111, 0.00011940993563056391, 0.00011940621847432125, 0.00011940248390583605, 0.00011939877819090267, 0.00011939506478883962, 0.00011939135351414502, 0.00011938764248972905, 0.0001193839209178561, 0.0001193802295045593, 0.00011937652312817524, 0.00011937279462000854, 0.00011936907639114314, 0.00011936535809076957, 0.00011936165065963979, 0.00011935791119284407, 0.00011935421488123674, 0.0001193505060556974, 0.00011934679649719917, 0.00011934308979902828, 0.00011933937820254685, 0.00011933564159607844, 0.00011933190988792056, 0.00011932818929928513, 0.00011932447385923886, 0.00011932073210418128, 0.00011931700709991555, 0.00011931327523086426, 0.00011930957196508614, 0.00011930586036860471, 0.00011930213250401167, 0.0001192984070170657, 0.00011929468528429935, 0.00011929094899961776, 0.00011928723936961136, 0.00011928351207708377, 0.00011927978853873579, 0.00011927605241494761, 0.00011927233500842632, 0.000119268619157208, 0.00011926489872946599, 0.00011926116146154687, 0.00011925743458019135, 0.00011925370760945059, 0.00011924997205772787, 0.00011924624346017595, 0.00011924251493413221, 0.00011923878976897306, 0.00011923506394236324, 0.00011923134138725277, 0.00011922760494167777, 0.00011922388998431175, 0.00011922015173315512, 0.00011921640187979582, 0.00011921268742298707, 0.00011920898514044647, 0.00011920525302111654, 0.0001192015270336133, 0.00011919780987737064, 0.00011919408176249895, 0.000119190342206318, 0.00011918662603331286, 0.00011918288778215623, 0.00011917915934549773, 0.00011917542576025005, 0.00011917168407670064, 0.00011916794394845423, 0.0001191642046425519, 0.00011916047751091773, 0.00011915673673909766, 0.00011915299105109002, 0.00011914925762673575, 0.00011914551962585776, 0.00011914177835348041, 0.00011913804158611859, 0.00011913430145787218, 0.00011913055233747175, 0.00011912681262039738, 0.0001191230603178828, 0.00011911930442208205, 0.0001191155587340744, 0.00011911182581027741, 0.00011910808838146489, 0.00011910433264655755, 0.00011910058091610844, 0.00011909682829180705, 0.00011909306880272009, 0.00011908930277063441, 0.00011908554163685924, 0.00011908178990641015, 0.00011907803671004329, 0.00011907428211926686, 0.00011907053979214381, 0.00011906679018906315, 0.00011906302383519064, 0.0001190592584466786, 0.00011905547312526056, 0.00011905172564954834, 0.00011904796476605181, 0.00011904416514299719, 0.00011904039821705921, 0.00011903661559507507, 0.00011903283370604982, 0.00011902906252537495, 0.00011902528832347936, 0.00011902149025572771, 0.00011901770239576917, 0.00011901390448891093, 0.0001190101352747111], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012062212120679417, 0.00012061828163419077, 0.0001206144306617971, 0.00012061059699054043, 0.00012060676045156105, 0.00012060291365040042, 0.00012059908374747358, 0.00012059524381936737, 0.00012059140481556847, 0.00012058755099915229, 0.00012058372806408063, 0.00012057991254716771, 0.00012057610001647845, 0.00012057227558829497, 0.00012056843421447731, 0.00012056459198745289, 0.0001205607597145073, 0.00012055693123359172, 0.0001205531169016882, 0.00012054930195357978, 0.00012054546555680153, 0.0001205416323832488, 0.00012053780963777864, 0.00012053398689230849, 0.00012053013819513284, 0.00012052630817370508, 0.00012052247246423227, 0.0001205186722339404, 0.00012051486693180837, 0.00012051105044318775, 0.00012050723701189136, 0.0001205034015868208, 0.00012049956618545042, 0.00012049573386510446, 0.0001204919249605439, 0.00012048811562937997, 0.00012048429954366255, 0.00012048047646638977, 0.00012047663758109181, 0.00012047281512002391, 0.00012046899483937327, 0.00012046518420469901, 0.0001204613633078435, 0.00012045752603415831, 0.00012045371378787129, 0.00012044988220223113, 0.0001204460711409535, 0.0001204422186991482, 0.0001204384025660304, 0.00012043456647735459, 0.00012043075989541243, 0.00012042693312091038, 0.00012042312954889204, 0.00012041930248998773, 0.00012041547552588416, 0.00012041165424982566, 0.00012040782038896748, 0.00012040401302491912, 0.00012040018698512289, 0.00012039637246361785, 0.00012039255730220775, 0.00012038870542920695, 0.00012038488867988427, 0.00012038106152617921, 0.00012037724901919013, 0.00012037343670180255, 0.00012036960817348661, 0.00012036579310687725, 0.00012036196533696733, 0.00012035814422681012, 0.0001203543383084732, 0.00012035052397656968, 0.00012034669758127062, 0.00012034288111635019, 0.00012033907955884782, 0.00012033526342573002, 0.00012033144622610378, 0.00012032764035516725, 0.00012032382725567348, 0.00012032000337259432, 0.00012031618598336657, 0.00012031238982950698, 0.00012030856442961581, 0.00012030473571169837, 0.00012030092664123649, 0.00012029710716639223, 0.00012029327835367404, 0.00012028944629403013, 0.00012028561383148303, 0.00012028178008912581, 0.00012027794857458622, 0.00012027411414492354, 0.00012027026957158054, 0.00012026645092624285, 0.00012026261846369575, 0.0001202587663062927, 0.00012025494460363081, 0.00012025111830313251, 0.00012024729261883907, 0.00012024346833285673, 0.00012023962868915277, 0.00012023582298411754, 0.00012023199782122824, 0.00012022814767834114, 0.00012022430812943793, 0.00012022047398417751, 0.00012021665052770172, 0.00012021279017003374, 0.0001202089806729685, 0.00012020515643438651, 0.00012020132641295875, 0.00012019750636931, 0.0001201936788838023, 0.00012018982020884764, 0.00012018597779222173, 0.00012018213914392565, 0.00012017831206132115, 0.0001201744508504464, 0.00012017060599270117, 0.00012016676850571428, 0.00012016294663715106, 0.0001201591200522505, 0.00012015527735122234, 0.00012015144145214803, 0.00012014760320675513, 0.00012014375024354571, 0.00012013992685817049, 0.00012013608811507366, 0.00012013226280998323, 0.00012012842797741737, 0.00012012461262640577, 0.00012012078627850709, 0.00012011695962250597, 0.00012011311798798624, 0.0001201092859757427, 0.00012010544808585263, 0.00012010160668833479, 0.00012009776879844472, 0.00012009393337337416, 0.00012009009799570397, 0.00012008626302093698, 0.00012008242252402626, 0.00012007856586358755, 0.00012007473434904796, 0.00012007088209684417, 0.00012006701185619792, 0.00012006317448771198, 0.00012005935157634051, 0.00012005550475147968, 0.00012005165982263387, 0.00012004782399466013, 0.0001200439781178068, 0.00012004013186175049, 0.00012003629624707842, 0.00012003244124565285, 0.00012002859074726294, 0.00012002474366170005, 0.00012002088152651799, 0.00012001701759012167, 0.00012001315088080339, 0.00012000930730286827, 0.0001200054452150866, 0.0001200015806624854, 0.00011999772938198929, 0.00011999387324295471, 0.00011999000949615989, 0.00011998615219581612, 0.0001199823003465155, 0.00011997843131457883, 0.00011997457050660729, 0.00011997069661613213, 0.00011996681950243147, 0.00011996295530533309, 0.00011995910848047227, 0.00011995525276804106, 0.0001199513889027453, 0.00011994752214602665, 0.00011994365868363408, 0.00011993978465095781, 0.00011993590229951567, 0.00011993203831571898, 0.00011992816911788099, 0.00011992430034664638, 0.0001199204303904024, 0.00011991657406176631, 0.00011991270754204954, 0.00011990881822275223, 0.0001199049398766418, 0.00011990103204749792, 0.00011989717249563631, 0.00011989329886586324, 0.00011988938060863677, 0.00011988549749878863, 0.00011988160026362864, 0.00011987771260334446, 0.00011987382219383851, 0.00011986993621626765, 0.00011986602459509375, 0.00011986213003805496, 0.00011985822097650132], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-14 21:06:12,245 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:92]: done!
[2017-12-14 21:06:12,245 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:152]: >> Executing classifier part ... 
[2017-12-14 21:06:12,245 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:97]: =======================================
[2017-12-14 21:06:12,245 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7ff8086a9400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-14 21:06:12,404 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:110]: training ... 
[2017-12-15 02:10:54,884 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:122]: trained!
[2017-12-15 02:10:54,887 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:125]: Training history: 
{'val_loss': [0.00011974980944755581, 0.00011974610496613858, 0.00011974241437518589, 0.00011973871650827559, 0.00011973500916653105, 0.00011973131857557835, 0.00011972761981481577, 0.00011972391794344721, 0.00011972020707992466, 0.00011971652229901184, 0.00011971284061082792, 0.00011970915811817695, 0.00011970547807468125, 0.00011970177505918176, 0.00011969806584034742, 0.00011969437083376444, 0.0001196906770606976, 0.0001196869917792275, 0.00011968330887540447, 0.00011967961011464188, 0.00011967590979857632, 0.00011967222247912299, 0.0001196685352490549, 0.00011966482177548366, 0.000119661124158852, 0.00011965741885509068, 0.00011965375450764113, 0.00011965007634123522, 0.00011964639956923889, 0.00011964271682630927, 0.00011963901593817824, 0.00011963531644445677, 0.00011963161923899716, 0.00011962793805137052, 0.00011962425915200574, 0.00011962057551522384, 0.00011961688444159092, 0.00011961318053223916, 0.00011960948822509008, 0.00011960580230004632, 0.00011960212291800131, 0.00011959843600972004, 0.0001195947357830397, 0.00011959104985799595, 0.00011958734936315992, 0.00011958366865821353, 0.00011957995026845471, 0.00011957626629200895, 0.00011957256236478014, 0.00011956888754138178, 0.0001195651944118886, 0.00011956152057172776, 0.00011955782204336676, 0.00011955412335411235, 0.0001195504343005856, 0.00011954673487837232, 0.00011954305032986109, 0.00011953935866628566, 0.00011953566848650502, 0.00011953197984415031, 0.00011952826399293199, 0.00011952457944442077, 0.00011952087977192885, 0.00011951720224909658, 0.00011951351843354423, 0.00011950981729513455, 0.00011950612759803415, 0.00011950242490432149, 0.00011949872605417368, 0.00011949503978946605, 0.00011949134755382515, 0.00011948764730926777, 0.00011948395532390551, 0.00011948027420778707, 0.00011947658238331821, 0.0001194728854102602, 0.00011946920070085556, 0.00011946550798253442, 0.00011946180103408489, 0.0001194581060275019, 0.00011945443031025126, 0.00011945072507799811, 0.00011944702279545751, 0.00011944333309835711, 0.00011943963923590505, 0.00011943593082153777, 0.00011943221595355696, 0.00011942850483975577, 0.00011942478759412788, 0.00011942107959093265, 0.00011941736062910838, 0.0001194136377522111, 0.00011940993563056391, 0.00011940621847432125, 0.00011940248390583605, 0.00011939877819090267, 0.00011939506478883962, 0.00011939135351414502, 0.00011938764248972905, 0.0001193839209178561, 0.0001193802295045593, 0.00011937652312817524, 0.00011937279462000854, 0.00011936907639114314, 0.00011936535809076957, 0.00011936165065963979, 0.00011935791119284407, 0.00011935421488123674, 0.0001193505060556974, 0.00011934679649719917, 0.00011934308979902828, 0.00011933937820254685, 0.00011933564159607844, 0.00011933190988792056, 0.00011932818929928513, 0.00011932447385923886, 0.00011932073210418128, 0.00011931700709991555, 0.00011931327523086426, 0.00011930957196508614, 0.00011930586036860471, 0.00011930213250401167, 0.0001192984070170657, 0.00011929468528429935, 0.00011929094899961776, 0.00011928723936961136, 0.00011928351207708377, 0.00011927978853873579, 0.00011927605241494761, 0.00011927233500842632, 0.000119268619157208, 0.00011926489872946599, 0.00011926116146154687, 0.00011925743458019135, 0.00011925370760945059, 0.00011924997205772787, 0.00011924624346017595, 0.00011924251493413221, 0.00011923878976897306, 0.00011923506394236324, 0.00011923134138725277, 0.00011922760494167777, 0.00011922388998431175, 0.00011922015173315512, 0.00011921640187979582, 0.00011921268742298707, 0.00011920898514044647, 0.00011920525302111654, 0.0001192015270336133, 0.00011919780987737064, 0.00011919408176249895, 0.000119190342206318, 0.00011918662603331286, 0.00011918288778215623, 0.00011917915934549773, 0.00011917542576025005, 0.00011917168407670064, 0.00011916794394845423, 0.0001191642046425519, 0.00011916047751091773, 0.00011915673673909766, 0.00011915299105109002, 0.00011914925762673575, 0.00011914551962585776, 0.00011914177835348041, 0.00011913804158611859, 0.00011913430145787218, 0.00011913055233747175, 0.00011912681262039738, 0.0001191230603178828, 0.00011911930442208205, 0.0001191155587340744, 0.00011911182581027741, 0.00011910808838146489, 0.00011910433264655755, 0.00011910058091610844, 0.00011909682829180705, 0.00011909306880272009, 0.00011908930277063441, 0.00011908554163685924, 0.00011908178990641015, 0.00011907803671004329, 0.00011907428211926686, 0.00011907053979214381, 0.00011906679018906315, 0.00011906302383519064, 0.0001190592584466786, 0.00011905547312526056, 0.00011905172564954834, 0.00011904796476605181, 0.00011904416514299719, 0.00011904039821705921, 0.00011903661559507507, 0.00011903283370604982, 0.00011902906252537495, 0.00011902528832347936, 0.00011902149025572771, 0.00011901770239576917, 0.00011901390448891093, 0.0001190101352747111], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012062212120679417, 0.00012061828163419077, 0.0001206144306617971, 0.00012061059699054043, 0.00012060676045156105, 0.00012060291365040042, 0.00012059908374747358, 0.00012059524381936737, 0.00012059140481556847, 0.00012058755099915229, 0.00012058372806408063, 0.00012057991254716771, 0.00012057610001647845, 0.00012057227558829497, 0.00012056843421447731, 0.00012056459198745289, 0.0001205607597145073, 0.00012055693123359172, 0.0001205531169016882, 0.00012054930195357978, 0.00012054546555680153, 0.0001205416323832488, 0.00012053780963777864, 0.00012053398689230849, 0.00012053013819513284, 0.00012052630817370508, 0.00012052247246423227, 0.0001205186722339404, 0.00012051486693180837, 0.00012051105044318775, 0.00012050723701189136, 0.0001205034015868208, 0.00012049956618545042, 0.00012049573386510446, 0.0001204919249605439, 0.00012048811562937997, 0.00012048429954366255, 0.00012048047646638977, 0.00012047663758109181, 0.00012047281512002391, 0.00012046899483937327, 0.00012046518420469901, 0.0001204613633078435, 0.00012045752603415831, 0.00012045371378787129, 0.00012044988220223113, 0.0001204460711409535, 0.0001204422186991482, 0.0001204384025660304, 0.00012043456647735459, 0.00012043075989541243, 0.00012042693312091038, 0.00012042312954889204, 0.00012041930248998773, 0.00012041547552588416, 0.00012041165424982566, 0.00012040782038896748, 0.00012040401302491912, 0.00012040018698512289, 0.00012039637246361785, 0.00012039255730220775, 0.00012038870542920695, 0.00012038488867988427, 0.00012038106152617921, 0.00012037724901919013, 0.00012037343670180255, 0.00012036960817348661, 0.00012036579310687725, 0.00012036196533696733, 0.00012035814422681012, 0.0001203543383084732, 0.00012035052397656968, 0.00012034669758127062, 0.00012034288111635019, 0.00012033907955884782, 0.00012033526342573002, 0.00012033144622610378, 0.00012032764035516725, 0.00012032382725567348, 0.00012032000337259432, 0.00012031618598336657, 0.00012031238982950698, 0.00012030856442961581, 0.00012030473571169837, 0.00012030092664123649, 0.00012029710716639223, 0.00012029327835367404, 0.00012028944629403013, 0.00012028561383148303, 0.00012028178008912581, 0.00012027794857458622, 0.00012027411414492354, 0.00012027026957158054, 0.00012026645092624285, 0.00012026261846369575, 0.0001202587663062927, 0.00012025494460363081, 0.00012025111830313251, 0.00012024729261883907, 0.00012024346833285673, 0.00012023962868915277, 0.00012023582298411754, 0.00012023199782122824, 0.00012022814767834114, 0.00012022430812943793, 0.00012022047398417751, 0.00012021665052770172, 0.00012021279017003374, 0.0001202089806729685, 0.00012020515643438651, 0.00012020132641295875, 0.00012019750636931, 0.0001201936788838023, 0.00012018982020884764, 0.00012018597779222173, 0.00012018213914392565, 0.00012017831206132115, 0.0001201744508504464, 0.00012017060599270117, 0.00012016676850571428, 0.00012016294663715106, 0.0001201591200522505, 0.00012015527735122234, 0.00012015144145214803, 0.00012014760320675513, 0.00012014375024354571, 0.00012013992685817049, 0.00012013608811507366, 0.00012013226280998323, 0.00012012842797741737, 0.00012012461262640577, 0.00012012078627850709, 0.00012011695962250597, 0.00012011311798798624, 0.0001201092859757427, 0.00012010544808585263, 0.00012010160668833479, 0.00012009776879844472, 0.00012009393337337416, 0.00012009009799570397, 0.00012008626302093698, 0.00012008242252402626, 0.00012007856586358755, 0.00012007473434904796, 0.00012007088209684417, 0.00012006701185619792, 0.00012006317448771198, 0.00012005935157634051, 0.00012005550475147968, 0.00012005165982263387, 0.00012004782399466013, 0.0001200439781178068, 0.00012004013186175049, 0.00012003629624707842, 0.00012003244124565285, 0.00012002859074726294, 0.00012002474366170005, 0.00012002088152651799, 0.00012001701759012167, 0.00012001315088080339, 0.00012000930730286827, 0.0001200054452150866, 0.0001200015806624854, 0.00011999772938198929, 0.00011999387324295471, 0.00011999000949615989, 0.00011998615219581612, 0.0001199823003465155, 0.00011997843131457883, 0.00011997457050660729, 0.00011997069661613213, 0.00011996681950243147, 0.00011996295530533309, 0.00011995910848047227, 0.00011995525276804106, 0.0001199513889027453, 0.00011994752214602665, 0.00011994365868363408, 0.00011993978465095781, 0.00011993590229951567, 0.00011993203831571898, 0.00011992816911788099, 0.00011992430034664638, 0.0001199204303904024, 0.00011991657406176631, 0.00011991270754204954, 0.00011990881822275223, 0.0001199049398766418, 0.00011990103204749792, 0.00011989717249563631, 0.00011989329886586324, 0.00011988938060863677, 0.00011988549749878863, 0.00011988160026362864, 0.00011987771260334446, 0.00011987382219383851, 0.00011986993621626765, 0.00011986602459509375, 0.00011986213003805496, 0.00011985822097650132], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 02:10:54,888 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:129]: evaluating model ... 
[2017-12-15 02:11:03,110 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:133]: evaluated! 
[2017-12-15 02:11:03,112 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:135]: generating reports ... 
[2017-12-15 02:11:05,442 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:138]: done!
[2017-12-15 02:11:05,442 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_9 finished!
[2018-04-29 11:38:17,512 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_9 was already executed!
[2018-04-29 11:42:07,308 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_9 was already executed!
[2018-04-29 13:12:02,721 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_9
[2018-04-29 13:12:02,722 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:146]: >> Printing header log
[2018-04-29 13:12:02,722 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_9
	layers = 9216,8294
	using GLOBAL obj = 
		{'numpy_seed': 666, 'autoencoder_configs': {'optimizer': <keras.optimizers.SGD object at 0x7fa2ec0fb860>, 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'loss_function': 'mse', 'hidden_layer_activation': 'relu'}, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'data_dir': '/home/dhiego/malware_dataset/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'store_history': True, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'shuffle_batches': True, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'batch': 32, 'mlp_configs': {'activation': 'sigmoid', 'classifier_dim': 9, 'optimizer': <keras.optimizers.SGD object at 0x7fa2ec0fb8d0>, 'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False}, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:12:02,722 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,705 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,705 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,706 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:57]: =======================================
[2018-04-29 13:12:20,706 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:62]: setting configurations for autoencoder: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7fa2ec0fb860>, 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'loss_function': 'mse', 'hidden_layer_activation': 'relu'}
[2018-04-29 13:12:20,751 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,931 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_9
[2018-04-29 13:14:12,931 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:146]: >> Printing header log
[2018-04-29 13:14:12,931 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_9
	layers = 9216,8294
	using GLOBAL obj = 
		{'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f4fb9fb1828>}, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'epochs': 200, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'batch': 32, 'mlp_configs': {'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7f4fb9fb1898>}, 'numpy_seed': 666, 'shuffle_batches': True, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'store_history': True, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:14:12,931 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:148]: >> Loading dataset... 
[2018-04-29 13:14:31,111 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:31,112 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:31,112 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:57]: =======================================
[2018-04-29 13:14:31,112 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f4fb9fb1828>}
[2018-04-29 13:14:31,160 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,712 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_9
[2018-04-29 13:16:33,712 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:146]: >> Printing header log
[2018-04-29 13:16:33,712 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_9
	layers = 9216,8294
	using GLOBAL obj = 
		{'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'mlp_configs': {'activation': 'sigmoid', 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7fb81785f898>, 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy'}, 'shuffle_batches': True, 'data_dir': '/home/dhiego/malware_dataset/', 'epochs': 200, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'batch': 32, 'numpy_seed': 666, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7fb81785f828>, 'output_layer_activation': 'relu', 'loss_function': 'mse'}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'store_history': True}
	=======================================
	
[2018-04-29 13:16:33,712 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:148]: >> Loading dataset... 
[2018-04-29 13:16:58,354 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:58,354 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:58,354 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:57]: =======================================
[2018-04-29 13:16:58,355 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7fb81785f828>, 'output_layer_activation': 'relu', 'loss_function': 'mse'}
[2018-04-29 13:16:58,437 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:21,095 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_9
[2018-04-29 14:30:21,096 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:146]: >> Printing header log
[2018-04-29 14:30:21,096 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_9
	layers = 9216,8294
	using GLOBAL obj = 
		{'shuffle_batches': True, 'epochs': 200, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'store_history': True, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'batch': 32, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'autoencoder_configs': {'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7fefb5fb58d0>, 'loss_function': 'mse'}, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'data_dir': '/home/dhiego/malware_dataset/', 'numpy_seed': 666, 'mlp_configs': {'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7fefb5fb5940>, 'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'activation': 'sigmoid'}}
	=======================================
	
[2018-04-29 14:30:21,096 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:148]: >> Loading dataset... 
[2018-04-29 14:30:39,126 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:39,126 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:39,126 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:57]: =======================================
[2018-04-29 14:30:39,126 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:62]: setting configurations for autoencoder: 
	 {'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7fefb5fb58d0>, 'loss_function': 'mse'}
[2018-04-29 14:30:39,171 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:73]: training and evaluate autoencoder
[2018-04-29 19:46:10,030 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:85]: trained and evaluated!
[2018-04-29 19:46:10,031 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:88]: Training history: 
{'val_loss': [0.00011909573750598448, 0.00011909123293738538, 0.00011908674152629194, 0.00011908226507828577, 0.00011907777153982389, 0.00011907328266727095, 0.0001190687919176282, 0.00011906426920382768, 0.00011905974003641365, 0.00011905519615619097, 0.00011905064421342068, 0.0001190460901254049, 0.00011904149981849444, 0.00011903691659089412, 0.00011903232748174574, 0.00011902770526430865, 0.00011902310074514685, 0.0001190184469210929, 0.0001190138472108564, 0.00011900921759232237, 0.00011900458044755211, 0.00011899993014527618, 0.00011899525467211984, 0.0001189905373309224, 0.00011898585899743876, 0.00011898114878918257, 0.0001189764039173347, 0.00011897163648465511, 0.00011896686463634523, 0.00011896210161929594, 0.0001189573140034317, 0.00011895251272950455, 0.00011894770475168523, 0.00011894286019743034, 0.0001189380124968154, 0.0001189331465616138, 0.0001189282677191852, 0.00011892339924544312, 0.00011891846922103255, 0.00011891353150949235, 0.00011890856624942465, 0.00011890359255139137, 0.00011889863210024897, 0.00011889362316655856, 0.00011888861920268685, 0.00011888359496624528, 0.0001188785743230899, 0.00011887353169116825, 0.0001188684899530989, 0.00011886342123856754, 0.0001188583501463891, 0.00011885325592131347, 0.00011884817052749844, 0.00011884305266896835, 0.00011883788103628469, 0.00011883271029745332, 0.00011882750718329212, 0.00011882229884903357, 0.00011881710015050266, 0.00011881185552584126, 0.00011880653846339055, 0.00011880125913938338, 0.00011879597237852518, 0.00011879064278426541, 0.0001187852921844769, 0.00011877992506629812, 0.00011877454911685877, 0.00011876917479423055, 0.00011876380742577315, 0.0001187583898585713, 0.00011875298512708831, 0.00011874752569184535, 0.00011874207796606737, 0.00011873659082140353, 0.00011873107425112241, 0.00011872547709111912, 0.00011871986137474235, 0.00011871420241379195, 0.00011870852743500858, 0.00011870274544422943, 0.00011869695249482125, 0.00011869114367059645, 0.00011868527542307163, 0.00011867941387943896, 0.00011867352569900815, 0.00011866752823619675, 0.00011866146947299553, 0.00011865533492899744, 0.00011864919132133717, 0.00011864296653401222, 0.00011863666303405487, 0.00011863037694634009, 0.00011862399521643062, 0.0001186176174909794, 0.00011861116807857477, 0.00011860471090753231, 0.00011859828192859097, 0.00011859179924700424, 0.00011858521193106881, 0.00011857863310673012, 0.00011857205379971118], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011989562188715792, 0.00011989108114450235, 0.00011988649947162268, 0.00011988192950663572, 0.00011987736940092683, 0.0001198727926576862, 0.00011986822835704409, 0.00011986365346241809, 0.00011985904538752935, 0.00011985444598690932, 0.0001198498301857594, 0.00011984520883876558, 0.00011984059521803292, 0.00011983596652398092, 0.00011983134529548803, 0.00011982672082006943, 0.00011982207797700539, 0.00011981746200995416, 0.0001198127890439516, 0.00011980815537286018, 0.00011980350430583101, 0.00011979884098580483, 0.00011979417110082666, 0.00011978949555150364, 0.0001197847756354293, 0.00011978009416105936, 0.00011977538711418692, 0.00011977065541911931, 0.00011976591907881491, 0.00011976116728598816, 0.00011975642189221208, 0.00011975164559339127, 0.00011974685303624173, 0.00011974205616565804, 0.0001197372347416799, 0.00011973242284517723, 0.00011972759594645576, 0.00011972274008610495, 0.00011971791325848403, 0.00011971300916825133, 0.00011970810564682312, 0.00011970316520050253, 0.0001196982117664791, 0.00011969326141348005, 0.00011968826541391957, 0.00011968326832415044, 0.0001196782518950282, 0.00011967322546442673, 0.00011966817414862822, 0.00011966314771802677, 0.00011965807419515242, 0.000119653008114137, 0.00011964790719384571, 0.00011964280077511087, 0.00011963765560656919, 0.00011963246538397076, 0.00011962727589607816, 0.00011962206164148943, 0.0001196168271943408, 0.00011961160528459147, 0.00011960633836818574, 0.00011960099186654982, 0.00011959567920878189, 0.00011959037624439073, 0.00011958501872216755, 0.00011957963802116082, 0.00011957423172395142, 0.00011956880511568119, 0.00011956336144327583, 0.00011955790558897402, 0.00011955240020127998, 0.00011954688725322606, 0.0001195413230890666, 0.00011953575070094202, 0.00011953012493999481, 0.00011952449695122995, 0.00011951881525783985, 0.00011951311413029585, 0.00011950734972325077, 0.00011950155870089495, 0.00011949571326290824, 0.000119489853486308, 0.00011948396752100038, 0.00011947800599949446, 0.00011947206723016872, 0.00011946610559016187, 0.00011946006604763815, 0.00011945398183026069, 0.00011944782817133335, 0.00011944166311261574, 0.00011943543102976742, 0.00011942913784783528, 0.00011942283461702356, 0.00011941646429098056, 0.00011941006772882981, 0.00011940359428327027, 0.00011939711183163942, 0.00011939064371862209, 0.00011938411564412997, 0.00011937748181940048, 0.00011937083552837226], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-04-29 19:46:10,032 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:92]: done!
[2018-04-29 19:46:10,032 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:152]: >> Executing classifier part ... 
[2018-04-29 19:46:10,032 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:97]: =======================================
[2018-04-29 19:46:10,032 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:101]: setting configurations for classifier: 
	 {'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7fefb5fb5940>, 'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'activation': 'sigmoid'}
[2018-04-29 19:46:10,537 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:110]: training ... 
[2018-04-30 00:18:33,890 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:122]: trained!
[2018-04-30 00:18:33,892 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:125]: Training history: 
{'val_loss': [0.00011909573750598448, 0.00011909123293738538, 0.00011908674152629194, 0.00011908226507828577, 0.00011907777153982389, 0.00011907328266727095, 0.0001190687919176282, 0.00011906426920382768, 0.00011905974003641365, 0.00011905519615619097, 0.00011905064421342068, 0.0001190460901254049, 0.00011904149981849444, 0.00011903691659089412, 0.00011903232748174574, 0.00011902770526430865, 0.00011902310074514685, 0.0001190184469210929, 0.0001190138472108564, 0.00011900921759232237, 0.00011900458044755211, 0.00011899993014527618, 0.00011899525467211984, 0.0001189905373309224, 0.00011898585899743876, 0.00011898114878918257, 0.0001189764039173347, 0.00011897163648465511, 0.00011896686463634523, 0.00011896210161929594, 0.0001189573140034317, 0.00011895251272950455, 0.00011894770475168523, 0.00011894286019743034, 0.0001189380124968154, 0.0001189331465616138, 0.0001189282677191852, 0.00011892339924544312, 0.00011891846922103255, 0.00011891353150949235, 0.00011890856624942465, 0.00011890359255139137, 0.00011889863210024897, 0.00011889362316655856, 0.00011888861920268685, 0.00011888359496624528, 0.0001188785743230899, 0.00011887353169116825, 0.0001188684899530989, 0.00011886342123856754, 0.0001188583501463891, 0.00011885325592131347, 0.00011884817052749844, 0.00011884305266896835, 0.00011883788103628469, 0.00011883271029745332, 0.00011882750718329212, 0.00011882229884903357, 0.00011881710015050266, 0.00011881185552584126, 0.00011880653846339055, 0.00011880125913938338, 0.00011879597237852518, 0.00011879064278426541, 0.0001187852921844769, 0.00011877992506629812, 0.00011877454911685877, 0.00011876917479423055, 0.00011876380742577315, 0.0001187583898585713, 0.00011875298512708831, 0.00011874752569184535, 0.00011874207796606737, 0.00011873659082140353, 0.00011873107425112241, 0.00011872547709111912, 0.00011871986137474235, 0.00011871420241379195, 0.00011870852743500858, 0.00011870274544422943, 0.00011869695249482125, 0.00011869114367059645, 0.00011868527542307163, 0.00011867941387943896, 0.00011867352569900815, 0.00011866752823619675, 0.00011866146947299553, 0.00011865533492899744, 0.00011864919132133717, 0.00011864296653401222, 0.00011863666303405487, 0.00011863037694634009, 0.00011862399521643062, 0.0001186176174909794, 0.00011861116807857477, 0.00011860471090753231, 0.00011859828192859097, 0.00011859179924700424, 0.00011858521193106881, 0.00011857863310673012, 0.00011857205379971118], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011989562188715792, 0.00011989108114450235, 0.00011988649947162268, 0.00011988192950663572, 0.00011987736940092683, 0.0001198727926576862, 0.00011986822835704409, 0.00011986365346241809, 0.00011985904538752935, 0.00011985444598690932, 0.0001198498301857594, 0.00011984520883876558, 0.00011984059521803292, 0.00011983596652398092, 0.00011983134529548803, 0.00011982672082006943, 0.00011982207797700539, 0.00011981746200995416, 0.0001198127890439516, 0.00011980815537286018, 0.00011980350430583101, 0.00011979884098580483, 0.00011979417110082666, 0.00011978949555150364, 0.0001197847756354293, 0.00011978009416105936, 0.00011977538711418692, 0.00011977065541911931, 0.00011976591907881491, 0.00011976116728598816, 0.00011975642189221208, 0.00011975164559339127, 0.00011974685303624173, 0.00011974205616565804, 0.0001197372347416799, 0.00011973242284517723, 0.00011972759594645576, 0.00011972274008610495, 0.00011971791325848403, 0.00011971300916825133, 0.00011970810564682312, 0.00011970316520050253, 0.0001196982117664791, 0.00011969326141348005, 0.00011968826541391957, 0.00011968326832415044, 0.0001196782518950282, 0.00011967322546442673, 0.00011966817414862822, 0.00011966314771802677, 0.00011965807419515242, 0.000119653008114137, 0.00011964790719384571, 0.00011964280077511087, 0.00011963765560656919, 0.00011963246538397076, 0.00011962727589607816, 0.00011962206164148943, 0.0001196168271943408, 0.00011961160528459147, 0.00011960633836818574, 0.00011960099186654982, 0.00011959567920878189, 0.00011959037624439073, 0.00011958501872216755, 0.00011957963802116082, 0.00011957423172395142, 0.00011956880511568119, 0.00011956336144327583, 0.00011955790558897402, 0.00011955240020127998, 0.00011954688725322606, 0.0001195413230890666, 0.00011953575070094202, 0.00011953012493999481, 0.00011952449695122995, 0.00011951881525783985, 0.00011951311413029585, 0.00011950734972325077, 0.00011950155870089495, 0.00011949571326290824, 0.000119489853486308, 0.00011948396752100038, 0.00011947800599949446, 0.00011947206723016872, 0.00011946610559016187, 0.00011946006604763815, 0.00011945398183026069, 0.00011944782817133335, 0.00011944166311261574, 0.00011943543102976742, 0.00011942913784783528, 0.00011942283461702356, 0.00011941646429098056, 0.00011941006772882981, 0.00011940359428327027, 0.00011939711183163942, 0.00011939064371862209, 0.00011938411564412997, 0.00011937748181940048, 0.00011937083552837226], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-04-30 00:18:33,892 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:129]: evaluating model ... 
[2018-04-30 00:18:39,380 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:133]: evaluated! 
[2018-04-30 00:18:39,380 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:135]: generating reports ... 
[2018-04-30 00:18:41,865 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:138]: done!
[2018-04-30 00:18:41,865 AE_BIGRAMA_1L_MINIDS_UNDER_F0_9.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_9 finished!
