[2018-01-18 00:19:28,515 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_FULLDS_UNDER_F0_5
[2018-01-18 00:19:28,515 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:146]: >> Printing header log
[2018-01-18 00:19:28,516 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_FULLDS_UNDER_F0_5
	layers = 9216,4608
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f37bb09fbe0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f37bb09f470>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2018-01-18 00:19:28,516 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:148]: >> Loading dataset... 
[2018-01-18 00:22:01,355 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (8147, 9216)
	trainy shape = (8147, 9)
	valx shape = (2721, 9216)
	valy shape = (2721, 9)
	=======================================
	
[2018-01-18 00:22:01,356 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:150]: >> Executing autoencoder part ... 
[2018-01-18 00:22:01,356 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:57]: =======================================
[2018-01-18 00:22:01,356 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f37bb09fbe0>, 'discard_decoder_function': True}
[2018-01-18 00:22:01,400 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:73]: training and evaluate autoencoder
[2018-01-19 03:15:02,453 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:85]: trained and evaluated!
[2018-01-19 03:15:02,455 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:88]: Training history: 
{'val_loss': [0.00011753226332965871, 0.00011751163840177169, 0.0001174910412861725, 0.00011747046190187611, 0.00011744989461476173, 0.00011742935595550557, 0.00011740882080721316, 0.00011738830156388063, 0.00011736781188698081, 0.00011734733665771016, 0.00011732687708204329, 0.00011730644087447402, 0.00011728602230996551, 0.000117265619845619, 0.0001172452268659552, 0.00011722485078597979, 0.00011720449170997884, 0.00011718415695401997, 0.00011716384264347525, 0.00011714353473716371, 0.00011712325639193685, 0.00011710298615963004, 0.0001170827412634858, 0.0001170625228292587, 0.00011704231902984087, 0.00011702213294300784, 0.00011700196520249794, 0.00011698181581633315, 0.00011696168948540779, 0.00011694157987233876, 0.00011692148293403601, 0.00011690141530278783, 0.00011688136424767387, 0.00011686132849586952, 0.00011684131413072779, 0.00011682131506354766, 0.0001168013348400549, 0.0001167813763080609, 0.00011676143533088563, 0.00011674151712015766, 0.00011672161162698002, 0.00011670171661053958, 0.00011668184795707823, 0.00011666199124838081, 0.00011664215555733385, 0.0001166223320142749, 0.00011660253111733321, 0.00011658273804451941, 0.00011656296861522534, 0.00011654320659291496, 0.00011652347676290657, 0.00011650376276833404, 0.0001164840696684079, 0.00011646439665023173, 0.00011644474291427914, 0.0001164251017595028, 0.00011640546899574263, 0.00011638585485640169, 0.00011636625710066692, 0.00011634666004284647, 0.00011632708057193278, 0.00011630751498175996, 0.00011628795837890562, 0.00011626840788347032, 0.00011624885607510035, 0.00011622933014571524, 0.0001162098124121442, 0.0001161903138618587, 0.0001161708328476739, 0.00011615136673035051, 0.00011613190345549052, 0.00011611244455262272, 0.00011609298108256064, 0.00011607352200588273, 0.0001160540811712419, 0.00011603463005239194, 0.00011601519751250323, 0.00011599575737577674, 0.00011597630091427214, 0.00011595684985157622, 0.00011593740718524448, 0.00011591794165085337, 0.00011589846037729044, 0.00011587897116729157, 0.00011585945936198148, 0.00011583994422219174, 0.00011582042524253608, 0.00011580088313811694, 0.000115781317272522, 0.0001157617011651161, 0.00011574203662887214, 0.00011572233721295485, 0.0001157026020135518, 0.00011568279090994704, 0.00011566291701811709, 0.00011564300031017787, 0.00011562297001066222, 0.00011560279504500072, 0.00011558248284959106, 0.00011556198096587515, 0.00011554127270006275], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011768728000307728, 0.00011766656168653182, 0.0001176458647664995, 0.00011762519473366245, 0.00011760453795955461, 0.00011758388899011026, 0.00011756326343733569, 0.00011754265160054947, 0.00011752205329845548, 0.00011750148252121894, 0.0001174809335861446, 0.00011746039385341065, 0.00011743987469287305, 0.000117419369833294, 0.00011739887743313372, 0.00011737839402633211, 0.00011735792896080233, 0.00011733747370044484, 0.00011731704761982997, 0.00011729663485063625, 0.00011727623221616293, 0.00011725586194081226, 0.00011723550004527138, 0.00011721516153960774, 0.00011719484944691163, 0.00011717455484259185, 0.00011715428027819018, 0.00011713402137402135, 0.00011711378009754998, 0.00011709357095425094, 0.00011707337779448124, 0.00011705319806312674, 0.00011703304627548621, 0.00011701290969617787, 0.00011699278952818622, 0.00011697269083351294, 0.0001169526056172676, 0.00011693254034538036, 0.00011691248916100452, 0.00011689246409130605, 0.00011687245730840127, 0.00011685245831944298, 0.00011683247886670328, 0.00011681252938676505, 0.00011679259448732073, 0.00011677267876239586, 0.00011675278330155327, 0.0001167329121915466, 0.00011671305168512957, 0.00011669322009767456, 0.00011667339409199627, 0.00011665359899417869, 0.00011663381542607914, 0.00011661405262218906, 0.00011659430697980646, 0.00011657457801220043, 0.00011655486208719718, 0.00011653515807147279, 0.00011651546944537669, 0.00011649580005899518, 0.0001164761349540597, 0.00011645649317648558, 0.00011643686255621334, 0.0001164172463128118, 0.00011639764193224873, 0.00011637804373897355, 0.0001163584705440204, 0.00011633890335684518, 0.00011631935470652723, 0.00011629981971057483, 0.00011628029560846444, 0.00011626077119913307, 0.00011624125244302517, 0.00011622172964928325, 0.00011620221648834836, 0.0001161827169246216, 0.00011616321024658547, 0.00011614371694873801, 0.00011612422699817042, 0.00011610471538048517, 0.00011608520295187938, 0.0001160656929140602, 0.00011604615711254579, 0.00011602660873640471, 0.00011600704519479952, 0.00011598745970028969, 0.00011596786508649629, 0.00011594826054875039, 0.0001159286200982995, 0.0001159089468198567, 0.00011588920908126997, 0.00011586944050269695, 0.00011584963378763908, 0.00011582977531477495, 0.00011580983791558773, 0.00011578982125785008, 0.00011576975840622124, 0.00011574955638348316, 0.00011572920763860827, 0.0001157086952815867, 0.00011568802313839151], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00012274456855284154, 0.00012274456855284154, 0.00012274456855284154]}
[2018-01-19 03:15:02,455 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:92]: done!
[2018-01-19 03:15:02,455 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:152]: >> Executing classifier part ... 
[2018-01-19 03:15:02,455 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:97]: =======================================
[2018-01-19 03:15:02,455 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f37bb09f470>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2018-01-19 03:15:02,715 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:110]: training ... 
[2018-01-21 12:42:14,010 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:122]: trained!
[2018-01-21 12:42:14,012 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:125]: Training history: 
{'val_loss': [0.00011753226332965871, 0.00011751163840177169, 0.0001174910412861725, 0.00011747046190187611, 0.00011744989461476173, 0.00011742935595550557, 0.00011740882080721316, 0.00011738830156388063, 0.00011736781188698081, 0.00011734733665771016, 0.00011732687708204329, 0.00011730644087447402, 0.00011728602230996551, 0.000117265619845619, 0.0001172452268659552, 0.00011722485078597979, 0.00011720449170997884, 0.00011718415695401997, 0.00011716384264347525, 0.00011714353473716371, 0.00011712325639193685, 0.00011710298615963004, 0.0001170827412634858, 0.0001170625228292587, 0.00011704231902984087, 0.00011702213294300784, 0.00011700196520249794, 0.00011698181581633315, 0.00011696168948540779, 0.00011694157987233876, 0.00011692148293403601, 0.00011690141530278783, 0.00011688136424767387, 0.00011686132849586952, 0.00011684131413072779, 0.00011682131506354766, 0.0001168013348400549, 0.0001167813763080609, 0.00011676143533088563, 0.00011674151712015766, 0.00011672161162698002, 0.00011670171661053958, 0.00011668184795707823, 0.00011666199124838081, 0.00011664215555733385, 0.0001166223320142749, 0.00011660253111733321, 0.00011658273804451941, 0.00011656296861522534, 0.00011654320659291496, 0.00011652347676290657, 0.00011650376276833404, 0.0001164840696684079, 0.00011646439665023173, 0.00011644474291427914, 0.0001164251017595028, 0.00011640546899574263, 0.00011638585485640169, 0.00011636625710066692, 0.00011634666004284647, 0.00011632708057193278, 0.00011630751498175996, 0.00011628795837890562, 0.00011626840788347032, 0.00011624885607510035, 0.00011622933014571524, 0.0001162098124121442, 0.0001161903138618587, 0.0001161708328476739, 0.00011615136673035051, 0.00011613190345549052, 0.00011611244455262272, 0.00011609298108256064, 0.00011607352200588273, 0.0001160540811712419, 0.00011603463005239194, 0.00011601519751250323, 0.00011599575737577674, 0.00011597630091427214, 0.00011595684985157622, 0.00011593740718524448, 0.00011591794165085337, 0.00011589846037729044, 0.00011587897116729157, 0.00011585945936198148, 0.00011583994422219174, 0.00011582042524253608, 0.00011580088313811694, 0.000115781317272522, 0.0001157617011651161, 0.00011574203662887214, 0.00011572233721295485, 0.0001157026020135518, 0.00011568279090994704, 0.00011566291701811709, 0.00011564300031017787, 0.00011562297001066222, 0.00011560279504500072, 0.00011558248284959106, 0.00011556198096587515, 0.00011554127270006275], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011768728000307728, 0.00011766656168653182, 0.0001176458647664995, 0.00011762519473366245, 0.00011760453795955461, 0.00011758388899011026, 0.00011756326343733569, 0.00011754265160054947, 0.00011752205329845548, 0.00011750148252121894, 0.0001174809335861446, 0.00011746039385341065, 0.00011743987469287305, 0.000117419369833294, 0.00011739887743313372, 0.00011737839402633211, 0.00011735792896080233, 0.00011733747370044484, 0.00011731704761982997, 0.00011729663485063625, 0.00011727623221616293, 0.00011725586194081226, 0.00011723550004527138, 0.00011721516153960774, 0.00011719484944691163, 0.00011717455484259185, 0.00011715428027819018, 0.00011713402137402135, 0.00011711378009754998, 0.00011709357095425094, 0.00011707337779448124, 0.00011705319806312674, 0.00011703304627548621, 0.00011701290969617787, 0.00011699278952818622, 0.00011697269083351294, 0.0001169526056172676, 0.00011693254034538036, 0.00011691248916100452, 0.00011689246409130605, 0.00011687245730840127, 0.00011685245831944298, 0.00011683247886670328, 0.00011681252938676505, 0.00011679259448732073, 0.00011677267876239586, 0.00011675278330155327, 0.0001167329121915466, 0.00011671305168512957, 0.00011669322009767456, 0.00011667339409199627, 0.00011665359899417869, 0.00011663381542607914, 0.00011661405262218906, 0.00011659430697980646, 0.00011657457801220043, 0.00011655486208719718, 0.00011653515807147279, 0.00011651546944537669, 0.00011649580005899518, 0.0001164761349540597, 0.00011645649317648558, 0.00011643686255621334, 0.0001164172463128118, 0.00011639764193224873, 0.00011637804373897355, 0.0001163584705440204, 0.00011633890335684518, 0.00011631935470652723, 0.00011629981971057483, 0.00011628029560846444, 0.00011626077119913307, 0.00011624125244302517, 0.00011622172964928325, 0.00011620221648834836, 0.0001161827169246216, 0.00011616321024658547, 0.00011614371694873801, 0.00011612422699817042, 0.00011610471538048517, 0.00011608520295187938, 0.0001160656929140602, 0.00011604615711254579, 0.00011602660873640471, 0.00011600704519479952, 0.00011598745970028969, 0.00011596786508649629, 0.00011594826054875039, 0.0001159286200982995, 0.0001159089468198567, 0.00011588920908126997, 0.00011586944050269695, 0.00011584963378763908, 0.00011582977531477495, 0.00011580983791558773, 0.00011578982125785008, 0.00011576975840622124, 0.00011574955638348316, 0.00011572920763860827, 0.0001157086952815867, 0.00011568802313839151], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00012274456855284154, 0.00012274456855284154, 0.00012274456855284154]}
[2018-01-21 12:42:14,012 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:129]: evaluating model ... 
[2018-01-21 12:43:02,485 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:133]: evaluated! 
[2018-01-21 12:43:02,486 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:135]: generating reports ... 
[2018-01-21 12:43:07,438 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:138]: done!
[2018-01-21 12:43:07,440 AE_BIGRAMA_1L_FULLDS_UNDER_F0_5.py:154]: >> experiment AE_BIGRAMA_1L_FULLDS_UNDER_F0_5 finished!
