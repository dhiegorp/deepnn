[2017-12-14 09:31:57,985 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_8
[2017-12-14 09:31:57,986 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:146]: >> Printing header log
[2017-12-14 09:31:57,986 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_8
	layers = 9216,16589
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fbafe4a6eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fbafe489400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,986 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:148]: >> Loading dataset... 
[2017-12-14 09:32:20,970 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:20,970 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:20,970 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:57]: =======================================
[2017-12-14 09:32:20,971 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fbafe4a6eb8>, 'discard_decoder_function': True}
[2017-12-14 09:32:21,016 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:55,000 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_8
[2017-12-14 10:18:55,000 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:146]: >> Printing header log
[2017-12-14 10:18:55,001 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_8
	layers = 9216,16589
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f641cfbdeb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f641cfa0400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:55,001 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:148]: >> Loading dataset... 
[2017-12-14 10:19:18,137 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:18,138 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:18,138 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:57]: =======================================
[2017-12-14 10:19:18,138 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f641cfbdeb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:18,186 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:73]: training and evaluate autoencoder
[2017-12-15 04:24:10,989 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:85]: trained and evaluated!
[2017-12-15 04:24:10,991 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:88]: Training history: 
{'val_loss': [0.00011823860147117845, 0.0001182355363444249, 0.00011823248192602174, 0.00011822943698245281, 0.00011822637929255028, 0.00011822331972555796, 0.00011822025468818962, 0.00011821719928654894, 0.00011821414404580168, 0.00011821108562294028, 0.00011820801232637682, 0.00011820496607778356, 0.00011820191019346265, 0.00011819883714717783, 0.00011819577643605457, 0.00011819270755512141, 0.00011818964471662971, 0.00011818658482785056, 0.00011818352869325101, 0.00011818046013410466, 0.00011817740424978376, 0.00011817436413301718, 0.00011817128985321621, 0.0001181682511308592, 0.0001181651935303419, 0.00011816213787842258, 0.00011815909229128001, 0.00011815603755109003, 0.00011815298999747243, 0.0001181499482538947, 0.00011814689939525276, 0.00011814383435788443, 0.00011814078492717702, 0.0001181377280596186, 0.00011813467732388687, 0.00011813162617698307, 0.0001181285697205967, 0.00011812551056477642, 0.00011812244912069428, 0.00011811939094811152, 0.00011811635050955812, 0.00011811328776045164, 0.00011811024919898806, 0.00011810719503086353, 0.00011810413129851955, 0.00011810107509241181, 0.00011809802092428729, 0.00011809494845006794, 0.00011809190727855566, 0.00011808884436855577, 0.0001180857751658358, 0.00011808270841227108, 0.00011807962735706977, 0.00011807656673533173, 0.00011807351240631381, 0.00011807044565274909, 0.00011806738070476599, 0.00011806431557801244, 0.00011806125765570831, 0.00011805821101382004, 0.00011805514295523098, 0.000118052082744665, 0.0001180490088045279, 0.0001180459331481944, 0.00011804285666951681, 0.00011803979261538599, 0.00011803673428190982, 0.00011803368061434258, 0.00011803060969542621, 0.00011802757049038897, 0.00011802449924968576, 0.00011802142808049075, 0.00011801835912804941, 0.00011801529205269786, 0.00011801223323654146, 0.00011800917057682021, 0.00011800610611151734, 0.00011800303642611713, 0.00011799995463795695, 0.0001179968829860817, 0.00011799380659678933, 0.00011799073739406934, 0.00011798765757238419, 0.00011798458159426388, 0.00011798151239154389, 0.00011797843755755449, 0.00011797537130454707, 0.00011797229123258327, 0.00011796920953380832, 0.00011796615014558644, 0.00011796306263677163, 0.00011795997251790814, 0.00011795688942472362, 0.00011795381859519247, 0.00011795074687180903, 0.00011794766222332154, 0.00011794457855807155, 0.0001179414842738564, 0.00011793838712931394, 0.00011793529627749157, 0.00011793218540337798, 0.00011792908425437725, 0.00011792598752100684, 0.00011792286895976359, 0.00011791973463096598, 0.00011791660364517595, 0.00011791348279567083, 0.00011791036407353417, 0.00011790723644862871, 0.0001179041244303842, 0.00011790102451489964, 0.0001178979008050672, 0.00011789478258348782, 0.00011789164040666715, 0.00011788852037950614, 0.00011788539904732078, 0.00011788225973082743, 0.00011787914109807599, 0.00011787599368328091, 0.00011787286573658865, 0.00011786972495417753, 0.00011786656284445087, 0.00011786342320617069, 0.00011786027327071216, 0.00011785711434309964, 0.00011785397479420468, 0.00011785079992026736, 0.00011784763192899264, 0.00011784443467299406, 0.00011784126700350616, 0.00011783808796421719, 0.00011783490908582162, 0.00011783173185211426, 0.00011782854709217064, 0.00011782535171326186, 0.00011782215649524649, 0.00011781896013310018, 0.00011781577775080366, 0.00011781256871383193, 0.00011780936189361388, 0.00011780616985771266, 0.00011780296148219161, 0.00011779974640277844, 0.00011779650942398423, 0.00011779327489434529, 0.00011779005924286664, 0.00011778683493889786, 0.00011778358725175325, 0.00011778034226404256, 0.00011777706196916654, 0.00011777379654799256, 0.00011777051993578797, 0.00011776724879395935, 0.00011776395476951221, 0.00011776065632943475, 0.0001177573650759297, 0.00011775405869844394, 0.00011775075559245757, 0.00011774742248878844, 0.00011774410189905133, 0.00011774076250266211, 0.00011773740733871854, 0.00011773405732336415, 0.00011773066840755824, 0.00011772729509841325, 0.00011772389906754315, 0.00011772049298977333, 0.00011771706933886756, 0.00011771361340201718, 0.0001177101832974979, 0.00011770673889134203, 0.00011770327510646858, 0.00011769980552943231, 0.00011769630350555804, 0.00011769280294760151, 0.00011768928980420478, 0.0001176857823814627, 0.00011768224832192246, 0.00011767870543112163, 0.00011767513885323521, 0.0001176715430821331, 0.00011766794495126095, 0.00011766432139922977, 0.00011766068697795478, 0.00011765699061271631, 0.0001176532938363058, 0.00011764961373917896, 0.00011764589824550156, 0.00011764215411279689, 0.00011763838799132597, 0.00011763459367775392, 0.00011763078195193934, 0.00011762696295016713, 0.00011762314124896102, 0.00011761930206400418, 0.00011761542118977668, 0.00011761153010775607, 0.00011760762332968931, 0.00011760370707678831, 0.00011759978297586422], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011907671136456976, 0.00011907356371894533, 0.00011907040737535204, 0.00011906726183904432, 0.0001190641309020522, 0.00011906097723658011, 0.00011905782847704688, 0.00011905466744081642, 0.00011905152451152934, 0.00011904837871451956, 0.00011904522476464522, 0.00011904206029188756, 0.00011903891857131005, 0.00011903577478881622, 0.00011903261273347769, 0.00011902946281263526, 0.00011902630246371024, 0.00011902315180816199, 0.00011902000274052631, 0.00011901685677761522, 0.00011901369986521741, 0.00011901055572722077, 0.00011900742367631984, 0.00011900426057817305, 0.0001190011347130211, 0.00011899799026692201, 0.00011899484520461805, 0.00011899170853218052, 0.00011898856439418387, 0.0001189854277928469, 0.00011898229638185103, 0.00011897915295486001, 0.00011897599786737667, 0.00011897285500919016, 0.00011896970091711469, 0.00011896655950463963, 0.00011896341906387226, 0.00011896027222405421, 0.00011895712751725306, 0.00011895397539229318, 0.00011895082348063498, 0.00011894769270954417, 0.00011894453740875913, 0.00011894140538155838, 0.00011893826148056362, 0.00011893510162934254, 0.00011893195026278865, 0.00011892880024714547, 0.00011892563726749962, 0.00011892250102166548, 0.00011891934112304403, 0.00011891617800119706, 0.00011891301298333507, 0.00011890984090281717, 0.00011890668515172857, 0.00011890353060934953, 0.00011890036917021588, 0.00011889720756518093, 0.0001188940439930304, 0.00011889089935732981, 0.00011888775337071853, 0.00011888459162348245, 0.00011888143639379797, 0.00011887827222914275, 0.00011887510171283723, 0.00011887193693197712, 0.00011886877776806149, 0.00011886562903222844, 0.00011886247354184189, 0.00011885930982749024, 0.00011885617014512886, 0.00011885300389485713, 0.00011884983335485142, 0.0001188466695693992, 0.0001188434978443841, 0.00011884034311240357, 0.00011883718063046167, 0.00011883401189167022, 0.00011883083718043148, 0.0001188276533209203, 0.00011882447223433108, 0.0001188212866684064, 0.00011881811299997591, 0.00011881492722074953, 0.00011881174400114342, 0.00011880857604445816, 0.00011880539360695825, 0.00011880221761590937, 0.00011879902475032688, 0.00011879583046273313, 0.00011879265961462499, 0.00011878946205640533, 0.00011878625776733238, 0.00011878305530317388, 0.00011877986705912798, 0.00011877667142062354, 0.00011877346957266991, 0.00011877026559169941, 0.00011876704933403169, 0.00011876383255495984, 0.00011876061525448386, 0.00011875738233558421, 0.00011875416129047858, 0.00011875094505651105, 0.00011874770474315284, 0.00011874445705903628, 0.00011874120051104952, 0.00011873796332611609, 0.00011873473453104908, 0.00011873149478649538, 0.00011872827641951096, 0.00011872506250816181, 0.00011872182236070493, 0.0001187185859341775, 0.00011871532443285152, 0.00011871208862252898, 0.00011870885191159929, 0.00011870558642864178, 0.00011870234272615674, 0.00011869907816750656, 0.00011869582166692017, 0.00011869256042629627, 0.00011868926932343588, 0.00011868600504918795, 0.00011868273579790059, 0.00011867946521940273, 0.00011867620395507864, 0.00011867290467565351, 0.0001186696121033815, 0.00011866629282099799, 0.00011866301181441755, 0.00011865971582931851, 0.00011865640889473278, 0.00011865311300443449, 0.00011864979696897669, 0.00011864646787471548, 0.0001186431404631676, 0.00011863981639334617, 0.00011863650360481407, 0.00011863314872474868, 0.00011862980230565028, 0.00011862646361281307, 0.00011862310648122985, 0.00011861974259509313, 0.00011861635844529598, 0.00011861298925031722, 0.00011860963349334489, 0.00011860627008121193, 0.0001186028816416808, 0.00011859949087953129, 0.00011859607295696669, 0.00011859267404195262, 0.00011858926645266984, 0.00011858585857898482, 0.00011858242117486596, 0.00011857897791680076, 0.000118575537621259, 0.00011857207777306243, 0.00011856861799596642, 0.00011856513425798067, 0.00011856166319959533, 0.00011855817050293864, 0.00011855465284998435, 0.0001185511318316034, 0.0001185475686268884, 0.00011854401352763758, 0.00011854043311658634, 0.00011853684983781239, 0.00011853324989780651, 0.00011852961611393262, 0.00011852600690715337, 0.00011852238414386676, 0.00011851874571475611, 0.00011851509593325555, 0.00011851141548371215, 0.00011850772401358149, 0.00011850401386770295, 0.00011850030301081878, 0.00011849656783754207, 0.000118492828706334, 0.00011848907537871354, 0.00011848531065130279, 0.00011848154838871156, 0.00011847777358872106, 0.00011847397947307761, 0.00011847014400060668, 0.00011846630402510009, 0.00011846248085302655, 0.00011845861300609927, 0.00011845473321427739, 0.00011845086477484542, 0.00011844697073921076, 0.00011844303688726081, 0.00011843908369595774, 0.00011843511912856457, 0.00011843112915457024, 0.00011842708381693749, 0.00011842304191583197, 0.00011841897373121833, 0.00011841489234560014], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 04:24:10,992 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:92]: done!
[2017-12-15 04:24:10,992 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:152]: >> Executing classifier part ... 
[2017-12-15 04:24:10,992 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:97]: =======================================
[2017-12-15 04:24:10,992 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f641cfa0400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-15 04:24:11,425 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:110]: training ... 
[2017-12-15 09:44:03,345 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:122]: trained!
[2017-12-15 09:44:03,348 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:125]: Training history: 
{'val_loss': [0.00011823860147117845, 0.0001182355363444249, 0.00011823248192602174, 0.00011822943698245281, 0.00011822637929255028, 0.00011822331972555796, 0.00011822025468818962, 0.00011821719928654894, 0.00011821414404580168, 0.00011821108562294028, 0.00011820801232637682, 0.00011820496607778356, 0.00011820191019346265, 0.00011819883714717783, 0.00011819577643605457, 0.00011819270755512141, 0.00011818964471662971, 0.00011818658482785056, 0.00011818352869325101, 0.00011818046013410466, 0.00011817740424978376, 0.00011817436413301718, 0.00011817128985321621, 0.0001181682511308592, 0.0001181651935303419, 0.00011816213787842258, 0.00011815909229128001, 0.00011815603755109003, 0.00011815298999747243, 0.0001181499482538947, 0.00011814689939525276, 0.00011814383435788443, 0.00011814078492717702, 0.0001181377280596186, 0.00011813467732388687, 0.00011813162617698307, 0.0001181285697205967, 0.00011812551056477642, 0.00011812244912069428, 0.00011811939094811152, 0.00011811635050955812, 0.00011811328776045164, 0.00011811024919898806, 0.00011810719503086353, 0.00011810413129851955, 0.00011810107509241181, 0.00011809802092428729, 0.00011809494845006794, 0.00011809190727855566, 0.00011808884436855577, 0.0001180857751658358, 0.00011808270841227108, 0.00011807962735706977, 0.00011807656673533173, 0.00011807351240631381, 0.00011807044565274909, 0.00011806738070476599, 0.00011806431557801244, 0.00011806125765570831, 0.00011805821101382004, 0.00011805514295523098, 0.000118052082744665, 0.0001180490088045279, 0.0001180459331481944, 0.00011804285666951681, 0.00011803979261538599, 0.00011803673428190982, 0.00011803368061434258, 0.00011803060969542621, 0.00011802757049038897, 0.00011802449924968576, 0.00011802142808049075, 0.00011801835912804941, 0.00011801529205269786, 0.00011801223323654146, 0.00011800917057682021, 0.00011800610611151734, 0.00011800303642611713, 0.00011799995463795695, 0.0001179968829860817, 0.00011799380659678933, 0.00011799073739406934, 0.00011798765757238419, 0.00011798458159426388, 0.00011798151239154389, 0.00011797843755755449, 0.00011797537130454707, 0.00011797229123258327, 0.00011796920953380832, 0.00011796615014558644, 0.00011796306263677163, 0.00011795997251790814, 0.00011795688942472362, 0.00011795381859519247, 0.00011795074687180903, 0.00011794766222332154, 0.00011794457855807155, 0.0001179414842738564, 0.00011793838712931394, 0.00011793529627749157, 0.00011793218540337798, 0.00011792908425437725, 0.00011792598752100684, 0.00011792286895976359, 0.00011791973463096598, 0.00011791660364517595, 0.00011791348279567083, 0.00011791036407353417, 0.00011790723644862871, 0.0001179041244303842, 0.00011790102451489964, 0.0001178979008050672, 0.00011789478258348782, 0.00011789164040666715, 0.00011788852037950614, 0.00011788539904732078, 0.00011788225973082743, 0.00011787914109807599, 0.00011787599368328091, 0.00011787286573658865, 0.00011786972495417753, 0.00011786656284445087, 0.00011786342320617069, 0.00011786027327071216, 0.00011785711434309964, 0.00011785397479420468, 0.00011785079992026736, 0.00011784763192899264, 0.00011784443467299406, 0.00011784126700350616, 0.00011783808796421719, 0.00011783490908582162, 0.00011783173185211426, 0.00011782854709217064, 0.00011782535171326186, 0.00011782215649524649, 0.00011781896013310018, 0.00011781577775080366, 0.00011781256871383193, 0.00011780936189361388, 0.00011780616985771266, 0.00011780296148219161, 0.00011779974640277844, 0.00011779650942398423, 0.00011779327489434529, 0.00011779005924286664, 0.00011778683493889786, 0.00011778358725175325, 0.00011778034226404256, 0.00011777706196916654, 0.00011777379654799256, 0.00011777051993578797, 0.00011776724879395935, 0.00011776395476951221, 0.00011776065632943475, 0.0001177573650759297, 0.00011775405869844394, 0.00011775075559245757, 0.00011774742248878844, 0.00011774410189905133, 0.00011774076250266211, 0.00011773740733871854, 0.00011773405732336415, 0.00011773066840755824, 0.00011772729509841325, 0.00011772389906754315, 0.00011772049298977333, 0.00011771706933886756, 0.00011771361340201718, 0.0001177101832974979, 0.00011770673889134203, 0.00011770327510646858, 0.00011769980552943231, 0.00011769630350555804, 0.00011769280294760151, 0.00011768928980420478, 0.0001176857823814627, 0.00011768224832192246, 0.00011767870543112163, 0.00011767513885323521, 0.0001176715430821331, 0.00011766794495126095, 0.00011766432139922977, 0.00011766068697795478, 0.00011765699061271631, 0.0001176532938363058, 0.00011764961373917896, 0.00011764589824550156, 0.00011764215411279689, 0.00011763838799132597, 0.00011763459367775392, 0.00011763078195193934, 0.00011762696295016713, 0.00011762314124896102, 0.00011761930206400418, 0.00011761542118977668, 0.00011761153010775607, 0.00011760762332968931, 0.00011760370707678831, 0.00011759978297586422], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011907671136456976, 0.00011907356371894533, 0.00011907040737535204, 0.00011906726183904432, 0.0001190641309020522, 0.00011906097723658011, 0.00011905782847704688, 0.00011905466744081642, 0.00011905152451152934, 0.00011904837871451956, 0.00011904522476464522, 0.00011904206029188756, 0.00011903891857131005, 0.00011903577478881622, 0.00011903261273347769, 0.00011902946281263526, 0.00011902630246371024, 0.00011902315180816199, 0.00011902000274052631, 0.00011901685677761522, 0.00011901369986521741, 0.00011901055572722077, 0.00011900742367631984, 0.00011900426057817305, 0.0001190011347130211, 0.00011899799026692201, 0.00011899484520461805, 0.00011899170853218052, 0.00011898856439418387, 0.0001189854277928469, 0.00011898229638185103, 0.00011897915295486001, 0.00011897599786737667, 0.00011897285500919016, 0.00011896970091711469, 0.00011896655950463963, 0.00011896341906387226, 0.00011896027222405421, 0.00011895712751725306, 0.00011895397539229318, 0.00011895082348063498, 0.00011894769270954417, 0.00011894453740875913, 0.00011894140538155838, 0.00011893826148056362, 0.00011893510162934254, 0.00011893195026278865, 0.00011892880024714547, 0.00011892563726749962, 0.00011892250102166548, 0.00011891934112304403, 0.00011891617800119706, 0.00011891301298333507, 0.00011890984090281717, 0.00011890668515172857, 0.00011890353060934953, 0.00011890036917021588, 0.00011889720756518093, 0.0001188940439930304, 0.00011889089935732981, 0.00011888775337071853, 0.00011888459162348245, 0.00011888143639379797, 0.00011887827222914275, 0.00011887510171283723, 0.00011887193693197712, 0.00011886877776806149, 0.00011886562903222844, 0.00011886247354184189, 0.00011885930982749024, 0.00011885617014512886, 0.00011885300389485713, 0.00011884983335485142, 0.0001188466695693992, 0.0001188434978443841, 0.00011884034311240357, 0.00011883718063046167, 0.00011883401189167022, 0.00011883083718043148, 0.0001188276533209203, 0.00011882447223433108, 0.0001188212866684064, 0.00011881811299997591, 0.00011881492722074953, 0.00011881174400114342, 0.00011880857604445816, 0.00011880539360695825, 0.00011880221761590937, 0.00011879902475032688, 0.00011879583046273313, 0.00011879265961462499, 0.00011878946205640533, 0.00011878625776733238, 0.00011878305530317388, 0.00011877986705912798, 0.00011877667142062354, 0.00011877346957266991, 0.00011877026559169941, 0.00011876704933403169, 0.00011876383255495984, 0.00011876061525448386, 0.00011875738233558421, 0.00011875416129047858, 0.00011875094505651105, 0.00011874770474315284, 0.00011874445705903628, 0.00011874120051104952, 0.00011873796332611609, 0.00011873473453104908, 0.00011873149478649538, 0.00011872827641951096, 0.00011872506250816181, 0.00011872182236070493, 0.0001187185859341775, 0.00011871532443285152, 0.00011871208862252898, 0.00011870885191159929, 0.00011870558642864178, 0.00011870234272615674, 0.00011869907816750656, 0.00011869582166692017, 0.00011869256042629627, 0.00011868926932343588, 0.00011868600504918795, 0.00011868273579790059, 0.00011867946521940273, 0.00011867620395507864, 0.00011867290467565351, 0.0001186696121033815, 0.00011866629282099799, 0.00011866301181441755, 0.00011865971582931851, 0.00011865640889473278, 0.00011865311300443449, 0.00011864979696897669, 0.00011864646787471548, 0.0001186431404631676, 0.00011863981639334617, 0.00011863650360481407, 0.00011863314872474868, 0.00011862980230565028, 0.00011862646361281307, 0.00011862310648122985, 0.00011861974259509313, 0.00011861635844529598, 0.00011861298925031722, 0.00011860963349334489, 0.00011860627008121193, 0.0001186028816416808, 0.00011859949087953129, 0.00011859607295696669, 0.00011859267404195262, 0.00011858926645266984, 0.00011858585857898482, 0.00011858242117486596, 0.00011857897791680076, 0.000118575537621259, 0.00011857207777306243, 0.00011856861799596642, 0.00011856513425798067, 0.00011856166319959533, 0.00011855817050293864, 0.00011855465284998435, 0.0001185511318316034, 0.0001185475686268884, 0.00011854401352763758, 0.00011854043311658634, 0.00011853684983781239, 0.00011853324989780651, 0.00011852961611393262, 0.00011852600690715337, 0.00011852238414386676, 0.00011851874571475611, 0.00011851509593325555, 0.00011851141548371215, 0.00011850772401358149, 0.00011850401386770295, 0.00011850030301081878, 0.00011849656783754207, 0.000118492828706334, 0.00011848907537871354, 0.00011848531065130279, 0.00011848154838871156, 0.00011847777358872106, 0.00011847397947307761, 0.00011847014400060668, 0.00011846630402510009, 0.00011846248085302655, 0.00011845861300609927, 0.00011845473321427739, 0.00011845086477484542, 0.00011844697073921076, 0.00011844303688726081, 0.00011843908369595774, 0.00011843511912856457, 0.00011843112915457024, 0.00011842708381693749, 0.00011842304191583197, 0.00011841897373121833, 0.00011841489234560014], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 09:44:03,349 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:129]: evaluating model ... 
[2017-12-15 09:44:06,240 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:133]: evaluated! 
[2017-12-15 09:44:06,241 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:135]: generating reports ... 
[2017-12-15 09:44:07,092 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:138]: done!
[2017-12-15 09:44:07,093 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_8 finished!
[2018-04-29 11:38:17,554 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_8 was already executed!
[2018-04-29 13:12:02,734 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_8
[2018-04-29 13:12:02,734 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:146]: >> Printing header log
[2018-04-29 13:12:02,735 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_8
	layers = 9216,16589
	using GLOBAL obj = 
		{'mlp_configs': {'activation': 'sigmoid', 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7f2119f7e898>, 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy'}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'numpy_seed': 666, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'batch': 32, 'epochs': 200, 'autoencoder_configs': {'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f2119f7e828>}, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'shuffle_batches': True, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'store_history': True}
	=======================================
	
[2018-04-29 13:12:02,735 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,736 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,736 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,736 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:57]: =======================================
[2018-04-29 13:12:20,736 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:62]: setting configurations for autoencoder: 
	 {'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f2119f7e828>}
[2018-04-29 13:12:20,781 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,842 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_8
[2018-04-29 13:14:12,842 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:146]: >> Printing header log
[2018-04-29 13:14:12,842 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_8
	layers = 9216,16589
	using GLOBAL obj = 
		{'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'shuffle_batches': True, 'batch': 32, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'epochs': 200, 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'numpy_seed': 666, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'autoencoder_configs': {'optimizer': <keras.optimizers.SGD object at 0x7fb849c92828>, 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'discard_decoder_function': True}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'mlp_configs': {'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'optimizer': <keras.optimizers.SGD object at 0x7fb849c92898>, 'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid'}, 'store_history': True, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:14:12,842 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:148]: >> Loading dataset... 
[2018-04-29 13:14:31,032 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:31,033 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:31,033 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:57]: =======================================
[2018-04-29 13:14:31,033 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:62]: setting configurations for autoencoder: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7fb849c92828>, 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'discard_decoder_function': True}
[2018-04-29 13:14:31,079 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,685 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_8
[2018-04-29 13:16:33,685 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:146]: >> Printing header log
[2018-04-29 13:16:33,685 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_8
	layers = 9216,16589
	using GLOBAL obj = 
		{'mlp_configs': {'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid', 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7f6736f60898>, 'classifier_dim': 9}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'epochs': 200, 'autoencoder_configs': {'loss_function': 'mse', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f6736f60828>, 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu'}, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'store_history': True, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'batch': 32, 'data_dir': '/home/dhiego/malware_dataset/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'shuffle_batches': True, 'numpy_seed': 666}
	=======================================
	
[2018-04-29 13:16:33,685 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:148]: >> Loading dataset... 
[2018-04-29 13:16:57,097 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:57,097 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:57,097 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:57]: =======================================
[2018-04-29 13:16:57,097 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:62]: setting configurations for autoencoder: 
	 {'loss_function': 'mse', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f6736f60828>, 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu'}
[2018-04-29 13:16:57,146 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:20,925 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_8
[2018-04-29 14:30:20,926 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:146]: >> Printing header log
[2018-04-29 14:30:20,926 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_8
	layers = 9216,16589
	using GLOBAL obj = 
		{'fullds_data_dir': '/home/dhiego/malware_dataset/', 'epochs': 200, 'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'store_history': True, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'mlp_configs': {'optimizer': <keras.optimizers.SGD object at 0x7f072be0fac8>, 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid', 'classifier_dim': 9}, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'autoencoder_configs': {'optimizer': <keras.optimizers.SGD object at 0x7f072be0fa58>, 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'discard_decoder_function': True}, 'shuffle_batches': True, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'batch': 32, 'data_dir': '/home/dhiego/malware_dataset/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/'}
	=======================================
	
[2018-04-29 14:30:20,926 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:148]: >> Loading dataset... 
[2018-04-29 14:30:37,391 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:37,391 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:37,391 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:57]: =======================================
[2018-04-29 14:30:37,391 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:62]: setting configurations for autoencoder: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7f072be0fa58>, 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'discard_decoder_function': True}
[2018-04-29 14:30:37,434 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:73]: training and evaluate autoencoder
[2018-04-29 23:33:00,628 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:85]: trained and evaluated!
[2018-04-29 23:33:00,629 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:88]: Training history: 
{'val_loss': [0.00011729276100135265, 0.0001172839186748682, 0.00011727507380984325, 0.00011726627333352287, 0.00011725738490213747, 0.00011724857250182759, 0.00011723975683001834, 0.00011723095438722292, 0.0001172220979199953, 0.00011721324438460318, 0.0001172044242077784, 0.00011719560240414246, 0.00011718673792799834, 0.00011717787059152691, 0.00011716897497356912, 0.00011716006137130334, 0.00011715112912327885, 0.00011714229072974446, 0.00011713343149157475, 0.00011712455900651415, 0.00011711570606106453, 0.0001171068636630719, 0.00011709802085390722, 0.00011708916594198257, 0.00011708035363105792, 0.00011707142474391802, 0.00011706257964649149, 0.00011705377763274518, 0.0001170449320526384, 0.00011703606046143009, 0.00011702721062658643, 0.00011701829064221532, 0.00011700939641866709, 0.00011700058092562829, 0.00011699167508200036, 0.00011698286759787805, 0.00011697405945230504, 0.00011696518525104805, 0.00011695628931130344, 0.00011694748901375353, 0.00011693857655561866, 0.00011692972672077499, 0.000116920872059129, 0.00011691203293263574, 0.00011690319822177277, 0.00011689438836000339, 0.00011688555323796837, 0.00011687676144989222, 0.00011686793081499567, 0.00011685909708737022, 0.0001168502940903864, 0.0001168414955984181, 0.00011683267102384006, 0.00011682383615208369, 0.00011681499784793453, 0.0001168061885582306, 0.00011679735753003906, 0.0001167885674581593, 0.00011677975057071093, 0.00011677096793568219, 0.0001167621588962569, 0.00011675330661225799, 0.0001167444967504886, 0.00011673569899147916, 0.00011672692715418605, 0.00011671813021752073, 0.00011670936286736609, 0.00011670058563120517, 0.00011669182221400059, 0.00011668296192108519, 0.00011667415844142112, 0.00011666542019509695, 0.00011665662636903757, 0.00011664782943237225, 0.00011663901949909467, 0.00011663022729984647, 0.00011662149152055461, 0.00011661267234484439, 0.00011660381230220762, 0.00011659498593992501, 0.00011658622693835073, 0.00011657744365974835, 0.00011656863871416653, 0.0001165598728478067, 0.00011655111409651106, 0.00011654233294527713, 0.00011653358588556939, 0.00011652481242146513, 0.00011651606085674187, 0.00011650737241586712, 0.00011649867104988829, 0.00011648992300694305, 0.00011648120937731084, 0.00011647244236682009, 0.00011646368075519713, 0.00011645497620710416, 0.00011644621287928483, 0.00011643746238718432, 0.00011642872145930329, 0.000116419930010891, 0.0001164112628973318], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322718259954, 0.0008143322475570033, 0.0016286644951140066, 0.0016286644951140066, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.003257328990228013, 0.003257328990228013, 0.004071661237785016, 0.004071661237785016, 0.004885993485342019, 0.005700325757168015, 0.006514657980456026, 0.006514657980456026, 0.006514657980456026, 0.006514657980456026, 0.006514657980456026, 0.006514657980456026, 0.007328990228013029, 0.007328990228013029, 0.007328990228013029, 0.008143322475570033, 0.008957654723127036, 0.008957654723127036, 0.008957654723127036, 0.009771986970684038, 0.009771986970684038, 0.010586319218241042, 0.012214983713355049, 0.012214983713355049, 0.012214983713355049, 0.012214983713355049, 0.012214983713355049, 0.013029315960912053, 0.013029315960912053, 0.014657980456026058, 0.014657980456026058, 0.015472312703583062, 0.015472312703583062], 'val_acc': [0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.009828009873775065, 0.009828009873775065, 0.009828009873775065, 0.009828009873775065, 0.009828009873775065, 0.009828009873775065, 0.009828009873775065, 0.009828009873775065, 0.009828009873775065, 0.012285012330777521, 0.01474201478777998, 0.01474201478777998, 0.017199017290547673], 'loss': [0.00011805113930564604, 0.0001180422142652737, 0.00011803328453226422, 0.00011802435418304984, 0.00011801546426635563, 0.00011800648130272464, 0.00011799758197705592, 0.00011798868077907238, 0.00011797976905820551, 0.0001179708049836241, 0.00011796185482105283, 0.00011795292385563358, 0.0001179439998580695, 0.00011793503950441753, 0.00011792607531133518, 0.0001179170763737777, 0.0001179080543759376, 0.0001178990197458975, 0.00011789007164524255, 0.00011788109178633615, 0.00011787210905970702, 0.00011786314012658713, 0.00011785417574390326, 0.00011784522882825771, 0.0001178362557002046, 0.00011782732746030693, 0.0001178182836108938, 0.00011780932001031613, 0.00011780038468406236, 0.00011779141731515484, 0.00011778241939670543, 0.00011777344394603392, 0.00011776440175563395, 0.0001177553838816265, 0.00011774644400493669, 0.0001177374185705694, 0.00011772848895606084, 0.00011771955633162846, 0.00011771055523735389, 0.00011770153662864064, 0.00011769261374498539, 0.00011768357529921506, 0.00011767459416049851, 0.00011766561361428665, 0.0001176566492079026, 0.00011764768589172718, 0.00011763875468930606, 0.00011762979360094828, 0.00011762087375091704, 0.00011761191228335626, 0.00011760295531883114, 0.00011759402781363929, 0.00011758510540398778, 0.00011757616287287694, 0.0001175672034435323, 0.00011755824572060118, 0.0001175493125036641, 0.0001175403637394039, 0.00011753144365237079, 0.00011752250754401081, 0.00011751359961517398, 0.00011750466511842677, 0.00011749569125566783, 0.00011748675647451837, 0.00011747783539207737, 0.00011746894246545933, 0.00011746001756728812, 0.00011745112885930347, 0.00011744222841972594, 0.00011743333959324037, 0.00011742435501799661, 0.00011741542459768167, 0.0001174065594002832, 0.00011739763848374352, 0.00011738871770940496, 0.00011737977828301872, 0.00011737085829078635, 0.00011736199361479201, 0.00011735304686504777, 0.00011734405859257474, 0.0001173351066524894, 0.00011732621801560532, 0.00011731730302411256, 0.0001173083667024509, 0.00011729947664355556, 0.00011729059132469775, 0.00011728167289667778, 0.00011727279883540912, 0.00011726390173755804, 0.00011725502023443045, 0.00011724620480742609, 0.00011723737167638153, 0.00011722849510289298, 0.00011721965040615683, 0.00011721075596272678, 0.00011720186128229485, 0.0001171930254494289, 0.00011718414152888217, 0.00011717525602042287, 0.00011716638181695308, 0.00011715746597225356]}
[2018-04-29 23:33:00,629 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:92]: done!
[2018-04-29 23:33:00,629 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:152]: >> Executing classifier part ... 
[2018-04-29 23:33:00,629 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:97]: =======================================
[2018-04-29 23:33:00,629 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:101]: setting configurations for classifier: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7f072be0fac8>, 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid', 'classifier_dim': 9}
[2018-04-29 23:33:00,888 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:110]: training ... 
[2018-04-30 04:08:42,139 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:122]: trained!
[2018-04-30 04:08:42,140 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:125]: Training history: 
{'val_loss': [0.00011729276100135265, 0.0001172839186748682, 0.00011727507380984325, 0.00011726627333352287, 0.00011725738490213747, 0.00011724857250182759, 0.00011723975683001834, 0.00011723095438722292, 0.0001172220979199953, 0.00011721324438460318, 0.0001172044242077784, 0.00011719560240414246, 0.00011718673792799834, 0.00011717787059152691, 0.00011716897497356912, 0.00011716006137130334, 0.00011715112912327885, 0.00011714229072974446, 0.00011713343149157475, 0.00011712455900651415, 0.00011711570606106453, 0.0001171068636630719, 0.00011709802085390722, 0.00011708916594198257, 0.00011708035363105792, 0.00011707142474391802, 0.00011706257964649149, 0.00011705377763274518, 0.0001170449320526384, 0.00011703606046143009, 0.00011702721062658643, 0.00011701829064221532, 0.00011700939641866709, 0.00011700058092562829, 0.00011699167508200036, 0.00011698286759787805, 0.00011697405945230504, 0.00011696518525104805, 0.00011695628931130344, 0.00011694748901375353, 0.00011693857655561866, 0.00011692972672077499, 0.000116920872059129, 0.00011691203293263574, 0.00011690319822177277, 0.00011689438836000339, 0.00011688555323796837, 0.00011687676144989222, 0.00011686793081499567, 0.00011685909708737022, 0.0001168502940903864, 0.0001168414955984181, 0.00011683267102384006, 0.00011682383615208369, 0.00011681499784793453, 0.0001168061885582306, 0.00011679735753003906, 0.0001167885674581593, 0.00011677975057071093, 0.00011677096793568219, 0.0001167621588962569, 0.00011675330661225799, 0.0001167444967504886, 0.00011673569899147916, 0.00011672692715418605, 0.00011671813021752073, 0.00011670936286736609, 0.00011670058563120517, 0.00011669182221400059, 0.00011668296192108519, 0.00011667415844142112, 0.00011666542019509695, 0.00011665662636903757, 0.00011664782943237225, 0.00011663901949909467, 0.00011663022729984647, 0.00011662149152055461, 0.00011661267234484439, 0.00011660381230220762, 0.00011659498593992501, 0.00011658622693835073, 0.00011657744365974835, 0.00011656863871416653, 0.0001165598728478067, 0.00011655111409651106, 0.00011654233294527713, 0.00011653358588556939, 0.00011652481242146513, 0.00011651606085674187, 0.00011650737241586712, 0.00011649867104988829, 0.00011648992300694305, 0.00011648120937731084, 0.00011647244236682009, 0.00011646368075519713, 0.00011645497620710416, 0.00011644621287928483, 0.00011643746238718432, 0.00011642872145930329, 0.000116419930010891, 0.0001164112628973318], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322718259954, 0.0008143322475570033, 0.0016286644951140066, 0.0016286644951140066, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.003257328990228013, 0.003257328990228013, 0.004071661237785016, 0.004071661237785016, 0.004885993485342019, 0.005700325757168015, 0.006514657980456026, 0.006514657980456026, 0.006514657980456026, 0.006514657980456026, 0.006514657980456026, 0.006514657980456026, 0.007328990228013029, 0.007328990228013029, 0.007328990228013029, 0.008143322475570033, 0.008957654723127036, 0.008957654723127036, 0.008957654723127036, 0.009771986970684038, 0.009771986970684038, 0.010586319218241042, 0.012214983713355049, 0.012214983713355049, 0.012214983713355049, 0.012214983713355049, 0.012214983713355049, 0.013029315960912053, 0.013029315960912053, 0.014657980456026058, 0.014657980456026058, 0.015472312703583062, 0.015472312703583062], 'val_acc': [0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.009828009873775065, 0.009828009873775065, 0.009828009873775065, 0.009828009873775065, 0.009828009873775065, 0.009828009873775065, 0.009828009873775065, 0.009828009873775065, 0.009828009873775065, 0.012285012330777521, 0.01474201478777998, 0.01474201478777998, 0.017199017290547673], 'loss': [0.00011805113930564604, 0.0001180422142652737, 0.00011803328453226422, 0.00011802435418304984, 0.00011801546426635563, 0.00011800648130272464, 0.00011799758197705592, 0.00011798868077907238, 0.00011797976905820551, 0.0001179708049836241, 0.00011796185482105283, 0.00011795292385563358, 0.0001179439998580695, 0.00011793503950441753, 0.00011792607531133518, 0.0001179170763737777, 0.0001179080543759376, 0.0001178990197458975, 0.00011789007164524255, 0.00011788109178633615, 0.00011787210905970702, 0.00011786314012658713, 0.00011785417574390326, 0.00011784522882825771, 0.0001178362557002046, 0.00011782732746030693, 0.0001178182836108938, 0.00011780932001031613, 0.00011780038468406236, 0.00011779141731515484, 0.00011778241939670543, 0.00011777344394603392, 0.00011776440175563395, 0.0001177553838816265, 0.00011774644400493669, 0.0001177374185705694, 0.00011772848895606084, 0.00011771955633162846, 0.00011771055523735389, 0.00011770153662864064, 0.00011769261374498539, 0.00011768357529921506, 0.00011767459416049851, 0.00011766561361428665, 0.0001176566492079026, 0.00011764768589172718, 0.00011763875468930606, 0.00011762979360094828, 0.00011762087375091704, 0.00011761191228335626, 0.00011760295531883114, 0.00011759402781363929, 0.00011758510540398778, 0.00011757616287287694, 0.0001175672034435323, 0.00011755824572060118, 0.0001175493125036641, 0.0001175403637394039, 0.00011753144365237079, 0.00011752250754401081, 0.00011751359961517398, 0.00011750466511842677, 0.00011749569125566783, 0.00011748675647451837, 0.00011747783539207737, 0.00011746894246545933, 0.00011746001756728812, 0.00011745112885930347, 0.00011744222841972594, 0.00011743333959324037, 0.00011742435501799661, 0.00011741542459768167, 0.0001174065594002832, 0.00011739763848374352, 0.00011738871770940496, 0.00011737977828301872, 0.00011737085829078635, 0.00011736199361479201, 0.00011735304686504777, 0.00011734405859257474, 0.0001173351066524894, 0.00011732621801560532, 0.00011731730302411256, 0.0001173083667024509, 0.00011729947664355556, 0.00011729059132469775, 0.00011728167289667778, 0.00011727279883540912, 0.00011726390173755804, 0.00011725502023443045, 0.00011724620480742609, 0.00011723737167638153, 0.00011722849510289298, 0.00011721965040615683, 0.00011721075596272678, 0.00011720186128229485, 0.0001171930254494289, 0.00011718414152888217, 0.00011717525602042287, 0.00011716638181695308, 0.00011715746597225356]}
[2018-04-30 04:08:42,140 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:129]: evaluating model ... 
[2018-04-30 04:08:44,074 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:133]: evaluated! 
[2018-04-30 04:08:44,075 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:135]: generating reports ... 
[2018-04-30 04:08:44,973 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:138]: done!
[2018-04-30 04:08:44,974 AE_BIGRAMA_1L_MINIDS_OVER_F1_8.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_8 finished!
