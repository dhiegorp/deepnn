[2017-12-14 09:31:57,986 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_8
[2017-12-14 09:31:57,987 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:146]: >> Printing header log
[2017-12-14 09:31:57,987 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_8
	layers = 9216,7373
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f6e1cd51eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f6e1cd34400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,987 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:148]: >> Loading dataset... 
[2017-12-14 09:32:20,140 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:20,140 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:20,140 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:57]: =======================================
[2017-12-14 09:32:20,140 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f6e1cd51eb8>, 'discard_decoder_function': True}
[2017-12-14 09:32:20,186 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:54,933 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_8
[2017-12-14 10:18:54,933 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:146]: >> Printing header log
[2017-12-14 10:18:54,933 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_8
	layers = 9216,7373
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7ffb6269eef0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7ffb62682438>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:54,933 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:148]: >> Loading dataset... 
[2017-12-14 10:19:17,344 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:17,344 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:17,344 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:57]: =======================================
[2017-12-14 10:19:17,344 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7ffb6269eef0>, 'discard_decoder_function': True}
[2017-12-14 10:19:17,384 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:73]: training and evaluate autoencoder
[2017-12-14 21:14:46,715 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:85]: trained and evaluated!
[2017-12-14 21:14:46,718 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:88]: Training history: 
{'val_loss': [0.00011791640837420538, 0.00011790848653680584, 0.00011790048965156829, 0.0001178924828624474, 0.00011788449112579904, 0.00011787646522611626, 0.00011786849156316114, 0.00011786052574822908, 0.00011785251456135495, 0.00011784454277548963, 0.00011783661620067295, 0.00011782860689088863, 0.00011782065333960995, 0.00011781268269787554, 0.00011780470299247895, 0.00011779671534967405, 0.00011778876081515785, 0.0001177807905845955, 0.00011777285877180444, 0.0001177649136406143, 0.00011775698944344471, 0.00011774906058036619, 0.00011774115100662002, 0.00011773319017938372, 0.00011772529515755278, 0.00011771734233923298, 0.00011770939434771551, 0.00011770147619298739, 0.0001176935593432836, 0.00011768561592828985, 0.00011767772557236785, 0.00011766981158299138, 0.00011766188239812604, 0.00011765389058996948, 0.00011764597660059301, 0.00011763806654416661, 0.00011763014822854508, 0.00011762219671524961, 0.00011761426409799148, 0.00011760634832091044, 0.00011759841618633254, 0.0001175905486415209, 0.00011758268157938949, 0.00011757478696873059, 0.00011756688672680229, 0.0001175589939932332, 0.00011755110681942533, 0.00011754321017078323, 0.00011753531850983688, 0.00011752742275504706, 0.00011751952528406086, 0.00011751163337283588, 0.00011750372217227854, 0.00011749581488679422, 0.00011748791423369387, 0.00011748003932353937, 0.00011747215844245161, 0.00011746425648432691, 0.00011745637503117369, 0.00011744850192660082, 0.00011744063134269138, 0.00011743276780233798, 0.00011742490989325398, 0.00011741704797971175, 0.00011740916080590388, 0.0001174012715941128, 0.00011739341148615218, 0.00011738556952339298, 0.00011737773737513189, 0.00011736990612072307, 0.00011736199050453544, 0.00011735409900448251, 0.00011734626015232927, 0.00011733842365994605, 0.00011733054172411259, 0.00011732264244754476, 0.00011731477636865087, 0.00011730697176891725, 0.00011729913390000152, 0.00011729128180095738, 0.00011728343567284652, 0.00011727562526307304, 0.00011726784109680271, 0.0001172599451096113, 0.00011725210387981096, 0.00011724430589458427, 0.00011723649328593417, 0.00011722865688293618, 0.00011722081647547996, 0.00011721294753625874, 0.00011720514236445967, 0.00011719733350998917, 0.00011718954492808853, 0.00011718178201762557, 0.00011717392746942617, 0.00011716613103950245, 0.00011715826390586285, 0.0001171504026537713, 0.0001171426160383457, 0.00011713479869227847, 0.00011712698052386714, 0.00011711918311070591, 0.00011711140688184387, 0.00011710360187093821, 0.00011709580340303128, 0.0001170880672187517, 0.00011708027087821321, 0.00011707248883931131, 0.0001170647189925546, 0.00011705695452678866, 0.00011704918106886872, 0.00011704134336084639, 0.0001170335628057393, 0.00011702582899910679, 0.00011701802938706893, 0.00011701029681395259, 0.00011700255287103516, 0.00011699479421530908, 0.00011698705338299759, 0.00011697926791170292, 0.00011697153149502173, 0.00011696376254211731, 0.00011695602049416673, 0.00011694826593228412, 0.00011694051414134361, 0.00011693273390802333, 0.00011692496021770179, 0.0001169172408020911, 0.00011690951525465372, 0.00011690172504594193, 0.00011689400489737237, 0.00011688625736116873, 0.00011687850835904735, 0.00011687079736352521, 0.00011686313574440338, 0.00011685543358014182, 0.00011684771073213836, 0.00011683997333221966, 0.00011683222604629467, 0.00011682447548887031, 0.00011681673277946904, 0.0001168090263604706, 0.00011680127646449692, 0.00011679354519640491, 0.00011678581793277114, 0.00011677805609493093, 0.00011677031771177472, 0.00011676264579547454, 0.00011675493708821425, 0.00011674724343342646, 0.00011673953701442804, 0.00011673184613058234, 0.00011672415206462251, 0.00011671648161424008, 0.00011670881084207084, 0.00011670107696393015, 0.00011669338861862495, 0.0001166856998621477, 0.00011667801078388362, 0.00011667027999847184, 0.00011666257293589976, 0.00011665488099730836, 0.00011664720695363975, 0.00011663952073570299, 0.00011663184227640409, 0.00011662417388188193, 0.00011661647645503749, 0.000116608769231572, 0.00011660106127514763, 0.00011659337080247398, 0.00011658567845271054, 0.00011657800351518964, 0.00011657031624250717, 0.00011656265716192584, 0.00011655496375741671, 0.00011654731971143084, 0.00011653966767440553, 0.00011653199445308101, 0.00011652433873338428, 0.00011651670271419195, 0.00011650901878451704, 0.00011650137009049928, 0.00011649372441770225, 0.00011648611896825812, 0.00011647847313456768, 0.00011647083679358852, 0.00011646321294866433, 0.00011645556622112161, 0.0001164479261080858, 0.00011644027578725687, 0.00011643262055024038, 0.00011642496531322388, 0.00011641739624356782, 0.00011640979578181946, 0.00011640212468786339, 0.00011639446471342978, 0.0001163868873130704, 0.00011637920166719911, 0.00011637155125698495, 0.0001163639463796063], 'val_acc': [0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0073710073710073713, 0.0073710073710073713, 0.0073710073710073713, 0.0073710073710073713, 0.0073710073710073713, 0.0073710073710073713, 0.014742014742014743, 0.014742014742014743, 0.019656019656019656, 0.019656019656019656, 0.019656019656019656, 0.019656019656019656, 0.019656019656019656, 0.019656019656019656, 0.024570024570024569, 0.024570024570024569, 0.024570024570024569, 0.024570024570024569, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.031941031941031942, 0.031941031941031942, 0.034398034398034398, 0.036855036855036855, 0.036855036855036855, 0.039312039312039311, 0.041769041769041768, 0.044226044226044224, 0.046683046728811919, 0.049140049185814376, 0.049140049185814376, 0.051597051688582071, 0.051597051688582071, 0.054054054145584528, 0.054054054145584528, 0.054054054145584528, 0.056511056602586984, 0.056511056602586984, 0.061425061516591897, 0.068796068887599274, 0.068796068887599274, 0.071253071344601723, 0.076167076258606636, 0.076167076258606636, 0.0786240787156091, 0.0786240787156091, 0.083538083629614013, 0.083538083629614013, 0.085995086086616462, 0.090909091000621389, 0.093366093457623839, 0.098280098371628752, 0.10319410328563366, 0.10319410328563366, 0.10319410328563366, 0.10319410328563366, 0.11056511065664104, 0.1179361180276484, 0.12285012294165333, 0.12285012294165333, 0.12776412785565824, 0.13022113014790584, 0.13759213751891322, 0.14250614243291812, 0.1449631448899206, 0.14742014734692305, 0.14742014734692305, 0.15233415226092795, 0.15233415226092795, 0.1572481574312182, 0.15970515988822065, 0.16461916480222558, 0.16461916480222558, 0.16707616725922803, 0.16953316971623048, 0.17199017217323292, 0.1744471746302354, 0.17690417708723785, 0.1793611795442403, 0.1793611795442403, 0.1793611795442403, 0.18181818200124278, 0.18427518445824523, 0.1916461918292526, 0.19410319428625505, 0.19901719920025995, 0.20393120411426488, 0.20638820657126733, 0.20884520902826981, 0.21621621639927716, 0.21867321885627963, 0.21867321885627963, 0.21867321885627963, 0.22113022131328208, 0.22113022131328208, 0.22604422564149196, 0.22850122835477973, 0.23095823022598716, 0.23341523268298961, 0.23587223513999209, 0.24078624005399699, 0.24570024496800191, 0.24815724742500436, 0.24815724742500436, 0.25307125233900929, 0.25307125233900929, 0.25307125233900929, 0.25307125233900929, 0.25798525725301419, 0.25798525725301419, 0.26289926216701909, 0.26289926216701909, 0.26535626462402157, 0.26781326708102404, 0.27272727309339462, 0.27272727309339462, 0.27518427555039709, 0.28009828046440199, 0.28501228537840689, 0.29238329274941427, 0.29729729766341917, 0.30221130257742412, 0.30712530749142902, 0.31203931240543392, 0.31203931240543392, 0.31695331731943888, 0.31695331731943888, 0.32186732223344378, 0.32432432410465117, 0.32432432410465117, 0.32678132656165365, 0.32923832901865607, 0.33169533147565855, 0.33169533147565855, 0.33169533147565855, 0.33169533147565855, 0.33169533147565855, 0.33415233393266103, 0.33415233393266103, 0.33906633884666593, 0.34398034376067083, 0.3513513511316782, 0.3513513511316782, 0.3513513511316782, 0.3513513511316782, 0.3513513511316782, 0.35380835358868068, 0.35380835358868068, 0.35380835358868068, 0.35380835358868068, 0.35380835358868068, 0.3562653560456831, 0.3562653560456831, 0.35872235850268558, 0.36117936095968806, 0.36117936095968806, 0.36117936095968806, 0.36117936095968806, 0.36117936095968806, 0.36363636341669048, 0.36363636341669048, 0.36363636341669048, 0.36363636341669048, 0.36363636341669048, 0.36363636341669048, 0.36363636341669048, 0.36363636341669048, 0.36609336587369296, 0.36855036833069543, 0.37100737078769785, 0.37100737078769785, 0.37100737078769785, 0.37346437324470033, 0.37346437324470033, 0.37592137570170281, 0.37592137570170281], 'loss': [0.00011868397737963198, 0.00011867583778717916, 0.00011866776078692198, 0.00011865960104930964, 0.00011865143849137496, 0.00011864328550831612, 0.00011863510567294464, 0.00011862697186333761, 0.0001186188539802567, 0.00011861068817539633, 0.00011860256021973565, 0.00011859447708112986, 0.00011858631438099406, 0.00011857820403457282, 0.00011857007349559169, 0.00011856194132129762, 0.0001185537982686174, 0.00011854568673718678, 0.00011853756006133625, 0.00011852947680422951, 0.00011852137150594825, 0.00011851329393688656, 0.00011850520947107025, 0.00011849714273299433, 0.00011848902546611831, 0.00011848097882580152, 0.00011847286369194237, 0.00011846475675834816, 0.00011845668362122157, 0.00011844861373102068, 0.00011844051291207488, 0.00011843245890099974, 0.00011842438652227915, 0.00011841630482938481, 0.00011840815189372632, 0.00011840007816409505, 0.00011839200628307839, 0.00011838393241124597, 0.00011837581805949302, 0.00011836772909064106, 0.00011835964559653246, 0.00011835156191282235, 0.00011834353572576753, 0.000118335507429396, 0.00011832745652304543, 0.00011831939379030123, 0.000118311338001712, 0.00011830329204870066, 0.00011829523303688592, 0.00011828718096922616, 0.00011827912695815101, 0.00011827106740123195, 0.00011826301559427426, 0.00011825494558557243, 0.00011824687704628224, 0.00011823881665985662, 0.00011823077985511771, 0.0001182227396138516, 0.00011821467479549088, 0.00011820663528893058, 0.00011819860566534854, 0.00011819057319774398, 0.000118182550399816, 0.00011817452914240021, 0.00011816651321752664, 0.00011815846487079635, 0.00011815041626336399, 0.00011814239147462024, 0.0001181343885611497, 0.00011812639728447131, 0.00011811840716910211, 0.00011811033256256389, 0.00011810227826708649, 0.00011809427490331239, 0.00011808627627957582, 0.00011807823497180126, 0.00011807017759529947, 0.00011806214816131893, 0.00011805418312074829, 0.00011804618639302673, 0.00011803817719900647, 0.0001180301710860106, 0.00011802219798737616, 0.0001180142531393654, 0.0001180061959524651, 0.00011799819194878594, 0.00011799023757329975, 0.00011798226400066155, 0.00011797426710703868, 0.0001179662686966038, 0.00011795824269915049, 0.00011795027912799148, 0.00011794230754616904, 0.00011793436049404084, 0.00011792643669179675, 0.00011791842858798511, 0.00011791047473390304, 0.00011790245466149664, 0.00011789443172136752, 0.00011788648801096579, 0.00011787851396432383, 0.00011787053742916218, 0.00011786258833881783, 0.00011785466192955309, 0.00011784669335765449, 0.00011783873689655179, 0.00011783084601386835, 0.00011782289180428348, 0.00011781495577274255, 0.00011780702775186505, 0.00011779911226838681, 0.00011779118583542189, 0.00011778319240202642, 0.00011777525895380594, 0.00011776736975383584, 0.00011775942291500932, 0.00011775153037331276, 0.0001177436370732102, 0.0001177357221111361, 0.00011772783120475248, 0.00011771989223438828, 0.00011771200459863056, 0.00011770408243169942, 0.00011769619818506853, 0.0001176882894324436, 0.00011768038504065321, 0.00011767245569256519, 0.00011766452989950533, 0.00011765666269330939, 0.00011764878593593781, 0.00011764084319724376, 0.00011763297966457691, 0.00011762508430255802, 0.00011761718055067269, 0.00011760931964872666, 0.00011760151129009669, 0.00011759365569699271, 0.00011758578839599601, 0.00011757790419676551, 0.00011757000426061039, 0.00011756210283134345, 0.00011755421329957072, 0.00011754635713766223, 0.00011753846232074764, 0.00011753058591887889, 0.00011752270849790206, 0.00011751479701975555, 0.00011750691417143574, 0.00011749908796656446, 0.00011749123841700832, 0.00011748339960363721, 0.00011747553841728893, 0.00011746769976981912, 0.00011745986162005326, 0.00011745203740599775, 0.00011744421999389609, 0.00011743633764328023, 0.00011742850560816277, 0.00011742066726879542, 0.00011741282433159164, 0.0001174049428815829, 0.00011739708610346953, 0.00011738923999044062, 0.00011738141828860499, 0.00011737357862202711, 0.00011736575386286728, 0.0001173579355501585, 0.00011735009901200539, 0.00011734224623922374, 0.00011733438569278053, 0.00011732654668980791, 0.00011731870626482403, 0.00011731088212186908, 0.00011730304627102141, 0.00011729524457214419, 0.00011728740476336518, 0.00011727961685799718, 0.00011727181816904378, 0.0001172639952347984, 0.00011725619329891929, 0.0001172484099913877, 0.00011724057866727588, 0.00011723278080782904, 0.0001172249824032779, 0.00011721723455122706, 0.00011720944294862977, 0.00011720166009140174, 0.00011719389195199024, 0.00011718609591745787, 0.00011717831220702307, 0.00011717051254636198, 0.00011716270755315867, 0.00011715490682598913, 0.00011714719317330909, 0.00011713944074712202, 0.00011713161601166238, 0.00011712381199016675, 0.00011711609046902441, 0.00011710825864720865, 0.00011710045822814156], 'acc': [0.0032573289902280132, 0.0048859934853420191, 0.0048859935096110113, 0.0048859934853420191, 0.0048859934853420191, 0.0048859934853420191, 0.0048859934853420191, 0.0065146579804560263, 0.0065146579804560263, 0.0065146579804560263, 0.0065146579804560263, 0.0073289902280130291, 0.0073289902280130291, 0.0073289902280130291, 0.0081433224755700327, 0.0081433224755700327, 0.0097719869706840382, 0.010586319218241042, 0.011400651465798045, 0.011400651465798045, 0.012214983713355049, 0.01221498373762404, 0.012214983713355049, 0.012214983713355049, 0.013029315960912053, 0.013843648208469055, 0.014657980456026058, 0.015472312703583062, 0.015472312703583062, 0.016286644951140065, 0.016286644951140065, 0.017100977198697069, 0.020358306213194073, 0.021986970684039087, 0.022801302931596091, 0.022801302931596091, 0.024429967426710098, 0.025244299674267102, 0.026872964169381109, 0.027687296416938109, 0.027687296416938109, 0.029315960936321109, 0.030130293183878113, 0.032573289902280131, 0.033387622149837134, 0.034201954397394138, 0.035830618892508145, 0.037459283436160132, 0.038273615635179156, 0.040716612402119157, 0.04071661237785016, 0.043159609120521171, 0.043973941392347171, 0.045602605887461171, 0.046416938135018175, 0.0496742671009772, 0.051302931644629186, 0.051302931620360197, 0.054560260586319222, 0.057817589576547229, 0.060260586343487237, 0.062703583061889251, 0.063517915309446255, 0.064332247581272248, 0.065960912076386255, 0.068403908794788276, 0.07166123783355427, 0.073289902328668277, 0.074918566799513295, 0.075732899047070298, 0.076547231318896292, 0.079804560309124306, 0.083061889275083331, 0.087947882760425339, 0.09283387627003635, 0.096905537507821368, 0.099348534250492379, 0.1009771987213374, 0.1050488599833914, 0.10993485342019543, 0.11237785021140444, 0.11563517920163245, 0.11726384367247746, 0.11970684039087948, 0.12296416942964547, 0.12703583064316151, 0.13192182415277251, 0.13843648213322854, 0.14250814332247558, 0.14332247561857056, 0.14657980460879858, 0.15065146584658359, 0.15553745930765661, 0.15798045607459663, 0.16123778491921067, 0.16368078178322665, 0.16612377850162866, 0.17182410433160367, 0.17263843653062266, 0.17671009762279374, 0.17752442999169571, 0.17996742671009772, 0.18403908794788273, 0.18566775244299674, 0.18973941370505076, 0.19462540719039276, 0.19706840400587075, 0.1986970684039088, 0.20195439744267479, 0.2035830619135198, 0.20521172638436483, 0.2084690554716688, 0.21091205211726385, 0.21335504890847284, 0.21416938101041588, 0.21579804562687485, 0.22068403913648588, 0.22231270363159988, 0.22312703583061888, 0.22719869697132794, 0.2312703583547269, 0.23208469055374592, 0.23371335509739791, 0.23697068394201198, 0.24185667762150595, 0.24592833885929094, 0.24755700325732899, 0.25325732901449699, 0.25651465798045603, 0.26058631921824105, 0.26302931598518103, 0.26547231275212102, 0.26791530947052306, 0.26872964174234903, 0.27117263846075107, 0.27442996752378607, 0.27768729641693812, 0.28257328990228014, 0.28338762214983715, 0.2866449510429892, 0.28827361553810321, 0.28990228022736914, 0.29234527697004015, 0.29315960912052119, 0.29397394117392622, 0.2947882736156352, 0.29723127035830621, 0.30130293159609123, 0.30293159613974319, 0.30456026058631924, 0.30700325732899025, 0.30944625416873722, 0.31107491846969926, 0.31188925086287023, 0.31351791535798423, 0.31596091214919325, 0.31677524434821225, 0.31677524434821225, 0.31758957654723124, 0.31758957654723124, 0.31758957635307933, 0.31840390884332626, 0.32247557003257327, 0.32573289902280128, 0.32899022811010531, 0.3306188925081433, 0.33143322485277632, 0.33469055374592832, 0.33794788254200442, 0.33876221498371334, 0.33957654713419438, 0.34039087938175139, 0.34201954407101731, 0.34201954407101731, 0.34527687286709341, 0.34771986960976442, 0.34853420195439738, 0.35016286635243543, 0.3501628664980494, 0.35016286644951139, 0.35179153084754944, 0.35260586328925836, 0.35586319208533446, 0.3583061889493504, 0.3607491857648284, 0.36074918571629044, 0.36156351781823348, 0.36482084700261341, 0.36482084690553745, 0.36726384355113251, 0.36807817599284143, 0.36889250794917056, 0.37052117266270546, 0.37052117263843648, 0.37052117263843648, 0.37052117273551244, 0.37214983718208849, 0.37296416957525941, 0.37377850143451258, 0.3745928338762215]}
[2017-12-14 21:14:46,719 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:92]: done!
[2017-12-14 21:14:46,719 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:152]: >> Executing classifier part ... 
[2017-12-14 21:14:46,719 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:97]: =======================================
[2017-12-14 21:14:46,720 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7ffb62682438>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-14 21:14:47,294 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:110]: training ... 
[2017-12-15 02:36:16,109 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:122]: trained!
[2017-12-15 02:36:16,111 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:125]: Training history: 
{'val_loss': [0.00011791640837420538, 0.00011790848653680584, 0.00011790048965156829, 0.0001178924828624474, 0.00011788449112579904, 0.00011787646522611626, 0.00011786849156316114, 0.00011786052574822908, 0.00011785251456135495, 0.00011784454277548963, 0.00011783661620067295, 0.00011782860689088863, 0.00011782065333960995, 0.00011781268269787554, 0.00011780470299247895, 0.00011779671534967405, 0.00011778876081515785, 0.0001177807905845955, 0.00011777285877180444, 0.0001177649136406143, 0.00011775698944344471, 0.00011774906058036619, 0.00011774115100662002, 0.00011773319017938372, 0.00011772529515755278, 0.00011771734233923298, 0.00011770939434771551, 0.00011770147619298739, 0.0001176935593432836, 0.00011768561592828985, 0.00011767772557236785, 0.00011766981158299138, 0.00011766188239812604, 0.00011765389058996948, 0.00011764597660059301, 0.00011763806654416661, 0.00011763014822854508, 0.00011762219671524961, 0.00011761426409799148, 0.00011760634832091044, 0.00011759841618633254, 0.0001175905486415209, 0.00011758268157938949, 0.00011757478696873059, 0.00011756688672680229, 0.0001175589939932332, 0.00011755110681942533, 0.00011754321017078323, 0.00011753531850983688, 0.00011752742275504706, 0.00011751952528406086, 0.00011751163337283588, 0.00011750372217227854, 0.00011749581488679422, 0.00011748791423369387, 0.00011748003932353937, 0.00011747215844245161, 0.00011746425648432691, 0.00011745637503117369, 0.00011744850192660082, 0.00011744063134269138, 0.00011743276780233798, 0.00011742490989325398, 0.00011741704797971175, 0.00011740916080590388, 0.0001174012715941128, 0.00011739341148615218, 0.00011738556952339298, 0.00011737773737513189, 0.00011736990612072307, 0.00011736199050453544, 0.00011735409900448251, 0.00011734626015232927, 0.00011733842365994605, 0.00011733054172411259, 0.00011732264244754476, 0.00011731477636865087, 0.00011730697176891725, 0.00011729913390000152, 0.00011729128180095738, 0.00011728343567284652, 0.00011727562526307304, 0.00011726784109680271, 0.0001172599451096113, 0.00011725210387981096, 0.00011724430589458427, 0.00011723649328593417, 0.00011722865688293618, 0.00011722081647547996, 0.00011721294753625874, 0.00011720514236445967, 0.00011719733350998917, 0.00011718954492808853, 0.00011718178201762557, 0.00011717392746942617, 0.00011716613103950245, 0.00011715826390586285, 0.0001171504026537713, 0.0001171426160383457, 0.00011713479869227847, 0.00011712698052386714, 0.00011711918311070591, 0.00011711140688184387, 0.00011710360187093821, 0.00011709580340303128, 0.0001170880672187517, 0.00011708027087821321, 0.00011707248883931131, 0.0001170647189925546, 0.00011705695452678866, 0.00011704918106886872, 0.00011704134336084639, 0.0001170335628057393, 0.00011702582899910679, 0.00011701802938706893, 0.00011701029681395259, 0.00011700255287103516, 0.00011699479421530908, 0.00011698705338299759, 0.00011697926791170292, 0.00011697153149502173, 0.00011696376254211731, 0.00011695602049416673, 0.00011694826593228412, 0.00011694051414134361, 0.00011693273390802333, 0.00011692496021770179, 0.0001169172408020911, 0.00011690951525465372, 0.00011690172504594193, 0.00011689400489737237, 0.00011688625736116873, 0.00011687850835904735, 0.00011687079736352521, 0.00011686313574440338, 0.00011685543358014182, 0.00011684771073213836, 0.00011683997333221966, 0.00011683222604629467, 0.00011682447548887031, 0.00011681673277946904, 0.0001168090263604706, 0.00011680127646449692, 0.00011679354519640491, 0.00011678581793277114, 0.00011677805609493093, 0.00011677031771177472, 0.00011676264579547454, 0.00011675493708821425, 0.00011674724343342646, 0.00011673953701442804, 0.00011673184613058234, 0.00011672415206462251, 0.00011671648161424008, 0.00011670881084207084, 0.00011670107696393015, 0.00011669338861862495, 0.0001166856998621477, 0.00011667801078388362, 0.00011667027999847184, 0.00011666257293589976, 0.00011665488099730836, 0.00011664720695363975, 0.00011663952073570299, 0.00011663184227640409, 0.00011662417388188193, 0.00011661647645503749, 0.000116608769231572, 0.00011660106127514763, 0.00011659337080247398, 0.00011658567845271054, 0.00011657800351518964, 0.00011657031624250717, 0.00011656265716192584, 0.00011655496375741671, 0.00011654731971143084, 0.00011653966767440553, 0.00011653199445308101, 0.00011652433873338428, 0.00011651670271419195, 0.00011650901878451704, 0.00011650137009049928, 0.00011649372441770225, 0.00011648611896825812, 0.00011647847313456768, 0.00011647083679358852, 0.00011646321294866433, 0.00011645556622112161, 0.0001164479261080858, 0.00011644027578725687, 0.00011643262055024038, 0.00011642496531322388, 0.00011641739624356782, 0.00011640979578181946, 0.00011640212468786339, 0.00011639446471342978, 0.0001163868873130704, 0.00011637920166719911, 0.00011637155125698495, 0.0001163639463796063], 'val_acc': [0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0073710073710073713, 0.0073710073710073713, 0.0073710073710073713, 0.0073710073710073713, 0.0073710073710073713, 0.0073710073710073713, 0.014742014742014743, 0.014742014742014743, 0.019656019656019656, 0.019656019656019656, 0.019656019656019656, 0.019656019656019656, 0.019656019656019656, 0.019656019656019656, 0.024570024570024569, 0.024570024570024569, 0.024570024570024569, 0.024570024570024569, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.029484029484029485, 0.031941031941031942, 0.031941031941031942, 0.034398034398034398, 0.036855036855036855, 0.036855036855036855, 0.039312039312039311, 0.041769041769041768, 0.044226044226044224, 0.046683046728811919, 0.049140049185814376, 0.049140049185814376, 0.051597051688582071, 0.051597051688582071, 0.054054054145584528, 0.054054054145584528, 0.054054054145584528, 0.056511056602586984, 0.056511056602586984, 0.061425061516591897, 0.068796068887599274, 0.068796068887599274, 0.071253071344601723, 0.076167076258606636, 0.076167076258606636, 0.0786240787156091, 0.0786240787156091, 0.083538083629614013, 0.083538083629614013, 0.085995086086616462, 0.090909091000621389, 0.093366093457623839, 0.098280098371628752, 0.10319410328563366, 0.10319410328563366, 0.10319410328563366, 0.10319410328563366, 0.11056511065664104, 0.1179361180276484, 0.12285012294165333, 0.12285012294165333, 0.12776412785565824, 0.13022113014790584, 0.13759213751891322, 0.14250614243291812, 0.1449631448899206, 0.14742014734692305, 0.14742014734692305, 0.15233415226092795, 0.15233415226092795, 0.1572481574312182, 0.15970515988822065, 0.16461916480222558, 0.16461916480222558, 0.16707616725922803, 0.16953316971623048, 0.17199017217323292, 0.1744471746302354, 0.17690417708723785, 0.1793611795442403, 0.1793611795442403, 0.1793611795442403, 0.18181818200124278, 0.18427518445824523, 0.1916461918292526, 0.19410319428625505, 0.19901719920025995, 0.20393120411426488, 0.20638820657126733, 0.20884520902826981, 0.21621621639927716, 0.21867321885627963, 0.21867321885627963, 0.21867321885627963, 0.22113022131328208, 0.22113022131328208, 0.22604422564149196, 0.22850122835477973, 0.23095823022598716, 0.23341523268298961, 0.23587223513999209, 0.24078624005399699, 0.24570024496800191, 0.24815724742500436, 0.24815724742500436, 0.25307125233900929, 0.25307125233900929, 0.25307125233900929, 0.25307125233900929, 0.25798525725301419, 0.25798525725301419, 0.26289926216701909, 0.26289926216701909, 0.26535626462402157, 0.26781326708102404, 0.27272727309339462, 0.27272727309339462, 0.27518427555039709, 0.28009828046440199, 0.28501228537840689, 0.29238329274941427, 0.29729729766341917, 0.30221130257742412, 0.30712530749142902, 0.31203931240543392, 0.31203931240543392, 0.31695331731943888, 0.31695331731943888, 0.32186732223344378, 0.32432432410465117, 0.32432432410465117, 0.32678132656165365, 0.32923832901865607, 0.33169533147565855, 0.33169533147565855, 0.33169533147565855, 0.33169533147565855, 0.33169533147565855, 0.33415233393266103, 0.33415233393266103, 0.33906633884666593, 0.34398034376067083, 0.3513513511316782, 0.3513513511316782, 0.3513513511316782, 0.3513513511316782, 0.3513513511316782, 0.35380835358868068, 0.35380835358868068, 0.35380835358868068, 0.35380835358868068, 0.35380835358868068, 0.3562653560456831, 0.3562653560456831, 0.35872235850268558, 0.36117936095968806, 0.36117936095968806, 0.36117936095968806, 0.36117936095968806, 0.36117936095968806, 0.36363636341669048, 0.36363636341669048, 0.36363636341669048, 0.36363636341669048, 0.36363636341669048, 0.36363636341669048, 0.36363636341669048, 0.36363636341669048, 0.36609336587369296, 0.36855036833069543, 0.37100737078769785, 0.37100737078769785, 0.37100737078769785, 0.37346437324470033, 0.37346437324470033, 0.37592137570170281, 0.37592137570170281], 'loss': [0.00011868397737963198, 0.00011867583778717916, 0.00011866776078692198, 0.00011865960104930964, 0.00011865143849137496, 0.00011864328550831612, 0.00011863510567294464, 0.00011862697186333761, 0.0001186188539802567, 0.00011861068817539633, 0.00011860256021973565, 0.00011859447708112986, 0.00011858631438099406, 0.00011857820403457282, 0.00011857007349559169, 0.00011856194132129762, 0.0001185537982686174, 0.00011854568673718678, 0.00011853756006133625, 0.00011852947680422951, 0.00011852137150594825, 0.00011851329393688656, 0.00011850520947107025, 0.00011849714273299433, 0.00011848902546611831, 0.00011848097882580152, 0.00011847286369194237, 0.00011846475675834816, 0.00011845668362122157, 0.00011844861373102068, 0.00011844051291207488, 0.00011843245890099974, 0.00011842438652227915, 0.00011841630482938481, 0.00011840815189372632, 0.00011840007816409505, 0.00011839200628307839, 0.00011838393241124597, 0.00011837581805949302, 0.00011836772909064106, 0.00011835964559653246, 0.00011835156191282235, 0.00011834353572576753, 0.000118335507429396, 0.00011832745652304543, 0.00011831939379030123, 0.000118311338001712, 0.00011830329204870066, 0.00011829523303688592, 0.00011828718096922616, 0.00011827912695815101, 0.00011827106740123195, 0.00011826301559427426, 0.00011825494558557243, 0.00011824687704628224, 0.00011823881665985662, 0.00011823077985511771, 0.0001182227396138516, 0.00011821467479549088, 0.00011820663528893058, 0.00011819860566534854, 0.00011819057319774398, 0.000118182550399816, 0.00011817452914240021, 0.00011816651321752664, 0.00011815846487079635, 0.00011815041626336399, 0.00011814239147462024, 0.0001181343885611497, 0.00011812639728447131, 0.00011811840716910211, 0.00011811033256256389, 0.00011810227826708649, 0.00011809427490331239, 0.00011808627627957582, 0.00011807823497180126, 0.00011807017759529947, 0.00011806214816131893, 0.00011805418312074829, 0.00011804618639302673, 0.00011803817719900647, 0.0001180301710860106, 0.00011802219798737616, 0.0001180142531393654, 0.0001180061959524651, 0.00011799819194878594, 0.00011799023757329975, 0.00011798226400066155, 0.00011797426710703868, 0.0001179662686966038, 0.00011795824269915049, 0.00011795027912799148, 0.00011794230754616904, 0.00011793436049404084, 0.00011792643669179675, 0.00011791842858798511, 0.00011791047473390304, 0.00011790245466149664, 0.00011789443172136752, 0.00011788648801096579, 0.00011787851396432383, 0.00011787053742916218, 0.00011786258833881783, 0.00011785466192955309, 0.00011784669335765449, 0.00011783873689655179, 0.00011783084601386835, 0.00011782289180428348, 0.00011781495577274255, 0.00011780702775186505, 0.00011779911226838681, 0.00011779118583542189, 0.00011778319240202642, 0.00011777525895380594, 0.00011776736975383584, 0.00011775942291500932, 0.00011775153037331276, 0.0001177436370732102, 0.0001177357221111361, 0.00011772783120475248, 0.00011771989223438828, 0.00011771200459863056, 0.00011770408243169942, 0.00011769619818506853, 0.0001176882894324436, 0.00011768038504065321, 0.00011767245569256519, 0.00011766452989950533, 0.00011765666269330939, 0.00011764878593593781, 0.00011764084319724376, 0.00011763297966457691, 0.00011762508430255802, 0.00011761718055067269, 0.00011760931964872666, 0.00011760151129009669, 0.00011759365569699271, 0.00011758578839599601, 0.00011757790419676551, 0.00011757000426061039, 0.00011756210283134345, 0.00011755421329957072, 0.00011754635713766223, 0.00011753846232074764, 0.00011753058591887889, 0.00011752270849790206, 0.00011751479701975555, 0.00011750691417143574, 0.00011749908796656446, 0.00011749123841700832, 0.00011748339960363721, 0.00011747553841728893, 0.00011746769976981912, 0.00011745986162005326, 0.00011745203740599775, 0.00011744421999389609, 0.00011743633764328023, 0.00011742850560816277, 0.00011742066726879542, 0.00011741282433159164, 0.0001174049428815829, 0.00011739708610346953, 0.00011738923999044062, 0.00011738141828860499, 0.00011737357862202711, 0.00011736575386286728, 0.0001173579355501585, 0.00011735009901200539, 0.00011734224623922374, 0.00011733438569278053, 0.00011732654668980791, 0.00011731870626482403, 0.00011731088212186908, 0.00011730304627102141, 0.00011729524457214419, 0.00011728740476336518, 0.00011727961685799718, 0.00011727181816904378, 0.0001172639952347984, 0.00011725619329891929, 0.0001172484099913877, 0.00011724057866727588, 0.00011723278080782904, 0.0001172249824032779, 0.00011721723455122706, 0.00011720944294862977, 0.00011720166009140174, 0.00011719389195199024, 0.00011718609591745787, 0.00011717831220702307, 0.00011717051254636198, 0.00011716270755315867, 0.00011715490682598913, 0.00011714719317330909, 0.00011713944074712202, 0.00011713161601166238, 0.00011712381199016675, 0.00011711609046902441, 0.00011710825864720865, 0.00011710045822814156], 'acc': [0.0032573289902280132, 0.0048859934853420191, 0.0048859935096110113, 0.0048859934853420191, 0.0048859934853420191, 0.0048859934853420191, 0.0048859934853420191, 0.0065146579804560263, 0.0065146579804560263, 0.0065146579804560263, 0.0065146579804560263, 0.0073289902280130291, 0.0073289902280130291, 0.0073289902280130291, 0.0081433224755700327, 0.0081433224755700327, 0.0097719869706840382, 0.010586319218241042, 0.011400651465798045, 0.011400651465798045, 0.012214983713355049, 0.01221498373762404, 0.012214983713355049, 0.012214983713355049, 0.013029315960912053, 0.013843648208469055, 0.014657980456026058, 0.015472312703583062, 0.015472312703583062, 0.016286644951140065, 0.016286644951140065, 0.017100977198697069, 0.020358306213194073, 0.021986970684039087, 0.022801302931596091, 0.022801302931596091, 0.024429967426710098, 0.025244299674267102, 0.026872964169381109, 0.027687296416938109, 0.027687296416938109, 0.029315960936321109, 0.030130293183878113, 0.032573289902280131, 0.033387622149837134, 0.034201954397394138, 0.035830618892508145, 0.037459283436160132, 0.038273615635179156, 0.040716612402119157, 0.04071661237785016, 0.043159609120521171, 0.043973941392347171, 0.045602605887461171, 0.046416938135018175, 0.0496742671009772, 0.051302931644629186, 0.051302931620360197, 0.054560260586319222, 0.057817589576547229, 0.060260586343487237, 0.062703583061889251, 0.063517915309446255, 0.064332247581272248, 0.065960912076386255, 0.068403908794788276, 0.07166123783355427, 0.073289902328668277, 0.074918566799513295, 0.075732899047070298, 0.076547231318896292, 0.079804560309124306, 0.083061889275083331, 0.087947882760425339, 0.09283387627003635, 0.096905537507821368, 0.099348534250492379, 0.1009771987213374, 0.1050488599833914, 0.10993485342019543, 0.11237785021140444, 0.11563517920163245, 0.11726384367247746, 0.11970684039087948, 0.12296416942964547, 0.12703583064316151, 0.13192182415277251, 0.13843648213322854, 0.14250814332247558, 0.14332247561857056, 0.14657980460879858, 0.15065146584658359, 0.15553745930765661, 0.15798045607459663, 0.16123778491921067, 0.16368078178322665, 0.16612377850162866, 0.17182410433160367, 0.17263843653062266, 0.17671009762279374, 0.17752442999169571, 0.17996742671009772, 0.18403908794788273, 0.18566775244299674, 0.18973941370505076, 0.19462540719039276, 0.19706840400587075, 0.1986970684039088, 0.20195439744267479, 0.2035830619135198, 0.20521172638436483, 0.2084690554716688, 0.21091205211726385, 0.21335504890847284, 0.21416938101041588, 0.21579804562687485, 0.22068403913648588, 0.22231270363159988, 0.22312703583061888, 0.22719869697132794, 0.2312703583547269, 0.23208469055374592, 0.23371335509739791, 0.23697068394201198, 0.24185667762150595, 0.24592833885929094, 0.24755700325732899, 0.25325732901449699, 0.25651465798045603, 0.26058631921824105, 0.26302931598518103, 0.26547231275212102, 0.26791530947052306, 0.26872964174234903, 0.27117263846075107, 0.27442996752378607, 0.27768729641693812, 0.28257328990228014, 0.28338762214983715, 0.2866449510429892, 0.28827361553810321, 0.28990228022736914, 0.29234527697004015, 0.29315960912052119, 0.29397394117392622, 0.2947882736156352, 0.29723127035830621, 0.30130293159609123, 0.30293159613974319, 0.30456026058631924, 0.30700325732899025, 0.30944625416873722, 0.31107491846969926, 0.31188925086287023, 0.31351791535798423, 0.31596091214919325, 0.31677524434821225, 0.31677524434821225, 0.31758957654723124, 0.31758957654723124, 0.31758957635307933, 0.31840390884332626, 0.32247557003257327, 0.32573289902280128, 0.32899022811010531, 0.3306188925081433, 0.33143322485277632, 0.33469055374592832, 0.33794788254200442, 0.33876221498371334, 0.33957654713419438, 0.34039087938175139, 0.34201954407101731, 0.34201954407101731, 0.34527687286709341, 0.34771986960976442, 0.34853420195439738, 0.35016286635243543, 0.3501628664980494, 0.35016286644951139, 0.35179153084754944, 0.35260586328925836, 0.35586319208533446, 0.3583061889493504, 0.3607491857648284, 0.36074918571629044, 0.36156351781823348, 0.36482084700261341, 0.36482084690553745, 0.36726384355113251, 0.36807817599284143, 0.36889250794917056, 0.37052117266270546, 0.37052117263843648, 0.37052117263843648, 0.37052117273551244, 0.37214983718208849, 0.37296416957525941, 0.37377850143451258, 0.3745928338762215]}
[2017-12-15 02:36:16,111 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:129]: evaluating model ... 
[2017-12-15 02:36:23,686 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:133]: evaluated! 
[2017-12-15 02:36:23,687 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:135]: generating reports ... 
[2017-12-15 02:36:26,140 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:138]: done!
[2017-12-15 02:36:26,141 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_8 finished!
[2018-04-29 11:38:17,513 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_8 was already executed!
[2018-04-29 11:42:07,459 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_8 was already executed!
[2018-04-29 13:12:02,744 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_8
[2018-04-29 13:12:02,744 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:146]: >> Printing header log
[2018-04-29 13:12:02,744 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_8
	layers = 9216,7373
	using GLOBAL obj = 
		{'data_dir': '/home/dhiego/malware_dataset/', 'epochs': 200, 'autoencoder_configs': {'optimizer': <keras.optimizers.SGD object at 0x7f423fbda828>, 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'loss_function': 'mse'}, 'store_history': True, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'numpy_seed': 666, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'shuffle_batches': True, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'batch': 32, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'mlp_configs': {'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7f423fbda898>, 'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy'}, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s'}
	=======================================
	
[2018-04-29 13:12:02,744 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,687 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,687 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,687 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:57]: =======================================
[2018-04-29 13:12:20,687 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:62]: setting configurations for autoencoder: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7f423fbda828>, 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'loss_function': 'mse'}
[2018-04-29 13:12:20,733 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,873 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_8
[2018-04-29 13:14:12,873 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:146]: >> Printing header log
[2018-04-29 13:14:12,873 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_8
	layers = 9216,7373
	using GLOBAL obj = 
		{'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f94d276b860>, 'discard_decoder_function': True, 'output_layer_activation': 'relu'}, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'mlp_configs': {'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7f94d276b8d0>, 'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy'}, 'data_dir': '/home/dhiego/malware_dataset/', 'epochs': 200, 'numpy_seed': 666, 'store_history': True, 'batch': 32, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:14:12,873 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:148]: >> Loading dataset... 
[2018-04-29 13:14:30,957 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:30,958 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:30,958 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:57]: =======================================
[2018-04-29 13:14:30,958 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f94d276b860>, 'discard_decoder_function': True, 'output_layer_activation': 'relu'}
[2018-04-29 13:14:31,005 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,887 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_8
[2018-04-29 13:16:33,888 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:146]: >> Printing header log
[2018-04-29 13:16:33,888 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_8
	layers = 9216,7373
	using GLOBAL obj = 
		{'data_dir': '/home/dhiego/malware_dataset/', 'mlp_configs': {'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7ff0d9163898>, 'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy'}, 'shuffle_batches': True, 'batch': 32, 'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'store_history': True, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'autoencoder_configs': {'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7ff0d9163828>, 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'loss_function': 'mse'}, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'epochs': 200, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:16:33,888 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:148]: >> Loading dataset... 
[2018-04-29 13:17:00,606 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:17:00,607 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:17:00,608 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:57]: =======================================
[2018-04-29 13:17:00,609 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:62]: setting configurations for autoencoder: 
	 {'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7ff0d9163828>, 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'loss_function': 'mse'}
[2018-04-29 13:17:00,806 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:21,166 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_8
[2018-04-29 14:30:21,166 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:146]: >> Printing header log
[2018-04-29 14:30:21,166 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_8
	layers = 9216,7373
	using GLOBAL obj = 
		{'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'shuffle_batches': True, 'batch': 32, 'epochs': 200, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'store_history': True, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'mlp_configs': {'activation': 'sigmoid', 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7fb594539908>, 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy'}, 'autoencoder_configs': {'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fb594539898>, 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'loss_function': 'mse'}, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'numpy_seed': 666, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/'}
	=======================================
	
[2018-04-29 14:30:21,166 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:148]: >> Loading dataset... 
[2018-04-29 14:30:39,358 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:39,359 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:39,359 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:57]: =======================================
[2018-04-29 14:30:39,359 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:62]: setting configurations for autoencoder: 
	 {'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fb594539898>, 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'loss_function': 'mse'}
[2018-04-29 14:30:39,451 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:73]: training and evaluate autoencoder
[2018-04-29 19:44:08,403 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:85]: trained and evaluated!
[2018-04-29 19:44:08,404 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:88]: Training history: 
{'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.0001212402723455826, 0.00012123667129166789, 0.00012123308313065527, 0.0001212295002784847, 0.00012122590254259626, 0.00012122232362465684, 0.00012121875056066377, 0.00012121516958080803, 0.00012121157819656989, 0.00012120800444527138, 0.00012120442818175298, 0.00012120085822248448, 0.00012119727375870116, 0.0001211937041312353, 0.0001211901211368636, 0.00012118654857057447, 0.00012118296235297725, 0.00012117939694414479, 0.00012117581677009542, 0.0001211722499629519, 0.00012116867886607441, 0.0001211651054939789, 0.00012116153927934007, 0.00012115796808766183, 0.00012115438940672429, 0.00012115081276400287, 0.00012114724583835842, 0.00012114368410305505, 0.00012114012435856745, 0.00012113655103387233, 0.00012113297654786799, 0.0001211294341045174, 0.00012112586845868306, 0.00012112231757806565, 0.00012111877079758071, 0.00012111521562722934, 0.00012111165327572109, 0.00012110809360233406, 0.00012110455198849003, 0.00012110098105751385, 0.00012109744555831824, 0.00012109389372969331, 0.00012109031848528298, 0.00012108676376523516, 0.00012108321179440912, 0.00012107966622263375, 0.00012107609126262568, 0.00012107254289422816, 0.00012106898466655256, 0.00012106544217580158, 0.0001210618996850506, 0.00012105834932583733, 0.00012105477839486115, 0.00012105122967096082, 0.00012104768312747776, 0.000121044127909726, 0.00012104059475684896, 0.00012103704029750322, 0.00012103349958426632, 0.00012102995616920802, 0.00012102640908062063, 0.00012102286924429068, 0.00012101933713422191, 0.00012101580144542479, 0.00012101226158539465, 0.00012100871641652246, 0.00012100517200605629, 0.00012100162918350268, 0.00012099808076770479, 0.00012099452514704984, 0.00012099098459971425, 0.00012098745677937944, 0.0001209839163268446, 0.00012098036779254578, 0.0001209768370333877, 0.0001209733193567332, 0.00012096977783768993, 0.00012096623039359973, 0.00012096268875605551, 0.00012095915460777059, 0.00012095561740216147, 0.00012095207495881086, 0.00012094853668669329, 0.00012094500187480313, 0.00012094146476399475, 0.00012093794680293801, 0.00012093441047423582, 0.00012093086554236552, 0.00012092733926254291, 0.00012092380902478896, 0.00012092027061047026, 0.00012091673032383674, 0.00012091319143551429, 0.00012090966461058737, 0.0001209061524797768, 0.00012090263722054144, 0.0001208991239047215, 0.00012089559461497504, 0.00012089208212866165, 0.00012088854452014935, 0.00012088502177165469], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.0001204160828482593, 0.0001204125932846859, 0.00012040910724289052, 0.00012040560525477033, 0.00012040212378949865, 0.00012039864175216151, 0.00012039515537070225, 0.0001203916624641213, 0.00012038818941893816, 0.00012038470541512599, 0.00012038123195877079, 0.00012037774623876222, 0.00012037426992207972, 0.00012037078845680804, 0.0001203673092797982, 0.00012036381996650345, 0.00012036035067549992, 0.00012035686676107296, 0.00012035339747006944, 0.00012034992303047673, 0.00012034644254844256, 0.00012034296998593965, 0.00012033949334747033, 0.00012033600673360947, 0.00012033252870073057, 0.00012032905622761289, 0.00012032558489862614, 0.0001203221206847036, 0.00012031863986300557, 0.00012031515775416023, 0.00012031170775248904, 0.00012030824500448426, 0.00012030478422295448, 0.00012030133275536555, 0.00012029786943529529, 0.00012029439991189015, 0.00012029092931586229, 0.00012028748757338623, 0.0001202840101125728, 0.00012028056731535104, 0.00012027710882208313, 0.00012027362465737753, 0.00012027016829147807, 0.00012026671469652069, 0.00012026326020771102, 0.00012025977441619427, 0.00012025632033855666, 0.00012025285498050318, 0.00012024940710620044, 0.00012024595916038951, 0.00012024249870064657, 0.00012023902721076641, 0.00012023557084486694, 0.00012023212093258098, 0.00012022865401922453, 0.00012022520934491297, 0.000120221750440473, 0.00012021830306672754, 0.0001202148514382452, 0.00012021139875501714, 0.00012020794639357592, 0.00012020450620640285, 0.00012020106455331202, 0.00012019761505219811, 0.00012019417307732046, 0.00012019071694382259, 0.00012018726712092185, 0.00012018381345445629, 0.00012018035079583672, 0.0001201769074265495, 0.00012017347206617877, 0.00012017002370919579, 0.00012016657290305754, 0.00012016313827564569, 0.00012015971403479741, 0.00012015626675043717, 0.00012015281422810254, 0.00012014937273590512, 0.0001201459293666179, 0.00012014248853587117, 0.00012013904418334644, 0.00012013559852579737, 0.00012013215712298518, 0.00012012871734698415, 0.00012012529228379177, 0.00012012184875361113, 0.00012011840449047163, 0.00012011496880831407, 0.00012011153173174695, 0.00012010809147306569, 0.00012010464279429589, 0.00012010119738702546, 0.0001200977707685301, 0.00012009435078241869, 0.00012009093235161027, 0.00012008751300907252, 0.00012008407872132454, 0.00012008066534972006, 0.00012007722099719533, 0.00012007379430719177, 0.00012007036590099183]}
[2018-04-29 19:44:08,404 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:92]: done!
[2018-04-29 19:44:08,405 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:152]: >> Executing classifier part ... 
[2018-04-29 19:44:08,405 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:97]: =======================================
[2018-04-29 19:44:08,405 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7fb594539908>, 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy'}
[2018-04-29 19:44:08,764 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:110]: training ... 
[2018-04-30 00:07:11,206 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:122]: trained!
[2018-04-30 00:07:11,208 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:125]: Training history: 
{'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.0001212402723455826, 0.00012123667129166789, 0.00012123308313065527, 0.0001212295002784847, 0.00012122590254259626, 0.00012122232362465684, 0.00012121875056066377, 0.00012121516958080803, 0.00012121157819656989, 0.00012120800444527138, 0.00012120442818175298, 0.00012120085822248448, 0.00012119727375870116, 0.0001211937041312353, 0.0001211901211368636, 0.00012118654857057447, 0.00012118296235297725, 0.00012117939694414479, 0.00012117581677009542, 0.0001211722499629519, 0.00012116867886607441, 0.0001211651054939789, 0.00012116153927934007, 0.00012115796808766183, 0.00012115438940672429, 0.00012115081276400287, 0.00012114724583835842, 0.00012114368410305505, 0.00012114012435856745, 0.00012113655103387233, 0.00012113297654786799, 0.0001211294341045174, 0.00012112586845868306, 0.00012112231757806565, 0.00012111877079758071, 0.00012111521562722934, 0.00012111165327572109, 0.00012110809360233406, 0.00012110455198849003, 0.00012110098105751385, 0.00012109744555831824, 0.00012109389372969331, 0.00012109031848528298, 0.00012108676376523516, 0.00012108321179440912, 0.00012107966622263375, 0.00012107609126262568, 0.00012107254289422816, 0.00012106898466655256, 0.00012106544217580158, 0.0001210618996850506, 0.00012105834932583733, 0.00012105477839486115, 0.00012105122967096082, 0.00012104768312747776, 0.000121044127909726, 0.00012104059475684896, 0.00012103704029750322, 0.00012103349958426632, 0.00012102995616920802, 0.00012102640908062063, 0.00012102286924429068, 0.00012101933713422191, 0.00012101580144542479, 0.00012101226158539465, 0.00012100871641652246, 0.00012100517200605629, 0.00012100162918350268, 0.00012099808076770479, 0.00012099452514704984, 0.00012099098459971425, 0.00012098745677937944, 0.0001209839163268446, 0.00012098036779254578, 0.0001209768370333877, 0.0001209733193567332, 0.00012096977783768993, 0.00012096623039359973, 0.00012096268875605551, 0.00012095915460777059, 0.00012095561740216147, 0.00012095207495881086, 0.00012094853668669329, 0.00012094500187480313, 0.00012094146476399475, 0.00012093794680293801, 0.00012093441047423582, 0.00012093086554236552, 0.00012092733926254291, 0.00012092380902478896, 0.00012092027061047026, 0.00012091673032383674, 0.00012091319143551429, 0.00012090966461058737, 0.0001209061524797768, 0.00012090263722054144, 0.0001208991239047215, 0.00012089559461497504, 0.00012089208212866165, 0.00012088854452014935, 0.00012088502177165469], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.0001204160828482593, 0.0001204125932846859, 0.00012040910724289052, 0.00012040560525477033, 0.00012040212378949865, 0.00012039864175216151, 0.00012039515537070225, 0.0001203916624641213, 0.00012038818941893816, 0.00012038470541512599, 0.00012038123195877079, 0.00012037774623876222, 0.00012037426992207972, 0.00012037078845680804, 0.0001203673092797982, 0.00012036381996650345, 0.00012036035067549992, 0.00012035686676107296, 0.00012035339747006944, 0.00012034992303047673, 0.00012034644254844256, 0.00012034296998593965, 0.00012033949334747033, 0.00012033600673360947, 0.00012033252870073057, 0.00012032905622761289, 0.00012032558489862614, 0.0001203221206847036, 0.00012031863986300557, 0.00012031515775416023, 0.00012031170775248904, 0.00012030824500448426, 0.00012030478422295448, 0.00012030133275536555, 0.00012029786943529529, 0.00012029439991189015, 0.00012029092931586229, 0.00012028748757338623, 0.0001202840101125728, 0.00012028056731535104, 0.00012027710882208313, 0.00012027362465737753, 0.00012027016829147807, 0.00012026671469652069, 0.00012026326020771102, 0.00012025977441619427, 0.00012025632033855666, 0.00012025285498050318, 0.00012024940710620044, 0.00012024595916038951, 0.00012024249870064657, 0.00012023902721076641, 0.00012023557084486694, 0.00012023212093258098, 0.00012022865401922453, 0.00012022520934491297, 0.000120221750440473, 0.00012021830306672754, 0.0001202148514382452, 0.00012021139875501714, 0.00012020794639357592, 0.00012020450620640285, 0.00012020106455331202, 0.00012019761505219811, 0.00012019417307732046, 0.00012019071694382259, 0.00012018726712092185, 0.00012018381345445629, 0.00012018035079583672, 0.0001201769074265495, 0.00012017347206617877, 0.00012017002370919579, 0.00012016657290305754, 0.00012016313827564569, 0.00012015971403479741, 0.00012015626675043717, 0.00012015281422810254, 0.00012014937273590512, 0.0001201459293666179, 0.00012014248853587117, 0.00012013904418334644, 0.00012013559852579737, 0.00012013215712298518, 0.00012012871734698415, 0.00012012529228379177, 0.00012012184875361113, 0.00012011840449047163, 0.00012011496880831407, 0.00012011153173174695, 0.00012010809147306569, 0.00012010464279429589, 0.00012010119738702546, 0.0001200977707685301, 0.00012009435078241869, 0.00012009093235161027, 0.00012008751300907252, 0.00012008407872132454, 0.00012008066534972006, 0.00012007722099719533, 0.00012007379430719177, 0.00012007036590099183]}
[2018-04-30 00:07:11,208 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:129]: evaluating model ... 
[2018-04-30 00:07:18,350 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:133]: evaluated! 
[2018-04-30 00:07:18,351 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:135]: generating reports ... 
[2018-04-30 00:07:21,277 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:138]: done!
[2018-04-30 00:07:21,278 AE_BIGRAMA_1L_MINIDS_UNDER_F0_8.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_8 finished!
