[2017-12-14 09:31:57,869 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_4
[2017-12-14 09:31:57,869 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:146]: >> Printing header log
[2017-12-14 09:31:57,869 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_4
	layers = 9216,3686
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f84080dae48>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f840813c390>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,869 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:148]: >> Loading dataset... 
[2017-12-14 09:32:20,750 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:20,750 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:20,750 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:57]: =======================================
[2017-12-14 09:32:20,751 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f84080dae48>, 'discard_decoder_function': True}
[2017-12-14 09:32:20,795 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:55,080 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_4
[2017-12-14 10:18:55,081 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:146]: >> Printing header log
[2017-12-14 10:18:55,081 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_4
	layers = 9216,3686
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f848b456eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f848b439400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:55,081 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:148]: >> Loading dataset... 
[2017-12-14 10:19:17,662 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:17,663 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:17,663 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:57]: =======================================
[2017-12-14 10:19:17,663 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f848b456eb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:17,710 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:73]: training and evaluate autoencoder
[2017-12-14 16:25:52,224 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:85]: trained and evaluated!
[2017-12-14 16:25:52,227 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:88]: Training history: 
{'val_loss': [0.00011649913801619993, 0.00011649599692987905, 0.00011649285825695934, 0.0001164897102700988, 0.0001164865617111728, 0.00011648339001934961, 0.00011648021620015799, 0.00011647704359660548, 0.00011647386487910332, 0.0001164706878599205, 0.00011646747823300628, 0.00011646430031997118, 0.00011646111799130578, 0.00011645792879785482, 0.00011645474353735393, 0.0001164515466567733, 0.0001164483132355112, 0.00011644510752366998, 0.00011644190590567223, 0.00011643869472345502, 0.0001164354890116138, 0.00011643226687076755, 0.00011642904399696244, 0.00011642579748970285, 0.00011642255793661406, 0.00011641932067178711, 0.00011641607539804369, 0.00011641283007066912, 0.00011640957820029582, 0.00011640632502489817, 0.00011640306702269817, 0.00011639979986745076, 0.00011639652305859866, 0.00011639325844189176, 0.00011638999895589697, 0.00011638670158844226, 0.00011638339490704673, 0.00011638006811397475, 0.00011637671663270261, 0.00011637339237817112, 0.00011637004842313524, 0.00011636669188265915, 0.00011636332111205467, 0.0001163599540241216, 0.00011635655302343277, 0.00011635315438251398, 0.00011634973579081214, 0.00011634631761028237, 0.00011634290531130063, 0.00011633949676649851, 0.0001163360703267736, 0.00011633261935974194, 0.00011632918081725707, 0.00011632572200220234, 0.00011632223170567006, 0.0001163187556392662, 0.00011631526223212797, 0.00011631175960043413, 0.00011630821903364925, 0.00011630466432612119, 0.00011630113087440052, 0.0001162975908796811, 0.0001162940388537099, 0.00011629045550715458, 0.0001162868352802539, 0.00011628323607675901, 0.00011627961863867747, 0.00011627594267114818, 0.0001162722755348795, 0.0001162685698914543, 0.00011626489564012142, 0.00011626118910284393, 0.00011625748329852531, 0.00011625377463387938, 0.00011625005558266989, 0.00011624632467897907, 0.00011624258478313423, 0.00011623883555547154, 0.00011623507898034303, 0.00011623129968348942, 0.00011622752651846247, 0.0001162237325088002, 0.00011621993637176949, 0.000116216113347662, 0.00011621227282192673, 0.00011620842788056115, 0.00011620459341514439, 0.000116200724697308, 0.00011619683218512373, 0.00011619292512102424, 0.00011618898348271828, 0.00011618503621314293, 0.00011618106498832629, 0.00011617707790657008, 0.00011617306572544166, 0.00011616907512190745, 0.00011616504324027463, 0.00011616099623466112, 0.00011615693181680506, 0.00011615286682688354, 0.00011614882096540095, 0.00011614468616561584, 0.00011614055553118239, 0.00011613643437158316, 0.00011613229360086478, 0.00011612814907596678, 0.00011612399588070159, 0.00011611979012692196, 0.00011611560807810497, 0.0001161113787802561, 0.00011610712615286253, 0.00011610286101153696, 0.00011609857477529744, 0.00011609430375242381, 0.00011608997967047845, 0.00011608565599970514, 0.00011608134082052857, 0.00011607697824930374, 0.00011607261567807891, 0.00011606822956278485, 0.00011606382644642029, 0.00011605941980827773, 0.00011605500884389011, 0.0001160506241945138, 0.00011604618053300951, 0.00011604170800007637, 0.00011603725305815623, 0.00011603278388610769, 0.00011602829329735835, 0.00011602376143051041, 0.0001160192156553209, 0.00011601466310473104, 0.00011601010687146977, 0.00011600552906061429, 0.00011600093971906431, 0.00011599637874838592, 0.00011599176243037393, 0.00011598715240508203, 0.00011598251041563237, 0.00011597786957031363, 0.00011597320632505659, 0.00011596855575462496, 0.00011596388703899193, 0.0001159591979614038, 0.00011595447112749507, 0.00011594970047694725, 0.00011594496579501544, 0.00011594017208307863, 0.00011593535818795717, 0.00011593056939220793, 0.00011592578958861269, 0.00011592096686223065, 0.00011591614994588846, 0.00011591129191234108, 0.00011590646780942652, 0.00011590162359483549, 0.00011589675084847948, 0.00011589186077926615, 0.000115886942249796, 0.00011588198621428391, 0.00011587700173639205, 0.00011587196984857491, 0.00011586696427576907, 0.00011586191930195445, 0.00011585685650472523, 0.00011585179027510322, 0.00011584676198057227, 0.00011584172167266658, 0.00011583661997498583, 0.00011583146514672516, 0.00011582633572174648, 0.00011582115504327768, 0.00011581597919161123, 0.00011581076885512355, 0.00011580550635341642, 0.00011580023380480959, 0.00011579492929204498, 0.00011578958423414064, 0.00011578426663537856, 0.00011577890228814187, 0.0001157735169174994, 0.00011576813147534875, 0.00011576271356848304, 0.00011575728463148011, 0.00011575184210792244, 0.00011574634579233414, 0.0001157408785090681, 0.00011573535670081259, 0.00011572978135080509, 0.00011572422798956385, 0.00011571863801613295, 0.00011571300896348006, 0.00011570736846951791, 0.00011570170353763423, 0.00011569602037116391, 0.0001156902942103986, 0.00011568458953784225, 0.00011567887490777143, 0.00011567314753136701, 0.00011566738533047749], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011724375764454839, 0.00011724057617875616, 0.00011723738618089639, 0.00011723419255690789, 0.00011723099258126909, 0.00011722779717976652, 0.00011722458087469843, 0.00011722135755437477, 0.0001172181271713952, 0.00011721488664473531, 0.00011721164628397673, 0.00011720836793181732, 0.00011720512489293753, 0.00011720186832125059, 0.00011719860513721128, 0.00011719534413358925, 0.00011719206514152477, 0.00011718874559843919, 0.00011718545271806473, 0.00011718216716104827, 0.00011717887387777062, 0.00011717557985978717, 0.00011717226304222317, 0.00011716894584545617, 0.00011716559852575064, 0.00011716225753399523, 0.0001171589248136053, 0.00011715558711617596, 0.00011715223965426931, 0.00011714889361437391, 0.00011714554198123424, 0.0001171421879543756, 0.0001171388255850509, 0.00011713544214625937, 0.00011713208006133693, 0.00011712871202766737, 0.00011712532614775651, 0.0001171219315224764, 0.00011711852900503379, 0.00011711511082176713, 0.00011711171712079434, 0.00011710831493515436, 0.00011710489663338677, 0.0001171014734730807, 0.00011709804275241478, 0.00011709459247909401, 0.00011709112236871617, 0.00011708763815672668, 0.0001170841346764846, 0.00011708063588887969, 0.00011707715307520125, 0.00011707365995194119, 0.0001170701282684758, 0.00011706660943051211, 0.00011706306203382229, 0.00011705947574512451, 0.00011705589128134117, 0.00011705228733600357, 0.00011704867419499315, 0.0001170450392261099, 0.00011704138863880296, 0.00011703776860103794, 0.00011703412742270552, 0.000117030482191641, 0.00011702682342776932, 0.00011702311861443299, 0.00011701944235982283, 0.00011701575634073533, 0.00011701202884631941, 0.00011700830640004347, 0.00011700454411375205, 0.00011700081432041794, 0.00011699705656086238, 0.00011699330463155297, 0.00011698953578030958, 0.00011698575330145827, 0.00011698195186245683, 0.00011697813890516418, 0.00011697431770020623, 0.00011697049431483101, 0.00011696664488294954, 0.00011696279862689321, 0.00011695893509340008, 0.00011695508108738239, 0.00011695120146146184, 0.00011694728884488004, 0.00011694337928562246, 0.00011693947934864108, 0.00011693555118473616, 0.00011693160481908713, 0.0001169276452050332, 0.00011692365755365724, 0.0001169196793112558, 0.00011691565319447526, 0.00011691161586750597, 0.00011690755725776813, 0.00011690351623356955, 0.00011689942852000127, 0.00011689532677592189, 0.00011689120227966233, 0.00011688707346996863, 0.0001168829585485849, 0.00011687874432341483, 0.00011687453343997124, 0.00011687032829197306, 0.0001168660980928765, 0.00011686187159100924, 0.00011685763975659974, 0.00011685336857987871, 0.00011684910207209465, 0.00011684479005995028, 0.00011684045652803549, 0.00011683611060092256, 0.00011683174490785311, 0.00011682741109153608, 0.00011682302155607783, 0.00011681864090818997, 0.00011681426803396366, 0.00011680986470499619, 0.00011680546324834355, 0.00011680101687983528, 0.0001167965616948572, 0.00011679210117733688, 0.0001167876308005385, 0.00011678317886248614, 0.00011677866656005577, 0.00011677411567371987, 0.00011676958431633863, 0.00011676503250569543, 0.00011676044794139285, 0.00011675582604929471, 0.00011675118894167609, 0.00011674654202217977, 0.00011674188775562526, 0.00011673720504884557, 0.00011673250274201067, 0.00011672781069735702, 0.00011672307523395957, 0.00011671832761236584, 0.00011671355823399984, 0.00011670877954146009, 0.0001167039882404205, 0.00011669919829029159, 0.00011669437587090559, 0.00011668953209765049, 0.00011668465540483473, 0.00011667974356464064, 0.00011667485568533631, 0.00011666990815265959, 0.00011666495322552433, 0.00011666003451227583, 0.0001166551014367136, 0.00011665011207320567, 0.00011664514176464861, 0.0001166401308102697, 0.00011663515358125784, 0.00011663015333936376, 0.00011662512515494843, 0.00011662007450275518, 0.0001166149902436958, 0.00011660989238072873, 0.00011660477612641601, 0.00011659961787537074, 0.00011659448130999719, 0.00011658930338779616, 0.00011658410991827201, 0.0001165788785521478, 0.00011657368401611522, 0.00011656848867427624, 0.00011656323799249907, 0.00011655794905861901, 0.00011655267654896899, 0.00011654734905488358, 0.00011654204123195394, 0.00011653670743361861, 0.00011653132085496536, 0.00011652592131230944, 0.00011652046628751419, 0.00011651496047321676, 0.00011650948930859372, 0.00011650398688342313, 0.00011649846357838721, 0.00011649293354249799, 0.00011648736373769387, 0.00011648177404843228, 0.00011647619035531818, 0.00011647055881152518, 0.00011646496573313676, 0.00011645932224444917, 0.00011645362571474158, 0.00011644795276672072, 0.00011644224685173882, 0.00011643650107304126, 0.00011643074498476205, 0.0001164249613331646, 0.00011641916500196675, 0.00011641331354413236, 0.00011640747851052803, 0.0001164016373622753, 0.00011639581119254116], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-14 16:25:52,228 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:92]: done!
[2017-12-14 16:25:52,228 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:152]: >> Executing classifier part ... 
[2017-12-14 16:25:52,229 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:97]: =======================================
[2017-12-14 16:25:52,274 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f848b439400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-14 16:25:52,371 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:110]: training ... 
[2017-12-14 19:16:22,620 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:122]: trained!
[2017-12-14 19:16:22,623 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:125]: Training history: 
{'val_loss': [0.00011649913801619993, 0.00011649599692987905, 0.00011649285825695934, 0.0001164897102700988, 0.0001164865617111728, 0.00011648339001934961, 0.00011648021620015799, 0.00011647704359660548, 0.00011647386487910332, 0.0001164706878599205, 0.00011646747823300628, 0.00011646430031997118, 0.00011646111799130578, 0.00011645792879785482, 0.00011645474353735393, 0.0001164515466567733, 0.0001164483132355112, 0.00011644510752366998, 0.00011644190590567223, 0.00011643869472345502, 0.0001164354890116138, 0.00011643226687076755, 0.00011642904399696244, 0.00011642579748970285, 0.00011642255793661406, 0.00011641932067178711, 0.00011641607539804369, 0.00011641283007066912, 0.00011640957820029582, 0.00011640632502489817, 0.00011640306702269817, 0.00011639979986745076, 0.00011639652305859866, 0.00011639325844189176, 0.00011638999895589697, 0.00011638670158844226, 0.00011638339490704673, 0.00011638006811397475, 0.00011637671663270261, 0.00011637339237817112, 0.00011637004842313524, 0.00011636669188265915, 0.00011636332111205467, 0.0001163599540241216, 0.00011635655302343277, 0.00011635315438251398, 0.00011634973579081214, 0.00011634631761028237, 0.00011634290531130063, 0.00011633949676649851, 0.0001163360703267736, 0.00011633261935974194, 0.00011632918081725707, 0.00011632572200220234, 0.00011632223170567006, 0.0001163187556392662, 0.00011631526223212797, 0.00011631175960043413, 0.00011630821903364925, 0.00011630466432612119, 0.00011630113087440052, 0.0001162975908796811, 0.0001162940388537099, 0.00011629045550715458, 0.0001162868352802539, 0.00011628323607675901, 0.00011627961863867747, 0.00011627594267114818, 0.0001162722755348795, 0.0001162685698914543, 0.00011626489564012142, 0.00011626118910284393, 0.00011625748329852531, 0.00011625377463387938, 0.00011625005558266989, 0.00011624632467897907, 0.00011624258478313423, 0.00011623883555547154, 0.00011623507898034303, 0.00011623129968348942, 0.00011622752651846247, 0.0001162237325088002, 0.00011621993637176949, 0.000116216113347662, 0.00011621227282192673, 0.00011620842788056115, 0.00011620459341514439, 0.000116200724697308, 0.00011619683218512373, 0.00011619292512102424, 0.00011618898348271828, 0.00011618503621314293, 0.00011618106498832629, 0.00011617707790657008, 0.00011617306572544166, 0.00011616907512190745, 0.00011616504324027463, 0.00011616099623466112, 0.00011615693181680506, 0.00011615286682688354, 0.00011614882096540095, 0.00011614468616561584, 0.00011614055553118239, 0.00011613643437158316, 0.00011613229360086478, 0.00011612814907596678, 0.00011612399588070159, 0.00011611979012692196, 0.00011611560807810497, 0.0001161113787802561, 0.00011610712615286253, 0.00011610286101153696, 0.00011609857477529744, 0.00011609430375242381, 0.00011608997967047845, 0.00011608565599970514, 0.00011608134082052857, 0.00011607697824930374, 0.00011607261567807891, 0.00011606822956278485, 0.00011606382644642029, 0.00011605941980827773, 0.00011605500884389011, 0.0001160506241945138, 0.00011604618053300951, 0.00011604170800007637, 0.00011603725305815623, 0.00011603278388610769, 0.00011602829329735835, 0.00011602376143051041, 0.0001160192156553209, 0.00011601466310473104, 0.00011601010687146977, 0.00011600552906061429, 0.00011600093971906431, 0.00011599637874838592, 0.00011599176243037393, 0.00011598715240508203, 0.00011598251041563237, 0.00011597786957031363, 0.00011597320632505659, 0.00011596855575462496, 0.00011596388703899193, 0.0001159591979614038, 0.00011595447112749507, 0.00011594970047694725, 0.00011594496579501544, 0.00011594017208307863, 0.00011593535818795717, 0.00011593056939220793, 0.00011592578958861269, 0.00011592096686223065, 0.00011591614994588846, 0.00011591129191234108, 0.00011590646780942652, 0.00011590162359483549, 0.00011589675084847948, 0.00011589186077926615, 0.000115886942249796, 0.00011588198621428391, 0.00011587700173639205, 0.00011587196984857491, 0.00011586696427576907, 0.00011586191930195445, 0.00011585685650472523, 0.00011585179027510322, 0.00011584676198057227, 0.00011584172167266658, 0.00011583661997498583, 0.00011583146514672516, 0.00011582633572174648, 0.00011582115504327768, 0.00011581597919161123, 0.00011581076885512355, 0.00011580550635341642, 0.00011580023380480959, 0.00011579492929204498, 0.00011578958423414064, 0.00011578426663537856, 0.00011577890228814187, 0.0001157735169174994, 0.00011576813147534875, 0.00011576271356848304, 0.00011575728463148011, 0.00011575184210792244, 0.00011574634579233414, 0.0001157408785090681, 0.00011573535670081259, 0.00011572978135080509, 0.00011572422798956385, 0.00011571863801613295, 0.00011571300896348006, 0.00011570736846951791, 0.00011570170353763423, 0.00011569602037116391, 0.0001156902942103986, 0.00011568458953784225, 0.00011567887490777143, 0.00011567314753136701, 0.00011566738533047749], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011724375764454839, 0.00011724057617875616, 0.00011723738618089639, 0.00011723419255690789, 0.00011723099258126909, 0.00011722779717976652, 0.00011722458087469843, 0.00011722135755437477, 0.0001172181271713952, 0.00011721488664473531, 0.00011721164628397673, 0.00011720836793181732, 0.00011720512489293753, 0.00011720186832125059, 0.00011719860513721128, 0.00011719534413358925, 0.00011719206514152477, 0.00011718874559843919, 0.00011718545271806473, 0.00011718216716104827, 0.00011717887387777062, 0.00011717557985978717, 0.00011717226304222317, 0.00011716894584545617, 0.00011716559852575064, 0.00011716225753399523, 0.0001171589248136053, 0.00011715558711617596, 0.00011715223965426931, 0.00011714889361437391, 0.00011714554198123424, 0.0001171421879543756, 0.0001171388255850509, 0.00011713544214625937, 0.00011713208006133693, 0.00011712871202766737, 0.00011712532614775651, 0.0001171219315224764, 0.00011711852900503379, 0.00011711511082176713, 0.00011711171712079434, 0.00011710831493515436, 0.00011710489663338677, 0.0001171014734730807, 0.00011709804275241478, 0.00011709459247909401, 0.00011709112236871617, 0.00011708763815672668, 0.0001170841346764846, 0.00011708063588887969, 0.00011707715307520125, 0.00011707365995194119, 0.0001170701282684758, 0.00011706660943051211, 0.00011706306203382229, 0.00011705947574512451, 0.00011705589128134117, 0.00011705228733600357, 0.00011704867419499315, 0.0001170450392261099, 0.00011704138863880296, 0.00011703776860103794, 0.00011703412742270552, 0.000117030482191641, 0.00011702682342776932, 0.00011702311861443299, 0.00011701944235982283, 0.00011701575634073533, 0.00011701202884631941, 0.00011700830640004347, 0.00011700454411375205, 0.00011700081432041794, 0.00011699705656086238, 0.00011699330463155297, 0.00011698953578030958, 0.00011698575330145827, 0.00011698195186245683, 0.00011697813890516418, 0.00011697431770020623, 0.00011697049431483101, 0.00011696664488294954, 0.00011696279862689321, 0.00011695893509340008, 0.00011695508108738239, 0.00011695120146146184, 0.00011694728884488004, 0.00011694337928562246, 0.00011693947934864108, 0.00011693555118473616, 0.00011693160481908713, 0.0001169276452050332, 0.00011692365755365724, 0.0001169196793112558, 0.00011691565319447526, 0.00011691161586750597, 0.00011690755725776813, 0.00011690351623356955, 0.00011689942852000127, 0.00011689532677592189, 0.00011689120227966233, 0.00011688707346996863, 0.0001168829585485849, 0.00011687874432341483, 0.00011687453343997124, 0.00011687032829197306, 0.0001168660980928765, 0.00011686187159100924, 0.00011685763975659974, 0.00011685336857987871, 0.00011684910207209465, 0.00011684479005995028, 0.00011684045652803549, 0.00011683611060092256, 0.00011683174490785311, 0.00011682741109153608, 0.00011682302155607783, 0.00011681864090818997, 0.00011681426803396366, 0.00011680986470499619, 0.00011680546324834355, 0.00011680101687983528, 0.0001167965616948572, 0.00011679210117733688, 0.0001167876308005385, 0.00011678317886248614, 0.00011677866656005577, 0.00011677411567371987, 0.00011676958431633863, 0.00011676503250569543, 0.00011676044794139285, 0.00011675582604929471, 0.00011675118894167609, 0.00011674654202217977, 0.00011674188775562526, 0.00011673720504884557, 0.00011673250274201067, 0.00011672781069735702, 0.00011672307523395957, 0.00011671832761236584, 0.00011671355823399984, 0.00011670877954146009, 0.0001167039882404205, 0.00011669919829029159, 0.00011669437587090559, 0.00011668953209765049, 0.00011668465540483473, 0.00011667974356464064, 0.00011667485568533631, 0.00011666990815265959, 0.00011666495322552433, 0.00011666003451227583, 0.0001166551014367136, 0.00011665011207320567, 0.00011664514176464861, 0.0001166401308102697, 0.00011663515358125784, 0.00011663015333936376, 0.00011662512515494843, 0.00011662007450275518, 0.0001166149902436958, 0.00011660989238072873, 0.00011660477612641601, 0.00011659961787537074, 0.00011659448130999719, 0.00011658930338779616, 0.00011658410991827201, 0.0001165788785521478, 0.00011657368401611522, 0.00011656848867427624, 0.00011656323799249907, 0.00011655794905861901, 0.00011655267654896899, 0.00011654734905488358, 0.00011654204123195394, 0.00011653670743361861, 0.00011653132085496536, 0.00011652592131230944, 0.00011652046628751419, 0.00011651496047321676, 0.00011650948930859372, 0.00011650398688342313, 0.00011649846357838721, 0.00011649293354249799, 0.00011648736373769387, 0.00011648177404843228, 0.00011647619035531818, 0.00011647055881152518, 0.00011646496573313676, 0.00011645932224444917, 0.00011645362571474158, 0.00011644795276672072, 0.00011644224685173882, 0.00011643650107304126, 0.00011643074498476205, 0.0001164249613331646, 0.00011641916500196675, 0.00011641331354413236, 0.00011640747851052803, 0.0001164016373622753, 0.00011639581119254116], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-14 19:16:22,623 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:129]: evaluating model ... 
[2017-12-14 19:16:28,215 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:133]: evaluated! 
[2017-12-14 19:16:28,215 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:135]: generating reports ... 
[2017-12-14 19:16:31,260 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:138]: done!
[2017-12-14 19:16:31,260 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_4 finished!
[2018-04-29 11:38:17,508 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_4 was already executed!
[2018-04-29 11:42:07,386 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_4 was already executed!
[2018-04-29 13:12:02,762 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_4
[2018-04-29 13:12:02,762 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:146]: >> Printing header log
[2018-04-29 13:12:02,762 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_4
	layers = 9216,3686
	using GLOBAL obj = 
		{'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'autoencoder_configs': {'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f5309f4f828>, 'discard_decoder_function': True, 'loss_function': 'mse', 'hidden_layer_activation': 'relu'}, 'batch': 32, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'epochs': 200, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'numpy_seed': 666, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'store_history': True, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_dir': '/home/dhiego/malware_dataset/', 'mlp_configs': {'activation': 'sigmoid', 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7f5309f4f898>}, 'shuffle_batches': True}
	=======================================
	
[2018-04-29 13:12:02,762 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,864 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,864 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,864 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:57]: =======================================
[2018-04-29 13:12:20,864 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:62]: setting configurations for autoencoder: 
	 {'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f5309f4f828>, 'discard_decoder_function': True, 'loss_function': 'mse', 'hidden_layer_activation': 'relu'}
[2018-04-29 13:12:20,909 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,892 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_4
[2018-04-29 13:14:12,892 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:146]: >> Printing header log
[2018-04-29 13:14:12,892 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_4
	layers = 9216,3686
	using GLOBAL obj = 
		{'shuffle_batches': True, 'batch': 32, 'epochs': 200, 'store_history': True, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'numpy_seed': 666, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'autoencoder_configs': {'discard_decoder_function': True, 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fdf7d6ea828>, 'loss_function': 'mse'}, 'data_dir': '/home/dhiego/malware_dataset/', 'mlp_configs': {'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7fdf7d6ea898>, 'loss_function': 'categorical_crossentropy'}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'fullds_data_dir': '/home/dhiego/malware_dataset/'}
	=======================================
	
[2018-04-29 13:14:12,892 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:148]: >> Loading dataset... 
[2018-04-29 13:14:30,878 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:30,879 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:30,879 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:57]: =======================================
[2018-04-29 13:14:30,879 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fdf7d6ea828>, 'loss_function': 'mse'}
[2018-04-29 13:14:30,927 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,682 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_4
[2018-04-29 13:16:33,683 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:146]: >> Printing header log
[2018-04-29 13:16:33,683 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_4
	layers = 9216,3686
	using GLOBAL obj = 
		{'numpy_seed': 666, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'autoencoder_configs': {'optimizer': <keras.optimizers.SGD object at 0x7fa9b78f1828>, 'output_layer_activation': 'relu', 'loss_function': 'mse', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu'}, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'epochs': 200, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'shuffle_batches': True, 'store_history': True, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'batch': 32, 'mlp_configs': {'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7fa9b78f1898>, 'use_last_dim_as_classifier': False}, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:16:33,683 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:148]: >> Loading dataset... 
[2018-04-29 13:16:58,089 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:58,089 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:58,089 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:57]: =======================================
[2018-04-29 13:16:58,089 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:62]: setting configurations for autoencoder: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7fa9b78f1828>, 'output_layer_activation': 'relu', 'loss_function': 'mse', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu'}
[2018-04-29 13:16:58,192 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:21,217 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_4
[2018-04-29 14:30:21,217 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:146]: >> Printing header log
[2018-04-29 14:30:21,217 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_4
	layers = 9216,3686
	using GLOBAL obj = 
		{'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'autoencoder_configs': {'optimizer': <keras.optimizers.SGD object at 0x7f6a56d50898>, 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'output_layer_activation': 'relu', 'discard_decoder_function': True}, 'numpy_seed': 666, 'store_history': True, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'mlp_configs': {'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7f6a56d50908>, 'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'use_last_dim_as_classifier': False}, 'batch': 32, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'shuffle_batches': True, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'epochs': 200, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'fullds_data_dir': '/home/dhiego/malware_dataset/'}
	=======================================
	
[2018-04-29 14:30:21,217 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:148]: >> Loading dataset... 
[2018-04-29 14:30:39,248 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:39,248 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:39,248 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:57]: =======================================
[2018-04-29 14:30:39,248 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:62]: setting configurations for autoencoder: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7f6a56d50898>, 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'output_layer_activation': 'relu', 'discard_decoder_function': True}
[2018-04-29 14:30:39,330 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:73]: training and evaluate autoencoder
[2018-04-29 17:45:47,214 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:85]: trained and evaluated!
[2018-04-29 17:45:47,215 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:88]: Training history: 
{'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.00011617959749739599, 0.00011617659645985139, 0.00011617360237647758, 0.00011617062348859266, 0.00011616762956611227, 0.0001161646271341581, 0.00011616162740163785, 0.00011615862252052841, 0.00011615563210194898, 0.00011615264822636829, 0.00011614964366704569, 0.00011614664034123924, 0.00011614364836735684, 0.00011614065819905605, 0.00011613766247099403, 0.00011613463747820814, 0.00011613163104179574, 0.00011612861698976185, 0.00011612560856899736, 0.00011612261069568986, 0.0001161196015419665, 0.00011611660522396198, 0.00011611360179089326, 0.00011611059190421103, 0.00011610757563542346, 0.00011610457130850244, 0.00011610157204078538, 0.00011609854686922904, 0.00011609553313898197, 0.00011609251679868623, 0.00011608951908627214, 0.00011608650306776321, 0.00011608349816877674, 0.0001160804837055708, 0.00011607747308592969, 0.00011607444979146315, 0.00011607143320088877, 0.000116068432878426, 0.00011606542086437533, 0.00011606240836764442, 0.00011605939635359374, 0.00011605638964902564, 0.00011605337649084403, 0.00011605035957848282, 0.00011604733955551566, 0.00011604431610015571, 0.00011604130008164678, 0.00011603827793131117, 0.00011603525815862265, 0.00011603222097369158, 0.00011602920953170637, 0.00011602620193328599, 0.00011602320177171663, 0.0001160201748839639, 0.00011601716083193, 0.00011601414424135563, 0.0001160111227524707, 0.00011600810101330715, 0.00011600507838029131, 0.00011600206579417517, 0.00011599904634327347, 0.00011599602240523329, 0.00011599302134981163, 0.00011599001162402281, 0.00011598699594517777, 0.00011598397077362142, 0.00011598095541656319, 0.00011597793768185788, 0.00011597491994715257, 0.00011597188864376954, 0.00011596888202858668, 0.00011596587843462454, 0.00011596285539043665, 0.00011595984779201627, 0.00011595682735787705, 0.00011595380186453389, 0.00011595077472650251, 0.00011594774310133267, 0.00011594472634986487, 0.0001159417082039875, 0.00011593868074416931, 0.00011593565360613794, 0.00011593263799880107, 0.0001159296198529237, 0.00011592659607577694, 0.00011592356909863898, 0.00011592054008351781, 0.0001159175195778704, 0.00011591448713035645, 0.00011591146498002084, 0.00011590844095259543, 0.00011590542738324178, 0.00011590239436366236, 0.00011589937532393271, 0.00011589634916913885, 0.00011589332170932067, 0.00011589029269419949, 0.00011588727553155964, 0.00011588424594437301, 0.00011588121692925183, 0.00011587818963032706], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011687017201293558, 0.00011686707906734428, 0.00011686396507598635, 0.0001168608582657853, 0.00011685777001283116, 0.00011685466905657645, 0.00011685155961565455, 0.00011684844837351838, 0.00011684533897999686, 0.00011684224108106636, 0.00011683915064769497, 0.00011683604680001736, 0.00011683293238205605, 0.00011682983479122798, 0.00011682673592058978, 0.0001168236352961377, 0.00011682050243943038, 0.00011681738204902178, 0.00011681426348352762, 0.00011681115382930403, 0.00011680805045563018, 0.0001168049390475927, 0.0001168018347970119, 0.0001167987205449519, 0.00011679560593738909, 0.0001167924841723696, 0.00011678937001511036, 0.00011678625919957758, 0.00011678312771748115, 0.00011678000469635171, 0.00011677687745658887, 0.00011677377097819043, 0.00011677064945017282, 0.00011676753780513346, 0.00011676442217846258, 0.00011676130797380297, 0.00011675817874322436, 0.00011675505396828105, 0.00011675194623377265, 0.00011674883188691191, 0.00011674571021669316, 0.00011674259203040201, 0.00011673947479211838, 0.00011673635805153867, 0.00011673323064587452, 0.00011673010390381562, 0.0001167269739385312, 0.00011672384973239239, 0.0001167207296737864, 0.00011671760051430837, 0.00011671446476617816, 0.00011671135148582585, 0.00011670823972228557, 0.00011670513976143874, 0.0001167020151997971, 0.0001166988944064853, 0.00011669577686009922, 0.00011669264888563057, 0.00011668953126814392, 0.00011668640599549666, 0.00011668328878091321, 0.00011668016929111173, 0.00011667704316525771, 0.00011667394033668818, 0.00011667082895235089, 0.00011666771038685673, 0.00011666457734054792, 0.00011666145704494006, 0.00011665833317060388, 0.00011665521441550822, 0.00011665207378513936, 0.00011664895877467335, 0.00011664585310208129, 0.00011664271856266065, 0.00011663960409729896, 0.00011663647481931998, 0.00011663334442743218, 0.00011663020915330573, 0.00011662706738532785, 0.00011662394626021343, 0.00011662082714961496, 0.00011661768692214927, 0.00011661455847367687, 0.00011661143218192154, 0.00011660830423115307, 0.00011660517336526152, 0.00011660204178836433, 0.0001165989076281467, 0.00011659578368270994, 0.00011659264132222738, 0.0001165895133714589, 0.00011658638084655422, 0.00011658326254176213, 0.00011658011947027392, 0.0001165769923964124, 0.00011657385769109044, 0.00011657072092385217, 0.00011656758529422289, 0.0001165644583625625, 0.00011656131574137785, 0.0001165581825765681]}
[2018-04-29 17:45:47,215 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:92]: done!
[2018-04-29 17:45:47,216 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:152]: >> Executing classifier part ... 
[2018-04-29 17:45:47,216 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:97]: =======================================
[2018-04-29 17:45:47,216 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7f6a56d50908>, 'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'use_last_dim_as_classifier': False}
[2018-04-29 17:45:47,653 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:110]: training ... 
[2018-04-29 20:48:56,839 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:122]: trained!
[2018-04-29 20:48:56,842 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:125]: Training history: 
{'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.00011617959749739599, 0.00011617659645985139, 0.00011617360237647758, 0.00011617062348859266, 0.00011616762956611227, 0.0001161646271341581, 0.00011616162740163785, 0.00011615862252052841, 0.00011615563210194898, 0.00011615264822636829, 0.00011614964366704569, 0.00011614664034123924, 0.00011614364836735684, 0.00011614065819905605, 0.00011613766247099403, 0.00011613463747820814, 0.00011613163104179574, 0.00011612861698976185, 0.00011612560856899736, 0.00011612261069568986, 0.0001161196015419665, 0.00011611660522396198, 0.00011611360179089326, 0.00011611059190421103, 0.00011610757563542346, 0.00011610457130850244, 0.00011610157204078538, 0.00011609854686922904, 0.00011609553313898197, 0.00011609251679868623, 0.00011608951908627214, 0.00011608650306776321, 0.00011608349816877674, 0.0001160804837055708, 0.00011607747308592969, 0.00011607444979146315, 0.00011607143320088877, 0.000116068432878426, 0.00011606542086437533, 0.00011606240836764442, 0.00011605939635359374, 0.00011605638964902564, 0.00011605337649084403, 0.00011605035957848282, 0.00011604733955551566, 0.00011604431610015571, 0.00011604130008164678, 0.00011603827793131117, 0.00011603525815862265, 0.00011603222097369158, 0.00011602920953170637, 0.00011602620193328599, 0.00011602320177171663, 0.0001160201748839639, 0.00011601716083193, 0.00011601414424135563, 0.0001160111227524707, 0.00011600810101330715, 0.00011600507838029131, 0.00011600206579417517, 0.00011599904634327347, 0.00011599602240523329, 0.00011599302134981163, 0.00011599001162402281, 0.00011598699594517777, 0.00011598397077362142, 0.00011598095541656319, 0.00011597793768185788, 0.00011597491994715257, 0.00011597188864376954, 0.00011596888202858668, 0.00011596587843462454, 0.00011596285539043665, 0.00011595984779201627, 0.00011595682735787705, 0.00011595380186453389, 0.00011595077472650251, 0.00011594774310133267, 0.00011594472634986487, 0.0001159417082039875, 0.00011593868074416931, 0.00011593565360613794, 0.00011593263799880107, 0.0001159296198529237, 0.00011592659607577694, 0.00011592356909863898, 0.00011592054008351781, 0.0001159175195778704, 0.00011591448713035645, 0.00011591146498002084, 0.00011590844095259543, 0.00011590542738324178, 0.00011590239436366236, 0.00011589937532393271, 0.00011589634916913885, 0.00011589332170932067, 0.00011589029269419949, 0.00011588727553155964, 0.00011588424594437301, 0.00011588121692925183, 0.00011587818963032706], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011687017201293558, 0.00011686707906734428, 0.00011686396507598635, 0.0001168608582657853, 0.00011685777001283116, 0.00011685466905657645, 0.00011685155961565455, 0.00011684844837351838, 0.00011684533897999686, 0.00011684224108106636, 0.00011683915064769497, 0.00011683604680001736, 0.00011683293238205605, 0.00011682983479122798, 0.00011682673592058978, 0.0001168236352961377, 0.00011682050243943038, 0.00011681738204902178, 0.00011681426348352762, 0.00011681115382930403, 0.00011680805045563018, 0.0001168049390475927, 0.0001168018347970119, 0.0001167987205449519, 0.00011679560593738909, 0.0001167924841723696, 0.00011678937001511036, 0.00011678625919957758, 0.00011678312771748115, 0.00011678000469635171, 0.00011677687745658887, 0.00011677377097819043, 0.00011677064945017282, 0.00011676753780513346, 0.00011676442217846258, 0.00011676130797380297, 0.00011675817874322436, 0.00011675505396828105, 0.00011675194623377265, 0.00011674883188691191, 0.00011674571021669316, 0.00011674259203040201, 0.00011673947479211838, 0.00011673635805153867, 0.00011673323064587452, 0.00011673010390381562, 0.0001167269739385312, 0.00011672384973239239, 0.0001167207296737864, 0.00011671760051430837, 0.00011671446476617816, 0.00011671135148582585, 0.00011670823972228557, 0.00011670513976143874, 0.0001167020151997971, 0.0001166988944064853, 0.00011669577686009922, 0.00011669264888563057, 0.00011668953126814392, 0.00011668640599549666, 0.00011668328878091321, 0.00011668016929111173, 0.00011667704316525771, 0.00011667394033668818, 0.00011667082895235089, 0.00011666771038685673, 0.00011666457734054792, 0.00011666145704494006, 0.00011665833317060388, 0.00011665521441550822, 0.00011665207378513936, 0.00011664895877467335, 0.00011664585310208129, 0.00011664271856266065, 0.00011663960409729896, 0.00011663647481931998, 0.00011663334442743218, 0.00011663020915330573, 0.00011662706738532785, 0.00011662394626021343, 0.00011662082714961496, 0.00011661768692214927, 0.00011661455847367687, 0.00011661143218192154, 0.00011660830423115307, 0.00011660517336526152, 0.00011660204178836433, 0.0001165989076281467, 0.00011659578368270994, 0.00011659264132222738, 0.0001165895133714589, 0.00011658638084655422, 0.00011658326254176213, 0.00011658011947027392, 0.0001165769923964124, 0.00011657385769109044, 0.00011657072092385217, 0.00011656758529422289, 0.0001165644583625625, 0.00011656131574137785, 0.0001165581825765681]}
[2018-04-29 20:48:56,842 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:129]: evaluating model ... 
[2018-04-29 20:49:04,107 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:133]: evaluated! 
[2018-04-29 20:49:04,115 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:135]: generating reports ... 
[2018-04-29 20:49:08,216 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:138]: done!
[2018-04-29 20:49:08,217 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_4 finished!
