[2017-12-14 09:31:57,869 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_4
[2017-12-14 09:31:57,869 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:146]: >> Printing header log
[2017-12-14 09:31:57,869 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_4
	layers = 9216,3686
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f84080dae48>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f840813c390>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,869 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:148]: >> Loading dataset... 
[2017-12-14 09:32:20,750 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:20,750 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:20,750 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:57]: =======================================
[2017-12-14 09:32:20,751 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f84080dae48>, 'discard_decoder_function': True}
[2017-12-14 09:32:20,795 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:55,080 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_4
[2017-12-14 10:18:55,081 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:146]: >> Printing header log
[2017-12-14 10:18:55,081 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_4
	layers = 9216,3686
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f848b456eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f848b439400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:55,081 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:148]: >> Loading dataset... 
[2017-12-14 10:19:17,662 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:17,663 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:17,663 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:57]: =======================================
[2017-12-14 10:19:17,663 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f848b456eb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:17,710 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:73]: training and evaluate autoencoder
[2017-12-14 16:25:52,224 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:85]: trained and evaluated!
[2017-12-14 16:25:52,227 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:88]: Training history: 
{'val_loss': [0.00011649913801619993, 0.00011649599692987905, 0.00011649285825695934, 0.0001164897102700988, 0.0001164865617111728, 0.00011648339001934961, 0.00011648021620015799, 0.00011647704359660548, 0.00011647386487910332, 0.0001164706878599205, 0.00011646747823300628, 0.00011646430031997118, 0.00011646111799130578, 0.00011645792879785482, 0.00011645474353735393, 0.0001164515466567733, 0.0001164483132355112, 0.00011644510752366998, 0.00011644190590567223, 0.00011643869472345502, 0.0001164354890116138, 0.00011643226687076755, 0.00011642904399696244, 0.00011642579748970285, 0.00011642255793661406, 0.00011641932067178711, 0.00011641607539804369, 0.00011641283007066912, 0.00011640957820029582, 0.00011640632502489817, 0.00011640306702269817, 0.00011639979986745076, 0.00011639652305859866, 0.00011639325844189176, 0.00011638999895589697, 0.00011638670158844226, 0.00011638339490704673, 0.00011638006811397475, 0.00011637671663270261, 0.00011637339237817112, 0.00011637004842313524, 0.00011636669188265915, 0.00011636332111205467, 0.0001163599540241216, 0.00011635655302343277, 0.00011635315438251398, 0.00011634973579081214, 0.00011634631761028237, 0.00011634290531130063, 0.00011633949676649851, 0.0001163360703267736, 0.00011633261935974194, 0.00011632918081725707, 0.00011632572200220234, 0.00011632223170567006, 0.0001163187556392662, 0.00011631526223212797, 0.00011631175960043413, 0.00011630821903364925, 0.00011630466432612119, 0.00011630113087440052, 0.0001162975908796811, 0.0001162940388537099, 0.00011629045550715458, 0.0001162868352802539, 0.00011628323607675901, 0.00011627961863867747, 0.00011627594267114818, 0.0001162722755348795, 0.0001162685698914543, 0.00011626489564012142, 0.00011626118910284393, 0.00011625748329852531, 0.00011625377463387938, 0.00011625005558266989, 0.00011624632467897907, 0.00011624258478313423, 0.00011623883555547154, 0.00011623507898034303, 0.00011623129968348942, 0.00011622752651846247, 0.0001162237325088002, 0.00011621993637176949, 0.000116216113347662, 0.00011621227282192673, 0.00011620842788056115, 0.00011620459341514439, 0.000116200724697308, 0.00011619683218512373, 0.00011619292512102424, 0.00011618898348271828, 0.00011618503621314293, 0.00011618106498832629, 0.00011617707790657008, 0.00011617306572544166, 0.00011616907512190745, 0.00011616504324027463, 0.00011616099623466112, 0.00011615693181680506, 0.00011615286682688354, 0.00011614882096540095, 0.00011614468616561584, 0.00011614055553118239, 0.00011613643437158316, 0.00011613229360086478, 0.00011612814907596678, 0.00011612399588070159, 0.00011611979012692196, 0.00011611560807810497, 0.0001161113787802561, 0.00011610712615286253, 0.00011610286101153696, 0.00011609857477529744, 0.00011609430375242381, 0.00011608997967047845, 0.00011608565599970514, 0.00011608134082052857, 0.00011607697824930374, 0.00011607261567807891, 0.00011606822956278485, 0.00011606382644642029, 0.00011605941980827773, 0.00011605500884389011, 0.0001160506241945138, 0.00011604618053300951, 0.00011604170800007637, 0.00011603725305815623, 0.00011603278388610769, 0.00011602829329735835, 0.00011602376143051041, 0.0001160192156553209, 0.00011601466310473104, 0.00011601010687146977, 0.00011600552906061429, 0.00011600093971906431, 0.00011599637874838592, 0.00011599176243037393, 0.00011598715240508203, 0.00011598251041563237, 0.00011597786957031363, 0.00011597320632505659, 0.00011596855575462496, 0.00011596388703899193, 0.0001159591979614038, 0.00011595447112749507, 0.00011594970047694725, 0.00011594496579501544, 0.00011594017208307863, 0.00011593535818795717, 0.00011593056939220793, 0.00011592578958861269, 0.00011592096686223065, 0.00011591614994588846, 0.00011591129191234108, 0.00011590646780942652, 0.00011590162359483549, 0.00011589675084847948, 0.00011589186077926615, 0.000115886942249796, 0.00011588198621428391, 0.00011587700173639205, 0.00011587196984857491, 0.00011586696427576907, 0.00011586191930195445, 0.00011585685650472523, 0.00011585179027510322, 0.00011584676198057227, 0.00011584172167266658, 0.00011583661997498583, 0.00011583146514672516, 0.00011582633572174648, 0.00011582115504327768, 0.00011581597919161123, 0.00011581076885512355, 0.00011580550635341642, 0.00011580023380480959, 0.00011579492929204498, 0.00011578958423414064, 0.00011578426663537856, 0.00011577890228814187, 0.0001157735169174994, 0.00011576813147534875, 0.00011576271356848304, 0.00011575728463148011, 0.00011575184210792244, 0.00011574634579233414, 0.0001157408785090681, 0.00011573535670081259, 0.00011572978135080509, 0.00011572422798956385, 0.00011571863801613295, 0.00011571300896348006, 0.00011570736846951791, 0.00011570170353763423, 0.00011569602037116391, 0.0001156902942103986, 0.00011568458953784225, 0.00011567887490777143, 0.00011567314753136701, 0.00011566738533047749], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011724375764454839, 0.00011724057617875616, 0.00011723738618089639, 0.00011723419255690789, 0.00011723099258126909, 0.00011722779717976652, 0.00011722458087469843, 0.00011722135755437477, 0.0001172181271713952, 0.00011721488664473531, 0.00011721164628397673, 0.00011720836793181732, 0.00011720512489293753, 0.00011720186832125059, 0.00011719860513721128, 0.00011719534413358925, 0.00011719206514152477, 0.00011718874559843919, 0.00011718545271806473, 0.00011718216716104827, 0.00011717887387777062, 0.00011717557985978717, 0.00011717226304222317, 0.00011716894584545617, 0.00011716559852575064, 0.00011716225753399523, 0.0001171589248136053, 0.00011715558711617596, 0.00011715223965426931, 0.00011714889361437391, 0.00011714554198123424, 0.0001171421879543756, 0.0001171388255850509, 0.00011713544214625937, 0.00011713208006133693, 0.00011712871202766737, 0.00011712532614775651, 0.0001171219315224764, 0.00011711852900503379, 0.00011711511082176713, 0.00011711171712079434, 0.00011710831493515436, 0.00011710489663338677, 0.0001171014734730807, 0.00011709804275241478, 0.00011709459247909401, 0.00011709112236871617, 0.00011708763815672668, 0.0001170841346764846, 0.00011708063588887969, 0.00011707715307520125, 0.00011707365995194119, 0.0001170701282684758, 0.00011706660943051211, 0.00011706306203382229, 0.00011705947574512451, 0.00011705589128134117, 0.00011705228733600357, 0.00011704867419499315, 0.0001170450392261099, 0.00011704138863880296, 0.00011703776860103794, 0.00011703412742270552, 0.000117030482191641, 0.00011702682342776932, 0.00011702311861443299, 0.00011701944235982283, 0.00011701575634073533, 0.00011701202884631941, 0.00011700830640004347, 0.00011700454411375205, 0.00011700081432041794, 0.00011699705656086238, 0.00011699330463155297, 0.00011698953578030958, 0.00011698575330145827, 0.00011698195186245683, 0.00011697813890516418, 0.00011697431770020623, 0.00011697049431483101, 0.00011696664488294954, 0.00011696279862689321, 0.00011695893509340008, 0.00011695508108738239, 0.00011695120146146184, 0.00011694728884488004, 0.00011694337928562246, 0.00011693947934864108, 0.00011693555118473616, 0.00011693160481908713, 0.0001169276452050332, 0.00011692365755365724, 0.0001169196793112558, 0.00011691565319447526, 0.00011691161586750597, 0.00011690755725776813, 0.00011690351623356955, 0.00011689942852000127, 0.00011689532677592189, 0.00011689120227966233, 0.00011688707346996863, 0.0001168829585485849, 0.00011687874432341483, 0.00011687453343997124, 0.00011687032829197306, 0.0001168660980928765, 0.00011686187159100924, 0.00011685763975659974, 0.00011685336857987871, 0.00011684910207209465, 0.00011684479005995028, 0.00011684045652803549, 0.00011683611060092256, 0.00011683174490785311, 0.00011682741109153608, 0.00011682302155607783, 0.00011681864090818997, 0.00011681426803396366, 0.00011680986470499619, 0.00011680546324834355, 0.00011680101687983528, 0.0001167965616948572, 0.00011679210117733688, 0.0001167876308005385, 0.00011678317886248614, 0.00011677866656005577, 0.00011677411567371987, 0.00011676958431633863, 0.00011676503250569543, 0.00011676044794139285, 0.00011675582604929471, 0.00011675118894167609, 0.00011674654202217977, 0.00011674188775562526, 0.00011673720504884557, 0.00011673250274201067, 0.00011672781069735702, 0.00011672307523395957, 0.00011671832761236584, 0.00011671355823399984, 0.00011670877954146009, 0.0001167039882404205, 0.00011669919829029159, 0.00011669437587090559, 0.00011668953209765049, 0.00011668465540483473, 0.00011667974356464064, 0.00011667485568533631, 0.00011666990815265959, 0.00011666495322552433, 0.00011666003451227583, 0.0001166551014367136, 0.00011665011207320567, 0.00011664514176464861, 0.0001166401308102697, 0.00011663515358125784, 0.00011663015333936376, 0.00011662512515494843, 0.00011662007450275518, 0.0001166149902436958, 0.00011660989238072873, 0.00011660477612641601, 0.00011659961787537074, 0.00011659448130999719, 0.00011658930338779616, 0.00011658410991827201, 0.0001165788785521478, 0.00011657368401611522, 0.00011656848867427624, 0.00011656323799249907, 0.00011655794905861901, 0.00011655267654896899, 0.00011654734905488358, 0.00011654204123195394, 0.00011653670743361861, 0.00011653132085496536, 0.00011652592131230944, 0.00011652046628751419, 0.00011651496047321676, 0.00011650948930859372, 0.00011650398688342313, 0.00011649846357838721, 0.00011649293354249799, 0.00011648736373769387, 0.00011648177404843228, 0.00011647619035531818, 0.00011647055881152518, 0.00011646496573313676, 0.00011645932224444917, 0.00011645362571474158, 0.00011644795276672072, 0.00011644224685173882, 0.00011643650107304126, 0.00011643074498476205, 0.0001164249613331646, 0.00011641916500196675, 0.00011641331354413236, 0.00011640747851052803, 0.0001164016373622753, 0.00011639581119254116], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-14 16:25:52,228 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:92]: done!
[2017-12-14 16:25:52,228 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:152]: >> Executing classifier part ... 
[2017-12-14 16:25:52,229 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:97]: =======================================
[2017-12-14 16:25:52,274 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f848b439400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-14 16:25:52,371 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:110]: training ... 
[2017-12-14 19:16:22,620 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:122]: trained!
[2017-12-14 19:16:22,623 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:125]: Training history: 
{'val_loss': [0.00011649913801619993, 0.00011649599692987905, 0.00011649285825695934, 0.0001164897102700988, 0.0001164865617111728, 0.00011648339001934961, 0.00011648021620015799, 0.00011647704359660548, 0.00011647386487910332, 0.0001164706878599205, 0.00011646747823300628, 0.00011646430031997118, 0.00011646111799130578, 0.00011645792879785482, 0.00011645474353735393, 0.0001164515466567733, 0.0001164483132355112, 0.00011644510752366998, 0.00011644190590567223, 0.00011643869472345502, 0.0001164354890116138, 0.00011643226687076755, 0.00011642904399696244, 0.00011642579748970285, 0.00011642255793661406, 0.00011641932067178711, 0.00011641607539804369, 0.00011641283007066912, 0.00011640957820029582, 0.00011640632502489817, 0.00011640306702269817, 0.00011639979986745076, 0.00011639652305859866, 0.00011639325844189176, 0.00011638999895589697, 0.00011638670158844226, 0.00011638339490704673, 0.00011638006811397475, 0.00011637671663270261, 0.00011637339237817112, 0.00011637004842313524, 0.00011636669188265915, 0.00011636332111205467, 0.0001163599540241216, 0.00011635655302343277, 0.00011635315438251398, 0.00011634973579081214, 0.00011634631761028237, 0.00011634290531130063, 0.00011633949676649851, 0.0001163360703267736, 0.00011633261935974194, 0.00011632918081725707, 0.00011632572200220234, 0.00011632223170567006, 0.0001163187556392662, 0.00011631526223212797, 0.00011631175960043413, 0.00011630821903364925, 0.00011630466432612119, 0.00011630113087440052, 0.0001162975908796811, 0.0001162940388537099, 0.00011629045550715458, 0.0001162868352802539, 0.00011628323607675901, 0.00011627961863867747, 0.00011627594267114818, 0.0001162722755348795, 0.0001162685698914543, 0.00011626489564012142, 0.00011626118910284393, 0.00011625748329852531, 0.00011625377463387938, 0.00011625005558266989, 0.00011624632467897907, 0.00011624258478313423, 0.00011623883555547154, 0.00011623507898034303, 0.00011623129968348942, 0.00011622752651846247, 0.0001162237325088002, 0.00011621993637176949, 0.000116216113347662, 0.00011621227282192673, 0.00011620842788056115, 0.00011620459341514439, 0.000116200724697308, 0.00011619683218512373, 0.00011619292512102424, 0.00011618898348271828, 0.00011618503621314293, 0.00011618106498832629, 0.00011617707790657008, 0.00011617306572544166, 0.00011616907512190745, 0.00011616504324027463, 0.00011616099623466112, 0.00011615693181680506, 0.00011615286682688354, 0.00011614882096540095, 0.00011614468616561584, 0.00011614055553118239, 0.00011613643437158316, 0.00011613229360086478, 0.00011612814907596678, 0.00011612399588070159, 0.00011611979012692196, 0.00011611560807810497, 0.0001161113787802561, 0.00011610712615286253, 0.00011610286101153696, 0.00011609857477529744, 0.00011609430375242381, 0.00011608997967047845, 0.00011608565599970514, 0.00011608134082052857, 0.00011607697824930374, 0.00011607261567807891, 0.00011606822956278485, 0.00011606382644642029, 0.00011605941980827773, 0.00011605500884389011, 0.0001160506241945138, 0.00011604618053300951, 0.00011604170800007637, 0.00011603725305815623, 0.00011603278388610769, 0.00011602829329735835, 0.00011602376143051041, 0.0001160192156553209, 0.00011601466310473104, 0.00011601010687146977, 0.00011600552906061429, 0.00011600093971906431, 0.00011599637874838592, 0.00011599176243037393, 0.00011598715240508203, 0.00011598251041563237, 0.00011597786957031363, 0.00011597320632505659, 0.00011596855575462496, 0.00011596388703899193, 0.0001159591979614038, 0.00011595447112749507, 0.00011594970047694725, 0.00011594496579501544, 0.00011594017208307863, 0.00011593535818795717, 0.00011593056939220793, 0.00011592578958861269, 0.00011592096686223065, 0.00011591614994588846, 0.00011591129191234108, 0.00011590646780942652, 0.00011590162359483549, 0.00011589675084847948, 0.00011589186077926615, 0.000115886942249796, 0.00011588198621428391, 0.00011587700173639205, 0.00011587196984857491, 0.00011586696427576907, 0.00011586191930195445, 0.00011585685650472523, 0.00011585179027510322, 0.00011584676198057227, 0.00011584172167266658, 0.00011583661997498583, 0.00011583146514672516, 0.00011582633572174648, 0.00011582115504327768, 0.00011581597919161123, 0.00011581076885512355, 0.00011580550635341642, 0.00011580023380480959, 0.00011579492929204498, 0.00011578958423414064, 0.00011578426663537856, 0.00011577890228814187, 0.0001157735169174994, 0.00011576813147534875, 0.00011576271356848304, 0.00011575728463148011, 0.00011575184210792244, 0.00011574634579233414, 0.0001157408785090681, 0.00011573535670081259, 0.00011572978135080509, 0.00011572422798956385, 0.00011571863801613295, 0.00011571300896348006, 0.00011570736846951791, 0.00011570170353763423, 0.00011569602037116391, 0.0001156902942103986, 0.00011568458953784225, 0.00011567887490777143, 0.00011567314753136701, 0.00011566738533047749], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011724375764454839, 0.00011724057617875616, 0.00011723738618089639, 0.00011723419255690789, 0.00011723099258126909, 0.00011722779717976652, 0.00011722458087469843, 0.00011722135755437477, 0.0001172181271713952, 0.00011721488664473531, 0.00011721164628397673, 0.00011720836793181732, 0.00011720512489293753, 0.00011720186832125059, 0.00011719860513721128, 0.00011719534413358925, 0.00011719206514152477, 0.00011718874559843919, 0.00011718545271806473, 0.00011718216716104827, 0.00011717887387777062, 0.00011717557985978717, 0.00011717226304222317, 0.00011716894584545617, 0.00011716559852575064, 0.00011716225753399523, 0.0001171589248136053, 0.00011715558711617596, 0.00011715223965426931, 0.00011714889361437391, 0.00011714554198123424, 0.0001171421879543756, 0.0001171388255850509, 0.00011713544214625937, 0.00011713208006133693, 0.00011712871202766737, 0.00011712532614775651, 0.0001171219315224764, 0.00011711852900503379, 0.00011711511082176713, 0.00011711171712079434, 0.00011710831493515436, 0.00011710489663338677, 0.0001171014734730807, 0.00011709804275241478, 0.00011709459247909401, 0.00011709112236871617, 0.00011708763815672668, 0.0001170841346764846, 0.00011708063588887969, 0.00011707715307520125, 0.00011707365995194119, 0.0001170701282684758, 0.00011706660943051211, 0.00011706306203382229, 0.00011705947574512451, 0.00011705589128134117, 0.00011705228733600357, 0.00011704867419499315, 0.0001170450392261099, 0.00011704138863880296, 0.00011703776860103794, 0.00011703412742270552, 0.000117030482191641, 0.00011702682342776932, 0.00011702311861443299, 0.00011701944235982283, 0.00011701575634073533, 0.00011701202884631941, 0.00011700830640004347, 0.00011700454411375205, 0.00011700081432041794, 0.00011699705656086238, 0.00011699330463155297, 0.00011698953578030958, 0.00011698575330145827, 0.00011698195186245683, 0.00011697813890516418, 0.00011697431770020623, 0.00011697049431483101, 0.00011696664488294954, 0.00011696279862689321, 0.00011695893509340008, 0.00011695508108738239, 0.00011695120146146184, 0.00011694728884488004, 0.00011694337928562246, 0.00011693947934864108, 0.00011693555118473616, 0.00011693160481908713, 0.0001169276452050332, 0.00011692365755365724, 0.0001169196793112558, 0.00011691565319447526, 0.00011691161586750597, 0.00011690755725776813, 0.00011690351623356955, 0.00011689942852000127, 0.00011689532677592189, 0.00011689120227966233, 0.00011688707346996863, 0.0001168829585485849, 0.00011687874432341483, 0.00011687453343997124, 0.00011687032829197306, 0.0001168660980928765, 0.00011686187159100924, 0.00011685763975659974, 0.00011685336857987871, 0.00011684910207209465, 0.00011684479005995028, 0.00011684045652803549, 0.00011683611060092256, 0.00011683174490785311, 0.00011682741109153608, 0.00011682302155607783, 0.00011681864090818997, 0.00011681426803396366, 0.00011680986470499619, 0.00011680546324834355, 0.00011680101687983528, 0.0001167965616948572, 0.00011679210117733688, 0.0001167876308005385, 0.00011678317886248614, 0.00011677866656005577, 0.00011677411567371987, 0.00011676958431633863, 0.00011676503250569543, 0.00011676044794139285, 0.00011675582604929471, 0.00011675118894167609, 0.00011674654202217977, 0.00011674188775562526, 0.00011673720504884557, 0.00011673250274201067, 0.00011672781069735702, 0.00011672307523395957, 0.00011671832761236584, 0.00011671355823399984, 0.00011670877954146009, 0.0001167039882404205, 0.00011669919829029159, 0.00011669437587090559, 0.00011668953209765049, 0.00011668465540483473, 0.00011667974356464064, 0.00011667485568533631, 0.00011666990815265959, 0.00011666495322552433, 0.00011666003451227583, 0.0001166551014367136, 0.00011665011207320567, 0.00011664514176464861, 0.0001166401308102697, 0.00011663515358125784, 0.00011663015333936376, 0.00011662512515494843, 0.00011662007450275518, 0.0001166149902436958, 0.00011660989238072873, 0.00011660477612641601, 0.00011659961787537074, 0.00011659448130999719, 0.00011658930338779616, 0.00011658410991827201, 0.0001165788785521478, 0.00011657368401611522, 0.00011656848867427624, 0.00011656323799249907, 0.00011655794905861901, 0.00011655267654896899, 0.00011654734905488358, 0.00011654204123195394, 0.00011653670743361861, 0.00011653132085496536, 0.00011652592131230944, 0.00011652046628751419, 0.00011651496047321676, 0.00011650948930859372, 0.00011650398688342313, 0.00011649846357838721, 0.00011649293354249799, 0.00011648736373769387, 0.00011648177404843228, 0.00011647619035531818, 0.00011647055881152518, 0.00011646496573313676, 0.00011645932224444917, 0.00011645362571474158, 0.00011644795276672072, 0.00011644224685173882, 0.00011643650107304126, 0.00011643074498476205, 0.0001164249613331646, 0.00011641916500196675, 0.00011641331354413236, 0.00011640747851052803, 0.0001164016373622753, 0.00011639581119254116], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-14 19:16:22,623 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:129]: evaluating model ... 
[2017-12-14 19:16:28,215 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:133]: evaluated! 
[2017-12-14 19:16:28,215 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:135]: generating reports ... 
[2017-12-14 19:16:31,260 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:138]: done!
[2017-12-14 19:16:31,260 AE_BIGRAMA_1L_MINIDS_UNDER_F0_4.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_4 finished!
