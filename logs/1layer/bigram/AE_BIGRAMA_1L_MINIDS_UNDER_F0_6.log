[2017-12-14 09:31:57,667 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_6
[2017-12-14 09:31:57,667 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:146]: >> Printing header log
[2017-12-14 09:31:57,668 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_6
	layers = 9216,5530
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fee2e954be0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fee2e954470>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,668 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:148]: >> Loading dataset... 
[2017-12-14 09:32:21,320 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:21,321 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:21,321 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:57]: =======================================
[2017-12-14 09:32:21,321 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fee2e954be0>, 'discard_decoder_function': True}
[2017-12-14 09:32:21,363 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:55,056 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_6
[2017-12-14 10:18:55,056 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:146]: >> Printing header log
[2017-12-14 10:18:55,056 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_6
	layers = 9216,5530
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f8f45177eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f8f4515a400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:55,056 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:148]: >> Loading dataset... 
[2017-12-14 10:19:17,447 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:17,448 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:17,448 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:57]: =======================================
[2017-12-14 10:19:17,448 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f8f45177eb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:17,496 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:73]: training and evaluate autoencoder
[2017-12-14 19:45:22,683 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:85]: trained and evaluated!
[2017-12-14 19:45:22,685 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:88]: Training history: 
{'val_loss': [0.00011872587433695239, 0.00011872292825344639, 0.0001187199734995732, 0.00011871702830991948, 0.00011871409824424645, 0.00011871114635070058, 0.00011870821228056931, 0.00011870526896800539, 0.00011870232762191651, 0.00011869939158531021, 0.0001186964429632637, 0.00011869350832106697, 0.00011869056500850305, 0.00011868763379869909, 0.00011868470618218133, 0.00011868176099252762, 0.00011867882994361706, 0.00011867590184441907, 0.00011867297807146615, 0.00011867005544264414, 0.00011866715111991698, 0.00011866421803302321, 0.00011866127986904846, 0.00011865834465478627, 0.00011865542186507085, 0.00011865249866418339, 0.00011864958003981964, 0.00011864665896630061, 0.00011864373061682397, 0.00011864080872096085, 0.00011863787677819801, 0.00011863494671252499, 0.00011863203479205338, 0.00011862912884251505, 0.00011862621250641315, 0.00011862329273791847, 0.00011862038408894622, 0.00011861745459533866, 0.00011861453352181965, 0.00011861162642815038, 0.00011860870780378662, 0.00011860579212913542, 0.0001186028793863197, 0.00011859996428373395, 0.0001185970437822804, 0.00011859413211208743, 0.00011859121618715758, 0.00011858830728790671, 0.00011858538564232221, 0.00011858248083691481, 0.00011857957177677052, 0.00011857666296690487, 0.00011857374189338585, 0.00011857083537178205, 0.00011856794486801124, 0.00011856503564697353, 0.00011856213640132736, 0.00011855922635794556, 0.00011855633177820831, 0.00011855342525660452, 0.00011855051390819837, 0.00011854761670053541, 0.00011854471033982503, 0.00011854180635676172, 0.000118538904161403, 0.00011853600205542949, 0.00011853310608128269, 0.00011853020781887403, 0.00011852731388271043, 0.00011852441225941716, 0.00011852152658244869, 0.0001185186378842595, 0.00011851573863861331, 0.00011851283710470529, 0.00011850993719760841, 0.000118507051931812, 0.00011850415129175625, 0.00011850125719469924, 0.00011849836129206062, 0.00011849546016932463, 0.00011849258536159997, 0.000118489691675715, 0.00011848679872278893, 0.00011848391231286159, 0.00011848102810181087, 0.00011847814741253816, 0.00011847524963280974, 0.00011847237123179888, 0.00011846948923750184, 0.00011846661973925976, 0.00011846372760867778, 0.000118460839071382, 0.0001184579604200925, 0.0001184550932994975, 0.00011845219732535069, 0.00011844931091542335, 0.00011844643267530591, 0.00011844356212231814, 0.00011844069614585407, 0.00011843779934936315, 0.00011843492691928557, 0.00011843204524677535, 0.00011842918000327016, 0.00011842631558210908, 0.00011842342647274782, 0.00011842055722478439, 0.00011841767850198671, 0.00011841480541045844, 0.00011841193854014209, 0.00011840907240278461, 0.00011840619718388789, 0.00011840333349568568, 0.00011840046793039367, 0.00011839759966566775, 0.00011839474840201231, 0.00011839187327250083, 0.00011838901269490457, 0.00011838616788486266, 0.0001183832964380226, 0.00011838044746262902, 0.00011837759832634204, 0.00011837475360568534, 0.00011837188394654986, 0.0001183690233689536, 0.00011836615837572705, 0.0001183632969757867, 0.00011836044374565624, 0.00011835758872782121, 0.00011835474146862402, 0.00011835189404853343, 0.00011834903861952635, 0.00011834618858938708, 0.00011834333863075598, 0.00011834050004192598, 0.0001183376401972886, 0.00011833480062522109, 0.00011833194977273771, 0.00011832911363306297, 0.00011832626776827536, 0.0001183234151995956, 0.00011832055674936778, 0.0001183176996041643, 0.00011831486158739977, 0.00011831202618068391, 0.00011830918743096049, 0.00011830634246002517, 0.0001183034917684352, 0.00011830065219636769, 0.00011829781131927584, 0.00011829497199748695, 0.00011829214157846686, 0.00011828930413376778, 0.00011828645998517655, 0.00011828360881090636, 0.00011828077193827274, 0.00011827794021422831, 0.00011827508789582717, 0.00011827224905671854, 0.00011826941472262542, 0.00011826657776060658, 0.00011826376017730532, 0.00011826093041973591, 0.00011825808487673512, 0.00011825525143649429, 0.00011825242527221107, 0.00011824959640849396, 0.00011824677555369332, 0.00011824395707653978, 0.00011824112976812563, 0.00011823829518375388, 0.00011823545570107159, 0.00011823263770659828, 0.00011822981236465917, 0.00011822698807746574, 0.00011822417294331975, 0.00011822135888179649, 0.00011821853525605378, 0.00011821571154092582, 0.00011821289395762456, 0.00011821006975981638, 0.00011820725462567038, 0.00011820442928373127, 0.00011820161513282279, 0.00011819879379534193, 0.00011819597522880315, 0.00011819315478517457, 0.00011819032159521238, 0.00011818751144876214, 0.00011818468839508488, 0.0001181818696676527, 0.00011817905176256461, 0.00011817624301052394, 0.00011817343736908922, 0.00011817062992207289, 0.00011816782933984213, 0.00011816501829953961, 0.00011816221993406251, 0.0001181594243395275, 0.00011815661860870755, 0.00011815380740751162], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011951018317548728, 0.00011950714862715841, 0.00011950412163918942, 0.00011950108360693296, 0.00011949805472294894, 0.00011949504129148729, 0.00011949201032188675, 0.00011948899255329076, 0.00011948596532831988, 0.0001194829442416976, 0.00011947992874831962, 0.00011947690479397464, 0.00011947388261714373, 0.0001194708597293072, 0.00011946785366860392, 0.00011946483677691486, 0.00011946181317807269, 0.0001194588005761176, 0.00011945579034418128, 0.00011945278489968288, 0.00011944978120899836, 0.00011944679555415664, 0.00011944377854396666, 0.00011944076411709711, 0.00011943775092263733, 0.0001194347420416117, 0.00011943173925153431, 0.0001194287413199954, 0.00011942573371877991, 0.00011942272929338956, 0.00011941972536570316, 0.00011941671411465877, 0.00011941370530473371, 0.00011941071230283383, 0.00011940772494157861, 0.00011940472712854063, 0.0001194017299791079, 0.00011939873534189507, 0.0001193957317460113, 0.00011939273322196768, 0.00011938973976976424, 0.00011938674138792174, 0.00011938374603970329, 0.00011938075438871409, 0.00011937775503516391, 0.00011937475155778108, 0.00011937176647174388, 0.00011936876778179895, 0.0001193657762493107, 0.00011936277613735451, 0.00011935978962930605, 0.00011935680587047936, 0.0001193538101193577, 0.00011935080998370133, 0.00011934782338085211, 0.00011934484820149335, 0.00011934185738001073, 0.0001193388792855289, 0.00011933588739753783, 0.0001193329123603802, 0.00011932992736914374, 0.00011932693863327763, 0.00011932395556175639, 0.00011932097111562425, 0.0001193179882337045, 0.00011931500094354984, 0.00011931202111895429, 0.00011930904437538315, 0.0001193060666838045, 0.00011930309373226339, 0.0001193001147608746, 0.00011929714671527233, 0.00011929417888297175, 0.00011929119962718071, 0.00011928821695856265, 0.00011928523748946992, 0.00011928226892246352, 0.00011927928589834265, 0.00011927631458211447, 0.00011927333978195872, 0.00011927035735034253, 0.00011926740428325887, 0.0001192644325878277, 0.00011926146022879127, 0.00011925849372370119, 0.00011925553205344942, 0.0001192525698617935, 0.00011924959354482573, 0.00011924663554810303, 0.00011924367764618108, 0.00011924072761272143, 0.00011923775442417844, 0.00011923478718438256, 0.00011923183165247937, 0.00011922888780476871, 0.0001192259127202107, 0.00011922294559891576, 0.00011921999220002946, 0.00011921704465508953, 0.00011921409594884039, 0.00011921111825726174, 0.00011920816639888764, 0.00011920520764375894, 0.00011920226026472031, 0.00011919932177325207, 0.00011919635308774473, 0.00011919339983105957, 0.00011919044707207835, 0.00011918749182457741, 0.00011918454691035831, 0.00011918159962612044, 0.00011917865331359025, 0.0001191757098450826, 0.00011917276339035129, 0.00011916981546620835, 0.00011916687908405681, 0.00011916393307962906, 0.00011916098551098894, 0.0001191580618084378, 0.0001191551130310881, 0.00011915218565500787, 0.00011914925752052164, 0.00011914632834322715, 0.00011914337930517538, 0.00011914043882289137, 0.00011913749220225875, 0.0001191345527390828, 0.00011913162213977705, 0.0001191286767515542, 0.00011912575093968635, 0.0001191228259099247, 0.00011911989433891125, 0.00011911696288639874, 0.00011911403330620106, 0.00011911110991175235, 0.00011910817144398429, 0.000119105253026575, 0.00011910231609931914, 0.00011909940180574249, 0.00011909647561467164, 0.00011909353816601165, 0.00011909060209196255, 0.0001190876647618035, 0.00011908474281306624, 0.00011908182688417666, 0.00011907890770836136, 0.00011907598270229989, 0.00011907305056248194, 0.00011907013534459798, 0.00011906721567107874, 0.00011906429654266382, 0.00011906138305489357, 0.00011905846186456231, 0.00011905553263986744, 0.0001190525993387403, 0.00011904968604057154, 0.00011904677504132098, 0.00011904384010488089, 0.00011904092230367648, 0.00011903800504757638, 0.00011903508864468304, 0.00011903219053833458, 0.00011902927866217708, 0.00011902635164159966, 0.00011902343455140087, 0.00011902052990380062, 0.00011901761933115345, 0.00011901472046639898, 0.0001190118223837507, 0.00011900890631265999, 0.00011900599417580043, 0.00011900307952672097, 0.00011900017755724192, 0.00011899727582476475, 0.00011899437148526694, 0.00011899147013199276, 0.00011898858261962818, 0.00011898567621821405, 0.00011898277005380178, 0.00011897987270585932, 0.0001189769699068737, 0.00011897407547405432, 0.00011897117528208935, 0.00011896828174987711, 0.00011896537741037929, 0.00011896248127114641, 0.00011895958480011089, 0.00011895667285285284, 0.00011895377557601094, 0.00011895087806216717, 0.00011894798298944273, 0.00011894508933872955, 0.00011894219886384152, 0.00011893931258388671, 0.00011893642931385572, 0.00011893354962255308, 0.00011893066609182003, 0.00011892778500220633, 0.00011892491225505866, 0.00011892202640170723], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-14 19:45:22,686 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:92]: done!
[2017-12-14 19:45:22,686 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:152]: >> Executing classifier part ... 
[2017-12-14 19:45:22,686 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:97]: =======================================
[2017-12-14 19:45:22,686 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f8f4515a400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-14 19:45:22,753 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:110]: training ... 
[2017-12-14 23:36:45,571 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:122]: trained!
[2017-12-14 23:36:45,573 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:125]: Training history: 
{'val_loss': [0.00011872587433695239, 0.00011872292825344639, 0.0001187199734995732, 0.00011871702830991948, 0.00011871409824424645, 0.00011871114635070058, 0.00011870821228056931, 0.00011870526896800539, 0.00011870232762191651, 0.00011869939158531021, 0.0001186964429632637, 0.00011869350832106697, 0.00011869056500850305, 0.00011868763379869909, 0.00011868470618218133, 0.00011868176099252762, 0.00011867882994361706, 0.00011867590184441907, 0.00011867297807146615, 0.00011867005544264414, 0.00011866715111991698, 0.00011866421803302321, 0.00011866127986904846, 0.00011865834465478627, 0.00011865542186507085, 0.00011865249866418339, 0.00011864958003981964, 0.00011864665896630061, 0.00011864373061682397, 0.00011864080872096085, 0.00011863787677819801, 0.00011863494671252499, 0.00011863203479205338, 0.00011862912884251505, 0.00011862621250641315, 0.00011862329273791847, 0.00011862038408894622, 0.00011861745459533866, 0.00011861453352181965, 0.00011861162642815038, 0.00011860870780378662, 0.00011860579212913542, 0.0001186028793863197, 0.00011859996428373395, 0.0001185970437822804, 0.00011859413211208743, 0.00011859121618715758, 0.00011858830728790671, 0.00011858538564232221, 0.00011858248083691481, 0.00011857957177677052, 0.00011857666296690487, 0.00011857374189338585, 0.00011857083537178205, 0.00011856794486801124, 0.00011856503564697353, 0.00011856213640132736, 0.00011855922635794556, 0.00011855633177820831, 0.00011855342525660452, 0.00011855051390819837, 0.00011854761670053541, 0.00011854471033982503, 0.00011854180635676172, 0.000118538904161403, 0.00011853600205542949, 0.00011853310608128269, 0.00011853020781887403, 0.00011852731388271043, 0.00011852441225941716, 0.00011852152658244869, 0.0001185186378842595, 0.00011851573863861331, 0.00011851283710470529, 0.00011850993719760841, 0.000118507051931812, 0.00011850415129175625, 0.00011850125719469924, 0.00011849836129206062, 0.00011849546016932463, 0.00011849258536159997, 0.000118489691675715, 0.00011848679872278893, 0.00011848391231286159, 0.00011848102810181087, 0.00011847814741253816, 0.00011847524963280974, 0.00011847237123179888, 0.00011846948923750184, 0.00011846661973925976, 0.00011846372760867778, 0.000118460839071382, 0.0001184579604200925, 0.0001184550932994975, 0.00011845219732535069, 0.00011844931091542335, 0.00011844643267530591, 0.00011844356212231814, 0.00011844069614585407, 0.00011843779934936315, 0.00011843492691928557, 0.00011843204524677535, 0.00011842918000327016, 0.00011842631558210908, 0.00011842342647274782, 0.00011842055722478439, 0.00011841767850198671, 0.00011841480541045844, 0.00011841193854014209, 0.00011840907240278461, 0.00011840619718388789, 0.00011840333349568568, 0.00011840046793039367, 0.00011839759966566775, 0.00011839474840201231, 0.00011839187327250083, 0.00011838901269490457, 0.00011838616788486266, 0.0001183832964380226, 0.00011838044746262902, 0.00011837759832634204, 0.00011837475360568534, 0.00011837188394654986, 0.0001183690233689536, 0.00011836615837572705, 0.0001183632969757867, 0.00011836044374565624, 0.00011835758872782121, 0.00011835474146862402, 0.00011835189404853343, 0.00011834903861952635, 0.00011834618858938708, 0.00011834333863075598, 0.00011834050004192598, 0.0001183376401972886, 0.00011833480062522109, 0.00011833194977273771, 0.00011832911363306297, 0.00011832626776827536, 0.0001183234151995956, 0.00011832055674936778, 0.0001183176996041643, 0.00011831486158739977, 0.00011831202618068391, 0.00011830918743096049, 0.00011830634246002517, 0.0001183034917684352, 0.00011830065219636769, 0.00011829781131927584, 0.00011829497199748695, 0.00011829214157846686, 0.00011828930413376778, 0.00011828645998517655, 0.00011828360881090636, 0.00011828077193827274, 0.00011827794021422831, 0.00011827508789582717, 0.00011827224905671854, 0.00011826941472262542, 0.00011826657776060658, 0.00011826376017730532, 0.00011826093041973591, 0.00011825808487673512, 0.00011825525143649429, 0.00011825242527221107, 0.00011824959640849396, 0.00011824677555369332, 0.00011824395707653978, 0.00011824112976812563, 0.00011823829518375388, 0.00011823545570107159, 0.00011823263770659828, 0.00011822981236465917, 0.00011822698807746574, 0.00011822417294331975, 0.00011822135888179649, 0.00011821853525605378, 0.00011821571154092582, 0.00011821289395762456, 0.00011821006975981638, 0.00011820725462567038, 0.00011820442928373127, 0.00011820161513282279, 0.00011819879379534193, 0.00011819597522880315, 0.00011819315478517457, 0.00011819032159521238, 0.00011818751144876214, 0.00011818468839508488, 0.0001181818696676527, 0.00011817905176256461, 0.00011817624301052394, 0.00011817343736908922, 0.00011817062992207289, 0.00011816782933984213, 0.00011816501829953961, 0.00011816221993406251, 0.0001181594243395275, 0.00011815661860870755, 0.00011815380740751162], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011951018317548728, 0.00011950714862715841, 0.00011950412163918942, 0.00011950108360693296, 0.00011949805472294894, 0.00011949504129148729, 0.00011949201032188675, 0.00011948899255329076, 0.00011948596532831988, 0.0001194829442416976, 0.00011947992874831962, 0.00011947690479397464, 0.00011947388261714373, 0.0001194708597293072, 0.00011946785366860392, 0.00011946483677691486, 0.00011946181317807269, 0.0001194588005761176, 0.00011945579034418128, 0.00011945278489968288, 0.00011944978120899836, 0.00011944679555415664, 0.00011944377854396666, 0.00011944076411709711, 0.00011943775092263733, 0.0001194347420416117, 0.00011943173925153431, 0.0001194287413199954, 0.00011942573371877991, 0.00011942272929338956, 0.00011941972536570316, 0.00011941671411465877, 0.00011941370530473371, 0.00011941071230283383, 0.00011940772494157861, 0.00011940472712854063, 0.0001194017299791079, 0.00011939873534189507, 0.0001193957317460113, 0.00011939273322196768, 0.00011938973976976424, 0.00011938674138792174, 0.00011938374603970329, 0.00011938075438871409, 0.00011937775503516391, 0.00011937475155778108, 0.00011937176647174388, 0.00011936876778179895, 0.0001193657762493107, 0.00011936277613735451, 0.00011935978962930605, 0.00011935680587047936, 0.0001193538101193577, 0.00011935080998370133, 0.00011934782338085211, 0.00011934484820149335, 0.00011934185738001073, 0.0001193388792855289, 0.00011933588739753783, 0.0001193329123603802, 0.00011932992736914374, 0.00011932693863327763, 0.00011932395556175639, 0.00011932097111562425, 0.0001193179882337045, 0.00011931500094354984, 0.00011931202111895429, 0.00011930904437538315, 0.0001193060666838045, 0.00011930309373226339, 0.0001193001147608746, 0.00011929714671527233, 0.00011929417888297175, 0.00011929119962718071, 0.00011928821695856265, 0.00011928523748946992, 0.00011928226892246352, 0.00011927928589834265, 0.00011927631458211447, 0.00011927333978195872, 0.00011927035735034253, 0.00011926740428325887, 0.0001192644325878277, 0.00011926146022879127, 0.00011925849372370119, 0.00011925553205344942, 0.0001192525698617935, 0.00011924959354482573, 0.00011924663554810303, 0.00011924367764618108, 0.00011924072761272143, 0.00011923775442417844, 0.00011923478718438256, 0.00011923183165247937, 0.00011922888780476871, 0.0001192259127202107, 0.00011922294559891576, 0.00011921999220002946, 0.00011921704465508953, 0.00011921409594884039, 0.00011921111825726174, 0.00011920816639888764, 0.00011920520764375894, 0.00011920226026472031, 0.00011919932177325207, 0.00011919635308774473, 0.00011919339983105957, 0.00011919044707207835, 0.00011918749182457741, 0.00011918454691035831, 0.00011918159962612044, 0.00011917865331359025, 0.0001191757098450826, 0.00011917276339035129, 0.00011916981546620835, 0.00011916687908405681, 0.00011916393307962906, 0.00011916098551098894, 0.0001191580618084378, 0.0001191551130310881, 0.00011915218565500787, 0.00011914925752052164, 0.00011914632834322715, 0.00011914337930517538, 0.00011914043882289137, 0.00011913749220225875, 0.0001191345527390828, 0.00011913162213977705, 0.0001191286767515542, 0.00011912575093968635, 0.0001191228259099247, 0.00011911989433891125, 0.00011911696288639874, 0.00011911403330620106, 0.00011911110991175235, 0.00011910817144398429, 0.000119105253026575, 0.00011910231609931914, 0.00011909940180574249, 0.00011909647561467164, 0.00011909353816601165, 0.00011909060209196255, 0.0001190876647618035, 0.00011908474281306624, 0.00011908182688417666, 0.00011907890770836136, 0.00011907598270229989, 0.00011907305056248194, 0.00011907013534459798, 0.00011906721567107874, 0.00011906429654266382, 0.00011906138305489357, 0.00011905846186456231, 0.00011905553263986744, 0.0001190525993387403, 0.00011904968604057154, 0.00011904677504132098, 0.00011904384010488089, 0.00011904092230367648, 0.00011903800504757638, 0.00011903508864468304, 0.00011903219053833458, 0.00011902927866217708, 0.00011902635164159966, 0.00011902343455140087, 0.00011902052990380062, 0.00011901761933115345, 0.00011901472046639898, 0.0001190118223837507, 0.00011900890631265999, 0.00011900599417580043, 0.00011900307952672097, 0.00011900017755724192, 0.00011899727582476475, 0.00011899437148526694, 0.00011899147013199276, 0.00011898858261962818, 0.00011898567621821405, 0.00011898277005380178, 0.00011897987270585932, 0.0001189769699068737, 0.00011897407547405432, 0.00011897117528208935, 0.00011896828174987711, 0.00011896537741037929, 0.00011896248127114641, 0.00011895958480011089, 0.00011895667285285284, 0.00011895377557601094, 0.00011895087806216717, 0.00011894798298944273, 0.00011894508933872955, 0.00011894219886384152, 0.00011893931258388671, 0.00011893642931385572, 0.00011893354962255308, 0.00011893066609182003, 0.00011892778500220633, 0.00011892491225505866, 0.00011892202640170723], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-14 23:36:45,574 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:129]: evaluating model ... 
[2017-12-14 23:36:52,541 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:133]: evaluated! 
[2017-12-14 23:36:52,542 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:135]: generating reports ... 
[2017-12-14 23:36:55,324 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:138]: done!
[2017-12-14 23:36:55,324 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_6 finished!
[2018-04-29 11:38:17,230 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_6 was already executed!
[2018-04-29 11:42:07,353 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_6 was already executed!
[2018-04-29 13:12:02,701 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_6
[2018-04-29 13:12:02,701 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:146]: >> Printing header log
[2018-04-29 13:12:02,701 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_6
	layers = 9216,5530
	using GLOBAL obj = 
		{'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'numpy_seed': 666, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'epochs': 200, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'batch': 32, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'store_history': True, 'shuffle_batches': True, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'autoencoder_configs': {'optimizer': <keras.optimizers.SGD object at 0x7f070ed20860>, 'loss_function': 'mse', 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'discard_decoder_function': True}, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'mlp_configs': {'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7f070ed208d0>, 'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'activation': 'sigmoid'}, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:12:02,701 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,686 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,686 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,687 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:57]: =======================================
[2018-04-29 13:12:20,687 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:62]: setting configurations for autoencoder: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7f070ed20860>, 'loss_function': 'mse', 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'discard_decoder_function': True}
[2018-04-29 13:12:20,733 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,878 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_6
[2018-04-29 13:14:12,878 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:146]: >> Printing header log
[2018-04-29 13:14:12,878 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_6
	layers = 9216,5530
	using GLOBAL obj = 
		{'mlp_configs': {'optimizer': <keras.optimizers.SGD object at 0x7fb78e93a898>, 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False, 'activation': 'sigmoid'}, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'epochs': 200, 'shuffle_batches': True, 'numpy_seed': 666, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'batch': 32, 'autoencoder_configs': {'optimizer': <keras.optimizers.SGD object at 0x7fb78e93a828>, 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'loss_function': 'mse'}, 'store_history': True}
	=======================================
	
[2018-04-29 13:14:12,878 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:148]: >> Loading dataset... 
[2018-04-29 13:14:31,017 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:31,017 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:31,017 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:57]: =======================================
[2018-04-29 13:14:31,017 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:62]: setting configurations for autoencoder: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7fb78e93a828>, 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'loss_function': 'mse'}
[2018-04-29 13:14:31,064 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,820 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_6
[2018-04-29 13:16:33,820 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:146]: >> Printing header log
[2018-04-29 13:16:33,821 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_6
	layers = 9216,5530
	using GLOBAL obj = 
		{'shuffle_batches': True, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'store_history': True, 'autoencoder_configs': {'optimizer': <keras.optimizers.SGD object at 0x7f31f3c64828>, 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'loss_function': 'mse'}, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'mlp_configs': {'optimizer': <keras.optimizers.SGD object at 0x7f31f3c64898>, 'activation': 'sigmoid', 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy', 'classifier_dim': 9}, 'data_dir': '/home/dhiego/malware_dataset/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'batch': 32, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'numpy_seed': 666, 'epochs': 200}
	=======================================
	
[2018-04-29 13:16:33,821 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:148]: >> Loading dataset... 
[2018-04-29 13:16:57,971 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:57,972 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:57,972 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:57]: =======================================
[2018-04-29 13:16:57,972 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:62]: setting configurations for autoencoder: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7f31f3c64828>, 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'loss_function': 'mse'}
[2018-04-29 13:16:58,021 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:20,929 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_6
[2018-04-29 14:30:20,930 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:146]: >> Printing header log
[2018-04-29 14:30:20,930 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_6
	layers = 9216,5530
	using GLOBAL obj = 
		{'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'epochs': 200, 'mlp_configs': {'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7fd0dd70c908>, 'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy'}, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'shuffle_batches': True, 'autoencoder_configs': {'optimizer': <keras.optimizers.SGD object at 0x7fd0dd70c898>, 'output_layer_activation': 'relu', 'loss_function': 'mse', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu'}, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'store_history': True, 'batch': 32, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'numpy_seed': 666, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/'}
	=======================================
	
[2018-04-29 14:30:20,930 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:148]: >> Loading dataset... 
[2018-04-29 14:30:38,000 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:38,000 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:38,001 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:57]: =======================================
[2018-04-29 14:30:38,001 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:62]: setting configurations for autoencoder: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7fd0dd70c898>, 'output_layer_activation': 'relu', 'loss_function': 'mse', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu'}
[2018-04-29 14:30:38,048 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:73]: training and evaluate autoencoder
[2018-04-29 18:41:12,296 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:85]: trained and evaluated!
[2018-04-29 18:41:12,297 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:88]: Training history: 
{'loss': [0.00011946999903329183, 0.00011946397044025483, 0.00011945790696054157, 0.00011945186552200284, 0.00011944575919235028, 0.00011943965440320992, 0.00011943349941707208, 0.00011942734668245205, 0.00011942112334497297, 0.00011941492193016748, 0.00011940865697515885, 0.00011940237644912694, 0.0001193960190633864, 0.00011938969895804107, 0.00011938342184483617, 0.00011937708782748067, 0.00011937071453891422, 0.00011936438301007842, 0.00011935799654420763, 0.00011935159085748462, 0.00011934517277556347, 0.00011933872400189929, 0.0001193322264295487, 0.00011932572046733167, 0.00011931920642335065, 0.00011931266467385025, 0.00011930615098537203, 0.00011929957570010609, 0.00011929301226493395, 0.00011928639868016472, 0.00011927973679441303, 0.00011927302957020233, 0.00011926627769483805, 0.00011925950939524372, 0.00011925270639857465, 0.00011924587306566536, 0.00011923897884697396, 0.00011923212918463536, 0.00011922521105245459, 0.00011921828377200139, 0.00011921134535245998, 0.00011920434412742124, 0.0001191973859656235, 0.00011919040905697733, 0.00011918342079594125, 0.00011917635837701796, 0.00011916927495972839, 0.0001191622413839335, 0.00011915513310514706, 0.00011914800579510993, 0.00011914087078251181, 0.00011913371382353991, 0.00011912656731635077, 0.00011911939324584338, 0.00011911220983746204, 0.00011910499922126525, 0.00011909779517002046, 0.00011909060318217119, 0.00011908333852954652, 0.00011907612803185067, 0.0001190688097931017, 0.0001190614807944675, 0.00011905415840818567, 0.00011904681566344264, 0.00011903948924812892, 0.00011903212152338808, 0.00011902474059764272, 0.00011901731603985186, 0.00011900996168201686, 0.0001190025330951941, 0.0001189950989862276, 0.00011898772133110813, 0.00011898029205697992, 0.00011897285820871549, 0.00011896537186453538, 0.00011895791495598834, 0.00011895043896879024, 0.000118942952482409, 0.00011893544755728175, 0.00011892793229887269, 0.00011892042708934319, 0.00011891295543927943, 0.00011890544217168611, 0.00011889792743468117, 0.00011889039060910132, 0.00011888286480410874, 0.0001188753001071082, 0.00011886774263866489, 0.00011886015644559412, 0.00011885256970741904, 0.00011884500119468829, 0.00011883746472461125, 0.00011882989611707973, 0.00011882232257990919, 0.00011881478115649293, 0.00011880720373249161, 0.00011879956674991867, 0.0001187919471395833, 0.00011878435013922696, 0.00011877675617249465, 0.00011876915824783098], 'val_loss': [0.00011867870752160787, 0.00011867276517372849, 0.00011866682398785706, 0.00011866078699889755, 0.0001186547631674437, 0.0001186486974143176, 0.0001186426193081529, 0.00011863647343010781, 0.00011863037603461077, 0.0001186241988538586, 0.00011861799421396417, 0.00011861170144023429, 0.00011860543458822071, 0.00011859917821215594, 0.00011859287172673197, 0.00011858653066710155, 0.00011858022001632592, 0.00011857386898130398, 0.00011856749490277008, 0.00011856112956611156, 0.00011855473782505648, 0.00011854831018689356, 0.00011854185925494006, 0.0001185354114335925, 0.00011852892510508844, 0.00011852249968367919, 0.00011851602905122128, 0.00011850958040752963, 0.00011850306147129415, 0.00011849649258659288, 0.00011848988986064403, 0.00011848326881072331, 0.00011847663231503507, 0.00011846996287195154, 0.00011846327086803629, 0.00011845652736035228, 0.00011844983830614959, 0.00011844307804767372, 0.00011843629883952938, 0.0001184295023800359, 0.00011842265311174932, 0.00011841584063442287, 0.00011840901049457523, 0.00011840217218491769, 0.00011839525237380864, 0.00011838830469238529, 0.00011838139421309411, 0.00011837440557535898, 0.00011836740444156889, 0.00011836041188876074, 0.00011835333931829588, 0.0001183462572014886, 0.0001183391570109881, 0.00011833204789984174, 0.0001183249114189384, 0.0001183177702721261, 0.00011831064433867973, 0.00011830345708696647, 0.00011829634218365733, 0.00011828913253200577, 0.00011828189239999124, 0.00011827466387017938, 0.00011826742552586943, 0.00011826021335355444, 0.00011825296315676316, 0.00011824570723931724, 0.00011823842132418858, 0.00011823120473624329, 0.0001182239187317294, 0.0001182166359272067, 0.00011820939693932309, 0.00011820210196053225, 0.00011819481229122399, 0.00011818747953823551, 0.00011818018275386304, 0.00011817285343326734, 0.00011816551193840348, 0.00011815815312068231, 0.00011815078464935646, 0.00011814342386516027, 0.00011813611147412687, 0.00011812874954579974, 0.0001181213881895381, 0.00011811402095172838, 0.00011810666662114569, 0.000118099279843725, 0.00011809191063944026, 0.00011808450775480136, 0.00011807709873833577, 0.00011806969168834522, 0.00011806231996339703, 0.00011805491005307915, 0.00011804749785449941, 0.00011804011434857809, 0.00011803270084497403, 0.0001180252341214048, 0.00011801779314078144, 0.00011801036460258191, 0.0001180029489716094, 0.00011799552565350721, 0.00011798803865736813], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-04-29 18:41:12,298 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:92]: done!
[2018-04-29 18:41:12,298 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:152]: >> Executing classifier part ... 
[2018-04-29 18:41:12,298 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:97]: =======================================
[2018-04-29 18:41:12,298 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7fd0dd70c908>, 'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy'}
[2018-04-29 18:41:12,672 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:110]: training ... 
[2018-04-29 22:28:45,494 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:122]: trained!
[2018-04-29 22:28:45,495 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:125]: Training history: 
{'loss': [0.00011946999903329183, 0.00011946397044025483, 0.00011945790696054157, 0.00011945186552200284, 0.00011944575919235028, 0.00011943965440320992, 0.00011943349941707208, 0.00011942734668245205, 0.00011942112334497297, 0.00011941492193016748, 0.00011940865697515885, 0.00011940237644912694, 0.0001193960190633864, 0.00011938969895804107, 0.00011938342184483617, 0.00011937708782748067, 0.00011937071453891422, 0.00011936438301007842, 0.00011935799654420763, 0.00011935159085748462, 0.00011934517277556347, 0.00011933872400189929, 0.0001193322264295487, 0.00011932572046733167, 0.00011931920642335065, 0.00011931266467385025, 0.00011930615098537203, 0.00011929957570010609, 0.00011929301226493395, 0.00011928639868016472, 0.00011927973679441303, 0.00011927302957020233, 0.00011926627769483805, 0.00011925950939524372, 0.00011925270639857465, 0.00011924587306566536, 0.00011923897884697396, 0.00011923212918463536, 0.00011922521105245459, 0.00011921828377200139, 0.00011921134535245998, 0.00011920434412742124, 0.0001191973859656235, 0.00011919040905697733, 0.00011918342079594125, 0.00011917635837701796, 0.00011916927495972839, 0.0001191622413839335, 0.00011915513310514706, 0.00011914800579510993, 0.00011914087078251181, 0.00011913371382353991, 0.00011912656731635077, 0.00011911939324584338, 0.00011911220983746204, 0.00011910499922126525, 0.00011909779517002046, 0.00011909060318217119, 0.00011908333852954652, 0.00011907612803185067, 0.0001190688097931017, 0.0001190614807944675, 0.00011905415840818567, 0.00011904681566344264, 0.00011903948924812892, 0.00011903212152338808, 0.00011902474059764272, 0.00011901731603985186, 0.00011900996168201686, 0.0001190025330951941, 0.0001189950989862276, 0.00011898772133110813, 0.00011898029205697992, 0.00011897285820871549, 0.00011896537186453538, 0.00011895791495598834, 0.00011895043896879024, 0.000118942952482409, 0.00011893544755728175, 0.00011892793229887269, 0.00011892042708934319, 0.00011891295543927943, 0.00011890544217168611, 0.00011889792743468117, 0.00011889039060910132, 0.00011888286480410874, 0.0001188753001071082, 0.00011886774263866489, 0.00011886015644559412, 0.00011885256970741904, 0.00011884500119468829, 0.00011883746472461125, 0.00011882989611707973, 0.00011882232257990919, 0.00011881478115649293, 0.00011880720373249161, 0.00011879956674991867, 0.0001187919471395833, 0.00011878435013922696, 0.00011877675617249465, 0.00011876915824783098], 'val_loss': [0.00011867870752160787, 0.00011867276517372849, 0.00011866682398785706, 0.00011866078699889755, 0.0001186547631674437, 0.0001186486974143176, 0.0001186426193081529, 0.00011863647343010781, 0.00011863037603461077, 0.0001186241988538586, 0.00011861799421396417, 0.00011861170144023429, 0.00011860543458822071, 0.00011859917821215594, 0.00011859287172673197, 0.00011858653066710155, 0.00011858022001632592, 0.00011857386898130398, 0.00011856749490277008, 0.00011856112956611156, 0.00011855473782505648, 0.00011854831018689356, 0.00011854185925494006, 0.0001185354114335925, 0.00011852892510508844, 0.00011852249968367919, 0.00011851602905122128, 0.00011850958040752963, 0.00011850306147129415, 0.00011849649258659288, 0.00011848988986064403, 0.00011848326881072331, 0.00011847663231503507, 0.00011846996287195154, 0.00011846327086803629, 0.00011845652736035228, 0.00011844983830614959, 0.00011844307804767372, 0.00011843629883952938, 0.0001184295023800359, 0.00011842265311174932, 0.00011841584063442287, 0.00011840901049457523, 0.00011840217218491769, 0.00011839525237380864, 0.00011838830469238529, 0.00011838139421309411, 0.00011837440557535898, 0.00011836740444156889, 0.00011836041188876074, 0.00011835333931829588, 0.0001183462572014886, 0.0001183391570109881, 0.00011833204789984174, 0.0001183249114189384, 0.0001183177702721261, 0.00011831064433867973, 0.00011830345708696647, 0.00011829634218365733, 0.00011828913253200577, 0.00011828189239999124, 0.00011827466387017938, 0.00011826742552586943, 0.00011826021335355444, 0.00011825296315676316, 0.00011824570723931724, 0.00011823842132418858, 0.00011823120473624329, 0.0001182239187317294, 0.0001182166359272067, 0.00011820939693932309, 0.00011820210196053225, 0.00011819481229122399, 0.00011818747953823551, 0.00011818018275386304, 0.00011817285343326734, 0.00011816551193840348, 0.00011815815312068231, 0.00011815078464935646, 0.00011814342386516027, 0.00011813611147412687, 0.00011812874954579974, 0.0001181213881895381, 0.00011811402095172838, 0.00011810666662114569, 0.000118099279843725, 0.00011809191063944026, 0.00011808450775480136, 0.00011807709873833577, 0.00011806969168834522, 0.00011806231996339703, 0.00011805491005307915, 0.00011804749785449941, 0.00011804011434857809, 0.00011803270084497403, 0.0001180252341214048, 0.00011801779314078144, 0.00011801036460258191, 0.0001180029489716094, 0.00011799552565350721, 0.00011798803865736813], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-04-29 22:28:45,495 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:129]: evaluating model ... 
[2018-04-29 22:28:51,450 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:133]: evaluated! 
[2018-04-29 22:28:51,451 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:135]: generating reports ... 
[2018-04-29 22:28:55,316 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:138]: done!
[2018-04-29 22:28:55,317 AE_BIGRAMA_1L_MINIDS_UNDER_F0_6.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_6 finished!
