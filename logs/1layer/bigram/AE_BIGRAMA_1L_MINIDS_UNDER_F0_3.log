[2017-12-14 09:31:57,846 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_3
[2017-12-14 09:31:57,846 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:146]: >> Printing header log
[2017-12-14 09:31:57,846 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_3
	layers = 9216,2765
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f0715bf4eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f0715bd7400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,846 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:148]: >> Loading dataset... 
[2017-12-14 09:32:19,791 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:19,791 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:19,792 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:57]: =======================================
[2017-12-14 09:32:19,792 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f0715bf4eb8>, 'discard_decoder_function': True}
[2017-12-14 09:32:19,836 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:55,066 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_3
[2017-12-14 10:18:55,067 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:146]: >> Printing header log
[2017-12-14 10:18:55,067 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_3
	layers = 9216,2765
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f18f820aeb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f18f81ed400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:55,067 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:148]: >> Loading dataset... 
[2017-12-14 10:19:17,911 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:17,912 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:17,912 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:57]: =======================================
[2017-12-14 10:19:17,912 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f18f820aeb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:17,963 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:73]: training and evaluate autoencoder
[2017-12-14 15:13:18,988 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:85]: trained and evaluated!
[2017-12-14 15:13:19,084 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:88]: Training history: 
{'val_loss': [0.00011415445513314515, 0.0001141504307062404, 0.00011414641764913674, 0.00011414238546359414, 0.00011413833021666255, 0.00011413427203789545, 0.00011413021646917704, 0.0001141261130078531, 0.00011412197701030591, 0.00011411785874678811, 0.00011411371865539745, 0.00011410954904900429, 0.00011410538321466778, 0.00011410116988102075, 0.00011409696235741358, 0.00011409268804516355, 0.00011408841307146283, 0.00011408413148325518, 0.00011407984049172147, 0.00011407554361863972, 0.00011407123725284669, 0.00011406692091166213, 0.00011406252937962322, 0.00011405812892693847, 0.00011405372185974681, 0.00011404929067642045, 0.00011404479940834339, 0.00011404031083970022, 0.00011403583437381702, 0.00011403136582746507, 0.0001140268215181933, 0.00011402224998218087, 0.0001140176840953149, 0.00011401309702414974, 0.00011400850816527999, 0.00011400387818920506, 0.00011399926095946373, 0.00011399462494094735, 0.00011398998000178514, 0.0001139852901018529, 0.0001139806301996034, 0.00011397595132980839, 0.00011397123388134868, 0.00011396652583621502, 0.00011396176580463237, 0.00011395700650600859, 0.00011395222015941461, 0.00011394740855255501, 0.00011394253759390356, 0.00011393763190015226, 0.00011393272432931115, 0.00011392782836067273, 0.00011392292479428986, 0.00011391795504708369, 0.00011391294797260603, 0.00011390792053617327, 0.00011390288289194739, 0.00011389782694162668, 0.00011389268752337944, 0.00011388755743695006, 0.00011388240414611533, 0.00011387718390574432, 0.00011387194609223734, 0.00011386670491784577, 0.00011386141527878322, 0.0001138560653046913, 0.00011385069931276642, 0.00011384534107947938, 0.00011383993616922591, 0.00011383450517636273, 0.0001138290601321416, 0.00011382356825006064, 0.00011381803868316727, 0.00011381250250176698, 0.00011380694474277249, 0.00011380137783073059, 0.00011379576488529591, 0.00011379015914431066, 0.00011378455789046389, 0.00011377894315732464, 0.00011377329302763484, 0.00011376758756848849, 0.00011376188316408782, 0.00011375615718209297, 0.00011375044927379135, 0.00011374471757114186, 0.00011373896674005342, 0.00011373319007663391, 0.00011372738014403228, 0.00011372156563490693, 0.00011371575480845301, 0.00011370994716411323, 0.0001137041211242934, 0.00011369826991359317, 0.00011369237186503311, 0.0001136864775885297, 0.00011368058518911609, 0.00011367468379754847, 0.00011366878691099639, 0.00011366284375864994, 0.00011365692838723257, 0.00011365095727518658, 0.00011364495755986743, 0.00011363895752276145, 0.00011363294472144482, 0.00011362688754930067, 0.00011362080479510408, 0.0001136147318554056, 0.00011360863635487538, 0.00011360256790231538, 0.00011359645629456696, 0.00011359037312919832, 0.00011358428865880534, 0.0001135781956074304, 0.00011357207107457792, 0.00011356600221084586, 0.00011355990858740547, 0.00011355381029805613, 0.0001135477165852305, 0.00011354162802099403, 0.00011353551968474499, 0.0001135294425618178, 0.00011352333569148652, 0.00011351721982900123, 0.00011351115922446429, 0.00011350505676976329, 0.00011349896223459356, 0.00011349285749163071, 0.00011348677171621338, 0.00011348069623797441, 0.00011347458911736447, 0.0001134685199318456, 0.00011346241191738337, 0.00011345631992075413, 0.00011345021231746398, 0.00011344407437682719, 0.00011343794723392604, 0.0001134318132977475, 0.00011342567576828278, 0.00011341957020297583, 0.00011341352726096008, 0.00011340742087330903, 0.00011340129650134996, 0.00011339518750365023, 0.0001133890830824742, 0.00011338297310153695, 0.00011337689574620817, 0.00011337083187017188, 0.00011336475083217167, 0.00011335865188137163, 0.00011335254787136767, 0.0001133464674769411, 0.00011334037033172269, 0.00011333429412052483, 0.00011332817048152465, 0.00011332206892067594, 0.00011331598101789017, 0.00011330985991743047, 0.00011330371094665648, 0.00011329763995555598, 0.00011329155548516299, 0.00011328546602707425, 0.00011327940999906103, 0.00011327335184367937, 0.00011326724799456881, 0.00011326117782581241, 0.00011325510479672869, 0.00011324903258998908, 0.00011324296642569092, 0.00011323687280225053, 0.00011323080508264939, 0.00011322474587252203, 0.00011321865682560534, 0.00011321257848703905, 0.00011320650472499646, 0.00011320042441995513, 0.0001131943593997879, 0.00011318826282663496, 0.00011318218865342031, 0.00011317613491366894, 0.00011317007799180343, 0.00011316398256278141, 0.00011315793879842157, 0.00011315185236155355, 0.00011314581955582274, 0.00011313978265624846, 0.0001131337414304291, 0.00011312766913430427, 0.00011312159676667124, 0.00011311555332409821, 0.00011310951912395784, 0.00011310349302211919, 0.00011309743184551679, 0.00011309138988673857, 0.00011308535511453272, 0.00011307931004514854, 0.00011307326685285415, 0.00011306724589960467, 0.00011306118758332959, 0.00011305515420553332], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011487284310542028, 0.00011486871033779524, 0.00011486455368038103, 0.00011486041178818373, 0.00011485625638687947, 0.00011485207358815825, 0.00011484789512657138, 0.00011484371649908321, 0.00011483951016607563, 0.00011483526408785336, 0.00011483103033372865, 0.00011482675417996822, 0.0001148224514582974, 0.00011481817340852195, 0.00011481384307613251, 0.00011480949307258729, 0.00011480508339196953, 0.00011480065610211234, 0.00011479622191550052, 0.00011479178275184931, 0.00011478732159442393, 0.0001147828440127685, 0.0001147783369480796, 0.00011477378141650693, 0.0001147692183008742, 0.00011476464023042307, 0.00011476004026099852, 0.00011475538170471004, 0.00011475073386090641, 0.00011474608926402848, 0.00011474144807997757, 0.00011473676909412734, 0.00011473204014828149, 0.00011472730935382102, 0.00011472254955033084, 0.00011471779958241852, 0.0001147130223829906, 0.00011470824947329662, 0.00011470345497273169, 0.00011469864613355323, 0.00011469377866011046, 0.00011468892573858293, 0.00011468404565664032, 0.00011467914223001287, 0.0001146742532367997, 0.00011466930748163707, 0.00011466436957123655, 0.00011465941177637859, 0.00011465441890524288, 0.00011464937650071493, 0.00011464430226683496, 0.00011463922139690242, 0.00011463414538550837, 0.00011462905515400172, 0.00011462389647635306, 0.00011461872360229201, 0.00011461352890035812, 0.00011460833571523622, 0.00011460312563188052, 0.00011459785899987705, 0.00011459260068663944, 0.0001145873299782037, 0.00011458201066068304, 0.00011457665749929438, 0.00011457128040071619, 0.00011456586346212253, 0.00011456038170351558, 0.00011455487058037613, 0.00011454934293820586, 0.00011454375497905798, 0.00011453813232283535, 0.00011453252656484654, 0.00011452685620014614, 0.00011452116682789522, 0.00011451548532410661, 0.00011450977737090857, 0.00011450404604932549, 0.00011449826929448263, 0.00011449250398683043, 0.00011448673896358049, 0.00011448094410179426, 0.00011447513288687853, 0.00011446928043363628, 0.0001144634284069974, 0.00011445754997834946, 0.00011445168697852369, 0.00011444579705528472, 0.00011443987042045505, 0.00011443391579570376, 0.00011442792611837491, 0.00011442192869108468, 0.00011441593529282635, 0.00011440993151388582, 0.00011440390282604807, 0.00011439785565206393, 0.00011439179724419083, 0.00011438574222544457, 0.00011437971121498842, 0.00011437366150508419, 0.0001143676151843068, 0.00011436153554106555, 0.00011435548173102887, 0.00011434936964223071, 0.00011434322084184184, 0.00011433708962699223, 0.00011433095819884092, 0.00011432481681661079, 0.00011431867185565233, 0.00011431253111332728, 0.00011430636458519804, 0.00011430021689871799, 0.00011429401842273578, 0.00011428785744045045, 0.00011428169273723566, 0.0001142755012528088, 0.00011426928658959841, 0.00011426312548881215, 0.00011425693966873014, 0.00011425075809098174, 0.00011424457217609898, 0.00011423839057465038, 0.00011423218901764377, 0.00011422601981139332, 0.00011421982154871279, 0.00011421361335565363, 0.00011420745991002799, 0.0001142012700135137, 0.00011419508521253977, 0.00011418888517234517, 0.00011418270916414086, 0.00011417654130880111, 0.00011417033766617798, 0.00011416415466641832, 0.0001141579344810642, 0.00011415173299515814, 0.00011414551008428244, 0.0001141392426407541, 0.0001141329929960667, 0.00011412673102728173, 0.00011412047230542246, 0.00011411424517591336, 0.00011410808170510832, 0.00011410185531030503, 0.00011409560184988742, 0.00011408937372497043, 0.00011408314180802341, 0.00011407690100350603, 0.0001140706984984919, 0.00011406449890860086, 0.000114058278628446, 0.00011405204299056952, 0.00011404580360806338, 0.00011403958849454942, 0.00011403335269077162, 0.00011402713963917399, 0.00011402087970860515, 0.00011401464186661121, 0.00011400841625391412, 0.0001140021566551479, 0.00011399586719414522, 0.00011398965824268006, 0.00011398343201377809, 0.0001139772034859579, 0.00011397100982111378, 0.00011396481380995108, 0.00011395858333871552, 0.00011395237611736404, 0.00011394617339904824, 0.00011393997098883486, 0.000113933768768223, 0.00011392754841696755, 0.00011392134930108027, 0.00011391515812475587, 0.00011390894071232369, 0.00011390272749482475, 0.00011389652503721101, 0.00011389031902456911, 0.0001138841203352852, 0.00011387789097795845, 0.00011387168664802988, 0.00011386550341126834, 0.00011385932095661298, 0.00011385309302129749, 0.00011384691924091083, 0.00011384069912665726, 0.00011383453532404961, 0.00011382837156884233, 0.00011382219852316147, 0.00011381599528344154, 0.00011380979161711822, 0.00011380361861883775, 0.00011379745626194154, 0.00011379130286371627, 0.00011378510834566539, 0.00011377893767000331, 0.00011377276983836375, 0.00011376659283475155, 0.00011376042033417503, 0.00011375426911636702, 0.00011374807926725312], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-14 15:13:19,084 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:92]: done!
[2017-12-14 15:13:19,084 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:152]: >> Executing classifier part ... 
[2017-12-14 15:13:19,085 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:97]: =======================================
[2017-12-14 15:13:19,085 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f18f81ed400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-14 15:13:19,502 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:110]: training ... 
[2017-12-14 17:57:31,968 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:122]: trained!
[2017-12-14 17:57:31,970 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:125]: Training history: 
{'val_loss': [0.00011415445513314515, 0.0001141504307062404, 0.00011414641764913674, 0.00011414238546359414, 0.00011413833021666255, 0.00011413427203789545, 0.00011413021646917704, 0.0001141261130078531, 0.00011412197701030591, 0.00011411785874678811, 0.00011411371865539745, 0.00011410954904900429, 0.00011410538321466778, 0.00011410116988102075, 0.00011409696235741358, 0.00011409268804516355, 0.00011408841307146283, 0.00011408413148325518, 0.00011407984049172147, 0.00011407554361863972, 0.00011407123725284669, 0.00011406692091166213, 0.00011406252937962322, 0.00011405812892693847, 0.00011405372185974681, 0.00011404929067642045, 0.00011404479940834339, 0.00011404031083970022, 0.00011403583437381702, 0.00011403136582746507, 0.0001140268215181933, 0.00011402224998218087, 0.0001140176840953149, 0.00011401309702414974, 0.00011400850816527999, 0.00011400387818920506, 0.00011399926095946373, 0.00011399462494094735, 0.00011398998000178514, 0.0001139852901018529, 0.0001139806301996034, 0.00011397595132980839, 0.00011397123388134868, 0.00011396652583621502, 0.00011396176580463237, 0.00011395700650600859, 0.00011395222015941461, 0.00011394740855255501, 0.00011394253759390356, 0.00011393763190015226, 0.00011393272432931115, 0.00011392782836067273, 0.00011392292479428986, 0.00011391795504708369, 0.00011391294797260603, 0.00011390792053617327, 0.00011390288289194739, 0.00011389782694162668, 0.00011389268752337944, 0.00011388755743695006, 0.00011388240414611533, 0.00011387718390574432, 0.00011387194609223734, 0.00011386670491784577, 0.00011386141527878322, 0.0001138560653046913, 0.00011385069931276642, 0.00011384534107947938, 0.00011383993616922591, 0.00011383450517636273, 0.0001138290601321416, 0.00011382356825006064, 0.00011381803868316727, 0.00011381250250176698, 0.00011380694474277249, 0.00011380137783073059, 0.00011379576488529591, 0.00011379015914431066, 0.00011378455789046389, 0.00011377894315732464, 0.00011377329302763484, 0.00011376758756848849, 0.00011376188316408782, 0.00011375615718209297, 0.00011375044927379135, 0.00011374471757114186, 0.00011373896674005342, 0.00011373319007663391, 0.00011372738014403228, 0.00011372156563490693, 0.00011371575480845301, 0.00011370994716411323, 0.0001137041211242934, 0.00011369826991359317, 0.00011369237186503311, 0.0001136864775885297, 0.00011368058518911609, 0.00011367468379754847, 0.00011366878691099639, 0.00011366284375864994, 0.00011365692838723257, 0.00011365095727518658, 0.00011364495755986743, 0.00011363895752276145, 0.00011363294472144482, 0.00011362688754930067, 0.00011362080479510408, 0.0001136147318554056, 0.00011360863635487538, 0.00011360256790231538, 0.00011359645629456696, 0.00011359037312919832, 0.00011358428865880534, 0.0001135781956074304, 0.00011357207107457792, 0.00011356600221084586, 0.00011355990858740547, 0.00011355381029805613, 0.0001135477165852305, 0.00011354162802099403, 0.00011353551968474499, 0.0001135294425618178, 0.00011352333569148652, 0.00011351721982900123, 0.00011351115922446429, 0.00011350505676976329, 0.00011349896223459356, 0.00011349285749163071, 0.00011348677171621338, 0.00011348069623797441, 0.00011347458911736447, 0.0001134685199318456, 0.00011346241191738337, 0.00011345631992075413, 0.00011345021231746398, 0.00011344407437682719, 0.00011343794723392604, 0.0001134318132977475, 0.00011342567576828278, 0.00011341957020297583, 0.00011341352726096008, 0.00011340742087330903, 0.00011340129650134996, 0.00011339518750365023, 0.0001133890830824742, 0.00011338297310153695, 0.00011337689574620817, 0.00011337083187017188, 0.00011336475083217167, 0.00011335865188137163, 0.00011335254787136767, 0.0001133464674769411, 0.00011334037033172269, 0.00011333429412052483, 0.00011332817048152465, 0.00011332206892067594, 0.00011331598101789017, 0.00011330985991743047, 0.00011330371094665648, 0.00011329763995555598, 0.00011329155548516299, 0.00011328546602707425, 0.00011327940999906103, 0.00011327335184367937, 0.00011326724799456881, 0.00011326117782581241, 0.00011325510479672869, 0.00011324903258998908, 0.00011324296642569092, 0.00011323687280225053, 0.00011323080508264939, 0.00011322474587252203, 0.00011321865682560534, 0.00011321257848703905, 0.00011320650472499646, 0.00011320042441995513, 0.0001131943593997879, 0.00011318826282663496, 0.00011318218865342031, 0.00011317613491366894, 0.00011317007799180343, 0.00011316398256278141, 0.00011315793879842157, 0.00011315185236155355, 0.00011314581955582274, 0.00011313978265624846, 0.0001131337414304291, 0.00011312766913430427, 0.00011312159676667124, 0.00011311555332409821, 0.00011310951912395784, 0.00011310349302211919, 0.00011309743184551679, 0.00011309138988673857, 0.00011308535511453272, 0.00011307931004514854, 0.00011307326685285415, 0.00011306724589960467, 0.00011306118758332959, 0.00011305515420553332], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011487284310542028, 0.00011486871033779524, 0.00011486455368038103, 0.00011486041178818373, 0.00011485625638687947, 0.00011485207358815825, 0.00011484789512657138, 0.00011484371649908321, 0.00011483951016607563, 0.00011483526408785336, 0.00011483103033372865, 0.00011482675417996822, 0.0001148224514582974, 0.00011481817340852195, 0.00011481384307613251, 0.00011480949307258729, 0.00011480508339196953, 0.00011480065610211234, 0.00011479622191550052, 0.00011479178275184931, 0.00011478732159442393, 0.0001147828440127685, 0.0001147783369480796, 0.00011477378141650693, 0.0001147692183008742, 0.00011476464023042307, 0.00011476004026099852, 0.00011475538170471004, 0.00011475073386090641, 0.00011474608926402848, 0.00011474144807997757, 0.00011473676909412734, 0.00011473204014828149, 0.00011472730935382102, 0.00011472254955033084, 0.00011471779958241852, 0.0001147130223829906, 0.00011470824947329662, 0.00011470345497273169, 0.00011469864613355323, 0.00011469377866011046, 0.00011468892573858293, 0.00011468404565664032, 0.00011467914223001287, 0.0001146742532367997, 0.00011466930748163707, 0.00011466436957123655, 0.00011465941177637859, 0.00011465441890524288, 0.00011464937650071493, 0.00011464430226683496, 0.00011463922139690242, 0.00011463414538550837, 0.00011462905515400172, 0.00011462389647635306, 0.00011461872360229201, 0.00011461352890035812, 0.00011460833571523622, 0.00011460312563188052, 0.00011459785899987705, 0.00011459260068663944, 0.0001145873299782037, 0.00011458201066068304, 0.00011457665749929438, 0.00011457128040071619, 0.00011456586346212253, 0.00011456038170351558, 0.00011455487058037613, 0.00011454934293820586, 0.00011454375497905798, 0.00011453813232283535, 0.00011453252656484654, 0.00011452685620014614, 0.00011452116682789522, 0.00011451548532410661, 0.00011450977737090857, 0.00011450404604932549, 0.00011449826929448263, 0.00011449250398683043, 0.00011448673896358049, 0.00011448094410179426, 0.00011447513288687853, 0.00011446928043363628, 0.0001144634284069974, 0.00011445754997834946, 0.00011445168697852369, 0.00011444579705528472, 0.00011443987042045505, 0.00011443391579570376, 0.00011442792611837491, 0.00011442192869108468, 0.00011441593529282635, 0.00011440993151388582, 0.00011440390282604807, 0.00011439785565206393, 0.00011439179724419083, 0.00011438574222544457, 0.00011437971121498842, 0.00011437366150508419, 0.0001143676151843068, 0.00011436153554106555, 0.00011435548173102887, 0.00011434936964223071, 0.00011434322084184184, 0.00011433708962699223, 0.00011433095819884092, 0.00011432481681661079, 0.00011431867185565233, 0.00011431253111332728, 0.00011430636458519804, 0.00011430021689871799, 0.00011429401842273578, 0.00011428785744045045, 0.00011428169273723566, 0.0001142755012528088, 0.00011426928658959841, 0.00011426312548881215, 0.00011425693966873014, 0.00011425075809098174, 0.00011424457217609898, 0.00011423839057465038, 0.00011423218901764377, 0.00011422601981139332, 0.00011421982154871279, 0.00011421361335565363, 0.00011420745991002799, 0.0001142012700135137, 0.00011419508521253977, 0.00011418888517234517, 0.00011418270916414086, 0.00011417654130880111, 0.00011417033766617798, 0.00011416415466641832, 0.0001141579344810642, 0.00011415173299515814, 0.00011414551008428244, 0.0001141392426407541, 0.0001141329929960667, 0.00011412673102728173, 0.00011412047230542246, 0.00011411424517591336, 0.00011410808170510832, 0.00011410185531030503, 0.00011409560184988742, 0.00011408937372497043, 0.00011408314180802341, 0.00011407690100350603, 0.0001140706984984919, 0.00011406449890860086, 0.000114058278628446, 0.00011405204299056952, 0.00011404580360806338, 0.00011403958849454942, 0.00011403335269077162, 0.00011402713963917399, 0.00011402087970860515, 0.00011401464186661121, 0.00011400841625391412, 0.0001140021566551479, 0.00011399586719414522, 0.00011398965824268006, 0.00011398343201377809, 0.0001139772034859579, 0.00011397100982111378, 0.00011396481380995108, 0.00011395858333871552, 0.00011395237611736404, 0.00011394617339904824, 0.00011393997098883486, 0.000113933768768223, 0.00011392754841696755, 0.00011392134930108027, 0.00011391515812475587, 0.00011390894071232369, 0.00011390272749482475, 0.00011389652503721101, 0.00011389031902456911, 0.0001138841203352852, 0.00011387789097795845, 0.00011387168664802988, 0.00011386550341126834, 0.00011385932095661298, 0.00011385309302129749, 0.00011384691924091083, 0.00011384069912665726, 0.00011383453532404961, 0.00011382837156884233, 0.00011382219852316147, 0.00011381599528344154, 0.00011380979161711822, 0.00011380361861883775, 0.00011379745626194154, 0.00011379130286371627, 0.00011378510834566539, 0.00011377893767000331, 0.00011377276983836375, 0.00011376659283475155, 0.00011376042033417503, 0.00011375426911636702, 0.00011374807926725312], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-14 17:57:31,970 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:129]: evaluating model ... 
[2017-12-14 17:57:37,808 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:133]: evaluated! 
[2017-12-14 17:57:37,810 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:135]: generating reports ... 
[2017-12-14 17:57:44,007 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:138]: done!
[2017-12-14 17:57:44,007 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_3 finished!
[2018-04-29 11:38:17,230 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_3 was already executed!
[2018-04-29 11:42:07,382 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_3 was already executed!
[2018-04-29 13:12:02,763 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_3
[2018-04-29 13:12:02,763 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:146]: >> Printing header log
[2018-04-29 13:12:02,763 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_3
	layers = 9216,2765
	using GLOBAL obj = 
		{'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'autoencoder_configs': {'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fcb7f80e828>, 'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'discard_decoder_function': True}, 'data_dir': '/home/dhiego/malware_dataset/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'batch': 32, 'shuffle_batches': True, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'store_history': True, 'mlp_configs': {'optimizer': <keras.optimizers.SGD object at 0x7fcb7f80e898>, 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'activation': 'sigmoid'}, 'numpy_seed': 666, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:12:02,763 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,738 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,738 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,738 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:57]: =======================================
[2018-04-29 13:12:20,738 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:62]: setting configurations for autoencoder: 
	 {'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fcb7f80e828>, 'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'discard_decoder_function': True}
[2018-04-29 13:12:20,786 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,890 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_3
[2018-04-29 13:14:12,891 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:146]: >> Printing header log
[2018-04-29 13:14:12,891 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_3
	layers = 9216,2765
	using GLOBAL obj = 
		{'store_history': True, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'mlp_configs': {'classifier_dim': 9, 'activation': 'sigmoid', 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f92ca8a1898>}, 'epochs': 200, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'autoencoder_configs': {'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f92ca8a1828>}, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'shuffle_batches': True, 'numpy_seed': 666, 'batch': 32, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/'}
	=======================================
	
[2018-04-29 13:14:12,891 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:148]: >> Loading dataset... 
[2018-04-29 13:14:30,922 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:30,922 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:30,923 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:57]: =======================================
[2018-04-29 13:14:30,923 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:62]: setting configurations for autoencoder: 
	 {'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f92ca8a1828>}
[2018-04-29 13:14:30,971 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,656 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_3
[2018-04-29 13:16:33,656 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:146]: >> Printing header log
[2018-04-29 13:16:33,656 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_3
	layers = 9216,2765
	using GLOBAL obj = 
		{'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'epochs': 200, 'mlp_configs': {'activation': 'sigmoid', 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'optimizer': <keras.optimizers.SGD object at 0x7f66258f1898>}, 'batch': 32, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'autoencoder_configs': {'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f66258f1828>}, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'store_history': True, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'numpy_seed': 666, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'shuffle_batches': True}
	=======================================
	
[2018-04-29 13:16:33,656 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:148]: >> Loading dataset... 
[2018-04-29 13:16:57,146 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:57,146 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:57,146 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:57]: =======================================
[2018-04-29 13:16:57,146 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:62]: setting configurations for autoencoder: 
	 {'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f66258f1828>}
[2018-04-29 13:16:57,244 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:21,096 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_3
[2018-04-29 14:30:21,096 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:146]: >> Printing header log
[2018-04-29 14:30:21,096 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_3
	layers = 9216,2765
	using GLOBAL obj = 
		{'mlp_configs': {'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7f38ba9a5908>, 'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy'}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'store_history': True, 'epochs': 200, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'batch': 32, 'data_dir': '/home/dhiego/malware_dataset/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'shuffle_batches': True, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'numpy_seed': 666, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f38ba9a5898>}, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/'}
	=======================================
	
[2018-04-29 14:30:21,096 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:148]: >> Loading dataset... 
[2018-04-29 14:30:39,122 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:39,122 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:39,122 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:57]: =======================================
[2018-04-29 14:30:39,122 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f38ba9a5898>}
[2018-04-29 14:30:39,168 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:73]: training and evaluate autoencoder
[2018-04-29 16:54:41,215 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:85]: trained and evaluated!
[2018-04-29 16:54:41,216 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:88]: Training history: 
{'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011517910285551762, 0.00011517667626440282, 0.0001151742649125087, 0.00011517185166459955, 0.00011516943900919511, 0.00011516701891193173, 0.00011516458976119666, 0.00011516217440397082, 0.00011515974999327328, 0.00011515734454272587, 0.00011515492591487414, 0.00011515250470370194, 0.00011515010418279357, 0.0001151476863844484, 0.00011514526605018314, 0.00011514284927094604, 0.00011514044152148044, 0.00011513801860389473, 0.00011513560741790191, 0.00011513318535352296, 0.00011513076433195225, 0.0001151283530274585, 0.00011512594648670247, 0.0001151235269582436, 0.00011512109989312505, 0.00011511868230808157, 0.00011511625766038215, 0.00011511382353260767, 0.00011511139440557279, 0.00011510895112952587, 0.00011510650723727408, 0.00011510406628384555, 0.00011510163661170635, 0.0001150992153294336, 0.00011509678103575781, 0.00011509433842331614, 0.00011509191417851993, 0.00011508949690157888, 0.0001150870618020967, 0.00011508461904745391, 0.00011508218473007793, 0.00011507974598076686, 0.00011507730775285992, 0.0001150748775356164, 0.00011507243724579313, 0.00011507001141308434, 0.00011506758484566972, 0.00011506515029129187, 0.00011506271839133503, 0.00011506027679800144, 0.00011505784323903146, 0.00011505540752334442, 0.00011505296292008699, 0.00011505051988104195, 0.00011504807051404679, 0.00011504563188323666, 0.00011504319621494998, 0.00011504075739453835, 0.00011503832938141229, 0.00011503587110314658, 0.00011503343176133081, 0.0001150309950028355, 0.00011502855153718708, 0.00011502610119848424, 0.00011502366595680095, 0.00011502123097581971, 0.000115018791515503, 0.00011501634278841293, 0.00011501389842215738, 0.00011501145787163203, 0.00011500901298397236, 0.00011500655935094343, 0.00011500409777835163, 0.00011500165113687806, 0.00011499920482720713, 0.0001149967506016735, 0.00011499430059477329, 0.00011499185831413426, 0.00011498941226516537, 0.00011498695936684226, 0.00011498449862375704, 0.00011498204349761628, 0.0001149795911206973, 0.00011497715369859674, 0.0001149747046871044, 0.0001149722596572436, 0.00011496979621233698, 0.00011496733601435607, 0.0001149648918614022, 0.00011496243882087797, 0.00011495999362511584, 0.00011495753975508505, 0.00011495508579025348, 0.00011495261829261477, 0.00011495015956404549, 0.00011494770040887284, 0.00011494524253351032, 0.0001149427780931958, 0.00011494031448238787, 0.00011493785120338256, 0.00011493539652754537], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.00011449601573878739, 0.00011449369488707892, 0.00011449136911918288, 0.00011448904335128683, 0.00011448671504485029, 0.00011448437611944859, 0.00011448204904652821, 0.00011447971355351928, 0.00011447739391744992, 0.00011447506169594037, 0.00011447273380067589, 0.00011447041473667199, 0.00011446808643023545, 0.00011446575665788116, 0.00011446342713580552, 0.00011446111077123553, 0.0001144587807486026, 0.00011445645882427139, 0.00011445412619158978, 0.00011445179175332655, 0.00011444946754073349, 0.00011444715460855629, 0.00011444482458592335, 0.00011444249456329043, 0.00011444016952835325, 0.00011443784122191671, 0.00011443550163506432, 0.0001144331636035149, 0.00011443082099544177, 0.00011442847838736864, 0.00011442613218600932, 0.00011442380330750733, 0.00011442148072172541, 0.000114419142690176, 0.00011441679811562785, 0.00011441447177566634, 0.00011441215172842492, 0.00011440981369687551, 0.00011440746568993458, 0.00011440513370082661, 0.00011440279305922851, 0.00011440045633270344, 0.00011439812402180865, 0.00011439578541819377, 0.0001143934507296519, 0.00011439112480086244, 0.00011438878267546955, 0.00011438644292772374, 0.0001143841010526095, 0.00011438176800875584, 0.00011437943137161598, 0.00011437709047973925, 0.00011437474787166613, 0.00011437239773735674, 0.0001143700566845866, 0.0001143677197971681, 0.00011436538364270848, 0.00011436305174298575, 0.00011436069817628359, 0.00011435835964417688, 0.00011435602406178273, 0.00011435368365258624, 0.00011435133704005487, 0.0001143490069280367, 0.00011434667020151163, 0.00011434433519118294, 0.00011434199004456931, 0.00011433965569569132, 0.00011433731675241257, 0.00011433497757673222, 0.00011433263185805313, 0.00011433027435840092, 0.00011432794090337521, 0.00011432560342601421, 0.00011432325549058145, 0.00011432091540317176, 0.00011431858014256444, 0.00011431624087749886, 0.00011431389785825369, 0.00011431154706249361, 0.00011430920052147042, 0.00011430685398044724, 0.00011430452460138796, 0.00011430217847153682, 0.0001142998450880193, 0.00011429748816043253, 0.0001142951333602142, 0.00011429279883256575, 0.00011429045849487743, 0.00011428812044545098, 0.00011428577798039421, 0.00011428343519355063, 0.00011428107588831679, 0.0001142787195327955, 0.00011427637061412522, 0.00011427401826306216, 0.00011427166206843427, 0.00011426930759000278, 0.00011426695556072655, 0.00011426460753590857, 0.000114262245209454]}
[2018-04-29 16:54:41,216 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:92]: done!
[2018-04-29 16:54:41,218 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:152]: >> Executing classifier part ... 
[2018-04-29 16:54:41,219 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:97]: =======================================
[2018-04-29 16:54:41,219 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7f38ba9a5908>, 'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy'}
[2018-04-29 16:54:41,595 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:110]: training ... 
[2018-04-29 19:25:31,956 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:122]: trained!
[2018-04-29 19:25:31,957 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:125]: Training history: 
{'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011517910285551762, 0.00011517667626440282, 0.0001151742649125087, 0.00011517185166459955, 0.00011516943900919511, 0.00011516701891193173, 0.00011516458976119666, 0.00011516217440397082, 0.00011515974999327328, 0.00011515734454272587, 0.00011515492591487414, 0.00011515250470370194, 0.00011515010418279357, 0.0001151476863844484, 0.00011514526605018314, 0.00011514284927094604, 0.00011514044152148044, 0.00011513801860389473, 0.00011513560741790191, 0.00011513318535352296, 0.00011513076433195225, 0.0001151283530274585, 0.00011512594648670247, 0.0001151235269582436, 0.00011512109989312505, 0.00011511868230808157, 0.00011511625766038215, 0.00011511382353260767, 0.00011511139440557279, 0.00011510895112952587, 0.00011510650723727408, 0.00011510406628384555, 0.00011510163661170635, 0.0001150992153294336, 0.00011509678103575781, 0.00011509433842331614, 0.00011509191417851993, 0.00011508949690157888, 0.0001150870618020967, 0.00011508461904745391, 0.00011508218473007793, 0.00011507974598076686, 0.00011507730775285992, 0.0001150748775356164, 0.00011507243724579313, 0.00011507001141308434, 0.00011506758484566972, 0.00011506515029129187, 0.00011506271839133503, 0.00011506027679800144, 0.00011505784323903146, 0.00011505540752334442, 0.00011505296292008699, 0.00011505051988104195, 0.00011504807051404679, 0.00011504563188323666, 0.00011504319621494998, 0.00011504075739453835, 0.00011503832938141229, 0.00011503587110314658, 0.00011503343176133081, 0.0001150309950028355, 0.00011502855153718708, 0.00011502610119848424, 0.00011502366595680095, 0.00011502123097581971, 0.000115018791515503, 0.00011501634278841293, 0.00011501389842215738, 0.00011501145787163203, 0.00011500901298397236, 0.00011500655935094343, 0.00011500409777835163, 0.00011500165113687806, 0.00011499920482720713, 0.0001149967506016735, 0.00011499430059477329, 0.00011499185831413426, 0.00011498941226516537, 0.00011498695936684226, 0.00011498449862375704, 0.00011498204349761628, 0.0001149795911206973, 0.00011497715369859674, 0.0001149747046871044, 0.0001149722596572436, 0.00011496979621233698, 0.00011496733601435607, 0.0001149648918614022, 0.00011496243882087797, 0.00011495999362511584, 0.00011495753975508505, 0.00011495508579025348, 0.00011495261829261477, 0.00011495015956404549, 0.00011494770040887284, 0.00011494524253351032, 0.0001149427780931958, 0.00011494031448238787, 0.00011493785120338256, 0.00011493539652754537], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.00011449601573878739, 0.00011449369488707892, 0.00011449136911918288, 0.00011448904335128683, 0.00011448671504485029, 0.00011448437611944859, 0.00011448204904652821, 0.00011447971355351928, 0.00011447739391744992, 0.00011447506169594037, 0.00011447273380067589, 0.00011447041473667199, 0.00011446808643023545, 0.00011446575665788116, 0.00011446342713580552, 0.00011446111077123553, 0.0001144587807486026, 0.00011445645882427139, 0.00011445412619158978, 0.00011445179175332655, 0.00011444946754073349, 0.00011444715460855629, 0.00011444482458592335, 0.00011444249456329043, 0.00011444016952835325, 0.00011443784122191671, 0.00011443550163506432, 0.0001144331636035149, 0.00011443082099544177, 0.00011442847838736864, 0.00011442613218600932, 0.00011442380330750733, 0.00011442148072172541, 0.000114419142690176, 0.00011441679811562785, 0.00011441447177566634, 0.00011441215172842492, 0.00011440981369687551, 0.00011440746568993458, 0.00011440513370082661, 0.00011440279305922851, 0.00011440045633270344, 0.00011439812402180865, 0.00011439578541819377, 0.0001143934507296519, 0.00011439112480086244, 0.00011438878267546955, 0.00011438644292772374, 0.0001143841010526095, 0.00011438176800875584, 0.00011437943137161598, 0.00011437709047973925, 0.00011437474787166613, 0.00011437239773735674, 0.0001143700566845866, 0.0001143677197971681, 0.00011436538364270848, 0.00011436305174298575, 0.00011436069817628359, 0.00011435835964417688, 0.00011435602406178273, 0.00011435368365258624, 0.00011435133704005487, 0.0001143490069280367, 0.00011434667020151163, 0.00011434433519118294, 0.00011434199004456931, 0.00011433965569569132, 0.00011433731675241257, 0.00011433497757673222, 0.00011433263185805313, 0.00011433027435840092, 0.00011432794090337521, 0.00011432560342601421, 0.00011432325549058145, 0.00011432091540317176, 0.00011431858014256444, 0.00011431624087749886, 0.00011431389785825369, 0.00011431154706249361, 0.00011430920052147042, 0.00011430685398044724, 0.00011430452460138796, 0.00011430217847153682, 0.0001142998450880193, 0.00011429748816043253, 0.0001142951333602142, 0.00011429279883256575, 0.00011429045849487743, 0.00011428812044545098, 0.00011428577798039421, 0.00011428343519355063, 0.00011428107588831679, 0.0001142787195327955, 0.00011427637061412522, 0.00011427401826306216, 0.00011427166206843427, 0.00011426930759000278, 0.00011426695556072655, 0.00011426460753590857, 0.000114262245209454]}
[2018-04-29 19:25:31,958 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:129]: evaluating model ... 
[2018-04-29 19:25:35,670 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:133]: evaluated! 
[2018-04-29 19:25:35,670 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:135]: generating reports ... 
[2018-04-29 19:25:38,907 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:138]: done!
[2018-04-29 19:25:38,907 AE_BIGRAMA_1L_MINIDS_UNDER_F0_3.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_3 finished!
