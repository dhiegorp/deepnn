[2017-12-14 09:31:57,937 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_1
[2017-12-14 09:31:57,937 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:146]: >> Printing header log
[2017-12-14 09:31:57,938 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_1
	layers = 9216,10138
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f2f35fdbef0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f2f35fbf400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,938 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:148]: >> Loading dataset... 
[2017-12-14 09:32:21,271 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:21,272 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:21,272 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:57]: =======================================
[2017-12-14 09:32:21,272 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f2f35fdbef0>, 'discard_decoder_function': True}
[2017-12-14 09:32:21,324 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:55,013 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_1
[2017-12-14 10:18:55,014 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:146]: >> Printing header log
[2017-12-14 10:18:55,014 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_1
	layers = 9216,10138
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fae17b87eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fae17b6a400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:55,014 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:148]: >> Loading dataset... 
[2017-12-14 10:19:18,058 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:18,058 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:18,058 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:57]: =======================================
[2017-12-14 10:19:18,058 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fae17b87eb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:18,099 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:73]: training and evaluate autoencoder
[2017-12-14 23:31:00,510 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:85]: trained and evaluated!
[2017-12-14 23:31:00,512 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:88]: Training history: 
{'val_loss': [0.00011954836839342636, 0.00011953989637206728, 0.0001195314368646402, 0.00011952295356286525, 0.00011951445424325732, 0.00011950598606546306, 0.00011949752754127351, 0.00011948900966529211, 0.00011948054370425152, 0.00011947201976795161, 0.00011946349903164289, 0.00011945497722271143, 0.00011944641283065704, 0.00011943789733232271, 0.00011942935525082139, 0.0001194208383580775, 0.00011941231901617836, 0.00011940378569442943, 0.00011939527623853658, 0.00011938679442055642, 0.00011937823371117344, 0.00011936971404748747, 0.00011936121399492067, 0.00011935267380838619, 0.000119344166569247, 0.00011933565253683043, 0.00011932709093359516, 0.00011931861598040057, 0.00011931013276801085, 0.00011930162610093713, 0.00011929312359921506, 0.0001192845843959181, 0.00011927603603957372, 0.00011926745219729356, 0.0001192589533782429, 0.00011925041197606932, 0.00011924188773585964, 0.00011923335514706961, 0.00011922482347000889, 0.00011921631009904302, 0.0001192077928845123, 0.00011919929333250279, 0.00011919081281954697, 0.00011918230011003179, 0.00011917377349217502, 0.00011916528530996659, 0.00011915677161721389, 0.00011914832560695635, 0.00011913986014647303, 0.00011913135176320293, 0.00011912286806813298, 0.00011911438732277556, 0.00011910587404119492, 0.00011909738593049466, 0.00011908883757415029, 0.00011908028759099474, 0.00011907182499083876, 0.00011906331269249563, 0.00011905480112711137, 0.00011904635602858316, 0.00011903785059502559, 0.00011902936927760272, 0.00011902088608309003, 0.0001190123933243579, 0.00011900390358684648, 0.00011899542014205517, 0.00011898695509274392, 0.00011897847443677174, 0.00011896992976309878, 0.00011896142915634356, 0.00011895296010257406, 0.00011894445851258133, 0.00011893599289120461, 0.00011892755367422444, 0.00011891907972214442, 0.00011891054486296956, 0.00011890204597241074, 0.00011889353513998536, 0.0001188850841599091, 0.00011887662533180976, 0.0001188682022220478, 0.00011885970488679194, 0.0001188512199045747, 0.00011884276508093361, 0.00011883432693657618, 0.00011882583802140887, 0.00011881739031283199, 0.00011880887188266217, 0.00011880041591489015, 0.00011879196427336319, 0.00011878343849572758, 0.00011877501449211333, 0.00011876658108517303, 0.00011875810958224827, 0.00011874963963462649, 0.00011874122886002608, 0.00011873276513361622, 0.00011872433712554372, 0.0001187158989811863, 0.00011870746239213185, 0.00011869901926007868, 0.00011869053272255845, 0.00011868209384524215, 0.00011867365995562161, 0.00011866521478558522, 0.00011865683320420051, 0.00011864833971250949, 0.00011863994547417639, 0.00011863151076221175, 0.00011862308357648336, 0.00011861464371592955, 0.00011860626107979913, 0.00011859786560794989, 0.00011858942959096091, 0.00011858101057504242, 0.00011857253662296239, 0.00011856412545506699, 0.00011855571911397393, 0.00011854730336955481, 0.0001185388796162192, 0.00011853044817575393, 0.00011852205540333859, 0.00011851366075383344, 0.00011850524239936564, 0.00011849684505042659, 0.00011848845923218203, 0.00011848008460496811, 0.00011847169281579028, 0.00011846331958298592, 0.00011845489951232174, 0.0001184464809075753, 0.00011843811854401475, 0.00011842972675483692, 0.00011842135793766286, 0.00011841299957856055, 0.00011840466272554427, 0.00011839625851181965, 0.00011838787212150963, 0.00011837948973565786, 0.00011837110734980608, 0.00011836274613037645, 0.00011835438131766065, 0.00011834595642019411, 0.00011833760190465664, 0.00011832925091089717, 0.00011832085879993251, 0.00011831248883862754, 0.00011830410677456258, 0.00011829574335625633, 0.00011828736840725558, 0.00011827897105831653, 0.00011827061647127087, 0.00011826232670639305, 0.00011825395667357988, 0.00011824562007084224, 0.00011823726614524727, 0.00011822892594922343, 0.00011822059801685296, 0.00011821224228567638, 0.00011820382434238064, 0.00011819547678101396, 0.00011818712130011601, 0.00011817882247157601, 0.00011817049037385388, 0.00011816216489063868, 0.00011815385665877262, 0.0001181454907913111, 0.00011813713178863515, 0.00011812878824960376, 0.0001181204640714129, 0.00011811212412566771, 0.00011810376406824606, 0.00011809542607109884, 0.00011808712234424829, 0.00011807884427095839, 0.00011807054519213974, 0.00011806227750541341, 0.00011805394687360904, 0.00011804561404292803, 0.00011803731226467548, 0.0001180289931635656, 0.00011802071158637473, 0.00011801243286951117, 0.00011800416600512894, 0.00011799589930164013, 0.00011798761397026964, 0.0001179793389360775, 0.00011797101027074817, 0.00011796269869587455, 0.00011795439284165557, 0.00011794610995944036, 0.0001179378408961815, 0.00011792956128546566, 0.0001179212825686021, 0.00011791300410201718, 0.0001179047072399522, 0.00011789646972967906, 0.00011788817597822005, 0.00011787991419091881, 0.00011787162558804896], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.0049140050055353881, 0.0049140050055353881, 0.0049140050055353881, 0.0049140050055353881, 0.0073710072977829916, 0.0073710072977829916, 0.0073710072977829916, 0.0073710072977829916, 0.0073710072977829916, 0.0073710072977829916, 0.0073710072977829916, 0.0073710072977829916, 0.0073710072977829916, 0.0098280097547854481, 0.0098280097547854481, 0.0098280097547854481, 0.0098280097547854481, 0.0098280097547854481, 0.0098280097547854481, 0.0098280097547854481, 0.0098280097547854481, 0.0098280097547854481, 0.0098280097547854481, 0.0098280097547854481, 0.012285012211787906, 0.012285012211787906, 0.012285012211787906, 0.012285012211787906, 0.012285012211787906, 0.012285012211787906, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.017199017125792821, 0.017199017125792821], 'loss': [0.0001203782144463348, 0.0001203696385571272, 0.00012036104093484751, 0.00012035246239121888, 0.00012034385481486036, 0.00012033523690522002, 0.00012032662508652791, 0.00012031803054527262, 0.00012030937246381418, 0.00012030075005113818, 0.00012029207367313486, 0.00012028338542133752, 0.00012027471224285953, 0.00012026598962579008, 0.00012025731962313724, 0.00012024862843251663, 0.00012023996108428481, 0.00012023129428115731, 0.00012022261494063054, 0.00012021396311602164, 0.00012020534259936066, 0.00012019662379802142, 0.00012018795637868903, 0.00012017930604719196, 0.00012017061355306103, 0.00012016195758091929, 0.00012015329001938579, 0.00012014458003451636, 0.00012013594897127186, 0.00012012731477960258, 0.00012011866003987061, 0.00012010999608076562, 0.00012010129500716676, 0.0001200925845008932, 0.00012008384166756366, 0.00012007519453559192, 0.00012006648763174688, 0.00012005781305495783, 0.00012004912454245842, 0.00012004042782969409, 0.00012003175868024801, 0.00012002308652087809, 0.00012001443064353711, 0.00012000579121412636, 0.0001199971128216071, 0.00011998843021045442, 0.00011997978679941215, 0.0001199711114405169, 0.00011996250523876927, 0.00011995387877336118, 0.00011994521434025245, 0.00011993656637877413, 0.00011992792261222905, 0.00011991924983665426, 0.0001199106091037332, 0.00011990189298051515, 0.0001198931909115084, 0.00011988456989714348, 0.00011987590375762123, 0.0001198672421922352, 0.00011985864520986057, 0.00011984998942732035, 0.00011984135611255801, 0.000119832722205291, 0.00011982407476521683, 0.00011981542644823571, 0.00011980678379559944, 0.00011979815351446113, 0.00011978950948721399, 0.00011978080291517157, 0.00011977213988037392, 0.00011976350379268964, 0.00011975483222582441, 0.00011974620163658367, 0.00011973759406022515, 0.00011972895299550146, 0.00011972024656566018, 0.00011971157672890865, 0.00011970290125151247, 0.00011969428000014566, 0.00011968565953088505, 0.00011967706828395584, 0.0001196684084723837, 0.00011965976072420708, 0.00011965114141625568, 0.0001196425412580559, 0.00011963388957564813, 0.00011962528041137704, 0.00011961659182777707, 0.00011960797920327858, 0.00011959935553449264, 0.00011959066626358723, 0.00011958207833468429, 0.0001195734847177363, 0.00011956484697103889, 0.00011955620936654259, 0.00011954763473344493, 0.000119539003362098, 0.00011953041235217067, 0.00011952181183846807, 0.00011951320276899772, 0.00011950459372322758, 0.00011949594196971925, 0.00011948733813799038, 0.00011947873736358573, 0.00011947012582929587, 0.00011946158582217239, 0.00011945291754963326, 0.00011944436543171388, 0.00011943576105488069, 0.00011942716644252484, 0.00011941856211309204, 0.00011941001103798091, 0.00011940144986658985, 0.00011939284297753677, 0.00011938424955019029, 0.00011937560739525797, 0.00011936702167047249, 0.00011935844127822923, 0.00011934985135851052, 0.00011934125522934265, 0.00011933265462083929, 0.00011932409136383173, 0.00011931552500209957, 0.00011930693448987616, 0.00011929836632692974, 0.00011928981288179985, 0.00011928126647562572, 0.00011927270077749881, 0.00011926415321001546, 0.00011925556393020182, 0.000119246974247485, 0.00011923843360045646, 0.0001192298626171877, 0.0001192213213065539, 0.00011921279355242745, 0.00011920428750767291, 0.00011919570976985067, 0.00011918715241418981, 0.00011917860159978076, 0.00011917004249030603, 0.00011916151606339009, 0.00011915297605626661, 0.00011914437720157715, 0.00011913585404528712, 0.00011912733242950927, 0.0001191187720402244, 0.00011911023032668741, 0.00011910168247480182, 0.0001190931467811125, 0.00011908459828932184, 0.00011907603000787448, 0.00011906750324915591, 0.00011905904775690166, 0.00011905050741797556, 0.0001190420177263505, 0.00011903351258220309, 0.00011902501037687894, 0.00011901653336485429, 0.00011900801883543256, 0.00011899944308842609, 0.00011899093934258974, 0.00011898243137811999, 0.00011897397891948977, 0.00011896548820875664, 0.00011895700690699802, 0.00011894853880624393, 0.00011894001538925183, 0.00011893149659379631, 0.00011892300401074836, 0.00011891451405842124, 0.00011890601687753689, 0.00011889749405304947, 0.00011888900234690847, 0.00011888053981569848, 0.00011887210408940074, 0.00011886365058796225, 0.00011885522446024053, 0.00011884673524261922, 0.00011883824754180993, 0.00011882979062754443, 0.00011882131593813817, 0.00011881288587618529, 0.00011880445998546543, 0.00011879603949838837, 0.00011878763432163253, 0.00011877920409377835, 0.00011877078476801048, 0.00011876230268414566, 0.000118753849917413, 0.00011874539269504505, 0.00011873696943504603, 0.00011872855458861363, 0.00011872012628047463, 0.00011871170216726885, 0.00011870328310220305, 0.00011869484135605764, 0.00011868646488022909, 0.00011867803744899703, 0.00011866963457115941], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00081433224755700329, 0.0024429967426710096, 0.0024429967426710096, 0.0032573289902280132, 0.0040716612377850164, 0.0048859934853420191, 0.0048859934853420191, 0.0048859934853420191, 0.0048859934853420191, 0.0048859934853420191, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257571680149, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0065146579804560263, 0.0073289902280130291, 0.0073289902280130291, 0.0073289902522820213, 0.0081433224755700327, 0.0081433224755700327, 0.0097719869706840382, 0.0097719869706840382, 0.0097719870192220226, 0.010586319218241042, 0.010586319218241042, 0.011400651465798045, 0.011400651490067039, 0.011400651465798045, 0.011400651465798045, 0.011400651490067039, 0.012214983713355049, 0.014657980456026058, 0.015472312703583062, 0.015472312703583062, 0.016286644975409059, 0.016286644951140065, 0.017100977198697069, 0.018729641693811076, 0.018729641693811076, 0.021172638436482084, 0.021986970684039087, 0.022801302931596091, 0.022801302931596091, 0.022801302931596091, 0.022801302955865084, 0.022801302955865084, 0.022801302931596091, 0.022801302931596091]}
[2017-12-14 23:31:00,512 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:92]: done!
[2017-12-14 23:31:00,512 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:152]: >> Executing classifier part ... 
[2017-12-14 23:31:00,512 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:97]: =======================================
[2017-12-14 23:31:00,513 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fae17b6a400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-14 23:31:00,615 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:110]: training ... 
[2017-12-15 05:10:29,163 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:122]: trained!
[2017-12-15 05:10:29,165 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:125]: Training history: 
{'val_loss': [0.00011954836839342636, 0.00011953989637206728, 0.0001195314368646402, 0.00011952295356286525, 0.00011951445424325732, 0.00011950598606546306, 0.00011949752754127351, 0.00011948900966529211, 0.00011948054370425152, 0.00011947201976795161, 0.00011946349903164289, 0.00011945497722271143, 0.00011944641283065704, 0.00011943789733232271, 0.00011942935525082139, 0.0001194208383580775, 0.00011941231901617836, 0.00011940378569442943, 0.00011939527623853658, 0.00011938679442055642, 0.00011937823371117344, 0.00011936971404748747, 0.00011936121399492067, 0.00011935267380838619, 0.000119344166569247, 0.00011933565253683043, 0.00011932709093359516, 0.00011931861598040057, 0.00011931013276801085, 0.00011930162610093713, 0.00011929312359921506, 0.0001192845843959181, 0.00011927603603957372, 0.00011926745219729356, 0.0001192589533782429, 0.00011925041197606932, 0.00011924188773585964, 0.00011923335514706961, 0.00011922482347000889, 0.00011921631009904302, 0.0001192077928845123, 0.00011919929333250279, 0.00011919081281954697, 0.00011918230011003179, 0.00011917377349217502, 0.00011916528530996659, 0.00011915677161721389, 0.00011914832560695635, 0.00011913986014647303, 0.00011913135176320293, 0.00011912286806813298, 0.00011911438732277556, 0.00011910587404119492, 0.00011909738593049466, 0.00011908883757415029, 0.00011908028759099474, 0.00011907182499083876, 0.00011906331269249563, 0.00011905480112711137, 0.00011904635602858316, 0.00011903785059502559, 0.00011902936927760272, 0.00011902088608309003, 0.0001190123933243579, 0.00011900390358684648, 0.00011899542014205517, 0.00011898695509274392, 0.00011897847443677174, 0.00011896992976309878, 0.00011896142915634356, 0.00011895296010257406, 0.00011894445851258133, 0.00011893599289120461, 0.00011892755367422444, 0.00011891907972214442, 0.00011891054486296956, 0.00011890204597241074, 0.00011889353513998536, 0.0001188850841599091, 0.00011887662533180976, 0.0001188682022220478, 0.00011885970488679194, 0.0001188512199045747, 0.00011884276508093361, 0.00011883432693657618, 0.00011882583802140887, 0.00011881739031283199, 0.00011880887188266217, 0.00011880041591489015, 0.00011879196427336319, 0.00011878343849572758, 0.00011877501449211333, 0.00011876658108517303, 0.00011875810958224827, 0.00011874963963462649, 0.00011874122886002608, 0.00011873276513361622, 0.00011872433712554372, 0.0001187158989811863, 0.00011870746239213185, 0.00011869901926007868, 0.00011869053272255845, 0.00011868209384524215, 0.00011867365995562161, 0.00011866521478558522, 0.00011865683320420051, 0.00011864833971250949, 0.00011863994547417639, 0.00011863151076221175, 0.00011862308357648336, 0.00011861464371592955, 0.00011860626107979913, 0.00011859786560794989, 0.00011858942959096091, 0.00011858101057504242, 0.00011857253662296239, 0.00011856412545506699, 0.00011855571911397393, 0.00011854730336955481, 0.0001185388796162192, 0.00011853044817575393, 0.00011852205540333859, 0.00011851366075383344, 0.00011850524239936564, 0.00011849684505042659, 0.00011848845923218203, 0.00011848008460496811, 0.00011847169281579028, 0.00011846331958298592, 0.00011845489951232174, 0.0001184464809075753, 0.00011843811854401475, 0.00011842972675483692, 0.00011842135793766286, 0.00011841299957856055, 0.00011840466272554427, 0.00011839625851181965, 0.00011838787212150963, 0.00011837948973565786, 0.00011837110734980608, 0.00011836274613037645, 0.00011835438131766065, 0.00011834595642019411, 0.00011833760190465664, 0.00011832925091089717, 0.00011832085879993251, 0.00011831248883862754, 0.00011830410677456258, 0.00011829574335625633, 0.00011828736840725558, 0.00011827897105831653, 0.00011827061647127087, 0.00011826232670639305, 0.00011825395667357988, 0.00011824562007084224, 0.00011823726614524727, 0.00011822892594922343, 0.00011822059801685296, 0.00011821224228567638, 0.00011820382434238064, 0.00011819547678101396, 0.00011818712130011601, 0.00011817882247157601, 0.00011817049037385388, 0.00011816216489063868, 0.00011815385665877262, 0.0001181454907913111, 0.00011813713178863515, 0.00011812878824960376, 0.0001181204640714129, 0.00011811212412566771, 0.00011810376406824606, 0.00011809542607109884, 0.00011808712234424829, 0.00011807884427095839, 0.00011807054519213974, 0.00011806227750541341, 0.00011805394687360904, 0.00011804561404292803, 0.00011803731226467548, 0.0001180289931635656, 0.00011802071158637473, 0.00011801243286951117, 0.00011800416600512894, 0.00011799589930164013, 0.00011798761397026964, 0.0001179793389360775, 0.00011797101027074817, 0.00011796269869587455, 0.00011795439284165557, 0.00011794610995944036, 0.0001179378408961815, 0.00011792956128546566, 0.0001179212825686021, 0.00011791300410201718, 0.0001179047072399522, 0.00011789646972967906, 0.00011788817597822005, 0.00011787991419091881, 0.00011787162558804896], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.002457002502767694, 0.0049140050055353881, 0.0049140050055353881, 0.0049140050055353881, 0.0049140050055353881, 0.0073710072977829916, 0.0073710072977829916, 0.0073710072977829916, 0.0073710072977829916, 0.0073710072977829916, 0.0073710072977829916, 0.0073710072977829916, 0.0073710072977829916, 0.0073710072977829916, 0.0098280097547854481, 0.0098280097547854481, 0.0098280097547854481, 0.0098280097547854481, 0.0098280097547854481, 0.0098280097547854481, 0.0098280097547854481, 0.0098280097547854481, 0.0098280097547854481, 0.0098280097547854481, 0.0098280097547854481, 0.012285012211787906, 0.012285012211787906, 0.012285012211787906, 0.012285012211787906, 0.012285012211787906, 0.012285012211787906, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.014742014668790363, 0.017199017125792821, 0.017199017125792821], 'loss': [0.0001203782144463348, 0.0001203696385571272, 0.00012036104093484751, 0.00012035246239121888, 0.00012034385481486036, 0.00012033523690522002, 0.00012032662508652791, 0.00012031803054527262, 0.00012030937246381418, 0.00012030075005113818, 0.00012029207367313486, 0.00012028338542133752, 0.00012027471224285953, 0.00012026598962579008, 0.00012025731962313724, 0.00012024862843251663, 0.00012023996108428481, 0.00012023129428115731, 0.00012022261494063054, 0.00012021396311602164, 0.00012020534259936066, 0.00012019662379802142, 0.00012018795637868903, 0.00012017930604719196, 0.00012017061355306103, 0.00012016195758091929, 0.00012015329001938579, 0.00012014458003451636, 0.00012013594897127186, 0.00012012731477960258, 0.00012011866003987061, 0.00012010999608076562, 0.00012010129500716676, 0.0001200925845008932, 0.00012008384166756366, 0.00012007519453559192, 0.00012006648763174688, 0.00012005781305495783, 0.00012004912454245842, 0.00012004042782969409, 0.00012003175868024801, 0.00012002308652087809, 0.00012001443064353711, 0.00012000579121412636, 0.0001199971128216071, 0.00011998843021045442, 0.00011997978679941215, 0.0001199711114405169, 0.00011996250523876927, 0.00011995387877336118, 0.00011994521434025245, 0.00011993656637877413, 0.00011992792261222905, 0.00011991924983665426, 0.0001199106091037332, 0.00011990189298051515, 0.0001198931909115084, 0.00011988456989714348, 0.00011987590375762123, 0.0001198672421922352, 0.00011985864520986057, 0.00011984998942732035, 0.00011984135611255801, 0.000119832722205291, 0.00011982407476521683, 0.00011981542644823571, 0.00011980678379559944, 0.00011979815351446113, 0.00011978950948721399, 0.00011978080291517157, 0.00011977213988037392, 0.00011976350379268964, 0.00011975483222582441, 0.00011974620163658367, 0.00011973759406022515, 0.00011972895299550146, 0.00011972024656566018, 0.00011971157672890865, 0.00011970290125151247, 0.00011969428000014566, 0.00011968565953088505, 0.00011967706828395584, 0.0001196684084723837, 0.00011965976072420708, 0.00011965114141625568, 0.0001196425412580559, 0.00011963388957564813, 0.00011962528041137704, 0.00011961659182777707, 0.00011960797920327858, 0.00011959935553449264, 0.00011959066626358723, 0.00011958207833468429, 0.0001195734847177363, 0.00011956484697103889, 0.00011955620936654259, 0.00011954763473344493, 0.000119539003362098, 0.00011953041235217067, 0.00011952181183846807, 0.00011951320276899772, 0.00011950459372322758, 0.00011949594196971925, 0.00011948733813799038, 0.00011947873736358573, 0.00011947012582929587, 0.00011946158582217239, 0.00011945291754963326, 0.00011944436543171388, 0.00011943576105488069, 0.00011942716644252484, 0.00011941856211309204, 0.00011941001103798091, 0.00011940144986658985, 0.00011939284297753677, 0.00011938424955019029, 0.00011937560739525797, 0.00011936702167047249, 0.00011935844127822923, 0.00011934985135851052, 0.00011934125522934265, 0.00011933265462083929, 0.00011932409136383173, 0.00011931552500209957, 0.00011930693448987616, 0.00011929836632692974, 0.00011928981288179985, 0.00011928126647562572, 0.00011927270077749881, 0.00011926415321001546, 0.00011925556393020182, 0.000119246974247485, 0.00011923843360045646, 0.0001192298626171877, 0.0001192213213065539, 0.00011921279355242745, 0.00011920428750767291, 0.00011919570976985067, 0.00011918715241418981, 0.00011917860159978076, 0.00011917004249030603, 0.00011916151606339009, 0.00011915297605626661, 0.00011914437720157715, 0.00011913585404528712, 0.00011912733242950927, 0.0001191187720402244, 0.00011911023032668741, 0.00011910168247480182, 0.0001190931467811125, 0.00011908459828932184, 0.00011907603000787448, 0.00011906750324915591, 0.00011905904775690166, 0.00011905050741797556, 0.0001190420177263505, 0.00011903351258220309, 0.00011902501037687894, 0.00011901653336485429, 0.00011900801883543256, 0.00011899944308842609, 0.00011899093934258974, 0.00011898243137811999, 0.00011897397891948977, 0.00011896548820875664, 0.00011895700690699802, 0.00011894853880624393, 0.00011894001538925183, 0.00011893149659379631, 0.00011892300401074836, 0.00011891451405842124, 0.00011890601687753689, 0.00011889749405304947, 0.00011888900234690847, 0.00011888053981569848, 0.00011887210408940074, 0.00011886365058796225, 0.00011885522446024053, 0.00011884673524261922, 0.00011883824754180993, 0.00011882979062754443, 0.00011882131593813817, 0.00011881288587618529, 0.00011880445998546543, 0.00011879603949838837, 0.00011878763432163253, 0.00011877920409377835, 0.00011877078476801048, 0.00011876230268414566, 0.000118753849917413, 0.00011874539269504505, 0.00011873696943504603, 0.00011872855458861363, 0.00011872012628047463, 0.00011871170216726885, 0.00011870328310220305, 0.00011869484135605764, 0.00011868646488022909, 0.00011867803744899703, 0.00011866963457115941], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00081433224755700329, 0.0024429967426710096, 0.0024429967426710096, 0.0032573289902280132, 0.0040716612377850164, 0.0048859934853420191, 0.0048859934853420191, 0.0048859934853420191, 0.0048859934853420191, 0.0048859934853420191, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257571680149, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0065146579804560263, 0.0073289902280130291, 0.0073289902280130291, 0.0073289902522820213, 0.0081433224755700327, 0.0081433224755700327, 0.0097719869706840382, 0.0097719869706840382, 0.0097719870192220226, 0.010586319218241042, 0.010586319218241042, 0.011400651465798045, 0.011400651490067039, 0.011400651465798045, 0.011400651465798045, 0.011400651490067039, 0.012214983713355049, 0.014657980456026058, 0.015472312703583062, 0.015472312703583062, 0.016286644975409059, 0.016286644951140065, 0.017100977198697069, 0.018729641693811076, 0.018729641693811076, 0.021172638436482084, 0.021986970684039087, 0.022801302931596091, 0.022801302931596091, 0.022801302931596091, 0.022801302955865084, 0.022801302955865084, 0.022801302931596091, 0.022801302931596091]}
[2017-12-15 05:10:29,165 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:129]: evaluating model ... 
[2017-12-15 05:10:36,174 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:133]: evaluated! 
[2017-12-15 05:10:36,175 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:135]: generating reports ... 
[2017-12-15 05:10:38,485 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:138]: done!
[2017-12-15 05:10:38,486 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_1 finished!
[2018-04-29 11:38:17,486 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_1 was already executed!
[2018-04-29 13:12:02,700 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_1
[2018-04-29 13:12:02,701 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:146]: >> Printing header log
[2018-04-29 13:12:02,701 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_1
	layers = 9216,10138
	using GLOBAL obj = 
		{'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'autoencoder_configs': {'discard_decoder_function': True, 'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f4626f3b828>}, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'batch': 32, 'epochs': 200, 'store_history': True, 'shuffle_batches': True, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'mlp_configs': {'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7f4626f3b898>, 'use_last_dim_as_classifier': False}, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'data_dir': '/home/dhiego/malware_dataset/', 'numpy_seed': 666, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'fullds_data_dir': '/home/dhiego/malware_dataset/'}
	=======================================
	
[2018-04-29 13:12:02,701 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,655 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,655 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,655 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:57]: =======================================
[2018-04-29 13:12:20,655 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f4626f3b828>}
[2018-04-29 13:12:20,703 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,867 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_1
[2018-04-29 13:14:12,868 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:146]: >> Printing header log
[2018-04-29 13:14:12,868 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_1
	layers = 9216,10138
	using GLOBAL obj = 
		{'fullds_data_dir': '/home/dhiego/malware_dataset/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'shuffle_batches': True, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'epochs': 200, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'autoencoder_configs': {'output_layer_activation': 'relu', 'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7fcc13e4a828>}, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7fcc13e4a898>}, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'batch': 32, 'data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'store_history': True, 'numpy_seed': 666}
	=======================================
	
[2018-04-29 13:14:12,868 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:148]: >> Loading dataset... 
[2018-04-29 13:14:30,948 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:30,948 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:30,948 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:57]: =======================================
[2018-04-29 13:14:30,948 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:62]: setting configurations for autoencoder: 
	 {'output_layer_activation': 'relu', 'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7fcc13e4a828>}
[2018-04-29 13:14:30,994 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,666 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_1
[2018-04-29 13:16:33,666 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:146]: >> Printing header log
[2018-04-29 13:16:33,666 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_1
	layers = 9216,10138
	using GLOBAL obj = 
		{'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'shuffle_batches': True, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'mlp_configs': {'activation': 'sigmoid', 'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f4861407898>}, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'batch': 32, 'autoencoder_configs': {'optimizer': <keras.optimizers.SGD object at 0x7f4861407828>, 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'output_layer_activation': 'relu'}, 'numpy_seed': 666, 'epochs': 200, 'data_dir': '/home/dhiego/malware_dataset/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'store_history': True, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/'}
	=======================================
	
[2018-04-29 13:16:33,666 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:148]: >> Loading dataset... 
[2018-04-29 13:16:56,362 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:56,363 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:56,363 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:57]: =======================================
[2018-04-29 13:16:56,363 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:62]: setting configurations for autoencoder: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7f4861407828>, 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'output_layer_activation': 'relu'}
[2018-04-29 13:16:56,453 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:21,179 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_1
[2018-04-29 14:30:21,179 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:146]: >> Printing header log
[2018-04-29 14:30:21,179 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_1
	layers = 9216,10138
	using GLOBAL obj = 
		{'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'autoencoder_configs': {'optimizer': <keras.optimizers.SGD object at 0x7fa4941db8d0>, 'loss_function': 'mse', 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu'}, 'mlp_configs': {'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid', 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7fa4941db940>}, 'store_history': True, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'data_dir': '/home/dhiego/malware_dataset/', 'epochs': 200, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'shuffle_batches': True, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'batch': 32, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'numpy_seed': 666}
	=======================================
	
[2018-04-29 14:30:21,179 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:148]: >> Loading dataset... 
[2018-04-29 14:30:39,321 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:39,322 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:39,322 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:57]: =======================================
[2018-04-29 14:30:39,322 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:62]: setting configurations for autoencoder: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7fa4941db8d0>, 'loss_function': 'mse', 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu'}
[2018-04-29 14:30:39,408 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:73]: training and evaluate autoencoder
[2018-04-29 20:59:00,925 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:85]: trained and evaluated!
[2018-04-29 20:59:00,926 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:88]: Training history: 
{'val_loss': [0.00012035702583102592, 0.00012035312396914673, 0.00012034925970269473, 0.00012034536250672447, 0.00012034146947610588, 0.00012033760701523549, 0.00012033372444268865, 0.00012032982855174274, 0.00012032595954787361, 0.00012032207191612283, 0.00012031818991564145, 0.00012031430040680088, 0.00012031039895609374, 0.00012030652480363544, 0.00012030264207019519, 0.0001202987580317306, 0.00012029487424354465, 0.00012029097361518162, 0.0001202870867163897, 0.00012028319148689448, 0.00012027930238922596, 0.00012027540176086292, 0.00012027148928001857, 0.00012026758318127955, 0.00012026368900653002, 0.00012025977261061266, 0.00012025585294319591, 0.00012025193041545186, 0.00012024800257822522, 0.00012024410636549249, 0.0001202401927405172, 0.00012023628304849199, 0.00012023237866594934, 0.00012022843293379992, 0.00012022448679047844, 0.0001202205428460336, 0.00012021663487020478, 0.00012021269641401297, 0.00012020875901256685, 0.0001202048138524829, 0.00012020088143873254, 0.00012019696504281518, 0.00012019300770846308, 0.00012018905396739717, 0.00012018507709343407, 0.00012018110880045293, 0.00012017713332089942, 0.00012017319314851121, 0.00012016922167341593, 0.0001201652445491742, 0.00012016127332435756, 0.00012015728567053589, 0.0001201532935116987, 0.00012014931281204782, 0.00012014530291918126, 0.00012014129556485518, 0.00012013727015471292, 0.00012013324621048841, 0.00012012921540147833, 0.00012012519971644895, 0.00012012117233983165, 0.0001201171318057087, 0.00012011311170504903, 0.00012010907224354881, 0.00012010501152624123, 0.0001201009622502429, 0.00012009687242910485, 0.00012009277763814809, 0.00012008870285160551, 0.00012008459265063532, 0.00012008048212787833, 0.00012007638647882337, 0.00012007228870239997, 0.00012006817662433998, 0.00012006405163905297, 0.00012005990858007273, 0.0001200557781959179, 0.00012005162231909587, 0.00012004746880204387, 0.00012004326830411568, 0.00012003907318717825, 0.00012003484653513215, 0.0001200305754943815, 0.00012002630438212263, 0.00012002204699943493, 0.00012001777531511061, 0.00012001350665200703, 0.00012000920194877924, 0.00012000487926124346, 0.00012000053712348191, 0.00011999617365840479, 0.00011999180466932055, 0.00011998742204005041, 0.0001199830076075159, 0.00011997860985426507, 0.00011997416043635206, 0.00011996972721504249, 0.00011996529047195491, 0.00011996084073225508, 0.00011995636639374031, 0.00011995190889540263], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012112726212451871, 0.00012112324878213934, 0.00012111919910737225, 0.0001211151847458848, 0.0001211111443378911, 0.0001211071018205807, 0.00012110309340784035, 0.00012109906101051008, 0.00012109502121872126, 0.00012109099870436925, 0.00012108695431474402, 0.00012108292594644566, 0.00012107889004148761, 0.00012107483903951003, 0.00012107081590895313, 0.00012106677905598759, 0.00012106274781996652, 0.00012105871629954319, 0.00012105466887629393, 0.00012105063790097492, 0.00012104659694787692, 0.00012104255717978828, 0.00012103850691251651, 0.0001210344409557205, 0.00012103038334139056, 0.000121026345350816, 0.00012102228057902938, 0.00012101822832094184, 0.00012101417298182991, 0.00012101009960687517, 0.0001210060656690327, 0.00012100200933451288, 0.00012099795605731728, 0.00012099390647735094, 0.00012098981980659092, 0.00012098572915419936, 0.00012098164949869488, 0.00012097759811751429, 0.00012097351142305407, 0.0001209694296345327, 0.00012096533564041469, 0.00012096125356749107, 0.00012095717964743201, 0.00012095306112361975, 0.00012094894231540524, 0.00012094480172671827, 0.00012094066955159791, 0.00012093652891551056, 0.00012093243053685783, 0.00012092828881056184, 0.00012092413708278666, 0.00012091999881671808, 0.00012091584223040443, 0.00012091169185353995, 0.00012090754166627698, 0.00012090337495998319, 0.00012089920559926839, 0.00012089500680292049, 0.00012089081227260638, 0.00012088660371178118, 0.00012088240136040514, 0.0001208781824899983, 0.00012087394390103532, 0.00012086972680814254, 0.00012086547866800393, 0.00012086120872369266, 0.00012085694709814727, 0.00012085265406985321, 0.0001208483626768721, 0.00012084409270886065, 0.00012083978453614667, 0.00012083547280840455, 0.00012083117131914349, 0.00012082686544534771, 0.00012082253174753162, 0.0001208181800375729, 0.00012081381993774774, 0.00012080946118883327, 0.00012080507144007335, 0.00012080068268672129, 0.0001207962449924817, 0.00012079181056886802, 0.000120787346425219, 0.00012078284405316727, 0.0001207783381023872, 0.00012077384736712761, 0.00012076934466327325, 0.00012076484361843203, 0.00012076031211884965, 0.0001207557531033494, 0.0001207511814793493, 0.00012074657800229697, 0.00012074196931120335, 0.00012073734753760615, 0.00012073269535666817, 0.00012072804113751405, 0.00012072332271455154, 0.00012071863941526715, 0.00012071395758539438, 0.00012070924959051443, 0.00012070448736960511], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-04-29 20:59:00,926 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:92]: done!
[2018-04-29 20:59:00,927 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:152]: >> Executing classifier part ... 
[2018-04-29 20:59:00,927 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:97]: =======================================
[2018-04-29 20:59:00,927 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:101]: setting configurations for classifier: 
	 {'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid', 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7fa4941db940>}
[2018-04-29 20:59:01,006 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:110]: training ... 
[2018-04-30 01:50:54,787 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:122]: trained!
[2018-04-30 01:50:54,789 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:125]: Training history: 
{'val_loss': [0.00012035702583102592, 0.00012035312396914673, 0.00012034925970269473, 0.00012034536250672447, 0.00012034146947610588, 0.00012033760701523549, 0.00012033372444268865, 0.00012032982855174274, 0.00012032595954787361, 0.00012032207191612283, 0.00012031818991564145, 0.00012031430040680088, 0.00012031039895609374, 0.00012030652480363544, 0.00012030264207019519, 0.0001202987580317306, 0.00012029487424354465, 0.00012029097361518162, 0.0001202870867163897, 0.00012028319148689448, 0.00012027930238922596, 0.00012027540176086292, 0.00012027148928001857, 0.00012026758318127955, 0.00012026368900653002, 0.00012025977261061266, 0.00012025585294319591, 0.00012025193041545186, 0.00012024800257822522, 0.00012024410636549249, 0.0001202401927405172, 0.00012023628304849199, 0.00012023237866594934, 0.00012022843293379992, 0.00012022448679047844, 0.0001202205428460336, 0.00012021663487020478, 0.00012021269641401297, 0.00012020875901256685, 0.0001202048138524829, 0.00012020088143873254, 0.00012019696504281518, 0.00012019300770846308, 0.00012018905396739717, 0.00012018507709343407, 0.00012018110880045293, 0.00012017713332089942, 0.00012017319314851121, 0.00012016922167341593, 0.0001201652445491742, 0.00012016127332435756, 0.00012015728567053589, 0.0001201532935116987, 0.00012014931281204782, 0.00012014530291918126, 0.00012014129556485518, 0.00012013727015471292, 0.00012013324621048841, 0.00012012921540147833, 0.00012012519971644895, 0.00012012117233983165, 0.0001201171318057087, 0.00012011311170504903, 0.00012010907224354881, 0.00012010501152624123, 0.0001201009622502429, 0.00012009687242910485, 0.00012009277763814809, 0.00012008870285160551, 0.00012008459265063532, 0.00012008048212787833, 0.00012007638647882337, 0.00012007228870239997, 0.00012006817662433998, 0.00012006405163905297, 0.00012005990858007273, 0.0001200557781959179, 0.00012005162231909587, 0.00012004746880204387, 0.00012004326830411568, 0.00012003907318717825, 0.00012003484653513215, 0.0001200305754943815, 0.00012002630438212263, 0.00012002204699943493, 0.00012001777531511061, 0.00012001350665200703, 0.00012000920194877924, 0.00012000487926124346, 0.00012000053712348191, 0.00011999617365840479, 0.00011999180466932055, 0.00011998742204005041, 0.0001199830076075159, 0.00011997860985426507, 0.00011997416043635206, 0.00011996972721504249, 0.00011996529047195491, 0.00011996084073225508, 0.00011995636639374031, 0.00011995190889540263], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012112726212451871, 0.00012112324878213934, 0.00012111919910737225, 0.0001211151847458848, 0.0001211111443378911, 0.0001211071018205807, 0.00012110309340784035, 0.00012109906101051008, 0.00012109502121872126, 0.00012109099870436925, 0.00012108695431474402, 0.00012108292594644566, 0.00012107889004148761, 0.00012107483903951003, 0.00012107081590895313, 0.00012106677905598759, 0.00012106274781996652, 0.00012105871629954319, 0.00012105466887629393, 0.00012105063790097492, 0.00012104659694787692, 0.00012104255717978828, 0.00012103850691251651, 0.0001210344409557205, 0.00012103038334139056, 0.000121026345350816, 0.00012102228057902938, 0.00012101822832094184, 0.00012101417298182991, 0.00012101009960687517, 0.0001210060656690327, 0.00012100200933451288, 0.00012099795605731728, 0.00012099390647735094, 0.00012098981980659092, 0.00012098572915419936, 0.00012098164949869488, 0.00012097759811751429, 0.00012097351142305407, 0.0001209694296345327, 0.00012096533564041469, 0.00012096125356749107, 0.00012095717964743201, 0.00012095306112361975, 0.00012094894231540524, 0.00012094480172671827, 0.00012094066955159791, 0.00012093652891551056, 0.00012093243053685783, 0.00012092828881056184, 0.00012092413708278666, 0.00012091999881671808, 0.00012091584223040443, 0.00012091169185353995, 0.00012090754166627698, 0.00012090337495998319, 0.00012089920559926839, 0.00012089500680292049, 0.00012089081227260638, 0.00012088660371178118, 0.00012088240136040514, 0.0001208781824899983, 0.00012087394390103532, 0.00012086972680814254, 0.00012086547866800393, 0.00012086120872369266, 0.00012085694709814727, 0.00012085265406985321, 0.0001208483626768721, 0.00012084409270886065, 0.00012083978453614667, 0.00012083547280840455, 0.00012083117131914349, 0.00012082686544534771, 0.00012082253174753162, 0.0001208181800375729, 0.00012081381993774774, 0.00012080946118883327, 0.00012080507144007335, 0.00012080068268672129, 0.0001207962449924817, 0.00012079181056886802, 0.000120787346425219, 0.00012078284405316727, 0.0001207783381023872, 0.00012077384736712761, 0.00012076934466327325, 0.00012076484361843203, 0.00012076031211884965, 0.0001207557531033494, 0.0001207511814793493, 0.00012074657800229697, 0.00012074196931120335, 0.00012073734753760615, 0.00012073269535666817, 0.00012072804113751405, 0.00012072332271455154, 0.00012071863941526715, 0.00012071395758539438, 0.00012070924959051443, 0.00012070448736960511], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-04-30 01:50:54,790 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:129]: evaluating model ... 
[2018-04-30 01:50:59,897 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:133]: evaluated! 
[2018-04-30 01:50:59,897 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:135]: generating reports ... 
[2018-04-30 01:51:01,897 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:138]: done!
[2018-04-30 01:51:01,898 AE_BIGRAMA_1L_MINIDS_OVER_F1_1.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_1 finished!
