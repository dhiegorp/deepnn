[2017-12-14 09:31:57,986 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_1
[2017-12-14 09:31:57,986 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:146]: >> Printing header log
[2017-12-14 09:31:57,986 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_1
	layers = 9216,922
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fd65cb1beb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fd65cafe400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,986 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:148]: >> Loading dataset... 
[2017-12-14 09:32:21,531 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:21,531 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:21,531 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:57]: =======================================
[2017-12-14 09:32:21,532 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fd65cb1beb8>, 'discard_decoder_function': True}
[2017-12-14 09:32:21,579 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:55,042 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_1
[2017-12-14 10:18:55,043 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:146]: >> Printing header log
[2017-12-14 10:18:55,043 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_1
	layers = 9216,922
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f6562be2eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f6562bc5400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:55,043 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:148]: >> Loading dataset... 
[2017-12-14 10:19:17,340 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:17,341 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:17,341 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:57]: =======================================
[2017-12-14 10:19:17,341 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f6562be2eb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:17,386 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:73]: training and evaluate autoencoder
[2017-12-14 12:31:43,526 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:85]: trained and evaluated!
[2017-12-14 12:31:43,802 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:88]: Training history: 
{'val_loss': [0.00010815043119568932, 0.00010814634155332172, 0.00010814226433550093, 0.00010813817657022314, 0.00010813403526319337, 0.00010812988962991854, 0.00010812574701786444, 0.00010812159239243561, 0.00010811741552796187, 0.00010811325591483728, 0.00010810905706159729, 0.0001081048179850044, 0.00010810058824022939, 0.0001080963257089525, 0.00010809203579004154, 0.00010808777408110875, 0.00010808348914989355, 0.00010807923831020457, 0.00010807497619009973, 0.00010807067917400161, 0.00010806636833894714, 0.0001080620504782137, 0.00010805772060410552, 0.00010805339915008588, 0.00010804904401571209, 0.00010804463927253637, 0.00010804023771147479, 0.00010803586124978543, 0.00010803147645739276, 0.00010802709370298329, 0.00010802268095089108, 0.00010801827163119167, 0.00010801386615505707, 0.00010800944228344242, 0.00010800503010341568, 0.00010800062389432221, 0.00010799620189979736, 0.00010799176503157046, 0.00010798734468173382, 0.00010798289563923877, 0.00010797847185700935, 0.00010797402958991463, 0.00010796962035960044, 0.00010796517932602189, 0.00010796072685113405, 0.0001079562572857905, 0.00010795179410255228, 0.00010794730042107403, 0.00010794282685127224, 0.00010793835785799416, 0.00010793387586810382, 0.00010792942387589622, 0.00010792495751054385, 0.00010792047215976892, 0.00010791602467257685, 0.00010791151627828997, 0.00010790702987276933, 0.00010790258451294571, 0.00010789810922694752, 0.00010789364750962704, 0.00010788918261019243, 0.00010788474697548169, 0.00010788028722463624, 0.0001078758285285365, 0.00010787134450066295, 0.0001078668533577252, 0.0001078623663801391, 0.00010785792060914342, 0.00010785345072201304, 0.00010784899497562584, 0.00010784451126953912, 0.00010784005380695552, 0.00010783558703043111, 0.0001078311087232122, 0.00010782665249414477, 0.00010782220189634674, 0.00010781773683601871, 0.00010781326653771629, 0.0001078087901969724, 0.00010780430143168173, 0.0001077998363713537, 0.00010779536550098582, 0.00010779090641159107, 0.00010778642295578297, 0.00010778195887869247, 0.00010777746798603336, 0.00010777296123643467, 0.0001077684894722145, 0.00010776400782198803, 0.00010775954594377414, 0.00010775507573485694, 0.0001077505862366074, 0.00010774609518305487, 0.00010774158703904663, 0.00010773713039880715, 0.00010773268373395918, 0.00010772822391160556, 0.00010772374985912352, 0.00010771929232503175, 0.00010771482039991817, 0.0001077103458647559, 0.00010770586650279129, 0.00010770139114528492, 0.00010769688684484151, 0.00010769240805494235, 0.00010768795795770161, 0.0001076834634717563, 0.00010767900364940268, 0.0001076745471700566, 0.00010767011375209953, 0.00010766564592082942, 0.0001076611766057645, 0.00010765673019119517, 0.00010765225769401613, 0.00010764777775998605, 0.0001076433276627453, 0.00010763886726832621, 0.0001076344013141459, 0.00010762995563253545, 0.00010762548558451166, 0.00010762102290183072, 0.0001076165534258724, 0.00010761208559460229, 0.0001076076341923372, 0.00010760317551411451, 0.00010759870571636936, 0.00010759425333086676, 0.00010758982441792521, 0.00010758532853757034, 0.00010758088530511516, 0.00010757641150291176, 0.00010757195446937728, 0.00010756750380007106, 0.00010756304912630661, 0.00010755858546038815, 0.00010755415147036561, 0.00010754970750495155, 0.00010754524850494203, 0.00010754079996300426, 0.00010753634798867372, 0.0001075318881663201, 0.00010752741820768154, 0.00010752296181772069, 0.00010751852954389454, 0.00010751406857740999, 0.00010750962019636564, 0.0001075051692052726, 0.00010750070995498443, 0.0001074962384410429, 0.00010749178916614627, 0.00010748736229118793, 0.00010748292233023211, 0.00010747845523192087, 0.00010747400284641827, 0.00010746956215250358, 0.0001074651356887173, 0.00010746066458594782, 0.000107456224624992, 0.00010745175050100178, 0.00010744734504274424, 0.00010744290222146111, 0.00010743849577996606, 0.00010743403996207067, 0.00010742960809941657, 0.00010742516241780613, 0.00010742074812828798, 0.00010741634699627551, 0.00010741191513362142, 0.00010740746103192242, 0.00010740303472902955, 0.00010739859542952443, 0.00010739415857917458, 0.00010738971756347306, 0.00010738533294985085, 0.00010738091923239817, 0.00010737647526698412, 0.00010737203301776645, 0.0001073676042657183, 0.00010736317542428494, 0.0001073587816576153, 0.00010735435707091883, 0.00010734993207305028, 0.00010734551803381079, 0.00010734110145603081, 0.00010733669615866667, 0.00010733226502897145, 0.00010732784231936477, 0.00010732345068006357, 0.00010731899927779849, 0.00010731460338376042, 0.00010731017447081887, 0.00010730576631312742, 0.00010730135399008431, 0.00010729690315988469, 0.00010729251209264895, 0.00010728810132490882, 0.000107283718427483, 0.00010727931640161824, 0.00010727489655233887, 0.00010727047908070659], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0073710073710073713, 0.0073710073710073713, 0.0073710073710073713, 0.0073710073710073713, 0.0073710073710073713, 0.0073710073710073713, 0.0098280098280098278, 0.0098280098280098278, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.014742014742014743, 0.014742014742014743, 0.017199017244782434, 0.022113022204552586, 0.022113022204552586, 0.024570024496800191, 0.024570024496800191, 0.024570024496800191, 0.027027026953802647, 0.027027026953802647, 0.027027026953802647, 0.031941031867807564, 0.03439803432481002, 0.03439803432481002, 0.03439803432481002, 0.03439803432481002, 0.03439803432481002, 0.03439803432481002, 0.03439803432481002, 0.03439803432481002, 0.036855036781812477, 0.04176904169581739, 0.04176904169581739, 0.04176904169581739, 0.04176904169581739, 0.04176904169581739, 0.04176904169581739, 0.04176904169581739, 0.04176904169581739, 0.04176904169581739, 0.044226044152819846, 0.044226044152819846, 0.044226044152819846, 0.044226044152819846, 0.044226044152819846, 0.046683046609822303, 0.046683046609822303, 0.046683046609822303, 0.046683046609822303, 0.046683046609822303, 0.046683046609822303, 0.046683046609822303, 0.049140049066824759, 0.049140049066824759, 0.049140049066824759, 0.051597051523827216, 0.051597051523827216, 0.051597051523827216, 0.051597051523827216, 0.051597051523827216, 0.051597051523827216, 0.051597051523827216, 0.051597051523827216, 0.054054054237115005, 0.056511056108322424, 0.056511056108322424, 0.056511056108322424, 0.056511056108322424, 0.056511056108322424, 0.056511056108322424, 0.056511056108322424, 0.056511056108322424, 0.056511056108322424, 0.058968058565324881, 0.058968058565324881, 0.058968058565324881, 0.061425061022327337, 0.061425061022327337, 0.061425061022327337, 0.061425061022327337, 0.061425061022327337, 0.061425061022327337, 0.061425061022327337, 0.063882063735615127, 0.066339066192617577, 0.066339066192617577, 0.066339066192617577, 0.066339066192617577, 0.066339066192617577, 0.066339066192617577, 0.066339066192617577, 0.066339066192617577, 0.066339066192617577, 0.066339066192617577, 0.066339066192617577, 0.068796068063825003, 0.068796068063825003, 0.068796068063825003, 0.068796068063825003, 0.068796068063825003, 0.068796068063825003, 0.068796068063825003, 0.071253070520827466, 0.071253070520827466, 0.071253070520827466, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916], 'loss': [0.00010863342395879735, 0.00010862926194514071, 0.00010862506556621198, 0.00010862089378807803, 0.00010861670359489826, 0.00010861247593172179, 0.00010860823961797683, 0.00010860398913151964, 0.00010859972098842263, 0.00010859543511758525, 0.00010859115382088409, 0.00010858683301597008, 0.00010858244923965745, 0.0001085780766261332, 0.00010857366952883589, 0.0001085692291564751, 0.00010856481440401718, 0.00010856037242004363, 0.00010855596435103863, 0.00010855154656495669, 0.00010854711029272836, 0.00010854267027587039, 0.0001085382187407212, 0.00010853376260773561, 0.00010852932576670277, 0.00010852486728739862, 0.00010852036953688347, 0.00010851588415786631, 0.00010851142051192121, 0.00010850694634309282, 0.00010850247210316384, 0.00010849797767067498, 0.00010849348762272084, 0.00010848899996848564, 0.00010848449389920464, 0.00010847999664639344, 0.00010847549756866779, 0.00010847097929379014, 0.00010846644191656122, 0.00010846193489927271, 0.00010845740275978526, 0.00010845288865614064, 0.00010844836118559016, 0.00010844386440678272, 0.00010843933169849077, 0.00010843478076475451, 0.00010843021063386623, 0.00010842564152208603, 0.00010842104169486261, 0.00010841646367181183, 0.00010841188451115205, 0.0001084072926234915, 0.00010840272758814358, 0.00010839815205361251, 0.00010839356047405439, 0.00010838900487138117, 0.00010838439494787779, 0.0001083798059990405, 0.00010837525843073089, 0.00010837068161638969, 0.00010836611589373632, 0.00010836154569174749, 0.00010835700449878835, 0.00010835244138315563, 0.00010834787243727673, 0.0001083432849341509, 0.00010833868721624418, 0.00010833409570778662, 0.00010832954204852878, 0.000108324964025478, 0.00010832039835022502, 0.00010831580887998361, 0.00010831124668865819, 0.00010830667155703032, 0.000108302086897927, 0.00010829752115157344, 0.00010829296265747732, 0.00010828839136527984, 0.00010828381737126099, 0.00010827923100574415, 0.00010827464407142282, 0.00010827007894127414, 0.00010826551516203617, 0.00010826095579103309, 0.00010825637418925398, 0.00010825180429536759, 0.00010824720949258395, 0.0001082426016309969, 0.00010823802239923656, 0.00010823343712392837, 0.00010822888009924387, 0.00010822430980245428, 0.00010821972457454646, 0.00010821513252098459, 0.00010821052477789849, 0.00010820596661560498, 0.00010820141885769387, 0.00010819686102720299, 0.00010819228828929407, 0.00010818772659567261, 0.00010818315532717532, 0.00010817858612059437, 0.00010817400250429931, 0.00010816942341474009, 0.00010816481875267838, 0.00010816024146433343, 0.00010815569207110936, 0.00010815109918804093, 0.00010814653839502659, 0.00010814198684508545, 0.00010813745442119575, 0.0001081328902153544, 0.00010812832010816631, 0.00010812377834640268, 0.00010811920530039131, 0.00010811462457551915, 0.00010811007416318702, 0.00010810551626159558, 0.00010810095212685479, 0.00010809640690486374, 0.00010809184167991432, 0.00010808727534105606, 0.00010808270767498731, 0.00010807814356394671, 0.00010807359106599805, 0.00010806903546332482, 0.00010806446701514986, 0.00010805991698202073, 0.00010805538837386125, 0.00010805079906952116, 0.00010804626128938904, 0.00010804168632366248, 0.00010803713197709919, 0.00010803258315267964, 0.0001080280323611445, 0.00010802346971951553, 0.00010801893992634666, 0.00010801440093750498, 0.00010800984168500285, 0.00010800529421149399, 0.00010800074460496823, 0.00010799618776988523, 0.0001079916225449358, 0.00010798706514104831, 0.00010798252923323102, 0.00010797797329875516, 0.00010797342577784593, 0.00010796887538921398, 0.00010796431689511784, 0.0001079597476648367, 0.00010795520339085317, 0.00010795067314738074, 0.00010794613486954469, 0.00010794156765377949, 0.0001079370142789239, 0.00010793247614328898, 0.00010792794507030999, 0.00010792337316190763, 0.00010791883429156689, 0.00010791426122185534, 0.00010790976024811469, 0.00010790521599783135, 0.00010790071011815184, 0.00010789615579528874, 0.0001078916263339225, 0.00010788708115933184, 0.00010788256731638928, 0.0001078780684993657, 0.00010787354005710753, 0.00010786898637414949, 0.00010786445786079077, 0.00010785991851644626, 0.0001078553796935059, 0.00010785084068096403, 0.00010784635361923354, 0.00010784184527473451, 0.00010783730337076975, 0.00010783276151420537, 0.00010782823086782975, 0.00010782370022145413, 0.00010781920640517014, 0.00010781467632759902, 0.00010781015966433414, 0.00010780564238486435, 0.0001078011256504989, 0.00010779662313624604, 0.00010779209137596159, 0.00010778756608582839, 0.00010778307212734328, 0.00010777852164391059, 0.00010777402351419244, 0.00010776949187240894, 0.00010776498243770128, 0.00010776046698314596, 0.0001077559198888401, 0.00010775142803967169, 0.00010774691507363607, 0.00010774243362885004, 0.00010773793305801258, 0.0001077334090713897], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.0016286644951140066, 0.0016286644951140066, 0.0024429967426710096, 0.0024429967669400018, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0032573289902280132, 0.0040716612377850164, 0.0057003257328990227, 0.0065146579804560263, 0.0073289902280130291, 0.0073289902280130291, 0.0081433224755700327, 0.0081433224755700327, 0.0081433224755700327, 0.0081433224755700327, 0.0081433224998390241, 0.0081433224998390241, 0.0089576547231270363, 0.0097719869706840382, 0.0097719869706840382, 0.010586319218241042, 0.011400651465798045, 0.01221498373762404, 0.013843648208469055, 0.015472312703583062, 0.015472312703583062, 0.016286644951140065, 0.016286644951140065, 0.016286644975409059, 0.016286644951140065, 0.017915309446254073, 0.018729641693811076, 0.02035830618892508, 0.02035830618892508, 0.02035830618892508, 0.020358306213194073, 0.02035830618892508, 0.021172638436482084, 0.021172638436482084, 0.021986970684039087, 0.021986970684039087, 0.021986970684039087, 0.022801302931596091, 0.022801302931596091, 0.024429967426710098, 0.026872964169381109, 0.026872964169381109, 0.027687296416938109, 0.029315960912052116, 0.030130293183878113, 0.03013029315960912, 0.031758957654723127, 0.031758957654723127, 0.032573289902280131, 0.033387622174106124, 0.034201954397394138, 0.034201954421663128, 0.034201954397394138, 0.035830618892508145, 0.036644951164334139, 0.038273615635179156, 0.038273615659448146, 0.040716612402119157, 0.041530944625407164, 0.042345276872964167, 0.043159609144790168, 0.044788273639904175, 0.044788273639904175, 0.044788273639904175, 0.045602605887461171, 0.046416938110749185, 0.046416938159287172, 0.047231270358306189, 0.048045602605863193, 0.049674267149515186, 0.04967426712524619, 0.051302931596091207, 0.0521172638679172, 0.053745928387300197, 0.054560260586319222, 0.056188925129971208, 0.056188925081433222, 0.057817589600816226, 0.058631921824104233, 0.058631921824104233, 0.059446254071661236, 0.059446254095930233, 0.06026058631921824, 0.060260586343487237, 0.06026058631921824, 0.06026058631921824, 0.06026058631921824, 0.06026058631921824, 0.060260586343487237, 0.061074918566775244, 0.061074918591044233, 0.061074918591044233, 0.062703583086158241, 0.062703583086158241, 0.063517915333715244, 0.064332247557003258, 0.064332247557003258, 0.064332247581272248, 0.065146579828829251, 0.065146579804560262, 0.067589576547231273, 0.070846905440383315, 0.072475570056842284, 0.073289902280130298, 0.073289902280130298, 0.074104234551956291, 0.074104234576225281, 0.074918566775244305, 0.074918566775244305, 0.076547231318896292, 0.076547231270358312, 0.077361563517915316, 0.078175895814010299, 0.078175895668396347, 0.078175895789741309, 0.078175895765472306, 0.078175895814010299, 0.078990228013029309, 0.079804560260586313, 0.079804560284855316, 0.08061889255668131, 0.081433224779969324, 0.08143322475570032, 0.081433224779969324, 0.081433224779969324, 0.08143322475570032, 0.082247557027526327, 0.083061889250814328, 0.083061889275083331, 0.083061889250814328, 0.083061889250814328, 0.083061889250814328, 0.083061889299352321, 0.083061889250814328, 0.083876221546909324, 0.085504885993485338, 0.085504886017754328, 0.086319218265311332, 0.086319218241042342, 0.086319218241042342]}
[2017-12-14 12:31:43,802 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:92]: done!
[2017-12-14 12:31:43,802 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:152]: >> Executing classifier part ... 
[2017-12-14 12:31:43,803 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:97]: =======================================
[2017-12-14 12:31:43,803 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f6562bc5400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-14 12:31:44,008 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:110]: training ... 
[2017-12-14 13:37:59,907 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:122]: trained!
[2017-12-14 13:37:59,910 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:125]: Training history: 
{'val_loss': [0.00010815043119568932, 0.00010814634155332172, 0.00010814226433550093, 0.00010813817657022314, 0.00010813403526319337, 0.00010812988962991854, 0.00010812574701786444, 0.00010812159239243561, 0.00010811741552796187, 0.00010811325591483728, 0.00010810905706159729, 0.0001081048179850044, 0.00010810058824022939, 0.0001080963257089525, 0.00010809203579004154, 0.00010808777408110875, 0.00010808348914989355, 0.00010807923831020457, 0.00010807497619009973, 0.00010807067917400161, 0.00010806636833894714, 0.0001080620504782137, 0.00010805772060410552, 0.00010805339915008588, 0.00010804904401571209, 0.00010804463927253637, 0.00010804023771147479, 0.00010803586124978543, 0.00010803147645739276, 0.00010802709370298329, 0.00010802268095089108, 0.00010801827163119167, 0.00010801386615505707, 0.00010800944228344242, 0.00010800503010341568, 0.00010800062389432221, 0.00010799620189979736, 0.00010799176503157046, 0.00010798734468173382, 0.00010798289563923877, 0.00010797847185700935, 0.00010797402958991463, 0.00010796962035960044, 0.00010796517932602189, 0.00010796072685113405, 0.0001079562572857905, 0.00010795179410255228, 0.00010794730042107403, 0.00010794282685127224, 0.00010793835785799416, 0.00010793387586810382, 0.00010792942387589622, 0.00010792495751054385, 0.00010792047215976892, 0.00010791602467257685, 0.00010791151627828997, 0.00010790702987276933, 0.00010790258451294571, 0.00010789810922694752, 0.00010789364750962704, 0.00010788918261019243, 0.00010788474697548169, 0.00010788028722463624, 0.0001078758285285365, 0.00010787134450066295, 0.0001078668533577252, 0.0001078623663801391, 0.00010785792060914342, 0.00010785345072201304, 0.00010784899497562584, 0.00010784451126953912, 0.00010784005380695552, 0.00010783558703043111, 0.0001078311087232122, 0.00010782665249414477, 0.00010782220189634674, 0.00010781773683601871, 0.00010781326653771629, 0.0001078087901969724, 0.00010780430143168173, 0.0001077998363713537, 0.00010779536550098582, 0.00010779090641159107, 0.00010778642295578297, 0.00010778195887869247, 0.00010777746798603336, 0.00010777296123643467, 0.0001077684894722145, 0.00010776400782198803, 0.00010775954594377414, 0.00010775507573485694, 0.0001077505862366074, 0.00010774609518305487, 0.00010774158703904663, 0.00010773713039880715, 0.00010773268373395918, 0.00010772822391160556, 0.00010772374985912352, 0.00010771929232503175, 0.00010771482039991817, 0.0001077103458647559, 0.00010770586650279129, 0.00010770139114528492, 0.00010769688684484151, 0.00010769240805494235, 0.00010768795795770161, 0.0001076834634717563, 0.00010767900364940268, 0.0001076745471700566, 0.00010767011375209953, 0.00010766564592082942, 0.0001076611766057645, 0.00010765673019119517, 0.00010765225769401613, 0.00010764777775998605, 0.0001076433276627453, 0.00010763886726832621, 0.0001076344013141459, 0.00010762995563253545, 0.00010762548558451166, 0.00010762102290183072, 0.0001076165534258724, 0.00010761208559460229, 0.0001076076341923372, 0.00010760317551411451, 0.00010759870571636936, 0.00010759425333086676, 0.00010758982441792521, 0.00010758532853757034, 0.00010758088530511516, 0.00010757641150291176, 0.00010757195446937728, 0.00010756750380007106, 0.00010756304912630661, 0.00010755858546038815, 0.00010755415147036561, 0.00010754970750495155, 0.00010754524850494203, 0.00010754079996300426, 0.00010753634798867372, 0.0001075318881663201, 0.00010752741820768154, 0.00010752296181772069, 0.00010751852954389454, 0.00010751406857740999, 0.00010750962019636564, 0.0001075051692052726, 0.00010750070995498443, 0.0001074962384410429, 0.00010749178916614627, 0.00010748736229118793, 0.00010748292233023211, 0.00010747845523192087, 0.00010747400284641827, 0.00010746956215250358, 0.0001074651356887173, 0.00010746066458594782, 0.000107456224624992, 0.00010745175050100178, 0.00010744734504274424, 0.00010744290222146111, 0.00010743849577996606, 0.00010743403996207067, 0.00010742960809941657, 0.00010742516241780613, 0.00010742074812828798, 0.00010741634699627551, 0.00010741191513362142, 0.00010740746103192242, 0.00010740303472902955, 0.00010739859542952443, 0.00010739415857917458, 0.00010738971756347306, 0.00010738533294985085, 0.00010738091923239817, 0.00010737647526698412, 0.00010737203301776645, 0.0001073676042657183, 0.00010736317542428494, 0.0001073587816576153, 0.00010735435707091883, 0.00010734993207305028, 0.00010734551803381079, 0.00010734110145603081, 0.00010733669615866667, 0.00010733226502897145, 0.00010732784231936477, 0.00010732345068006357, 0.00010731899927779849, 0.00010731460338376042, 0.00010731017447081887, 0.00010730576631312742, 0.00010730135399008431, 0.00010729690315988469, 0.00010729251209264895, 0.00010728810132490882, 0.000107283718427483, 0.00010727931640161824, 0.00010727489655233887, 0.00010727047908070659], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0073710073710073713, 0.0073710073710073713, 0.0073710073710073713, 0.0073710073710073713, 0.0073710073710073713, 0.0073710073710073713, 0.0098280098280098278, 0.0098280098280098278, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.012285012285012284, 0.014742014742014743, 0.014742014742014743, 0.017199017244782434, 0.022113022204552586, 0.022113022204552586, 0.024570024496800191, 0.024570024496800191, 0.024570024496800191, 0.027027026953802647, 0.027027026953802647, 0.027027026953802647, 0.031941031867807564, 0.03439803432481002, 0.03439803432481002, 0.03439803432481002, 0.03439803432481002, 0.03439803432481002, 0.03439803432481002, 0.03439803432481002, 0.03439803432481002, 0.036855036781812477, 0.04176904169581739, 0.04176904169581739, 0.04176904169581739, 0.04176904169581739, 0.04176904169581739, 0.04176904169581739, 0.04176904169581739, 0.04176904169581739, 0.04176904169581739, 0.044226044152819846, 0.044226044152819846, 0.044226044152819846, 0.044226044152819846, 0.044226044152819846, 0.046683046609822303, 0.046683046609822303, 0.046683046609822303, 0.046683046609822303, 0.046683046609822303, 0.046683046609822303, 0.046683046609822303, 0.049140049066824759, 0.049140049066824759, 0.049140049066824759, 0.051597051523827216, 0.051597051523827216, 0.051597051523827216, 0.051597051523827216, 0.051597051523827216, 0.051597051523827216, 0.051597051523827216, 0.051597051523827216, 0.054054054237115005, 0.056511056108322424, 0.056511056108322424, 0.056511056108322424, 0.056511056108322424, 0.056511056108322424, 0.056511056108322424, 0.056511056108322424, 0.056511056108322424, 0.056511056108322424, 0.058968058565324881, 0.058968058565324881, 0.058968058565324881, 0.061425061022327337, 0.061425061022327337, 0.061425061022327337, 0.061425061022327337, 0.061425061022327337, 0.061425061022327337, 0.061425061022327337, 0.063882063735615127, 0.066339066192617577, 0.066339066192617577, 0.066339066192617577, 0.066339066192617577, 0.066339066192617577, 0.066339066192617577, 0.066339066192617577, 0.066339066192617577, 0.066339066192617577, 0.066339066192617577, 0.066339066192617577, 0.068796068063825003, 0.068796068063825003, 0.068796068063825003, 0.068796068063825003, 0.068796068063825003, 0.068796068063825003, 0.068796068063825003, 0.071253070520827466, 0.071253070520827466, 0.071253070520827466, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916, 0.073710072977829916], 'loss': [0.00010863342395879735, 0.00010862926194514071, 0.00010862506556621198, 0.00010862089378807803, 0.00010861670359489826, 0.00010861247593172179, 0.00010860823961797683, 0.00010860398913151964, 0.00010859972098842263, 0.00010859543511758525, 0.00010859115382088409, 0.00010858683301597008, 0.00010858244923965745, 0.0001085780766261332, 0.00010857366952883589, 0.0001085692291564751, 0.00010856481440401718, 0.00010856037242004363, 0.00010855596435103863, 0.00010855154656495669, 0.00010854711029272836, 0.00010854267027587039, 0.0001085382187407212, 0.00010853376260773561, 0.00010852932576670277, 0.00010852486728739862, 0.00010852036953688347, 0.00010851588415786631, 0.00010851142051192121, 0.00010850694634309282, 0.00010850247210316384, 0.00010849797767067498, 0.00010849348762272084, 0.00010848899996848564, 0.00010848449389920464, 0.00010847999664639344, 0.00010847549756866779, 0.00010847097929379014, 0.00010846644191656122, 0.00010846193489927271, 0.00010845740275978526, 0.00010845288865614064, 0.00010844836118559016, 0.00010844386440678272, 0.00010843933169849077, 0.00010843478076475451, 0.00010843021063386623, 0.00010842564152208603, 0.00010842104169486261, 0.00010841646367181183, 0.00010841188451115205, 0.0001084072926234915, 0.00010840272758814358, 0.00010839815205361251, 0.00010839356047405439, 0.00010838900487138117, 0.00010838439494787779, 0.0001083798059990405, 0.00010837525843073089, 0.00010837068161638969, 0.00010836611589373632, 0.00010836154569174749, 0.00010835700449878835, 0.00010835244138315563, 0.00010834787243727673, 0.0001083432849341509, 0.00010833868721624418, 0.00010833409570778662, 0.00010832954204852878, 0.000108324964025478, 0.00010832039835022502, 0.00010831580887998361, 0.00010831124668865819, 0.00010830667155703032, 0.000108302086897927, 0.00010829752115157344, 0.00010829296265747732, 0.00010828839136527984, 0.00010828381737126099, 0.00010827923100574415, 0.00010827464407142282, 0.00010827007894127414, 0.00010826551516203617, 0.00010826095579103309, 0.00010825637418925398, 0.00010825180429536759, 0.00010824720949258395, 0.0001082426016309969, 0.00010823802239923656, 0.00010823343712392837, 0.00010822888009924387, 0.00010822430980245428, 0.00010821972457454646, 0.00010821513252098459, 0.00010821052477789849, 0.00010820596661560498, 0.00010820141885769387, 0.00010819686102720299, 0.00010819228828929407, 0.00010818772659567261, 0.00010818315532717532, 0.00010817858612059437, 0.00010817400250429931, 0.00010816942341474009, 0.00010816481875267838, 0.00010816024146433343, 0.00010815569207110936, 0.00010815109918804093, 0.00010814653839502659, 0.00010814198684508545, 0.00010813745442119575, 0.0001081328902153544, 0.00010812832010816631, 0.00010812377834640268, 0.00010811920530039131, 0.00010811462457551915, 0.00010811007416318702, 0.00010810551626159558, 0.00010810095212685479, 0.00010809640690486374, 0.00010809184167991432, 0.00010808727534105606, 0.00010808270767498731, 0.00010807814356394671, 0.00010807359106599805, 0.00010806903546332482, 0.00010806446701514986, 0.00010805991698202073, 0.00010805538837386125, 0.00010805079906952116, 0.00010804626128938904, 0.00010804168632366248, 0.00010803713197709919, 0.00010803258315267964, 0.0001080280323611445, 0.00010802346971951553, 0.00010801893992634666, 0.00010801440093750498, 0.00010800984168500285, 0.00010800529421149399, 0.00010800074460496823, 0.00010799618776988523, 0.0001079916225449358, 0.00010798706514104831, 0.00010798252923323102, 0.00010797797329875516, 0.00010797342577784593, 0.00010796887538921398, 0.00010796431689511784, 0.0001079597476648367, 0.00010795520339085317, 0.00010795067314738074, 0.00010794613486954469, 0.00010794156765377949, 0.0001079370142789239, 0.00010793247614328898, 0.00010792794507030999, 0.00010792337316190763, 0.00010791883429156689, 0.00010791426122185534, 0.00010790976024811469, 0.00010790521599783135, 0.00010790071011815184, 0.00010789615579528874, 0.0001078916263339225, 0.00010788708115933184, 0.00010788256731638928, 0.0001078780684993657, 0.00010787354005710753, 0.00010786898637414949, 0.00010786445786079077, 0.00010785991851644626, 0.0001078553796935059, 0.00010785084068096403, 0.00010784635361923354, 0.00010784184527473451, 0.00010783730337076975, 0.00010783276151420537, 0.00010782823086782975, 0.00010782370022145413, 0.00010781920640517014, 0.00010781467632759902, 0.00010781015966433414, 0.00010780564238486435, 0.0001078011256504989, 0.00010779662313624604, 0.00010779209137596159, 0.00010778756608582839, 0.00010778307212734328, 0.00010777852164391059, 0.00010777402351419244, 0.00010776949187240894, 0.00010776498243770128, 0.00010776046698314596, 0.0001077559198888401, 0.00010775142803967169, 0.00010774691507363607, 0.00010774243362885004, 0.00010773793305801258, 0.0001077334090713897], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.0016286644951140066, 0.0016286644951140066, 0.0024429967426710096, 0.0024429967669400018, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0032573289902280132, 0.0040716612377850164, 0.0057003257328990227, 0.0065146579804560263, 0.0073289902280130291, 0.0073289902280130291, 0.0081433224755700327, 0.0081433224755700327, 0.0081433224755700327, 0.0081433224755700327, 0.0081433224998390241, 0.0081433224998390241, 0.0089576547231270363, 0.0097719869706840382, 0.0097719869706840382, 0.010586319218241042, 0.011400651465798045, 0.01221498373762404, 0.013843648208469055, 0.015472312703583062, 0.015472312703583062, 0.016286644951140065, 0.016286644951140065, 0.016286644975409059, 0.016286644951140065, 0.017915309446254073, 0.018729641693811076, 0.02035830618892508, 0.02035830618892508, 0.02035830618892508, 0.020358306213194073, 0.02035830618892508, 0.021172638436482084, 0.021172638436482084, 0.021986970684039087, 0.021986970684039087, 0.021986970684039087, 0.022801302931596091, 0.022801302931596091, 0.024429967426710098, 0.026872964169381109, 0.026872964169381109, 0.027687296416938109, 0.029315960912052116, 0.030130293183878113, 0.03013029315960912, 0.031758957654723127, 0.031758957654723127, 0.032573289902280131, 0.033387622174106124, 0.034201954397394138, 0.034201954421663128, 0.034201954397394138, 0.035830618892508145, 0.036644951164334139, 0.038273615635179156, 0.038273615659448146, 0.040716612402119157, 0.041530944625407164, 0.042345276872964167, 0.043159609144790168, 0.044788273639904175, 0.044788273639904175, 0.044788273639904175, 0.045602605887461171, 0.046416938110749185, 0.046416938159287172, 0.047231270358306189, 0.048045602605863193, 0.049674267149515186, 0.04967426712524619, 0.051302931596091207, 0.0521172638679172, 0.053745928387300197, 0.054560260586319222, 0.056188925129971208, 0.056188925081433222, 0.057817589600816226, 0.058631921824104233, 0.058631921824104233, 0.059446254071661236, 0.059446254095930233, 0.06026058631921824, 0.060260586343487237, 0.06026058631921824, 0.06026058631921824, 0.06026058631921824, 0.06026058631921824, 0.060260586343487237, 0.061074918566775244, 0.061074918591044233, 0.061074918591044233, 0.062703583086158241, 0.062703583086158241, 0.063517915333715244, 0.064332247557003258, 0.064332247557003258, 0.064332247581272248, 0.065146579828829251, 0.065146579804560262, 0.067589576547231273, 0.070846905440383315, 0.072475570056842284, 0.073289902280130298, 0.073289902280130298, 0.074104234551956291, 0.074104234576225281, 0.074918566775244305, 0.074918566775244305, 0.076547231318896292, 0.076547231270358312, 0.077361563517915316, 0.078175895814010299, 0.078175895668396347, 0.078175895789741309, 0.078175895765472306, 0.078175895814010299, 0.078990228013029309, 0.079804560260586313, 0.079804560284855316, 0.08061889255668131, 0.081433224779969324, 0.08143322475570032, 0.081433224779969324, 0.081433224779969324, 0.08143322475570032, 0.082247557027526327, 0.083061889250814328, 0.083061889275083331, 0.083061889250814328, 0.083061889250814328, 0.083061889250814328, 0.083061889299352321, 0.083061889250814328, 0.083876221546909324, 0.085504885993485338, 0.085504886017754328, 0.086319218265311332, 0.086319218241042342, 0.086319218241042342]}
[2017-12-14 13:37:59,910 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:129]: evaluating model ... 
[2017-12-14 13:38:01,500 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:133]: evaluated! 
[2017-12-14 13:38:01,501 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:135]: generating reports ... 
[2017-12-14 13:38:04,489 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:138]: done!
[2017-12-14 13:38:04,489 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_1 finished!
[2018-04-29 11:38:17,520 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_1 was already executed!
[2018-04-29 11:42:07,387 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_1 was already executed!
[2018-04-29 11:48:16,020 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_1 was already executed!
[2018-04-29 13:12:02,681 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_1
[2018-04-29 13:12:02,681 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:146]: >> Printing header log
[2018-04-29 13:12:02,681 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_1
	layers = 9216,922
	using GLOBAL obj = 
		{'numpy_seed': 666, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'epochs': 200, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_dir': '/home/dhiego/malware_dataset/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'autoencoder_configs': {'output_layer_activation': 'relu', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7effd9119828>}, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'shuffle_batches': True, 'mlp_configs': {'activation': 'sigmoid', 'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7effd9119898>}, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'batch': 32, 'store_history': True}
	=======================================
	
[2018-04-29 13:12:02,681 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,682 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,682 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,682 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:57]: =======================================
[2018-04-29 13:12:20,682 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:62]: setting configurations for autoencoder: 
	 {'output_layer_activation': 'relu', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7effd9119828>}
[2018-04-29 13:12:20,730 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,875 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_1
[2018-04-29 13:14:12,875 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:146]: >> Printing header log
[2018-04-29 13:14:12,875 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_1
	layers = 9216,922
	using GLOBAL obj = 
		{'fullds_data_dir': '/home/dhiego/malware_dataset/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'store_history': True, 'batch': 32, 'shuffle_batches': True, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'data_dir': '/home/dhiego/malware_dataset/', 'mlp_configs': {'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid', 'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7fa8b0b2a898>}, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'autoencoder_configs': {'loss_function': 'mse', 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fa8b0b2a828>}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'epochs': 200, 'numpy_seed': 666}
	=======================================
	
[2018-04-29 13:14:12,875 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:148]: >> Loading dataset... 
[2018-04-29 13:14:30,820 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:30,820 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:30,820 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:57]: =======================================
[2018-04-29 13:14:30,820 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:62]: setting configurations for autoencoder: 
	 {'loss_function': 'mse', 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fa8b0b2a828>}
[2018-04-29 13:14:30,867 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,783 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_1
[2018-04-29 13:16:33,784 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:146]: >> Printing header log
[2018-04-29 13:16:33,784 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_1
	layers = 9216,922
	using GLOBAL obj = 
		{'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'epochs': 200, 'data_dir': '/home/dhiego/malware_dataset/', 'batch': 32, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'autoencoder_configs': {'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f1d33117828>, 'loss_function': 'mse', 'output_layer_activation': 'relu'}, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'mlp_configs': {'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7f1d33117898>, 'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid'}, 'shuffle_batches': True, 'numpy_seed': 666, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'store_history': True}
	=======================================
	
[2018-04-29 13:16:33,784 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:148]: >> Loading dataset... 
[2018-04-29 13:16:58,509 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:58,509 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:58,509 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:57]: =======================================
[2018-04-29 13:16:58,510 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f1d33117828>, 'loss_function': 'mse', 'output_layer_activation': 'relu'}
[2018-04-29 13:16:58,559 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:21,121 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_1
[2018-04-29 14:30:21,121 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:146]: >> Printing header log
[2018-04-29 14:30:21,121 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_1
	layers = 9216,922
	using GLOBAL obj = 
		{'numpy_seed': 666, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f86b862c898>}, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'shuffle_batches': True, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'mlp_configs': {'use_last_dim_as_classifier': False, 'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'optimizer': <keras.optimizers.SGD object at 0x7f86b862c908>}, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'data_dir': '/home/dhiego/malware_dataset/', 'batch': 32, 'epochs': 200, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'store_history': True, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s'}
	=======================================
	
[2018-04-29 14:30:21,122 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:148]: >> Loading dataset... 
[2018-04-29 14:30:40,212 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:40,213 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:40,213 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:57]: =======================================
[2018-04-29 14:30:40,213 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f86b862c898>}
[2018-04-29 14:30:40,322 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:73]: training and evaluate autoencoder
[2018-04-29 15:37:12,986 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:85]: trained and evaluated!
[2018-04-29 15:37:12,988 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:88]: Training history: 
{'loss': [0.00010874042025724038, 0.00010873695211397811, 0.00010873345960692293, 0.0001087299785233582, 0.00010872649895660549, 0.00010872301884474846, 0.00010871952470238033, 0.00010871599967866766, 0.00010871246287596172, 0.00010870891419946177, 0.00010870536049852202, 0.0001087017598475105, 0.00010869811812407376, 0.00010869445559187225, 0.00010869079173246022, 0.00010868707758124996, 0.00010868332745315483, 0.00010867957526314336, 0.00010867582155631988, 0.00010867202665857024, 0.00010866819009589067, 0.00010866435061808803, 0.00010866049578256377, 0.00010865663743941175, 0.00010865273638852153, 0.00010864878831645898, 0.00010864485714263025, 0.00010864090089400296, 0.00010863696780045902, 0.00010863302970617548, 0.00010862906523358307, 0.0001086250966608582, 0.0001086211481147919, 0.00010861716811857656, 0.00010861318911776912, 0.00010860920781804346, 0.00010860520075621381, 0.00010860120516527499, 0.00010859719457211737, 0.00010859317250806893, 0.00010858913916273116, 0.00010858510726310483, 0.00010858106239947584, 0.00010857703080795196, 0.0001085730118486281, 0.00010856897082442952, 0.00010856491050827818, 0.00010856085270434673, 0.00010855678148610906, 0.00010855273498716713, 0.00010854865142113168, 0.00010854457366164222, 0.00010854048912389907, 0.00010853640015422084, 0.00010853229089718196, 0.0001085281877547915, 0.00010852405994050569, 0.00010851994409481464, 0.00010851583749219678, 0.0001085117634299366, 0.00010850765798862794, 0.00010850355605494704, 0.00010849942437753063, 0.00010849531310597581, 0.0001084911917855414, 0.0001084870771959603, 0.00010848294435723469, 0.00010847879582898485, 0.00010847464865164571, 0.00010847050526633658, 0.00010846638088857798, 0.00010846223532285158, 0.00010845810295812973, 0.00010845396082893056, 0.00010844980744214225, 0.00010844565678087552, 0.00010844152553006249, 0.00010843739017911698, 0.00010843323643682586, 0.00010842907231385253, 0.00010842491437662817, 0.00010842072010701613, 0.000108416548684385, 0.00010841238818754039, 0.00010840823039251717, 0.00010840407477791121, 0.00010839991067863807, 0.00010839573292805682, 0.00010839154413318813, 0.0001083873573528354, 0.00010838314879201019, 0.00010837897158653327, 0.00010837479978469914, 0.00010837061080022894, 0.00010836644926057607, 0.00010836224283276775, 0.00010835808280992689, 0.00010835391370991415, 0.0001083497292521798, 0.0001083455412157171, 0.00010834138420280008], 'val_loss': [0.00010833000758601609, 0.00010832665129581865, 0.00010832330310392291, 0.00010831994690311071, 0.00010831656168785326, 0.00010831317770611197, 0.00010830975888200855, 0.00010830631783673728, 0.00010830284320049706, 0.00010829937412401808, 0.00010829581414276153, 0.00010829222169678993, 0.00010828863121729335, 0.0001082850391824938, 0.00010828140488635813, 0.00010827773862606468, 0.00010827408046407296, 0.0001082704004027002, 0.00010826665551915962, 0.0001082628870915498, 0.00010825911516003901, 0.00010825532098948332, 0.00010825152436977238, 0.00010824768990435562, 0.00010824380736756287, 0.00010823994764188049, 0.00010823605219786071, 0.00010823218290795886, 0.00010822831402922905, 0.00010822442602206031, 0.00010822052003058355, 0.00010821664666471527, 0.00010821273470230525, 0.00010820883264377856, 0.00010820492902994889, 0.00010820099443519896, 0.00010819707512532313, 0.0001081931468232933, 0.00010818920682967556, 0.00010818525964948543, 0.00010818130968047617, 0.00010817734222771618, 0.00010817338866542073, 0.00010816944377349245, 0.00010816548620673876, 0.00010816150337971944, 0.00010815752423537153, 0.00010815353111117387, 0.00010814957150643697, 0.00010814556964036393, 0.00010814157619437945, 0.00010813756272610374, 0.00010813355399524513, 0.00010812951371140082, 0.00010812547882642432, 0.000108121415087896, 0.00010811735993034964, 0.00010811330995714654, 0.00010810928637046294, 0.00010810522819169584, 0.0001081011792553614, 0.0001080971013760899, 0.00010809304255374915, 0.00010808898004873699, 0.00010808492285320742, 0.0001080808403974122, 0.00010807674805561071, 0.00010807265816296448, 0.00010806857138092422, 0.00010806450143906102, 0.0001080604108849641, 0.00010805632090293264, 0.00010805222593320543, 0.00010804812606516768, 0.0001080440279848345, 0.00010803994610110477, 0.0001080358578352697, 0.00010803175069127433, 0.00010802763913164868, 0.00010802353230944013, 0.00010801939034095968, 0.00010801527223833528, 0.000108011162394906, 0.0001080070602386064, 0.00010800295922643773, 0.00010799885453159763, 0.00010799473226362158, 0.00010799059789288555, 0.0001079864703869351, 0.00010798232425310299, 0.0001079782069728227, 0.00010797408854841148, 0.0001079699554111916, 0.00010796584744485214, 0.00010796169918365159, 0.00010795759473909013, 0.00010795348089120262, 0.00010794935208022783, 0.00010794521869272931, 0.00010794111694760176, 0.00010793698683160263], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-04-29 15:37:12,988 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:92]: done!
[2018-04-29 15:37:12,988 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:152]: >> Executing classifier part ... 
[2018-04-29 15:37:12,988 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:97]: =======================================
[2018-04-29 15:37:12,988 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:101]: setting configurations for classifier: 
	 {'use_last_dim_as_classifier': False, 'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'optimizer': <keras.optimizers.SGD object at 0x7f86b862c908>}
[2018-04-29 15:37:13,119 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:110]: training ... 
[2018-04-29 16:44:32,571 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:122]: trained!
[2018-04-29 16:44:32,573 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:125]: Training history: 
{'loss': [0.00010874042025724038, 0.00010873695211397811, 0.00010873345960692293, 0.0001087299785233582, 0.00010872649895660549, 0.00010872301884474846, 0.00010871952470238033, 0.00010871599967866766, 0.00010871246287596172, 0.00010870891419946177, 0.00010870536049852202, 0.0001087017598475105, 0.00010869811812407376, 0.00010869445559187225, 0.00010869079173246022, 0.00010868707758124996, 0.00010868332745315483, 0.00010867957526314336, 0.00010867582155631988, 0.00010867202665857024, 0.00010866819009589067, 0.00010866435061808803, 0.00010866049578256377, 0.00010865663743941175, 0.00010865273638852153, 0.00010864878831645898, 0.00010864485714263025, 0.00010864090089400296, 0.00010863696780045902, 0.00010863302970617548, 0.00010862906523358307, 0.0001086250966608582, 0.0001086211481147919, 0.00010861716811857656, 0.00010861318911776912, 0.00010860920781804346, 0.00010860520075621381, 0.00010860120516527499, 0.00010859719457211737, 0.00010859317250806893, 0.00010858913916273116, 0.00010858510726310483, 0.00010858106239947584, 0.00010857703080795196, 0.0001085730118486281, 0.00010856897082442952, 0.00010856491050827818, 0.00010856085270434673, 0.00010855678148610906, 0.00010855273498716713, 0.00010854865142113168, 0.00010854457366164222, 0.00010854048912389907, 0.00010853640015422084, 0.00010853229089718196, 0.0001085281877547915, 0.00010852405994050569, 0.00010851994409481464, 0.00010851583749219678, 0.0001085117634299366, 0.00010850765798862794, 0.00010850355605494704, 0.00010849942437753063, 0.00010849531310597581, 0.0001084911917855414, 0.0001084870771959603, 0.00010848294435723469, 0.00010847879582898485, 0.00010847464865164571, 0.00010847050526633658, 0.00010846638088857798, 0.00010846223532285158, 0.00010845810295812973, 0.00010845396082893056, 0.00010844980744214225, 0.00010844565678087552, 0.00010844152553006249, 0.00010843739017911698, 0.00010843323643682586, 0.00010842907231385253, 0.00010842491437662817, 0.00010842072010701613, 0.000108416548684385, 0.00010841238818754039, 0.00010840823039251717, 0.00010840407477791121, 0.00010839991067863807, 0.00010839573292805682, 0.00010839154413318813, 0.0001083873573528354, 0.00010838314879201019, 0.00010837897158653327, 0.00010837479978469914, 0.00010837061080022894, 0.00010836644926057607, 0.00010836224283276775, 0.00010835808280992689, 0.00010835391370991415, 0.0001083497292521798, 0.0001083455412157171, 0.00010834138420280008], 'val_loss': [0.00010833000758601609, 0.00010832665129581865, 0.00010832330310392291, 0.00010831994690311071, 0.00010831656168785326, 0.00010831317770611197, 0.00010830975888200855, 0.00010830631783673728, 0.00010830284320049706, 0.00010829937412401808, 0.00010829581414276153, 0.00010829222169678993, 0.00010828863121729335, 0.0001082850391824938, 0.00010828140488635813, 0.00010827773862606468, 0.00010827408046407296, 0.0001082704004027002, 0.00010826665551915962, 0.0001082628870915498, 0.00010825911516003901, 0.00010825532098948332, 0.00010825152436977238, 0.00010824768990435562, 0.00010824380736756287, 0.00010823994764188049, 0.00010823605219786071, 0.00010823218290795886, 0.00010822831402922905, 0.00010822442602206031, 0.00010822052003058355, 0.00010821664666471527, 0.00010821273470230525, 0.00010820883264377856, 0.00010820492902994889, 0.00010820099443519896, 0.00010819707512532313, 0.0001081931468232933, 0.00010818920682967556, 0.00010818525964948543, 0.00010818130968047617, 0.00010817734222771618, 0.00010817338866542073, 0.00010816944377349245, 0.00010816548620673876, 0.00010816150337971944, 0.00010815752423537153, 0.00010815353111117387, 0.00010814957150643697, 0.00010814556964036393, 0.00010814157619437945, 0.00010813756272610374, 0.00010813355399524513, 0.00010812951371140082, 0.00010812547882642432, 0.000108121415087896, 0.00010811735993034964, 0.00010811330995714654, 0.00010810928637046294, 0.00010810522819169584, 0.0001081011792553614, 0.0001080971013760899, 0.00010809304255374915, 0.00010808898004873699, 0.00010808492285320742, 0.0001080808403974122, 0.00010807674805561071, 0.00010807265816296448, 0.00010806857138092422, 0.00010806450143906102, 0.0001080604108849641, 0.00010805632090293264, 0.00010805222593320543, 0.00010804812606516768, 0.0001080440279848345, 0.00010803994610110477, 0.0001080358578352697, 0.00010803175069127433, 0.00010802763913164868, 0.00010802353230944013, 0.00010801939034095968, 0.00010801527223833528, 0.000108011162394906, 0.0001080070602386064, 0.00010800295922643773, 0.00010799885453159763, 0.00010799473226362158, 0.00010799059789288555, 0.0001079864703869351, 0.00010798232425310299, 0.0001079782069728227, 0.00010797408854841148, 0.0001079699554111916, 0.00010796584744485214, 0.00010796169918365159, 0.00010795759473909013, 0.00010795348089120262, 0.00010794935208022783, 0.00010794521869272931, 0.00010794111694760176, 0.00010793698683160263], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-04-29 16:44:32,573 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:129]: evaluating model ... 
[2018-04-29 16:44:35,226 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:133]: evaluated! 
[2018-04-29 16:44:35,226 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:135]: generating reports ... 
[2018-04-29 16:44:40,234 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:138]: done!
[2018-04-29 16:44:40,234 AE_BIGRAMA_1L_MINIDS_UNDER_F0_1.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_1 finished!
