[2017-12-14 09:31:57,850 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_4
[2017-12-14 09:31:57,850 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:146]: >> Printing header log
[2017-12-14 09:31:57,850 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_4
	layers = 9216,12902
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7faba2d23eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7faba2d06400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,850 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:148]: >> Loading dataset... 
[2017-12-14 09:32:23,738 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:23,738 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:23,738 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:57]: =======================================
[2017-12-14 09:32:23,738 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7faba2d23eb8>, 'discard_decoder_function': True}
[2017-12-14 09:32:23,783 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:54,863 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_4
[2017-12-14 10:18:54,863 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:146]: >> Printing header log
[2017-12-14 10:18:54,863 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_4
	layers = 9216,12902
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fcf620e0ef0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fcf620c4438>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:54,863 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:148]: >> Loading dataset... 
[2017-12-14 10:19:17,347 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:17,347 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:17,347 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:57]: =======================================
[2017-12-14 10:19:17,347 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fcf620e0ef0>, 'discard_decoder_function': True}
[2017-12-14 10:19:17,388 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:73]: training and evaluate autoencoder
[2017-12-15 02:12:24,190 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:85]: trained and evaluated!
[2017-12-15 02:12:24,230 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:88]: Training history: 
{'val_loss': [0.00011946200016649733, 0.00011945811811238482, 0.00011945419061482205, 0.00011945027652504358, 0.00011944635696488911, 0.00011944239423166919, 0.00011943847842569433, 0.00011943452893936531, 0.0001194305689055793, 0.00011942662447845421, 0.00011942267155973241, 0.0001194186901092456, 0.0001194146936062863, 0.00011941071313903701, 0.00011940671223832446, 0.00011940269646390984, 0.00011939866401021156, 0.0001193946335229883, 0.00011939056796100133, 0.00011938650967497196, 0.00011938243038341385, 0.00011937833612876846, 0.00011937425658693171, 0.00011937014631445334, 0.00011936602157944497, 0.00011936188801317601, 0.0001193576861029612, 0.00011935351048988068, 0.00011934927795628653, 0.00011934504182940619, 0.00011934081812707263, 0.00011933656950413733, 0.00011933230287901695, 0.00011932804043712529, 0.00011932374284896171, 0.0001193194451714129, 0.00011931510911184691, 0.00011931074410934385, 0.00011930635286333767, 0.00011930193505204152, 0.0001192975047446904, 0.00011929299547442828, 0.00011928849678737723, 0.00011928394909934381, 0.00011927940141131039, 0.00011927479830443521, 0.00011927019446460113, 0.00011926554426958748, 0.00011926092251695358, 0.0001192563042860977, 0.00011925163995034086, 0.00011924693074319923, 0.00011924215826919275, 0.00011923735047014388, 0.00011923251194045341, 0.00011922768247442512, 0.00011922279873368598, 0.0001192178939874181, 0.00011921292457987579, 0.00011920794283717193, 0.00011920291490018189, 0.00011919784600688007, 0.0001191927563404511, 0.00011918767223378336, 0.00011918253744569096, 0.0001191774048564752, 0.00011917222618023551, 0.00011916697090085486, 0.00011916167539812131, 0.00011915638170096936, 0.00011915103331793452, 0.00011914564610595635, 0.00011914023543929416, 0.00011913475683985817, 0.00011912921751209784, 0.00011912366731509369, 0.00011911804698643913, 0.0001191123210938295, 0.00011910660769727483, 0.00011910086430303743, 0.00011909503222077613, 0.00011908919343462269, 0.00011908329604751333, 0.00011907736826942621, 0.00011907139199091401, 0.00011906538914711185, 0.00011905926981647968, 0.00011905311739543586, 0.00011904694863477223, 0.00011904070997485979, 0.00011903446568367792, 0.00011902817773675038, 0.00011902184949496176, 0.00011901547327118237, 0.00011900902773809668, 0.00011900260936024345, 0.00011899615737354425, 0.0001189896271568929, 0.00011898307732912238, 0.00011897647885791042, 0.00011896986077557929, 0.00011896317949789148, 0.00011895642644386505, 0.00011894965073962167, 0.00011894285730134893, 0.00011893596469910349, 0.00011892903294612788, 0.00011892204405811412, 0.00011891496026086454, 0.00011890780461576816, 0.00011890065755165371, 0.00011889346536587585, 0.0001188862462930212, 0.00011887896173654802, 0.00011887170792859349, 0.00011886440996433602, 0.00011885709442694257, 0.00011884979108169432, 0.00011884241612100085, 0.00011883497855489322, 0.00011882751074082422, 0.00011881997613811233, 0.00011881246720683813, 0.0001188049142086462, 0.00011879727661627384, 0.00011878952727448859, 0.0001187818187281217, 0.00011877400383120975, 0.0001187662490190485, 0.0001187584625645163, 0.00011875070228197907, 0.00011874282157966176, 0.00011873497446831339, 0.00011872711387767253, 0.00011871926326242319, 0.00011871147600342394, 0.00011870366707744525, 0.00011869584286659247, 0.00011868794737995833, 0.00011868002386211649, 0.00011867202284728138, 0.00011866407521330484, 0.00011865608204649281, 0.00011864808249757545, 0.00011864007642353641, 0.00011863197880114614, 0.00011862386564360314, 0.00011861578895523343, 0.00011860769746466987, 0.00011859958545125778, 0.00011859145039433376, 0.00011858327135987722, 0.00011857508392320918, 0.00011856685438609844, 0.00011855858741445394, 0.0001185503486527876, 0.00011854212574806083, 0.00011853385175073735, 0.00011852554260714195, 0.00011851727695839882, 0.00011850894525397171, 0.00011850063146234442, 0.0001184922815412077, 0.00011848398761097822, 0.00011847568265061153, 0.00011846734712049663, 0.00011845902982496837, 0.00011845068791274815, 0.00011844234297930719, 0.00011843402870499967, 0.0001184256856486485, 0.00011841736892518572, 0.00011840904442520803, 0.0001184006746426735, 0.00011839235366447382, 0.00011838401232431905, 0.00011837567237857386, 0.00011836733497136915, 0.00011835900916636714, 0.00011835062965871973, 0.00011834226384488935, 0.00011833392738516807, 0.00011832555985514129, 0.00011831714720345109, 0.0001183087900599879, 0.00011830042801821416, 0.00011829200376432128, 0.00011828364662085809, 0.00011827524689427194, 0.00011826686883466525, 0.0001182585810183854, 0.00011825016804490838, 0.00011824179348920264, 0.00011823345295351492, 0.00011822507080006475, 0.00011821672379288649, 0.00011820834246178041, 0.00011819998140324419, 0.00011819156492586621, 0.00011818315229205305], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012031410015355007, 0.00012031000990406171, 0.00012030593558109944, 0.00012030182644256151, 0.0001202977322351418, 0.00012029362693603426, 0.00012028948265011802, 0.0001202853657853189, 0.00012028121344133884, 0.00012027704732754974, 0.00012027288903482258, 0.00012026870599909949, 0.0001202644937647452, 0.00012026026536686291, 0.00012025606747112216, 0.00012025183641881885, 0.00012024760645672418, 0.00012024335542516267, 0.00012023910190508146, 0.00012023480520325831, 0.00012023050769562879, 0.00012022619054054368, 0.00012022186459268895, 0.00012021756443063841, 0.00012021324066320094, 0.0001202089060884779, 0.00012020455238770341, 0.00012020014033706689, 0.00012019574772058424, 0.00012019131739710303, 0.00012018688285498841, 0.00012018246670421941, 0.00012017801974320645, 0.00012017355351394092, 0.00012016910226319397, 0.00012016461944379708, 0.00012016013918402043, 0.00012015562304215967, 0.00012015108523832738, 0.00012014651934977269, 0.00012014193819829714, 0.00012013735334959233, 0.00012013270965332153, 0.00012012808688431645, 0.00012012343169345463, 0.0001201187882341857, 0.00012011409429351705, 0.00012010937731626598, 0.00012010461687287072, 0.00012009988494080124, 0.00012009514646747996, 0.00012009035594854655, 0.00012008553950160784, 0.00012008066242958908, 0.00012007573736469028, 0.00012007078039933888, 0.00012006582021076196, 0.00012006081451782471, 0.00012005575654227348, 0.00012005065702029325, 0.0001200455292950897, 0.0001200403319150346, 0.0001200350960695749, 0.00012002982540853953, 0.00012002452173314273, 0.00012001915918647976, 0.00012001377590215258, 0.00012000834099881666, 0.00012000286213163263, 0.00011999731358589683, 0.00011999175788270437, 0.00011998613202695641, 0.0001199804618518575, 0.00011997477162639983, 0.00011996901655722871, 0.00011996322854479671, 0.00011995740704399956, 0.00011995153809542667, 0.00011994554069183663, 0.00011993955300532355, 0.00011993350163640618, 0.00011992740258271123, 0.0001199212835023577, 0.00011991512178536656, 0.00011990895717695252, 0.00011990273881651286, 0.00011989650547755458, 0.00011989017572623286, 0.00011988384919813667, 0.00011987751667389301, 0.00011987111141377338, 0.00011986469373475542, 0.00011985823896494376, 0.00011985176343376769, 0.00011984527159686251, 0.0001198387371707201, 0.00011983223156400589, 0.00011982566912424165, 0.00011981903655562209, 0.00011981236014165536, 0.00011980566370103004, 0.00011979892386536111, 0.00011979212532432731, 0.00011978525807644947, 0.00011977834044197264, 0.00011977138322818241, 0.00011976434311113572, 0.00011975723267563222, 0.00011975005744381562, 0.00011974280428578197, 0.00011973549204318047, 0.00011972815370667234, 0.00011972078929995778, 0.00011971338991176622, 0.00011970590560580224, 0.00011969843639685781, 0.00011969089926317552, 0.00011968333579858474, 0.00011967579781169569, 0.00011966817995517421, 0.00011966051612028864, 0.0001196528047665268, 0.00011964500261734601, 0.00011963721103844892, 0.00011962934563346723, 0.00011962142088321562, 0.00011961341728243965, 0.00011960543382682318, 0.00011959733869592244, 0.00011958928598835761, 0.00011958121128701862, 0.00011957314554435058, 0.00011956497362484178, 0.00011955681440863358, 0.0001195486545762205, 0.00011954049438830458, 0.0001195323927872526, 0.00011952426810221782, 0.00011951611692037324, 0.00011950791134659795, 0.00011949970911454913, 0.00011949142999909151, 0.00011948319378097356, 0.00011947488522988286, 0.00011946657305266344, 0.00011945827099542416, 0.00011944989428259371, 0.00011944150012642515, 0.0001194331295756435, 0.00011942474174742505, 0.00011941631884292885, 0.00011940787387355792, 0.00011939944561281929, 0.00011939103015018203, 0.00011938257906616269, 0.00011937411392793206, 0.00011936568749210789, 0.00011935727115256366, 0.00011934881449900021, 0.00011934031682041191, 0.00011933187334415281, 0.00011932337922059267, 0.00011931490782551249, 0.00011930640438777859, 0.00011929794169066729, 0.00011928949364027196, 0.00011928102527881581, 0.0001192725602827863, 0.00011926407909952862, 0.00011925558632687917, 0.00011924712905711084, 0.00011923862490837132, 0.00011923015010046412, 0.00011922165654570847, 0.0001192131209468199, 0.00011920462253352578, 0.00011919610089404776, 0.00011918758437381026, 0.00011917906951258591, 0.00011917056861077207, 0.00011916201059150598, 0.00011915347354690595, 0.00011914496875826136, 0.00011913642638111911, 0.00011912784037193137, 0.00011911932735932165, 0.00011911080996217722, 0.00011910223881300714, 0.0001190937325786511, 0.00011908517958382478, 0.00011907665780214562, 0.00011906821008355294, 0.00011905963803377573, 0.0001190511025533881, 0.00011904260245738066, 0.00011903405806572246, 0.0001190255530163758, 0.00011901701222714614, 0.0001190084863690347, 0.00011899991280244548], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 02:12:24,231 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:92]: done!
[2017-12-15 02:12:24,232 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:152]: >> Executing classifier part ... 
[2017-12-15 02:12:24,232 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:97]: =======================================
[2017-12-15 02:12:24,232 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fcf620c4438>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-15 02:12:25,035 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:110]: training ... 
[2017-12-15 07:47:31,652 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:122]: trained!
[2017-12-15 07:47:31,654 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:125]: Training history: 
{'val_loss': [0.00011946200016649733, 0.00011945811811238482, 0.00011945419061482205, 0.00011945027652504358, 0.00011944635696488911, 0.00011944239423166919, 0.00011943847842569433, 0.00011943452893936531, 0.0001194305689055793, 0.00011942662447845421, 0.00011942267155973241, 0.0001194186901092456, 0.0001194146936062863, 0.00011941071313903701, 0.00011940671223832446, 0.00011940269646390984, 0.00011939866401021156, 0.0001193946335229883, 0.00011939056796100133, 0.00011938650967497196, 0.00011938243038341385, 0.00011937833612876846, 0.00011937425658693171, 0.00011937014631445334, 0.00011936602157944497, 0.00011936188801317601, 0.0001193576861029612, 0.00011935351048988068, 0.00011934927795628653, 0.00011934504182940619, 0.00011934081812707263, 0.00011933656950413733, 0.00011933230287901695, 0.00011932804043712529, 0.00011932374284896171, 0.0001193194451714129, 0.00011931510911184691, 0.00011931074410934385, 0.00011930635286333767, 0.00011930193505204152, 0.0001192975047446904, 0.00011929299547442828, 0.00011928849678737723, 0.00011928394909934381, 0.00011927940141131039, 0.00011927479830443521, 0.00011927019446460113, 0.00011926554426958748, 0.00011926092251695358, 0.0001192563042860977, 0.00011925163995034086, 0.00011924693074319923, 0.00011924215826919275, 0.00011923735047014388, 0.00011923251194045341, 0.00011922768247442512, 0.00011922279873368598, 0.0001192178939874181, 0.00011921292457987579, 0.00011920794283717193, 0.00011920291490018189, 0.00011919784600688007, 0.0001191927563404511, 0.00011918767223378336, 0.00011918253744569096, 0.0001191774048564752, 0.00011917222618023551, 0.00011916697090085486, 0.00011916167539812131, 0.00011915638170096936, 0.00011915103331793452, 0.00011914564610595635, 0.00011914023543929416, 0.00011913475683985817, 0.00011912921751209784, 0.00011912366731509369, 0.00011911804698643913, 0.0001191123210938295, 0.00011910660769727483, 0.00011910086430303743, 0.00011909503222077613, 0.00011908919343462269, 0.00011908329604751333, 0.00011907736826942621, 0.00011907139199091401, 0.00011906538914711185, 0.00011905926981647968, 0.00011905311739543586, 0.00011904694863477223, 0.00011904070997485979, 0.00011903446568367792, 0.00011902817773675038, 0.00011902184949496176, 0.00011901547327118237, 0.00011900902773809668, 0.00011900260936024345, 0.00011899615737354425, 0.0001189896271568929, 0.00011898307732912238, 0.00011897647885791042, 0.00011896986077557929, 0.00011896317949789148, 0.00011895642644386505, 0.00011894965073962167, 0.00011894285730134893, 0.00011893596469910349, 0.00011892903294612788, 0.00011892204405811412, 0.00011891496026086454, 0.00011890780461576816, 0.00011890065755165371, 0.00011889346536587585, 0.0001188862462930212, 0.00011887896173654802, 0.00011887170792859349, 0.00011886440996433602, 0.00011885709442694257, 0.00011884979108169432, 0.00011884241612100085, 0.00011883497855489322, 0.00011882751074082422, 0.00011881997613811233, 0.00011881246720683813, 0.0001188049142086462, 0.00011879727661627384, 0.00011878952727448859, 0.0001187818187281217, 0.00011877400383120975, 0.0001187662490190485, 0.0001187584625645163, 0.00011875070228197907, 0.00011874282157966176, 0.00011873497446831339, 0.00011872711387767253, 0.00011871926326242319, 0.00011871147600342394, 0.00011870366707744525, 0.00011869584286659247, 0.00011868794737995833, 0.00011868002386211649, 0.00011867202284728138, 0.00011866407521330484, 0.00011865608204649281, 0.00011864808249757545, 0.00011864007642353641, 0.00011863197880114614, 0.00011862386564360314, 0.00011861578895523343, 0.00011860769746466987, 0.00011859958545125778, 0.00011859145039433376, 0.00011858327135987722, 0.00011857508392320918, 0.00011856685438609844, 0.00011855858741445394, 0.0001185503486527876, 0.00011854212574806083, 0.00011853385175073735, 0.00011852554260714195, 0.00011851727695839882, 0.00011850894525397171, 0.00011850063146234442, 0.0001184922815412077, 0.00011848398761097822, 0.00011847568265061153, 0.00011846734712049663, 0.00011845902982496837, 0.00011845068791274815, 0.00011844234297930719, 0.00011843402870499967, 0.0001184256856486485, 0.00011841736892518572, 0.00011840904442520803, 0.0001184006746426735, 0.00011839235366447382, 0.00011838401232431905, 0.00011837567237857386, 0.00011836733497136915, 0.00011835900916636714, 0.00011835062965871973, 0.00011834226384488935, 0.00011833392738516807, 0.00011832555985514129, 0.00011831714720345109, 0.0001183087900599879, 0.00011830042801821416, 0.00011829200376432128, 0.00011828364662085809, 0.00011827524689427194, 0.00011826686883466525, 0.0001182585810183854, 0.00011825016804490838, 0.00011824179348920264, 0.00011823345295351492, 0.00011822507080006475, 0.00011821672379288649, 0.00011820834246178041, 0.00011819998140324419, 0.00011819156492586621, 0.00011818315229205305], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012031410015355007, 0.00012031000990406171, 0.00012030593558109944, 0.00012030182644256151, 0.0001202977322351418, 0.00012029362693603426, 0.00012028948265011802, 0.0001202853657853189, 0.00012028121344133884, 0.00012027704732754974, 0.00012027288903482258, 0.00012026870599909949, 0.0001202644937647452, 0.00012026026536686291, 0.00012025606747112216, 0.00012025183641881885, 0.00012024760645672418, 0.00012024335542516267, 0.00012023910190508146, 0.00012023480520325831, 0.00012023050769562879, 0.00012022619054054368, 0.00012022186459268895, 0.00012021756443063841, 0.00012021324066320094, 0.0001202089060884779, 0.00012020455238770341, 0.00012020014033706689, 0.00012019574772058424, 0.00012019131739710303, 0.00012018688285498841, 0.00012018246670421941, 0.00012017801974320645, 0.00012017355351394092, 0.00012016910226319397, 0.00012016461944379708, 0.00012016013918402043, 0.00012015562304215967, 0.00012015108523832738, 0.00012014651934977269, 0.00012014193819829714, 0.00012013735334959233, 0.00012013270965332153, 0.00012012808688431645, 0.00012012343169345463, 0.0001201187882341857, 0.00012011409429351705, 0.00012010937731626598, 0.00012010461687287072, 0.00012009988494080124, 0.00012009514646747996, 0.00012009035594854655, 0.00012008553950160784, 0.00012008066242958908, 0.00012007573736469028, 0.00012007078039933888, 0.00012006582021076196, 0.00012006081451782471, 0.00012005575654227348, 0.00012005065702029325, 0.0001200455292950897, 0.0001200403319150346, 0.0001200350960695749, 0.00012002982540853953, 0.00012002452173314273, 0.00012001915918647976, 0.00012001377590215258, 0.00012000834099881666, 0.00012000286213163263, 0.00011999731358589683, 0.00011999175788270437, 0.00011998613202695641, 0.0001199804618518575, 0.00011997477162639983, 0.00011996901655722871, 0.00011996322854479671, 0.00011995740704399956, 0.00011995153809542667, 0.00011994554069183663, 0.00011993955300532355, 0.00011993350163640618, 0.00011992740258271123, 0.0001199212835023577, 0.00011991512178536656, 0.00011990895717695252, 0.00011990273881651286, 0.00011989650547755458, 0.00011989017572623286, 0.00011988384919813667, 0.00011987751667389301, 0.00011987111141377338, 0.00011986469373475542, 0.00011985823896494376, 0.00011985176343376769, 0.00011984527159686251, 0.0001198387371707201, 0.00011983223156400589, 0.00011982566912424165, 0.00011981903655562209, 0.00011981236014165536, 0.00011980566370103004, 0.00011979892386536111, 0.00011979212532432731, 0.00011978525807644947, 0.00011977834044197264, 0.00011977138322818241, 0.00011976434311113572, 0.00011975723267563222, 0.00011975005744381562, 0.00011974280428578197, 0.00011973549204318047, 0.00011972815370667234, 0.00011972078929995778, 0.00011971338991176622, 0.00011970590560580224, 0.00011969843639685781, 0.00011969089926317552, 0.00011968333579858474, 0.00011967579781169569, 0.00011966817995517421, 0.00011966051612028864, 0.0001196528047665268, 0.00011964500261734601, 0.00011963721103844892, 0.00011962934563346723, 0.00011962142088321562, 0.00011961341728243965, 0.00011960543382682318, 0.00011959733869592244, 0.00011958928598835761, 0.00011958121128701862, 0.00011957314554435058, 0.00011956497362484178, 0.00011955681440863358, 0.0001195486545762205, 0.00011954049438830458, 0.0001195323927872526, 0.00011952426810221782, 0.00011951611692037324, 0.00011950791134659795, 0.00011949970911454913, 0.00011949142999909151, 0.00011948319378097356, 0.00011947488522988286, 0.00011946657305266344, 0.00011945827099542416, 0.00011944989428259371, 0.00011944150012642515, 0.0001194331295756435, 0.00011942474174742505, 0.00011941631884292885, 0.00011940787387355792, 0.00011939944561281929, 0.00011939103015018203, 0.00011938257906616269, 0.00011937411392793206, 0.00011936568749210789, 0.00011935727115256366, 0.00011934881449900021, 0.00011934031682041191, 0.00011933187334415281, 0.00011932337922059267, 0.00011931490782551249, 0.00011930640438777859, 0.00011929794169066729, 0.00011928949364027196, 0.00011928102527881581, 0.0001192725602827863, 0.00011926407909952862, 0.00011925558632687917, 0.00011924712905711084, 0.00011923862490837132, 0.00011923015010046412, 0.00011922165654570847, 0.0001192131209468199, 0.00011920462253352578, 0.00011919610089404776, 0.00011918758437381026, 0.00011917906951258591, 0.00011917056861077207, 0.00011916201059150598, 0.00011915347354690595, 0.00011914496875826136, 0.00011913642638111911, 0.00011912784037193137, 0.00011911932735932165, 0.00011911080996217722, 0.00011910223881300714, 0.0001190937325786511, 0.00011908517958382478, 0.00011907665780214562, 0.00011906821008355294, 0.00011905963803377573, 0.0001190511025533881, 0.00011904260245738066, 0.00011903405806572246, 0.0001190255530163758, 0.00011901701222714614, 0.0001190084863690347, 0.00011899991280244548], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 07:47:31,655 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:129]: evaluating model ... 
[2017-12-15 07:47:35,763 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:133]: evaluated! 
[2017-12-15 07:47:35,764 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:135]: generating reports ... 
[2017-12-15 07:47:37,511 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:138]: done!
[2017-12-15 07:47:37,512 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_4 finished!
[2018-04-29 11:38:17,469 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_4 was already executed!
[2018-04-29 13:12:02,673 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_4
[2018-04-29 13:12:02,673 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:146]: >> Printing header log
[2018-04-29 13:12:02,673 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_4
	layers = 9216,12902
	using GLOBAL obj = 
		{'fullds_data_dir': '/home/dhiego/malware_dataset/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'autoencoder_configs': {'optimizer': <keras.optimizers.SGD object at 0x7f27f7a77828>, 'loss_function': 'mse', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu'}, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'numpy_seed': 666, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'store_history': True, 'shuffle_batches': True, 'batch': 32, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'mlp_configs': {'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7f27f7a77898>, 'classifier_dim': 9, 'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy'}, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'data_dir': '/home/dhiego/malware_dataset/'}
	=======================================
	
[2018-04-29 13:12:02,673 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,806 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,806 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,806 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:57]: =======================================
[2018-04-29 13:12:20,806 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:62]: setting configurations for autoencoder: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7f27f7a77828>, 'loss_function': 'mse', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu'}
[2018-04-29 13:12:20,852 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,870 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_4
[2018-04-29 13:14:12,870 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:146]: >> Printing header log
[2018-04-29 13:14:12,871 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_4
	layers = 9216,12902
	using GLOBAL obj = 
		{'store_history': True, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'epochs': 200, 'shuffle_batches': True, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'data_dir': '/home/dhiego/malware_dataset/', 'numpy_seed': 666, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'batch': 32, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'mlp_configs': {'classifier_dim': 9, 'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f7055a01898>, 'use_last_dim_as_classifier': False}, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'autoencoder_configs': {'discard_decoder_function': True, 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f7055a01828>, 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu'}}
	=======================================
	
[2018-04-29 13:14:12,871 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:148]: >> Loading dataset... 
[2018-04-29 13:14:30,835 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:30,835 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:30,835 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:57]: =======================================
[2018-04-29 13:14:30,835 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f7055a01828>, 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu'}
[2018-04-29 13:14:30,883 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,718 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_4
[2018-04-29 13:16:33,719 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:146]: >> Printing header log
[2018-04-29 13:16:33,719 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_4
	layers = 9216,12902
	using GLOBAL obj = 
		{'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'numpy_seed': 666, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'batch': 32, 'mlp_configs': {'optimizer': <keras.optimizers.SGD object at 0x7fed5f463898>, 'activation': 'sigmoid', 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False}, 'shuffle_batches': True, 'epochs': 200, 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'store_history': True, 'autoencoder_configs': {'optimizer': <keras.optimizers.SGD object at 0x7fed5f463828>, 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'output_layer_activation': 'relu'}, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:16:33,719 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:148]: >> Loading dataset... 
[2018-04-29 13:16:58,386 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:58,387 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:58,387 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:57]: =======================================
[2018-04-29 13:16:58,387 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:62]: setting configurations for autoencoder: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7fed5f463828>, 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'output_layer_activation': 'relu'}
[2018-04-29 13:16:58,450 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:20,937 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_4
[2018-04-29 14:30:20,937 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:146]: >> Printing header log
[2018-04-29 14:30:20,938 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_4
	layers = 9216,12902
	using GLOBAL obj = 
		{'numpy_seed': 666, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'store_history': True, 'shuffle_batches': True, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'epochs': 200, 'data_dir': '/home/dhiego/malware_dataset/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'mlp_configs': {'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f19625f5940>}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'batch': 32, 'autoencoder_configs': {'discard_decoder_function': True, 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f19625f58d0>, 'hidden_layer_activation': 'relu'}}
	=======================================
	
[2018-04-29 14:30:20,938 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:148]: >> Loading dataset... 
[2018-04-29 14:30:37,323 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:37,323 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:37,323 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:57]: =======================================
[2018-04-29 14:30:37,324 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f19625f58d0>, 'hidden_layer_activation': 'relu'}
[2018-04-29 14:30:37,368 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:73]: training and evaluate autoencoder
[2018-04-29 22:10:58,428 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:85]: trained and evaluated!
[2018-04-29 22:10:58,429 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:88]: Training history: 
{'val_loss': [0.00011837082619750781, 0.00011836232771812104, 0.0001183538084656071, 0.00011834536762181578, 0.00011833688868203998, 0.00011832838635908838, 0.00011831985886525637, 0.0001183113817310622, 0.00011830284892774762, 0.00011829434923272172, 0.00011828579975012346, 0.00011827725141165613, 0.00011826872220162774, 0.00011826017811789729, 0.00011825165185758144, 0.00011824308054711034, 0.00011823453630248648, 0.00011822597432383324, 0.0001182174380166177, 0.00011820886876200685, 0.00011820035771505692, 0.00011819176589961435, 0.00011818319877237196, 0.00011817462869541701, 0.00011816604848217713, 0.00011815747511584577, 0.000118148916051151, 0.00011814031470724306, 0.00011813166404056594, 0.00011812308482844063, 0.00011811443695058264, 0.00011810588128252655, 0.00011809724001917549, 0.00011808864176799646, 0.00011808000533144775, 0.00011807139056187847, 0.00011806279695873133, 0.00011805416715456657, 0.00011804556636484706, 0.00011803696452038184, 0.0001180283466402066, 0.00011801968871544891, 0.00011801102285328291, 0.0001180024291786276, 0.00011799377052091104, 0.0001179851175838491, 0.00011797644403455344, 0.000117967790114254, 0.00011795920085522897, 0.00011795054807906046, 0.00011794193225474547, 0.00011793327792327398, 0.00011792466887435933, 0.00011791599083792519, 0.00011790733952767442, 0.00011789869107775098, 0.00011789010483994669, 0.00011788147634080627, 0.0001178728177545979, 0.00011786420568446251, 0.00011785555020886008, 0.00011784683210996646, 0.00011783818063882228, 0.00011782949908061013, 0.00011782084875359687, 0.00011781219197297012, 0.00011780351139799548, 0.00011779489083626338, 0.0001177861796021553, 0.00011777746445297422, 0.00011776878421766345, 0.00011776008117124232, 0.00011775142063643595, 0.00011774276124576052, 0.00011773407226857439, 0.0001177253973427462, 0.00011771666617573214, 0.00011770793640312764, 0.00011769922322042158, 0.00011769051920863998, 0.00011768185000346644, 0.00011767316315364875, 0.00011766447433735603, 0.00011765582508296553, 0.00011764716349341346, 0.00011763847655421054, 0.00011762982656686116, 0.00011762117731247065, 0.00011761252936310448, 0.00011760384855572825, 0.0001175952005348539, 0.0001175865279866728, 0.0001175778814317162, 0.00011756921101090355, 0.00011756047853886514, 0.00011755185847769032, 0.00011754322034282226, 0.00011753455751975404, 0.00011752595258255991, 0.00011751731085440565, 0.00011750873346573899], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0016286644951140066, 0.0016286644951140066, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.003257328990228013, 0.003257328990228013, 0.004071661237785016, 0.004071661237785016, 0.004885993485342019, 0.005700325732899023, 0.005700325732899023, 0.008957654723127036, 0.008957654723127036, 0.009771986970684038, 0.009771986970684038, 0.010586319218241042, 0.010586319218241042, 0.010586319218241042, 0.012214983713355049, 0.013029315960912053, 0.013029315960912053, 0.013029315985181044, 0.013029315960912053, 0.013843648208469055, 0.013843648208469055, 0.013843648208469055, 0.015472312703583062, 0.015472312703583062, 0.015472312703583062, 0.01710097719869707, 0.018729641693811076, 0.02035830618892508, 0.02035830618892508, 0.02035830618892508, 0.021172638436482084, 0.021172638436482084, 0.021172638436482084, 0.021986970684039087, 0.02280130293159609, 0.024429967426710098], 'loss': [0.00011912892918225305, 0.00011912020663628416, 0.00011911148214689987, 0.0001191027311133054, 0.00011909405535150696, 0.0001190853456036394, 0.00011907662580689228, 0.00011906787581610606, 0.00011905918543129182, 0.00011905043138777352, 0.0001190417127286354, 0.00011903294593441613, 0.0001190241782632899, 0.00011901543488485606, 0.00011900667117166117, 0.00011899792120457515, 0.00011898913469179973, 0.0001189803712867073, 0.00011897159522571465, 0.00011896284404991905, 0.00011895406979014065, 0.00011894535207901004, 0.00011893654726968974, 0.0001189277681276727, 0.0001189189894596594, 0.00011891019744844045, 0.00011890140707253445, 0.00011889263724469115, 0.00011888381511053368, 0.00011887495766309657, 0.00011886617494235118, 0.0001188573197464319, 0.00011884855923276236, 0.00011883971361171892, 0.00011883090704858473, 0.00011882206505366998, 0.00011881325469850578, 0.00011880446572091085, 0.00011879563877561526, 0.00011878684486838131, 0.00011877805302306366, 0.00011876924281010059, 0.00011876039640695093, 0.0001187515411162309, 0.00011874275377394892, 0.0001187339021330578, 0.00011872505670161584, 0.0001187161912435153, 0.00011870733851241554, 0.0001186985472359024, 0.00011868969701702255, 0.00011868088649595703, 0.00011867203267464864, 0.00011866322556641014, 0.00011865435795159253, 0.00011864551749718997, 0.00011863667955500733, 0.00011862789986788595, 0.00011861907621691647, 0.00011861023154388051, 0.00011860143043178949, 0.00011859258966928452, 0.00011858367873052384, 0.0001185748367356091, 0.00011856595940371454, 0.00011855710982473974, 0.00011854824960438068, 0.0001185393715377803, 0.00011853055912069976, 0.00011852164398700588, 0.00011851274103520845, 0.00011850387413139646, 0.00011849497440282456, 0.0001184861276204719, 0.00011847727797039654, 0.00011846839705977364, 0.0001184595368157144, 0.00011845061014002911, 0.00011844169261261627, 0.00011843279636797195, 0.00011842390867909538, 0.00011841505872091758, 0.00011840618878348157, 0.00011839731012437651, 0.00011838848268137698, 0.00011837963720253466, 0.00011837076304646524, 0.00011836193313864621, 0.0001183531047002388, 0.00011834426891477324, 0.00011833540753310497, 0.00011832657388065629, 0.00011831771266488935, 0.00011830888581439453, 0.00011830002635244147, 0.00011829110953603425, 0.00011828230411050908, 0.00011827347884792683, 0.0001182646293400526, 0.00011825584389378562, 0.00011824701697219024], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.009828009828009828, 0.009828009828009828, 0.009828009828009828, 0.009828009828009828, 0.014742014742014743, 0.014742014742014743, 0.0171990171990172, 0.0171990171990172, 0.0171990171990172, 0.0171990171990172, 0.0171990171990172, 0.0171990171990172, 0.0171990171990172, 0.0171990171990172, 0.0171990171990172, 0.019656019656019656, 0.019656019656019656, 0.019656019656019656, 0.022113022113022112, 0.022113022113022112, 0.02457002457002457, 0.02457002457002457, 0.02702702702702703, 0.02702702702702703, 0.02702702702702703, 0.02702702702702703, 0.02702702702702703]}
[2018-04-29 22:10:58,429 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:92]: done!
[2018-04-29 22:10:58,430 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:152]: >> Executing classifier part ... 
[2018-04-29 22:10:58,430 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:97]: =======================================
[2018-04-29 22:10:58,435 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:101]: setting configurations for classifier: 
	 {'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f19625f5940>}
[2018-04-29 22:10:58,675 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:110]: training ... 
[2018-04-30 03:09:07,060 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:122]: trained!
[2018-04-30 03:09:07,062 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:125]: Training history: 
{'val_loss': [0.00011837082619750781, 0.00011836232771812104, 0.0001183538084656071, 0.00011834536762181578, 0.00011833688868203998, 0.00011832838635908838, 0.00011831985886525637, 0.0001183113817310622, 0.00011830284892774762, 0.00011829434923272172, 0.00011828579975012346, 0.00011827725141165613, 0.00011826872220162774, 0.00011826017811789729, 0.00011825165185758144, 0.00011824308054711034, 0.00011823453630248648, 0.00011822597432383324, 0.0001182174380166177, 0.00011820886876200685, 0.00011820035771505692, 0.00011819176589961435, 0.00011818319877237196, 0.00011817462869541701, 0.00011816604848217713, 0.00011815747511584577, 0.000118148916051151, 0.00011814031470724306, 0.00011813166404056594, 0.00011812308482844063, 0.00011811443695058264, 0.00011810588128252655, 0.00011809724001917549, 0.00011808864176799646, 0.00011808000533144775, 0.00011807139056187847, 0.00011806279695873133, 0.00011805416715456657, 0.00011804556636484706, 0.00011803696452038184, 0.0001180283466402066, 0.00011801968871544891, 0.00011801102285328291, 0.0001180024291786276, 0.00011799377052091104, 0.0001179851175838491, 0.00011797644403455344, 0.000117967790114254, 0.00011795920085522897, 0.00011795054807906046, 0.00011794193225474547, 0.00011793327792327398, 0.00011792466887435933, 0.00011791599083792519, 0.00011790733952767442, 0.00011789869107775098, 0.00011789010483994669, 0.00011788147634080627, 0.0001178728177545979, 0.00011786420568446251, 0.00011785555020886008, 0.00011784683210996646, 0.00011783818063882228, 0.00011782949908061013, 0.00011782084875359687, 0.00011781219197297012, 0.00011780351139799548, 0.00011779489083626338, 0.0001177861796021553, 0.00011777746445297422, 0.00011776878421766345, 0.00011776008117124232, 0.00011775142063643595, 0.00011774276124576052, 0.00011773407226857439, 0.0001177253973427462, 0.00011771666617573214, 0.00011770793640312764, 0.00011769922322042158, 0.00011769051920863998, 0.00011768185000346644, 0.00011767316315364875, 0.00011766447433735603, 0.00011765582508296553, 0.00011764716349341346, 0.00011763847655421054, 0.00011762982656686116, 0.00011762117731247065, 0.00011761252936310448, 0.00011760384855572825, 0.0001175952005348539, 0.0001175865279866728, 0.0001175778814317162, 0.00011756921101090355, 0.00011756047853886514, 0.00011755185847769032, 0.00011754322034282226, 0.00011753455751975404, 0.00011752595258255991, 0.00011751731085440565, 0.00011750873346573899], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0016286644951140066, 0.0016286644951140066, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.003257328990228013, 0.003257328990228013, 0.004071661237785016, 0.004071661237785016, 0.004885993485342019, 0.005700325732899023, 0.005700325732899023, 0.008957654723127036, 0.008957654723127036, 0.009771986970684038, 0.009771986970684038, 0.010586319218241042, 0.010586319218241042, 0.010586319218241042, 0.012214983713355049, 0.013029315960912053, 0.013029315960912053, 0.013029315985181044, 0.013029315960912053, 0.013843648208469055, 0.013843648208469055, 0.013843648208469055, 0.015472312703583062, 0.015472312703583062, 0.015472312703583062, 0.01710097719869707, 0.018729641693811076, 0.02035830618892508, 0.02035830618892508, 0.02035830618892508, 0.021172638436482084, 0.021172638436482084, 0.021172638436482084, 0.021986970684039087, 0.02280130293159609, 0.024429967426710098], 'loss': [0.00011912892918225305, 0.00011912020663628416, 0.00011911148214689987, 0.0001191027311133054, 0.00011909405535150696, 0.0001190853456036394, 0.00011907662580689228, 0.00011906787581610606, 0.00011905918543129182, 0.00011905043138777352, 0.0001190417127286354, 0.00011903294593441613, 0.0001190241782632899, 0.00011901543488485606, 0.00011900667117166117, 0.00011899792120457515, 0.00011898913469179973, 0.0001189803712867073, 0.00011897159522571465, 0.00011896284404991905, 0.00011895406979014065, 0.00011894535207901004, 0.00011893654726968974, 0.0001189277681276727, 0.0001189189894596594, 0.00011891019744844045, 0.00011890140707253445, 0.00011889263724469115, 0.00011888381511053368, 0.00011887495766309657, 0.00011886617494235118, 0.0001188573197464319, 0.00011884855923276236, 0.00011883971361171892, 0.00011883090704858473, 0.00011882206505366998, 0.00011881325469850578, 0.00011880446572091085, 0.00011879563877561526, 0.00011878684486838131, 0.00011877805302306366, 0.00011876924281010059, 0.00011876039640695093, 0.0001187515411162309, 0.00011874275377394892, 0.0001187339021330578, 0.00011872505670161584, 0.0001187161912435153, 0.00011870733851241554, 0.0001186985472359024, 0.00011868969701702255, 0.00011868088649595703, 0.00011867203267464864, 0.00011866322556641014, 0.00011865435795159253, 0.00011864551749718997, 0.00011863667955500733, 0.00011862789986788595, 0.00011861907621691647, 0.00011861023154388051, 0.00011860143043178949, 0.00011859258966928452, 0.00011858367873052384, 0.0001185748367356091, 0.00011856595940371454, 0.00011855710982473974, 0.00011854824960438068, 0.0001185393715377803, 0.00011853055912069976, 0.00011852164398700588, 0.00011851274103520845, 0.00011850387413139646, 0.00011849497440282456, 0.0001184861276204719, 0.00011847727797039654, 0.00011846839705977364, 0.0001184595368157144, 0.00011845061014002911, 0.00011844169261261627, 0.00011843279636797195, 0.00011842390867909538, 0.00011841505872091758, 0.00011840618878348157, 0.00011839731012437651, 0.00011838848268137698, 0.00011837963720253466, 0.00011837076304646524, 0.00011836193313864621, 0.0001183531047002388, 0.00011834426891477324, 0.00011833540753310497, 0.00011832657388065629, 0.00011831771266488935, 0.00011830888581439453, 0.00011830002635244147, 0.00011829110953603425, 0.00011828230411050908, 0.00011827347884792683, 0.0001182646293400526, 0.00011825584389378562, 0.00011824701697219024], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.009828009828009828, 0.009828009828009828, 0.009828009828009828, 0.009828009828009828, 0.014742014742014743, 0.014742014742014743, 0.0171990171990172, 0.0171990171990172, 0.0171990171990172, 0.0171990171990172, 0.0171990171990172, 0.0171990171990172, 0.0171990171990172, 0.0171990171990172, 0.0171990171990172, 0.019656019656019656, 0.019656019656019656, 0.019656019656019656, 0.022113022113022112, 0.022113022113022112, 0.02457002457002457, 0.02457002457002457, 0.02702702702702703, 0.02702702702702703, 0.02702702702702703, 0.02702702702702703, 0.02702702702702703]}
[2018-04-30 03:09:07,062 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:129]: evaluating model ... 
[2018-04-30 03:09:11,068 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:133]: evaluated! 
[2018-04-30 03:09:11,068 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:135]: generating reports ... 
[2018-04-30 03:09:12,851 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:138]: done!
[2018-04-30 03:09:12,856 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_4 finished!
