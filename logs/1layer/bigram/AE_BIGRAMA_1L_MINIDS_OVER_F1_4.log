[2017-12-14 09:31:57,850 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_4
[2017-12-14 09:31:57,850 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:146]: >> Printing header log
[2017-12-14 09:31:57,850 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_4
	layers = 9216,12902
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7faba2d23eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7faba2d06400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,850 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:148]: >> Loading dataset... 
[2017-12-14 09:32:23,738 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:23,738 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:23,738 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:57]: =======================================
[2017-12-14 09:32:23,738 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7faba2d23eb8>, 'discard_decoder_function': True}
[2017-12-14 09:32:23,783 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:54,863 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_4
[2017-12-14 10:18:54,863 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:146]: >> Printing header log
[2017-12-14 10:18:54,863 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_4
	layers = 9216,12902
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fcf620e0ef0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fcf620c4438>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:54,863 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:148]: >> Loading dataset... 
[2017-12-14 10:19:17,347 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:17,347 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:17,347 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:57]: =======================================
[2017-12-14 10:19:17,347 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fcf620e0ef0>, 'discard_decoder_function': True}
[2017-12-14 10:19:17,388 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:73]: training and evaluate autoencoder
[2017-12-15 02:12:24,190 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:85]: trained and evaluated!
[2017-12-15 02:12:24,230 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:88]: Training history: 
{'val_loss': [0.00011946200016649733, 0.00011945811811238482, 0.00011945419061482205, 0.00011945027652504358, 0.00011944635696488911, 0.00011944239423166919, 0.00011943847842569433, 0.00011943452893936531, 0.0001194305689055793, 0.00011942662447845421, 0.00011942267155973241, 0.0001194186901092456, 0.0001194146936062863, 0.00011941071313903701, 0.00011940671223832446, 0.00011940269646390984, 0.00011939866401021156, 0.0001193946335229883, 0.00011939056796100133, 0.00011938650967497196, 0.00011938243038341385, 0.00011937833612876846, 0.00011937425658693171, 0.00011937014631445334, 0.00011936602157944497, 0.00011936188801317601, 0.0001193576861029612, 0.00011935351048988068, 0.00011934927795628653, 0.00011934504182940619, 0.00011934081812707263, 0.00011933656950413733, 0.00011933230287901695, 0.00011932804043712529, 0.00011932374284896171, 0.0001193194451714129, 0.00011931510911184691, 0.00011931074410934385, 0.00011930635286333767, 0.00011930193505204152, 0.0001192975047446904, 0.00011929299547442828, 0.00011928849678737723, 0.00011928394909934381, 0.00011927940141131039, 0.00011927479830443521, 0.00011927019446460113, 0.00011926554426958748, 0.00011926092251695358, 0.0001192563042860977, 0.00011925163995034086, 0.00011924693074319923, 0.00011924215826919275, 0.00011923735047014388, 0.00011923251194045341, 0.00011922768247442512, 0.00011922279873368598, 0.0001192178939874181, 0.00011921292457987579, 0.00011920794283717193, 0.00011920291490018189, 0.00011919784600688007, 0.0001191927563404511, 0.00011918767223378336, 0.00011918253744569096, 0.0001191774048564752, 0.00011917222618023551, 0.00011916697090085486, 0.00011916167539812131, 0.00011915638170096936, 0.00011915103331793452, 0.00011914564610595635, 0.00011914023543929416, 0.00011913475683985817, 0.00011912921751209784, 0.00011912366731509369, 0.00011911804698643913, 0.0001191123210938295, 0.00011910660769727483, 0.00011910086430303743, 0.00011909503222077613, 0.00011908919343462269, 0.00011908329604751333, 0.00011907736826942621, 0.00011907139199091401, 0.00011906538914711185, 0.00011905926981647968, 0.00011905311739543586, 0.00011904694863477223, 0.00011904070997485979, 0.00011903446568367792, 0.00011902817773675038, 0.00011902184949496176, 0.00011901547327118237, 0.00011900902773809668, 0.00011900260936024345, 0.00011899615737354425, 0.0001189896271568929, 0.00011898307732912238, 0.00011897647885791042, 0.00011896986077557929, 0.00011896317949789148, 0.00011895642644386505, 0.00011894965073962167, 0.00011894285730134893, 0.00011893596469910349, 0.00011892903294612788, 0.00011892204405811412, 0.00011891496026086454, 0.00011890780461576816, 0.00011890065755165371, 0.00011889346536587585, 0.0001188862462930212, 0.00011887896173654802, 0.00011887170792859349, 0.00011886440996433602, 0.00011885709442694257, 0.00011884979108169432, 0.00011884241612100085, 0.00011883497855489322, 0.00011882751074082422, 0.00011881997613811233, 0.00011881246720683813, 0.0001188049142086462, 0.00011879727661627384, 0.00011878952727448859, 0.0001187818187281217, 0.00011877400383120975, 0.0001187662490190485, 0.0001187584625645163, 0.00011875070228197907, 0.00011874282157966176, 0.00011873497446831339, 0.00011872711387767253, 0.00011871926326242319, 0.00011871147600342394, 0.00011870366707744525, 0.00011869584286659247, 0.00011868794737995833, 0.00011868002386211649, 0.00011867202284728138, 0.00011866407521330484, 0.00011865608204649281, 0.00011864808249757545, 0.00011864007642353641, 0.00011863197880114614, 0.00011862386564360314, 0.00011861578895523343, 0.00011860769746466987, 0.00011859958545125778, 0.00011859145039433376, 0.00011858327135987722, 0.00011857508392320918, 0.00011856685438609844, 0.00011855858741445394, 0.0001185503486527876, 0.00011854212574806083, 0.00011853385175073735, 0.00011852554260714195, 0.00011851727695839882, 0.00011850894525397171, 0.00011850063146234442, 0.0001184922815412077, 0.00011848398761097822, 0.00011847568265061153, 0.00011846734712049663, 0.00011845902982496837, 0.00011845068791274815, 0.00011844234297930719, 0.00011843402870499967, 0.0001184256856486485, 0.00011841736892518572, 0.00011840904442520803, 0.0001184006746426735, 0.00011839235366447382, 0.00011838401232431905, 0.00011837567237857386, 0.00011836733497136915, 0.00011835900916636714, 0.00011835062965871973, 0.00011834226384488935, 0.00011833392738516807, 0.00011832555985514129, 0.00011831714720345109, 0.0001183087900599879, 0.00011830042801821416, 0.00011829200376432128, 0.00011828364662085809, 0.00011827524689427194, 0.00011826686883466525, 0.0001182585810183854, 0.00011825016804490838, 0.00011824179348920264, 0.00011823345295351492, 0.00011822507080006475, 0.00011821672379288649, 0.00011820834246178041, 0.00011819998140324419, 0.00011819156492586621, 0.00011818315229205305], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012031410015355007, 0.00012031000990406171, 0.00012030593558109944, 0.00012030182644256151, 0.0001202977322351418, 0.00012029362693603426, 0.00012028948265011802, 0.0001202853657853189, 0.00012028121344133884, 0.00012027704732754974, 0.00012027288903482258, 0.00012026870599909949, 0.0001202644937647452, 0.00012026026536686291, 0.00012025606747112216, 0.00012025183641881885, 0.00012024760645672418, 0.00012024335542516267, 0.00012023910190508146, 0.00012023480520325831, 0.00012023050769562879, 0.00012022619054054368, 0.00012022186459268895, 0.00012021756443063841, 0.00012021324066320094, 0.0001202089060884779, 0.00012020455238770341, 0.00012020014033706689, 0.00012019574772058424, 0.00012019131739710303, 0.00012018688285498841, 0.00012018246670421941, 0.00012017801974320645, 0.00012017355351394092, 0.00012016910226319397, 0.00012016461944379708, 0.00012016013918402043, 0.00012015562304215967, 0.00012015108523832738, 0.00012014651934977269, 0.00012014193819829714, 0.00012013735334959233, 0.00012013270965332153, 0.00012012808688431645, 0.00012012343169345463, 0.0001201187882341857, 0.00012011409429351705, 0.00012010937731626598, 0.00012010461687287072, 0.00012009988494080124, 0.00012009514646747996, 0.00012009035594854655, 0.00012008553950160784, 0.00012008066242958908, 0.00012007573736469028, 0.00012007078039933888, 0.00012006582021076196, 0.00012006081451782471, 0.00012005575654227348, 0.00012005065702029325, 0.0001200455292950897, 0.0001200403319150346, 0.0001200350960695749, 0.00012002982540853953, 0.00012002452173314273, 0.00012001915918647976, 0.00012001377590215258, 0.00012000834099881666, 0.00012000286213163263, 0.00011999731358589683, 0.00011999175788270437, 0.00011998613202695641, 0.0001199804618518575, 0.00011997477162639983, 0.00011996901655722871, 0.00011996322854479671, 0.00011995740704399956, 0.00011995153809542667, 0.00011994554069183663, 0.00011993955300532355, 0.00011993350163640618, 0.00011992740258271123, 0.0001199212835023577, 0.00011991512178536656, 0.00011990895717695252, 0.00011990273881651286, 0.00011989650547755458, 0.00011989017572623286, 0.00011988384919813667, 0.00011987751667389301, 0.00011987111141377338, 0.00011986469373475542, 0.00011985823896494376, 0.00011985176343376769, 0.00011984527159686251, 0.0001198387371707201, 0.00011983223156400589, 0.00011982566912424165, 0.00011981903655562209, 0.00011981236014165536, 0.00011980566370103004, 0.00011979892386536111, 0.00011979212532432731, 0.00011978525807644947, 0.00011977834044197264, 0.00011977138322818241, 0.00011976434311113572, 0.00011975723267563222, 0.00011975005744381562, 0.00011974280428578197, 0.00011973549204318047, 0.00011972815370667234, 0.00011972078929995778, 0.00011971338991176622, 0.00011970590560580224, 0.00011969843639685781, 0.00011969089926317552, 0.00011968333579858474, 0.00011967579781169569, 0.00011966817995517421, 0.00011966051612028864, 0.0001196528047665268, 0.00011964500261734601, 0.00011963721103844892, 0.00011962934563346723, 0.00011962142088321562, 0.00011961341728243965, 0.00011960543382682318, 0.00011959733869592244, 0.00011958928598835761, 0.00011958121128701862, 0.00011957314554435058, 0.00011956497362484178, 0.00011955681440863358, 0.0001195486545762205, 0.00011954049438830458, 0.0001195323927872526, 0.00011952426810221782, 0.00011951611692037324, 0.00011950791134659795, 0.00011949970911454913, 0.00011949142999909151, 0.00011948319378097356, 0.00011947488522988286, 0.00011946657305266344, 0.00011945827099542416, 0.00011944989428259371, 0.00011944150012642515, 0.0001194331295756435, 0.00011942474174742505, 0.00011941631884292885, 0.00011940787387355792, 0.00011939944561281929, 0.00011939103015018203, 0.00011938257906616269, 0.00011937411392793206, 0.00011936568749210789, 0.00011935727115256366, 0.00011934881449900021, 0.00011934031682041191, 0.00011933187334415281, 0.00011932337922059267, 0.00011931490782551249, 0.00011930640438777859, 0.00011929794169066729, 0.00011928949364027196, 0.00011928102527881581, 0.0001192725602827863, 0.00011926407909952862, 0.00011925558632687917, 0.00011924712905711084, 0.00011923862490837132, 0.00011923015010046412, 0.00011922165654570847, 0.0001192131209468199, 0.00011920462253352578, 0.00011919610089404776, 0.00011918758437381026, 0.00011917906951258591, 0.00011917056861077207, 0.00011916201059150598, 0.00011915347354690595, 0.00011914496875826136, 0.00011913642638111911, 0.00011912784037193137, 0.00011911932735932165, 0.00011911080996217722, 0.00011910223881300714, 0.0001190937325786511, 0.00011908517958382478, 0.00011907665780214562, 0.00011906821008355294, 0.00011905963803377573, 0.0001190511025533881, 0.00011904260245738066, 0.00011903405806572246, 0.0001190255530163758, 0.00011901701222714614, 0.0001190084863690347, 0.00011899991280244548], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 02:12:24,231 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:92]: done!
[2017-12-15 02:12:24,232 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:152]: >> Executing classifier part ... 
[2017-12-15 02:12:24,232 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:97]: =======================================
[2017-12-15 02:12:24,232 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fcf620c4438>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-15 02:12:25,035 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:110]: training ... 
[2017-12-15 07:47:31,652 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:122]: trained!
[2017-12-15 07:47:31,654 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:125]: Training history: 
{'val_loss': [0.00011946200016649733, 0.00011945811811238482, 0.00011945419061482205, 0.00011945027652504358, 0.00011944635696488911, 0.00011944239423166919, 0.00011943847842569433, 0.00011943452893936531, 0.0001194305689055793, 0.00011942662447845421, 0.00011942267155973241, 0.0001194186901092456, 0.0001194146936062863, 0.00011941071313903701, 0.00011940671223832446, 0.00011940269646390984, 0.00011939866401021156, 0.0001193946335229883, 0.00011939056796100133, 0.00011938650967497196, 0.00011938243038341385, 0.00011937833612876846, 0.00011937425658693171, 0.00011937014631445334, 0.00011936602157944497, 0.00011936188801317601, 0.0001193576861029612, 0.00011935351048988068, 0.00011934927795628653, 0.00011934504182940619, 0.00011934081812707263, 0.00011933656950413733, 0.00011933230287901695, 0.00011932804043712529, 0.00011932374284896171, 0.0001193194451714129, 0.00011931510911184691, 0.00011931074410934385, 0.00011930635286333767, 0.00011930193505204152, 0.0001192975047446904, 0.00011929299547442828, 0.00011928849678737723, 0.00011928394909934381, 0.00011927940141131039, 0.00011927479830443521, 0.00011927019446460113, 0.00011926554426958748, 0.00011926092251695358, 0.0001192563042860977, 0.00011925163995034086, 0.00011924693074319923, 0.00011924215826919275, 0.00011923735047014388, 0.00011923251194045341, 0.00011922768247442512, 0.00011922279873368598, 0.0001192178939874181, 0.00011921292457987579, 0.00011920794283717193, 0.00011920291490018189, 0.00011919784600688007, 0.0001191927563404511, 0.00011918767223378336, 0.00011918253744569096, 0.0001191774048564752, 0.00011917222618023551, 0.00011916697090085486, 0.00011916167539812131, 0.00011915638170096936, 0.00011915103331793452, 0.00011914564610595635, 0.00011914023543929416, 0.00011913475683985817, 0.00011912921751209784, 0.00011912366731509369, 0.00011911804698643913, 0.0001191123210938295, 0.00011910660769727483, 0.00011910086430303743, 0.00011909503222077613, 0.00011908919343462269, 0.00011908329604751333, 0.00011907736826942621, 0.00011907139199091401, 0.00011906538914711185, 0.00011905926981647968, 0.00011905311739543586, 0.00011904694863477223, 0.00011904070997485979, 0.00011903446568367792, 0.00011902817773675038, 0.00011902184949496176, 0.00011901547327118237, 0.00011900902773809668, 0.00011900260936024345, 0.00011899615737354425, 0.0001189896271568929, 0.00011898307732912238, 0.00011897647885791042, 0.00011896986077557929, 0.00011896317949789148, 0.00011895642644386505, 0.00011894965073962167, 0.00011894285730134893, 0.00011893596469910349, 0.00011892903294612788, 0.00011892204405811412, 0.00011891496026086454, 0.00011890780461576816, 0.00011890065755165371, 0.00011889346536587585, 0.0001188862462930212, 0.00011887896173654802, 0.00011887170792859349, 0.00011886440996433602, 0.00011885709442694257, 0.00011884979108169432, 0.00011884241612100085, 0.00011883497855489322, 0.00011882751074082422, 0.00011881997613811233, 0.00011881246720683813, 0.0001188049142086462, 0.00011879727661627384, 0.00011878952727448859, 0.0001187818187281217, 0.00011877400383120975, 0.0001187662490190485, 0.0001187584625645163, 0.00011875070228197907, 0.00011874282157966176, 0.00011873497446831339, 0.00011872711387767253, 0.00011871926326242319, 0.00011871147600342394, 0.00011870366707744525, 0.00011869584286659247, 0.00011868794737995833, 0.00011868002386211649, 0.00011867202284728138, 0.00011866407521330484, 0.00011865608204649281, 0.00011864808249757545, 0.00011864007642353641, 0.00011863197880114614, 0.00011862386564360314, 0.00011861578895523343, 0.00011860769746466987, 0.00011859958545125778, 0.00011859145039433376, 0.00011858327135987722, 0.00011857508392320918, 0.00011856685438609844, 0.00011855858741445394, 0.0001185503486527876, 0.00011854212574806083, 0.00011853385175073735, 0.00011852554260714195, 0.00011851727695839882, 0.00011850894525397171, 0.00011850063146234442, 0.0001184922815412077, 0.00011848398761097822, 0.00011847568265061153, 0.00011846734712049663, 0.00011845902982496837, 0.00011845068791274815, 0.00011844234297930719, 0.00011843402870499967, 0.0001184256856486485, 0.00011841736892518572, 0.00011840904442520803, 0.0001184006746426735, 0.00011839235366447382, 0.00011838401232431905, 0.00011837567237857386, 0.00011836733497136915, 0.00011835900916636714, 0.00011835062965871973, 0.00011834226384488935, 0.00011833392738516807, 0.00011832555985514129, 0.00011831714720345109, 0.0001183087900599879, 0.00011830042801821416, 0.00011829200376432128, 0.00011828364662085809, 0.00011827524689427194, 0.00011826686883466525, 0.0001182585810183854, 0.00011825016804490838, 0.00011824179348920264, 0.00011823345295351492, 0.00011822507080006475, 0.00011821672379288649, 0.00011820834246178041, 0.00011819998140324419, 0.00011819156492586621, 0.00011818315229205305], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012031410015355007, 0.00012031000990406171, 0.00012030593558109944, 0.00012030182644256151, 0.0001202977322351418, 0.00012029362693603426, 0.00012028948265011802, 0.0001202853657853189, 0.00012028121344133884, 0.00012027704732754974, 0.00012027288903482258, 0.00012026870599909949, 0.0001202644937647452, 0.00012026026536686291, 0.00012025606747112216, 0.00012025183641881885, 0.00012024760645672418, 0.00012024335542516267, 0.00012023910190508146, 0.00012023480520325831, 0.00012023050769562879, 0.00012022619054054368, 0.00012022186459268895, 0.00012021756443063841, 0.00012021324066320094, 0.0001202089060884779, 0.00012020455238770341, 0.00012020014033706689, 0.00012019574772058424, 0.00012019131739710303, 0.00012018688285498841, 0.00012018246670421941, 0.00012017801974320645, 0.00012017355351394092, 0.00012016910226319397, 0.00012016461944379708, 0.00012016013918402043, 0.00012015562304215967, 0.00012015108523832738, 0.00012014651934977269, 0.00012014193819829714, 0.00012013735334959233, 0.00012013270965332153, 0.00012012808688431645, 0.00012012343169345463, 0.0001201187882341857, 0.00012011409429351705, 0.00012010937731626598, 0.00012010461687287072, 0.00012009988494080124, 0.00012009514646747996, 0.00012009035594854655, 0.00012008553950160784, 0.00012008066242958908, 0.00012007573736469028, 0.00012007078039933888, 0.00012006582021076196, 0.00012006081451782471, 0.00012005575654227348, 0.00012005065702029325, 0.0001200455292950897, 0.0001200403319150346, 0.0001200350960695749, 0.00012002982540853953, 0.00012002452173314273, 0.00012001915918647976, 0.00012001377590215258, 0.00012000834099881666, 0.00012000286213163263, 0.00011999731358589683, 0.00011999175788270437, 0.00011998613202695641, 0.0001199804618518575, 0.00011997477162639983, 0.00011996901655722871, 0.00011996322854479671, 0.00011995740704399956, 0.00011995153809542667, 0.00011994554069183663, 0.00011993955300532355, 0.00011993350163640618, 0.00011992740258271123, 0.0001199212835023577, 0.00011991512178536656, 0.00011990895717695252, 0.00011990273881651286, 0.00011989650547755458, 0.00011989017572623286, 0.00011988384919813667, 0.00011987751667389301, 0.00011987111141377338, 0.00011986469373475542, 0.00011985823896494376, 0.00011985176343376769, 0.00011984527159686251, 0.0001198387371707201, 0.00011983223156400589, 0.00011982566912424165, 0.00011981903655562209, 0.00011981236014165536, 0.00011980566370103004, 0.00011979892386536111, 0.00011979212532432731, 0.00011978525807644947, 0.00011977834044197264, 0.00011977138322818241, 0.00011976434311113572, 0.00011975723267563222, 0.00011975005744381562, 0.00011974280428578197, 0.00011973549204318047, 0.00011972815370667234, 0.00011972078929995778, 0.00011971338991176622, 0.00011970590560580224, 0.00011969843639685781, 0.00011969089926317552, 0.00011968333579858474, 0.00011967579781169569, 0.00011966817995517421, 0.00011966051612028864, 0.0001196528047665268, 0.00011964500261734601, 0.00011963721103844892, 0.00011962934563346723, 0.00011962142088321562, 0.00011961341728243965, 0.00011960543382682318, 0.00011959733869592244, 0.00011958928598835761, 0.00011958121128701862, 0.00011957314554435058, 0.00011956497362484178, 0.00011955681440863358, 0.0001195486545762205, 0.00011954049438830458, 0.0001195323927872526, 0.00011952426810221782, 0.00011951611692037324, 0.00011950791134659795, 0.00011949970911454913, 0.00011949142999909151, 0.00011948319378097356, 0.00011947488522988286, 0.00011946657305266344, 0.00011945827099542416, 0.00011944989428259371, 0.00011944150012642515, 0.0001194331295756435, 0.00011942474174742505, 0.00011941631884292885, 0.00011940787387355792, 0.00011939944561281929, 0.00011939103015018203, 0.00011938257906616269, 0.00011937411392793206, 0.00011936568749210789, 0.00011935727115256366, 0.00011934881449900021, 0.00011934031682041191, 0.00011933187334415281, 0.00011932337922059267, 0.00011931490782551249, 0.00011930640438777859, 0.00011929794169066729, 0.00011928949364027196, 0.00011928102527881581, 0.0001192725602827863, 0.00011926407909952862, 0.00011925558632687917, 0.00011924712905711084, 0.00011923862490837132, 0.00011923015010046412, 0.00011922165654570847, 0.0001192131209468199, 0.00011920462253352578, 0.00011919610089404776, 0.00011918758437381026, 0.00011917906951258591, 0.00011917056861077207, 0.00011916201059150598, 0.00011915347354690595, 0.00011914496875826136, 0.00011913642638111911, 0.00011912784037193137, 0.00011911932735932165, 0.00011911080996217722, 0.00011910223881300714, 0.0001190937325786511, 0.00011908517958382478, 0.00011907665780214562, 0.00011906821008355294, 0.00011905963803377573, 0.0001190511025533881, 0.00011904260245738066, 0.00011903405806572246, 0.0001190255530163758, 0.00011901701222714614, 0.0001190084863690347, 0.00011899991280244548], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 07:47:31,655 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:129]: evaluating model ... 
[2017-12-15 07:47:35,763 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:133]: evaluated! 
[2017-12-15 07:47:35,764 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:135]: generating reports ... 
[2017-12-15 07:47:37,511 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:138]: done!
[2017-12-15 07:47:37,512 AE_BIGRAMA_1L_MINIDS_OVER_F1_4.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_4 finished!
