[2017-12-14 09:31:57,903 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_5
[2017-12-14 09:31:57,903 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:146]: >> Printing header log
[2017-12-14 09:31:57,903 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_5
	layers = 9216,4608
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f3752bb0eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f3752b93400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,903 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:148]: >> Loading dataset... 
[2017-12-14 09:32:21,808 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:21,809 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:21,809 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:57]: =======================================
[2017-12-14 09:32:21,809 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f3752bb0eb8>, 'discard_decoder_function': True}
[2017-12-14 09:32:21,852 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:55,066 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_5
[2017-12-14 10:18:55,066 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:146]: >> Printing header log
[2017-12-14 10:18:55,066 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_5
	layers = 9216,4608
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f5bd5d43eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f5bd5d26400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:55,067 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:148]: >> Loading dataset... 
[2017-12-14 10:19:17,196 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:17,197 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:17,197 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:57]: =======================================
[2017-12-14 10:19:17,197 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f5bd5d43eb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:17,240 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:73]: training and evaluate autoencoder
[2017-12-14 17:31:35,218 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:85]: trained and evaluated!
[2017-12-14 17:31:35,219 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:88]: Training history: 
{'val_loss': [0.00011775317376680301, 0.00011775037808288278, 0.00011774756041019629, 0.00011774473628389628, 0.0001177419123184897, 0.00011773911123570164, 0.00011773630271606257, 0.00011773347900093463, 0.00011773066860420575, 0.00011772784921532287, 0.00011772503325883276, 0.00011772220643309885, 0.000117719389010691, 0.00011771657321509431, 0.00011771376666193027, 0.00011771094670098193, 0.00011770814472434159, 0.00011770533588291569, 0.00011770254003810204, 0.00011769972563691492, 0.00011769692938092922, 0.00011769412887020664, 0.00011769132803769722, 0.00011768854347329943, 0.0001176857414966591, 0.00011768293412115096, 0.00011768013157244516, 0.00011767732918463276, 0.00011767452263146873, 0.00011767173079111333, 0.00011766892963681708, 0.00011766613223670045, 0.00011766333115391241, 0.0001176605320375994, 0.00011765772720063174, 0.00011765491975361541, 0.00011765211834904053, 0.00011764931964389956, 0.00011764651325162893, 0.00011764371626268436, 0.00011764091755754339, 0.00011763812097977087, 0.00011763532227462991, 0.00011763251171700762, 0.00011762970916830182, 0.00011762690114922003, 0.00011762410587647184, 0.00011762130440038878, 0.0001176184938427665, 0.00011761568745049587, 0.00011761288842356807, 0.00011761008456983793, 0.00011760727499545317, 0.0001176044664043059, 0.00011760167595836006, 0.00011759886581190983, 0.00011759607854807813, 0.0001175932648083417, 0.00011759046594230732, 0.00011758767091983778, 0.00011758486960464814, 0.00011758206991626966, 0.00011757927130051392, 0.00011757647275626636, 0.00011757366155507043, 0.00011757086097283967, 0.00011756807281515568, 0.00011756527934798911, 0.00011756248636350279, 0.00011755968733657499, 0.0001175568767789527, 0.00011755407072634594, 0.00011755127373740137, 0.00011754848493614374, 0.00011754569865554955, 0.00011754288704318156, 0.00011754010419498017, 0.00011753732127527058, 0.00011753452151538393, 0.00011753172820911077, 0.00011752894160672977, 0.00011752614976637436, 0.00011752336471929634, 0.00011752056381527874, 0.00011751777737379116, 0.00011751497704183902, 0.00011751218643499977, 0.00011750939884938126, 0.00011750660604366539, 0.00011750381690274388, 0.00011750103013946946, 0.00011749824639741578, 0.00011749545995592819, 0.00011749268635015944, 0.0001174899028583844, 0.00011748711126830763, 0.00011748433766253887, 0.00011748154819983055, 0.00011747877042871014, 0.00011747598840285285, 0.00011747322150097624, 0.00011747043661479163, 0.00011746766129282649, 0.00011746487665692052, 0.00011746210330143041, 0.00011745931424989413, 0.00011745654293238724, 0.00011745376646629115, 0.00011745097585945192, 0.00011744821108494375, 0.00011744541532951533, 0.00011744264327904955, 0.00011743984605770339, 0.00011743706526536224, 0.00011743428381157041, 0.00011743150359129473, 0.00011742869982694982, 0.00011742591617428137, 0.00011742313700875139, 0.00011742035907673757, 0.00011741757917824871, 0.00011741481007749549, 0.00011741202356449971, 0.00011740923295766046, 0.00011740644170724757, 0.00011740365306688335, 0.00011740086589243689, 0.00011739807439174536, 0.00011739527913687422, 0.00011739249310655867, 0.00011738970716562837, 0.00011738692751741815, 0.00011738414427592175, 0.00011738135849588484, 0.0001173785758264539, 0.00011737579004641701, 0.00011737298923178464, 0.00011737020271878887, 0.00011736742478677504, 0.00011736466083461099, 0.00011736189663216828, 0.00011735912098841632, 0.00011735633358156826, 0.0001173535491065557, 0.00011735076668740339, 0.00011734799218778236, 0.0001173452255361844, 0.00011734245389689067, 0.0001173396656676985, 0.00011733689542281435, 0.0001173341209231933, 0.00011733132926160835, 0.00011732855043574225, 0.00011732576449481194, 0.00011732298608011788, 0.00011732020242744942, 0.00011731742630101722, 0.00011731465834439492, 0.00011731188343360183, 0.0001173091124557588, 0.00011730633035839332, 0.00011730354597276598, 0.00011730076886309627, 0.0001172980003344085, 0.00011729522150854239, 0.0001172924499586339, 0.00011728967079310392, 0.0001172868899292546, 0.00011728413080389289, 0.00011728136014783667, 0.00011727858303816696, 0.0001172758208021993, 0.00011727304884111874, 0.00011727028873251952, 0.0001172675179155699, 0.00011726474922598873, 0.00011726197939227663, 0.00011725921381330141, 0.00011725644757287549, 0.00011725366604757547, 0.00011725090406188645, 0.00011724813136784703, 0.00011724536954305141, 0.00011724259153952941, 0.00011723982186671072, 0.00011723705365980978, 0.00011723429592885764, 0.00011723153181580017, 0.00011722875756645776, 0.00011722600727235665, 0.00011722322256494249, 0.00011722043817931516, 0.00011721766205288296, 0.00011721488780354056, 0.00011721212892845748, 0.00011720936481540003, 0.00011720658084094473, 0.00011720380700277439, 0.00011720104567853606, 0.00011719828099341312], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011850850363125485, 0.00011850558075821027, 0.00011850269620836915, 0.00011849979058906121, 0.00011849686856922339, 0.00011849396119610155, 0.00011849106221284614, 0.0001184881655996095, 0.00011848524941001785, 0.0001184823501423602, 0.00011847943639388788, 0.0001184765256316392, 0.00011847360839923929, 0.00011847070067061464, 0.00011846778976616484, 0.0001184648887209931, 0.00011846197753214104, 0.00011845908525603875, 0.00011845617515739533, 0.0001184532890907422, 0.00011845038079331304, 0.00011844748875421262, 0.00011844459560120339, 0.00011844170740153337, 0.0001184388241078022, 0.00011843592721016332, 0.00011843302915121522, 0.00011843012919625213, 0.00011842722962049204, 0.00011842432587349891, 0.00011842143750792757, 0.00011841854290920689, 0.00011841565236321829, 0.00011841275774079743, 0.00011840986491959081, 0.00011840696081709486, 0.00011840406221304247, 0.00011840116676111503, 0.00011839826716165474, 0.00011839537332134006, 0.0001183924727027717, 0.00011838958490600488, 0.00011838669357791009, 0.00011838379682247232, 0.00011838090236595277, 0.0001183780131234745, 0.00011837511534892866, 0.00011837223094128867, 0.00011836933916289032, 0.00011836643956343003, 0.00011836354683702417, 0.00011836065531932788, 0.00011835776010440232, 0.00011835486214025498, 0.00011835197088326076, 0.0001183490962163979, 0.0001183462005037684, 0.00011834333223595621, 0.00011834043173588879, 0.0001183375430859152, 0.00011833466372641519, 0.00011833177322782697, 0.000118328890716202, 0.00011832600227953011, 0.00011832312036040983, 0.00011832022291766662, 0.0001183173397187362, 0.00011831446308475776, 0.00011831158557387239, 0.00011830870621437237, 0.00011830582251773801, 0.00011830292836932089, 0.00011830003907944224, 0.00011829715737362365, 0.00011829428254085947, 0.00011829140692598912, 0.00011828851704360577, 0.000118285644912663, 0.00011828277865936677, 0.00011827989692984798, 0.00011827700818507365, 0.00011827414050976615, 0.00011827126705161287, 0.00011826839442296614, 0.00011826550899621809, 0.00011826263762368133, 0.0001182597546143524, 0.00011825688482972821, 0.00011825401030506648, 0.00011825113343408616, 0.00011824825909902594, 0.00011824538711028429, 0.00011824251784706422, 0.0001182396434883038, 0.00011823678778159107, 0.00011823391721486068, 0.00011823104873374682, 0.00011822819075181607, 0.00011822532030358662, 0.00011822246016493881, 0.00011821959929158516, 0.00011821674505428408, 0.00011821388064960248, 0.00011821102089015767, 0.00011820815515826557, 0.00011820529184379261, 0.00011820242596969936, 0.0001181995711161934, 0.00011819671014803901, 0.00011819383453316866, 0.00011819098200228108, 0.00011818810574750564, 0.00011818524558515765, 0.00011818236006360884, 0.00011817949535082479, 0.00011817662703561224, 0.00011817375800939404, 0.00011817087341215255, 0.00011816800407783191, 0.00011816514083445952, 0.0001181622759083738, 0.00011815940704805691, 0.00011815654809441848, 0.0001181536771247849, 0.00011815080030120497, 0.00011814792153420964, 0.00011814504402332427, 0.00011814216433202162, 0.00011813929461849799, 0.00011813641464279309, 0.00011813354684898466, 0.00011813067410183701, 0.00011812781242267699, 0.00011812494981920965, 0.00011812208413471793, 0.00011811922191045359, 0.00011811635568085755, 0.00011811347385653802, 0.00011811060930965529, 0.00011810775383994445, 0.00011810490652309816, 0.00011810206295088152, 0.00011809920769447236, 0.0001180963364878369, 0.00011809347144325024, 0.0001180906078917754, 0.0001180877469710214, 0.00011808490079178412, 0.00011808205288243313, 0.00011807918371401382, 0.0001180763314438283, 0.0001180734782019351, 0.00011807060287146698, 0.00011806774842086422, 0.00011806488200166666, 0.00011806203025288528, 0.00011805916399958904, 0.00011805631049699378, 0.00011805346341714937, 0.00011805060768673646, 0.00011804775887677834, 0.00011804489606000932, 0.00011804202648868681, 0.00011803917815273245, 0.0001180363236547293, 0.00011803346332647998, 0.00011803061515642694, 0.0001180277566056917, 0.00011802489217730992, 0.00011802205403243626, 0.00011801920287615957, 0.00011801634543933315, 0.00011801349601317015, 0.00011801064476209271, 0.00011800780699642205, 0.00011800495235621778, 0.00011800210487347018, 0.00011799925412009668, 0.00011799640438583124, 0.00011799355581287501, 0.00011799068944107784, 0.00011798785212571074, 0.00011798499494958638, 0.000117982151661772, 0.0001179792916890255, 0.00011797643735692366, 0.00011797358821516293, 0.00011797074687076392, 0.00011796790156843359, 0.00011796504673862781, 0.00011796220691104082, 0.00011795934471047669, 0.00011795647788837595, 0.00011795361341259379, 0.00011795075547806342, 0.00011794791453656761, 0.0001179450593986594, 0.00011794219677149187, 0.00011793934040117391, 0.00011793648943449871], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-14 17:31:35,220 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:92]: done!
[2017-12-14 17:31:35,220 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:152]: >> Executing classifier part ... 
[2017-12-14 17:31:35,220 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:97]: =======================================
[2017-12-14 17:31:35,220 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f5bd5d26400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-14 17:31:35,449 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:110]: training ... 
[2017-12-14 20:57:20,696 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:122]: trained!
[2017-12-14 20:57:20,699 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:125]: Training history: 
{'val_loss': [0.00011775317376680301, 0.00011775037808288278, 0.00011774756041019629, 0.00011774473628389628, 0.0001177419123184897, 0.00011773911123570164, 0.00011773630271606257, 0.00011773347900093463, 0.00011773066860420575, 0.00011772784921532287, 0.00011772503325883276, 0.00011772220643309885, 0.000117719389010691, 0.00011771657321509431, 0.00011771376666193027, 0.00011771094670098193, 0.00011770814472434159, 0.00011770533588291569, 0.00011770254003810204, 0.00011769972563691492, 0.00011769692938092922, 0.00011769412887020664, 0.00011769132803769722, 0.00011768854347329943, 0.0001176857414966591, 0.00011768293412115096, 0.00011768013157244516, 0.00011767732918463276, 0.00011767452263146873, 0.00011767173079111333, 0.00011766892963681708, 0.00011766613223670045, 0.00011766333115391241, 0.0001176605320375994, 0.00011765772720063174, 0.00011765491975361541, 0.00011765211834904053, 0.00011764931964389956, 0.00011764651325162893, 0.00011764371626268436, 0.00011764091755754339, 0.00011763812097977087, 0.00011763532227462991, 0.00011763251171700762, 0.00011762970916830182, 0.00011762690114922003, 0.00011762410587647184, 0.00011762130440038878, 0.0001176184938427665, 0.00011761568745049587, 0.00011761288842356807, 0.00011761008456983793, 0.00011760727499545317, 0.0001176044664043059, 0.00011760167595836006, 0.00011759886581190983, 0.00011759607854807813, 0.0001175932648083417, 0.00011759046594230732, 0.00011758767091983778, 0.00011758486960464814, 0.00011758206991626966, 0.00011757927130051392, 0.00011757647275626636, 0.00011757366155507043, 0.00011757086097283967, 0.00011756807281515568, 0.00011756527934798911, 0.00011756248636350279, 0.00011755968733657499, 0.0001175568767789527, 0.00011755407072634594, 0.00011755127373740137, 0.00011754848493614374, 0.00011754569865554955, 0.00011754288704318156, 0.00011754010419498017, 0.00011753732127527058, 0.00011753452151538393, 0.00011753172820911077, 0.00011752894160672977, 0.00011752614976637436, 0.00011752336471929634, 0.00011752056381527874, 0.00011751777737379116, 0.00011751497704183902, 0.00011751218643499977, 0.00011750939884938126, 0.00011750660604366539, 0.00011750381690274388, 0.00011750103013946946, 0.00011749824639741578, 0.00011749545995592819, 0.00011749268635015944, 0.0001174899028583844, 0.00011748711126830763, 0.00011748433766253887, 0.00011748154819983055, 0.00011747877042871014, 0.00011747598840285285, 0.00011747322150097624, 0.00011747043661479163, 0.00011746766129282649, 0.00011746487665692052, 0.00011746210330143041, 0.00011745931424989413, 0.00011745654293238724, 0.00011745376646629115, 0.00011745097585945192, 0.00011744821108494375, 0.00011744541532951533, 0.00011744264327904955, 0.00011743984605770339, 0.00011743706526536224, 0.00011743428381157041, 0.00011743150359129473, 0.00011742869982694982, 0.00011742591617428137, 0.00011742313700875139, 0.00011742035907673757, 0.00011741757917824871, 0.00011741481007749549, 0.00011741202356449971, 0.00011740923295766046, 0.00011740644170724757, 0.00011740365306688335, 0.00011740086589243689, 0.00011739807439174536, 0.00011739527913687422, 0.00011739249310655867, 0.00011738970716562837, 0.00011738692751741815, 0.00011738414427592175, 0.00011738135849588484, 0.0001173785758264539, 0.00011737579004641701, 0.00011737298923178464, 0.00011737020271878887, 0.00011736742478677504, 0.00011736466083461099, 0.00011736189663216828, 0.00011735912098841632, 0.00011735633358156826, 0.0001173535491065557, 0.00011735076668740339, 0.00011734799218778236, 0.0001173452255361844, 0.00011734245389689067, 0.0001173396656676985, 0.00011733689542281435, 0.0001173341209231933, 0.00011733132926160835, 0.00011732855043574225, 0.00011732576449481194, 0.00011732298608011788, 0.00011732020242744942, 0.00011731742630101722, 0.00011731465834439492, 0.00011731188343360183, 0.0001173091124557588, 0.00011730633035839332, 0.00011730354597276598, 0.00011730076886309627, 0.0001172980003344085, 0.00011729522150854239, 0.0001172924499586339, 0.00011728967079310392, 0.0001172868899292546, 0.00011728413080389289, 0.00011728136014783667, 0.00011727858303816696, 0.0001172758208021993, 0.00011727304884111874, 0.00011727028873251952, 0.0001172675179155699, 0.00011726474922598873, 0.00011726197939227663, 0.00011725921381330141, 0.00011725644757287549, 0.00011725366604757547, 0.00011725090406188645, 0.00011724813136784703, 0.00011724536954305141, 0.00011724259153952941, 0.00011723982186671072, 0.00011723705365980978, 0.00011723429592885764, 0.00011723153181580017, 0.00011722875756645776, 0.00011722600727235665, 0.00011722322256494249, 0.00011722043817931516, 0.00011721766205288296, 0.00011721488780354056, 0.00011721212892845748, 0.00011720936481540003, 0.00011720658084094473, 0.00011720380700277439, 0.00011720104567853606, 0.00011719828099341312], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011850850363125485, 0.00011850558075821027, 0.00011850269620836915, 0.00011849979058906121, 0.00011849686856922339, 0.00011849396119610155, 0.00011849106221284614, 0.0001184881655996095, 0.00011848524941001785, 0.0001184823501423602, 0.00011847943639388788, 0.0001184765256316392, 0.00011847360839923929, 0.00011847070067061464, 0.00011846778976616484, 0.0001184648887209931, 0.00011846197753214104, 0.00011845908525603875, 0.00011845617515739533, 0.0001184532890907422, 0.00011845038079331304, 0.00011844748875421262, 0.00011844459560120339, 0.00011844170740153337, 0.0001184388241078022, 0.00011843592721016332, 0.00011843302915121522, 0.00011843012919625213, 0.00011842722962049204, 0.00011842432587349891, 0.00011842143750792757, 0.00011841854290920689, 0.00011841565236321829, 0.00011841275774079743, 0.00011840986491959081, 0.00011840696081709486, 0.00011840406221304247, 0.00011840116676111503, 0.00011839826716165474, 0.00011839537332134006, 0.0001183924727027717, 0.00011838958490600488, 0.00011838669357791009, 0.00011838379682247232, 0.00011838090236595277, 0.0001183780131234745, 0.00011837511534892866, 0.00011837223094128867, 0.00011836933916289032, 0.00011836643956343003, 0.00011836354683702417, 0.00011836065531932788, 0.00011835776010440232, 0.00011835486214025498, 0.00011835197088326076, 0.0001183490962163979, 0.0001183462005037684, 0.00011834333223595621, 0.00011834043173588879, 0.0001183375430859152, 0.00011833466372641519, 0.00011833177322782697, 0.000118328890716202, 0.00011832600227953011, 0.00011832312036040983, 0.00011832022291766662, 0.0001183173397187362, 0.00011831446308475776, 0.00011831158557387239, 0.00011830870621437237, 0.00011830582251773801, 0.00011830292836932089, 0.00011830003907944224, 0.00011829715737362365, 0.00011829428254085947, 0.00011829140692598912, 0.00011828851704360577, 0.000118285644912663, 0.00011828277865936677, 0.00011827989692984798, 0.00011827700818507365, 0.00011827414050976615, 0.00011827126705161287, 0.00011826839442296614, 0.00011826550899621809, 0.00011826263762368133, 0.0001182597546143524, 0.00011825688482972821, 0.00011825401030506648, 0.00011825113343408616, 0.00011824825909902594, 0.00011824538711028429, 0.00011824251784706422, 0.0001182396434883038, 0.00011823678778159107, 0.00011823391721486068, 0.00011823104873374682, 0.00011822819075181607, 0.00011822532030358662, 0.00011822246016493881, 0.00011821959929158516, 0.00011821674505428408, 0.00011821388064960248, 0.00011821102089015767, 0.00011820815515826557, 0.00011820529184379261, 0.00011820242596969936, 0.0001181995711161934, 0.00011819671014803901, 0.00011819383453316866, 0.00011819098200228108, 0.00011818810574750564, 0.00011818524558515765, 0.00011818236006360884, 0.00011817949535082479, 0.00011817662703561224, 0.00011817375800939404, 0.00011817087341215255, 0.00011816800407783191, 0.00011816514083445952, 0.0001181622759083738, 0.00011815940704805691, 0.00011815654809441848, 0.0001181536771247849, 0.00011815080030120497, 0.00011814792153420964, 0.00011814504402332427, 0.00011814216433202162, 0.00011813929461849799, 0.00011813641464279309, 0.00011813354684898466, 0.00011813067410183701, 0.00011812781242267699, 0.00011812494981920965, 0.00011812208413471793, 0.00011811922191045359, 0.00011811635568085755, 0.00011811347385653802, 0.00011811060930965529, 0.00011810775383994445, 0.00011810490652309816, 0.00011810206295088152, 0.00011809920769447236, 0.0001180963364878369, 0.00011809347144325024, 0.0001180906078917754, 0.0001180877469710214, 0.00011808490079178412, 0.00011808205288243313, 0.00011807918371401382, 0.0001180763314438283, 0.0001180734782019351, 0.00011807060287146698, 0.00011806774842086422, 0.00011806488200166666, 0.00011806203025288528, 0.00011805916399958904, 0.00011805631049699378, 0.00011805346341714937, 0.00011805060768673646, 0.00011804775887677834, 0.00011804489606000932, 0.00011804202648868681, 0.00011803917815273245, 0.0001180363236547293, 0.00011803346332647998, 0.00011803061515642694, 0.0001180277566056917, 0.00011802489217730992, 0.00011802205403243626, 0.00011801920287615957, 0.00011801634543933315, 0.00011801349601317015, 0.00011801064476209271, 0.00011800780699642205, 0.00011800495235621778, 0.00011800210487347018, 0.00011799925412009668, 0.00011799640438583124, 0.00011799355581287501, 0.00011799068944107784, 0.00011798785212571074, 0.00011798499494958638, 0.000117982151661772, 0.0001179792916890255, 0.00011797643735692366, 0.00011797358821516293, 0.00011797074687076392, 0.00011796790156843359, 0.00011796504673862781, 0.00011796220691104082, 0.00011795934471047669, 0.00011795647788837595, 0.00011795361341259379, 0.00011795075547806342, 0.00011794791453656761, 0.0001179450593986594, 0.00011794219677149187, 0.00011793934040117391, 0.00011793648943449871], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-14 20:57:20,699 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:129]: evaluating model ... 
[2017-12-14 20:57:28,353 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:133]: evaluated! 
[2017-12-14 20:57:28,354 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:135]: generating reports ... 
[2017-12-14 20:57:31,768 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:138]: done!
[2017-12-14 20:57:31,768 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_5 finished!
[2018-04-29 11:38:17,232 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_5 was already executed!
[2018-04-29 11:42:07,380 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_5 was already executed!
[2018-04-29 13:12:02,670 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_5
[2018-04-29 13:12:02,670 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:146]: >> Printing header log
[2018-04-29 13:12:02,670 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_5
	layers = 9216,4608
	using GLOBAL obj = 
		{'store_history': True, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'autoencoder_configs': {'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fe1c9daf828>, 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu'}, 'batch': 32, 'shuffle_batches': True, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'mlp_configs': {'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fe1c9daf898>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'activation': 'sigmoid'}, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'numpy_seed': 666, 'epochs': 200, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9]}
	=======================================
	
[2018-04-29 13:12:02,670 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,651 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,651 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,651 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:57]: =======================================
[2018-04-29 13:12:20,651 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:62]: setting configurations for autoencoder: 
	 {'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fe1c9daf828>, 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu'}
[2018-04-29 13:12:20,700 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,852 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_5
[2018-04-29 13:14:12,852 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:146]: >> Printing header log
[2018-04-29 13:14:12,852 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_5
	layers = 9216,4608
	using GLOBAL obj = 
		{'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'epochs': 200, 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'shuffle_batches': True, 'autoencoder_configs': {'discard_decoder_function': True, 'loss_function': 'mse', 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f49661f4860>, 'hidden_layer_activation': 'relu'}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'store_history': True, 'batch': 32, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'mlp_configs': {'activation': 'sigmoid', 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f49661f48d0>, 'use_last_dim_as_classifier': False}, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'numpy_seed': 666}
	=======================================
	
[2018-04-29 13:14:12,852 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:148]: >> Loading dataset... 
[2018-04-29 13:14:30,947 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:30,947 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:30,947 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:57]: =======================================
[2018-04-29 13:14:30,947 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'loss_function': 'mse', 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f49661f4860>, 'hidden_layer_activation': 'relu'}
[2018-04-29 13:14:30,993 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,781 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_5
[2018-04-29 13:16:33,781 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:146]: >> Printing header log
[2018-04-29 13:16:33,781 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_5
	layers = 9216,4608
	using GLOBAL obj = 
		{'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'numpy_seed': 666, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'mlp_configs': {'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7ff4216d7898>}, 'epochs': 200, 'autoencoder_configs': {'loss_function': 'mse', 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7ff4216d7828>}, 'shuffle_batches': True, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'batch': 32, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'store_history': True, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s'}
	=======================================
	
[2018-04-29 13:16:33,782 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:148]: >> Loading dataset... 
[2018-04-29 13:16:58,935 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:58,935 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:58,935 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:57]: =======================================
[2018-04-29 13:16:58,935 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:62]: setting configurations for autoencoder: 
	 {'loss_function': 'mse', 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7ff4216d7828>}
[2018-04-29 13:16:58,990 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:21,131 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_5
[2018-04-29 14:30:21,132 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:146]: >> Printing header log
[2018-04-29 14:30:21,132 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_5
	layers = 9216,4608
	using GLOBAL obj = 
		{'store_history': True, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_dir': '/home/dhiego/malware_dataset/', 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f47c6bcd898>}, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'epochs': 200, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'shuffle_batches': True, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'batch': 32, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'mlp_configs': {'activation': 'sigmoid', 'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f47c6bcd908>}, 'numpy_seed': 666, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/'}
	=======================================
	
[2018-04-29 14:30:21,132 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:148]: >> Loading dataset... 
[2018-04-29 14:30:39,517 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:39,517 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:39,517 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:57]: =======================================
[2018-04-29 14:30:39,517 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f47c6bcd898>}
[2018-04-29 14:30:39,596 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:73]: training and evaluate autoencoder
[2018-04-29 18:02:02,867 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:85]: trained and evaluated!
[2018-04-29 18:02:02,868 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:88]: Training history: 
{'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011830669613035566, 0.00011830092259873834, 0.00011829509194966872, 0.00011828925082511618, 0.00011828341654991786, 0.00011827757142003358, 0.00011827170974741833, 0.00011826584691349386, 0.00011825997289308083, 0.00011825408097902611, 0.0001182481822867177, 0.00011824226442095747, 0.00011823633100787414, 0.00011823036289771607, 0.00011822435433133763, 0.00011821834962808978, 0.00011821232276516648, 0.00011820624930767421, 0.00011820015117828658, 0.00011819404174390944, 0.00011818793311533867, 0.00011818178422014905, 0.00011817560622112899, 0.000118169414973704, 0.0001181632404823117, 0.00011815700083910351, 0.00011815074474796507, 0.00011814452299839856, 0.00011813824116885631, 0.00011813198026657978, 0.00011812570782231186, 0.00011811938146011698, 0.00011811307173545384, 0.00011810673203005331, 0.0001181004040088453, 0.00011809406700526616, 0.0001180877098565275, 0.00011808133663906161, 0.00011807495180850375, 0.00011806847734383614, 0.00011806202383013442, 0.00011805554422252609, 0.00011804910258261839, 0.00011804259891931957, 0.00011803610658471045, 0.00011802958917530277, 0.0001180230656275465, 0.00011801649970385467, 0.00011800995468372837, 0.00011800337771574909, 0.00011799677702388195, 0.0001179901417771412, 0.00011798356743988275, 0.00011797693532156676, 0.00011797025570807469, 0.00011796357953110982, 0.00011795686526794339, 0.00011795014346811726, 0.00011794339531368246, 0.00011793667178374264, 0.00011792989305606573, 0.00011792315966684783, 0.00011791637508522459, 0.00011790961280547792, 0.00011790285744618606, 0.00011789610350890546, 0.00011788933909614191, 0.00011788254783106573, 0.00011787570854940934, 0.00011786889777907872, 0.00011786205513199567, 0.00011785515958609375, 0.00011784832378836495, 0.00011784143625312645, 0.00011783455260471875, 0.00011782768656555047, 0.00011782081313192364, 0.00011781394631064919, 0.00011780707268742087, 0.00011780022146086988, 0.00011779335636970912, 0.0001177864544721569, 0.00011777953418325905, 0.00011777266328555232, 0.00011776574998820984, 0.00011775884211821032, 0.00011775187457113827, 0.00011774492363789778, 0.00011773796751431617, 0.00011773100911551656, 0.00011772399587448268, 0.00011771703024712582, 0.00011771004414280682, 0.00011770302751264611, 0.00011769609215042891, 0.00011768909545212601, 0.00011768213285839318, 0.00011767515554684381, 0.00011766817970470607, 0.00011766118153699155, 0.00011765418998162938], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.00011750368625729366, 0.00011749806389065587, 0.00011749237802475164, 0.00011748668840466782, 0.00011748098432205398, 0.00011747527140817955, 0.00011746951648324765, 0.00011746375869798842, 0.00011745796773293232, 0.00011745214358807933, 0.0001174463102007937, 0.00011744048548387524, 0.00011743461834472726, 0.00011742871244814411, 0.00011742279618287445, 0.00011741685506851122, 0.00011741086498891971, 0.00011740484745018596, 0.00011739882572822351, 0.00011739282569111753, 0.00011738679007869053, 0.00011738074449087203, 0.00011737465313781643, 0.0001173685621244247, 0.0001173624088095286, 0.00011735625011364172, 0.00011735010988474309, 0.00011734392256770599, 0.00011733775584502557, 0.00011733158175700232, 0.00011732535673727577, 0.00011731912762370573, 0.00011731288206325363, 0.0001173066575977155, 0.00011730041883054077, 0.00011729415496399383, 0.0001172878820337847, 0.00011728161040859993, 0.0001172752433914991, 0.00011726890065142638, 0.00011726252668015476, 0.00011725619702607951, 0.00011724981275762955, 0.00011724344663438101, 0.00011723706179386559, 0.00011723066599472112, 0.00011722422875658465, 0.0001172177878536538, 0.00011721133681443801, 0.00011720485928144045, 0.000117198348568646, 0.00011719189434731608, 0.00011718539230488882, 0.00011717882266935162, 0.00011717225351649467, 0.00011716564912798057, 0.00011715904295176189, 0.00011715240970969599, 0.0001171458184965646, 0.00011713917038079662, 0.0001171325753241004, 0.00011712593064072521, 0.00011711930149250277, 0.00011711268117554093, 0.00011710607319374063, 0.00011709946791137425, 0.00011709284350056892, 0.00011708618648203218, 0.00011707954882433596, 0.00011707288942815215, 0.00011706617566787224, 0.00011705952379792466, 0.00011705281150356251, 0.00011704608467516218, 0.00011703937770815965, 0.00011703266101604424, 0.00011702594391275678, 0.00011701922273350289, 0.00011701253482343112, 0.00011700582851787929, 0.00011699908665488349, 0.000116992324590826, 0.00011698561714114323, 0.00011697887724462246, 0.00011697214306875634, 0.00011696536096453058, 0.00011695858997982728, 0.00011695182308896744, 0.00011694504482830653, 0.00011693821579683572, 0.00011693144432945219, 0.0001169246556107195, 0.00011691782421947867, 0.00011691106101129025, 0.00011690424426134987, 0.00011689746592918076, 0.00011689065944066463, 0.0001168838429052488, 0.00011687701467824506, 0.00011687018555738902, 0.00011686340000289343]}
[2018-04-29 18:02:02,868 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:92]: done!
[2018-04-29 18:02:02,869 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:152]: >> Executing classifier part ... 
[2018-04-29 18:02:02,869 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:97]: =======================================
[2018-04-29 18:02:02,869 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f47c6bcd908>}
[2018-04-29 18:02:02,937 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:110]: training ... 
[2018-04-29 21:35:52,161 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:122]: trained!
[2018-04-29 21:35:52,163 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:125]: Training history: 
{'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011830669613035566, 0.00011830092259873834, 0.00011829509194966872, 0.00011828925082511618, 0.00011828341654991786, 0.00011827757142003358, 0.00011827170974741833, 0.00011826584691349386, 0.00011825997289308083, 0.00011825408097902611, 0.0001182481822867177, 0.00011824226442095747, 0.00011823633100787414, 0.00011823036289771607, 0.00011822435433133763, 0.00011821834962808978, 0.00011821232276516648, 0.00011820624930767421, 0.00011820015117828658, 0.00011819404174390944, 0.00011818793311533867, 0.00011818178422014905, 0.00011817560622112899, 0.000118169414973704, 0.0001181632404823117, 0.00011815700083910351, 0.00011815074474796507, 0.00011814452299839856, 0.00011813824116885631, 0.00011813198026657978, 0.00011812570782231186, 0.00011811938146011698, 0.00011811307173545384, 0.00011810673203005331, 0.0001181004040088453, 0.00011809406700526616, 0.0001180877098565275, 0.00011808133663906161, 0.00011807495180850375, 0.00011806847734383614, 0.00011806202383013442, 0.00011805554422252609, 0.00011804910258261839, 0.00011804259891931957, 0.00011803610658471045, 0.00011802958917530277, 0.0001180230656275465, 0.00011801649970385467, 0.00011800995468372837, 0.00011800337771574909, 0.00011799677702388195, 0.0001179901417771412, 0.00011798356743988275, 0.00011797693532156676, 0.00011797025570807469, 0.00011796357953110982, 0.00011795686526794339, 0.00011795014346811726, 0.00011794339531368246, 0.00011793667178374264, 0.00011792989305606573, 0.00011792315966684783, 0.00011791637508522459, 0.00011790961280547792, 0.00011790285744618606, 0.00011789610350890546, 0.00011788933909614191, 0.00011788254783106573, 0.00011787570854940934, 0.00011786889777907872, 0.00011786205513199567, 0.00011785515958609375, 0.00011784832378836495, 0.00011784143625312645, 0.00011783455260471875, 0.00011782768656555047, 0.00011782081313192364, 0.00011781394631064919, 0.00011780707268742087, 0.00011780022146086988, 0.00011779335636970912, 0.0001177864544721569, 0.00011777953418325905, 0.00011777266328555232, 0.00011776574998820984, 0.00011775884211821032, 0.00011775187457113827, 0.00011774492363789778, 0.00011773796751431617, 0.00011773100911551656, 0.00011772399587448268, 0.00011771703024712582, 0.00011771004414280682, 0.00011770302751264611, 0.00011769609215042891, 0.00011768909545212601, 0.00011768213285839318, 0.00011767515554684381, 0.00011766817970470607, 0.00011766118153699155, 0.00011765418998162938], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.00011750368625729366, 0.00011749806389065587, 0.00011749237802475164, 0.00011748668840466782, 0.00011748098432205398, 0.00011747527140817955, 0.00011746951648324765, 0.00011746375869798842, 0.00011745796773293232, 0.00011745214358807933, 0.0001174463102007937, 0.00011744048548387524, 0.00011743461834472726, 0.00011742871244814411, 0.00011742279618287445, 0.00011741685506851122, 0.00011741086498891971, 0.00011740484745018596, 0.00011739882572822351, 0.00011739282569111753, 0.00011738679007869053, 0.00011738074449087203, 0.00011737465313781643, 0.0001173685621244247, 0.0001173624088095286, 0.00011735625011364172, 0.00011735010988474309, 0.00011734392256770599, 0.00011733775584502557, 0.00011733158175700232, 0.00011732535673727577, 0.00011731912762370573, 0.00011731288206325363, 0.0001173066575977155, 0.00011730041883054077, 0.00011729415496399383, 0.0001172878820337847, 0.00011728161040859993, 0.0001172752433914991, 0.00011726890065142638, 0.00011726252668015476, 0.00011725619702607951, 0.00011724981275762955, 0.00011724344663438101, 0.00011723706179386559, 0.00011723066599472112, 0.00011722422875658465, 0.0001172177878536538, 0.00011721133681443801, 0.00011720485928144045, 0.000117198348568646, 0.00011719189434731608, 0.00011718539230488882, 0.00011717882266935162, 0.00011717225351649467, 0.00011716564912798057, 0.00011715904295176189, 0.00011715240970969599, 0.0001171458184965646, 0.00011713917038079662, 0.0001171325753241004, 0.00011712593064072521, 0.00011711930149250277, 0.00011711268117554093, 0.00011710607319374063, 0.00011709946791137425, 0.00011709284350056892, 0.00011708618648203218, 0.00011707954882433596, 0.00011707288942815215, 0.00011706617566787224, 0.00011705952379792466, 0.00011705281150356251, 0.00011704608467516218, 0.00011703937770815965, 0.00011703266101604424, 0.00011702594391275678, 0.00011701922273350289, 0.00011701253482343112, 0.00011700582851787929, 0.00011699908665488349, 0.000116992324590826, 0.00011698561714114323, 0.00011697887724462246, 0.00011697214306875634, 0.00011696536096453058, 0.00011695858997982728, 0.00011695182308896744, 0.00011694504482830653, 0.00011693821579683572, 0.00011693144432945219, 0.0001169246556107195, 0.00011691782421947867, 0.00011691106101129025, 0.00011690424426134987, 0.00011689746592918076, 0.00011689065944066463, 0.0001168838429052488, 0.00011687701467824506, 0.00011687018555738902, 0.00011686340000289343]}
[2018-04-29 21:35:52,163 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:129]: evaluating model ... 
[2018-04-29 21:35:58,127 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:133]: evaluated! 
[2018-04-29 21:35:58,132 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:135]: generating reports ... 
[2018-04-29 21:36:01,249 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:138]: done!
[2018-04-29 21:36:01,249 AE_BIGRAMA_1L_MINIDS_UNDER_F0_5.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_5 finished!
