[2017-12-14 09:31:57,885 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_9
[2017-12-14 09:31:57,885 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:146]: >> Printing header log
[2017-12-14 09:31:57,885 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_9
	layers = 9216,17510
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f81233745c0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f8123374ac8>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,886 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:148]: >> Loading dataset... 
[2017-12-14 09:32:19,827 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:19,828 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:19,828 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:57]: =======================================
[2017-12-14 09:32:19,828 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f81233745c0>, 'discard_decoder_function': True}
[2017-12-14 09:32:19,878 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:55,056 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_9
[2017-12-14 10:18:55,057 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:146]: >> Printing header log
[2017-12-14 10:18:55,057 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_9
	layers = 9216,17510
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f2ee0821eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f2ee0804400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:55,057 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:148]: >> Loading dataset... 
[2017-12-14 10:19:17,121 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:17,122 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:17,122 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:57]: =======================================
[2017-12-14 10:19:17,122 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f2ee0821eb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:17,169 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:73]: training and evaluate autoencoder
[2017-12-15 05:52:34,062 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:85]: trained and evaluated!
[2017-12-15 05:52:34,065 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:88]: Training history: 
{'val_loss': [0.00011784389298063124, 0.0001178379564785458, 0.00011783194934425267, 0.00011782593681109172, 0.00011781990507798366, 0.00011781385659408373, 0.00011780776511588882, 0.00011780165711930365, 0.00011779554072050698, 0.00011778931795328819, 0.00011778307214255744, 0.00011777680058888084, 0.00011777050599169229, 0.00011776413536342822, 0.00011775777642675207, 0.00011775141454036336, 0.00011774505476346605, 0.00011773861845493596, 0.00011773215132637902, 0.00011772570684803902, 0.00011771925655965915, 0.00011771279320315886, 0.00011770628120321711, 0.00011769976895299674, 0.00011769314911871512, 0.00011768652839058122, 0.00011767993832158076, 0.00011767335438440698, 0.0001176666578754762, 0.00011765995246377666, 0.00011765319547680017, 0.00011764648468410986, 0.00011763968836763274, 0.00011763295133143929, 0.00011762620120924836, 0.00011761945861329368, 0.00011761273105193447, 0.00011760592402710696, 0.00011759911537546829, 0.00011759232413607217, 0.00011758542972824511, 0.00011757856794602652, 0.00011757174311566146, 0.00011756486048880754, 0.00011755794205423101, 0.00011755095813603596, 0.00011754400805908848, 0.00011753702446268025, 0.00011753001430098205, 0.00011752302857720537, 0.00011751601333842619, 0.0001175090147074225, 0.00011750195917378223, 0.00011749485067045545, 0.00011748776310114923, 0.00011748057064721567, 0.0001174733996099829, 0.00011746616775504055, 0.00011745895626205328, 0.00011745174820145881, 0.00011744450090074893, 0.0001174372664357579, 0.00011743002371157173, 0.00011742276185894665, 0.00011741552158391575, 0.00011740827698263978, 0.00011740103418694544, 0.0001173937387433514, 0.00011738639776692188, 0.00011737910101830351, 0.00011737174246873803, 0.00011736434589469626, 0.00011735701938079675, 0.00011734962951064713, 0.00011734225533654369, 0.00011733486628873818, 0.00011732746630018069, 0.00011732009827578097, 0.00011731273368377403, 0.00011730533843263366, 0.00011729789453805184, 0.00011729050354164836, 0.00011728304509515131, 0.0001172755143896354, 0.00011726797640816187, 0.00011726046025456119, 0.00011725281804991104, 0.00011724523389202838, 0.00011723758408963379, 0.00011722996519665127, 0.000117222316699281, 0.00011721457624238749, 0.00011720676565384355, 0.00011719904344941373, 0.00011719123475583663, 0.00011718347372246348, 0.00011717567058864762, 0.00011716787317548638, 0.00011716004281492986, 0.00011715223599844257, 0.00011714442272834178, 0.00011713659451303075, 0.00011712868569012049, 0.0001171207546460424, 0.00011711279392606837, 0.00011710484890214051, 0.00011709691197651436, 0.00011708896679169309, 0.00011708100108402331, 0.00011707307740528805, 0.00011706511316353601, 0.00011705716102454395, 0.000117049227692204, 0.0001170412565856664, 0.00011703328932269364, 0.00011702532819154757, 0.0001170173402448329, 0.00011700937158745057, 0.00011700132627329613, 0.000116993308829456, 0.00011698526253206403, 0.0001169772272648093, 0.00011696917099202582, 0.00011696105447359822, 0.00011695297277965571, 0.00011694488978068886, 0.00011693678177173503, 0.00011692869566216222, 0.00011692054317511861, 0.00011691242780082193, 0.00011690426574955884, 0.00011689606290287739, 0.00011688793421018163, 0.00011687986315308133, 0.00011687168368957571, 0.00011686347219040411, 0.00011685530148665089, 0.00011684713952477303, 0.00011683897355843692, 0.0001168308403786027, 0.0001168226387654374, 0.00011681442392325826, 0.00011680618766437832, 0.00011679797069483072, 0.0001167896987891216, 0.00011678147559836209, 0.00011677323345793411, 0.0001167649803588771, 0.00011675668401524644, 0.00011674846974513277, 0.00011674020234443916, 0.00011673191206112701, 0.00011672362333311784, 0.00011671536689105327, 0.00011670707571388883, 0.0001166987629591302, 0.00011669045519206733, 0.00011668213989876819, 0.00011667385462102885, 0.00011666555797348843, 0.00011665716527258126, 0.00011664886118818981, 0.00011664055440436445, 0.00011663226397803593, 0.00011662393872722232, 0.00011661560816692614, 0.0001166072145006585, 0.0001165987727630149, 0.00011659040836147112, 0.00011658204576550898, 0.00011657360377758672, 0.00011656516915500731, 0.00011655673281623151, 0.00011654834937563404, 0.00011653995506579276, 0.00011653155698389484, 0.00011652316471203677, 0.00011651479728927227, 0.00011650637419738736, 0.00011649794275692208, 0.00011648956029956213, 0.0001164811825796193, 0.00011647283510763783, 0.00011646443269949485, 0.00011645600503108622, 0.00011644757393028481, 0.00011643915353783382, 0.00011643076453747511, 0.00011642238862311389, 0.00011641399627974765, 0.00011640562640782789, 0.00011639724918844233, 0.00011638885798920701, 0.00011638040228959067, 0.00011637198713511408, 0.00011636358824874908, 0.00011635519336684233, 0.00011634679930727971, 0.00011633842600296717, 0.00011633002418476668], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011857953340179934, 0.00011857367981094806, 0.00011856778258805129, 0.00011856178319364549, 0.00011855579098039656, 0.00011854979928855175, 0.0001185437999652465, 0.00011853775364446912, 0.00011853171320133827, 0.00011852565932020102, 0.00011851947994657007, 0.00011851329275187718, 0.00011850709124227094, 0.00011850087634205867, 0.00011849459294830403, 0.00011848830559661806, 0.000118482015329809, 0.00011847570430163555, 0.00011846929738250279, 0.00011846284789783298, 0.00011845642197114971, 0.00011844996281680333, 0.00011844350482376614, 0.00011843699040058212, 0.00011843045796525547, 0.0001184238104418175, 0.00011841718225773266, 0.00011841059249165204, 0.00011840401123393879, 0.0001183973078254583, 0.00011839064155517189, 0.00011838392091665495, 0.00011837724651720417, 0.00011837047935521886, 0.00011836377044829482, 0.00011835704310262479, 0.00011835030677458362, 0.00011834358177523217, 0.00011833678553311658, 0.00011832999666175936, 0.00011832321153503179, 0.00011831632542180456, 0.0001183094612786513, 0.00011830264766429815, 0.00011829578624666648, 0.00011828888231089812, 0.00011828189369435923, 0.00011827492342176559, 0.00011826790956452683, 0.00011826086167381858, 0.00011825382617830847, 0.000118246771438246, 0.00011823971961330658, 0.00011823260358455879, 0.00011822544292835762, 0.00011821830267801802, 0.00011821107871861558, 0.00011820386002065481, 0.00011819657553097306, 0.00011818927731888265, 0.00011818199465411536, 0.00011817465999113632, 0.00011816733812625862, 0.00011816000870102106, 0.00011815264917654516, 0.0001181453184240971, 0.00011813796989650827, 0.00011813062373893821, 0.00011812321968180968, 0.00011811578282362141, 0.00011810838435973717, 0.00011810089848956081, 0.00011809338628847595, 0.00011808593836230004, 0.00011807841639673787, 0.00011807089454967662, 0.00011806335895650652, 0.00011805584229978639, 0.00011804836183325282, 0.00011804087155484155, 0.00011803335963815893, 0.00011802577007966152, 0.00011801822792153945, 0.00011801058041608319, 0.00011800287697818403, 0.00011799514844178614, 0.00011798743222948583, 0.00011797961081205246, 0.00011797184321774527, 0.00011796399399999177, 0.00011795618234703573, 0.00011794833260787809, 0.00011794039069869062, 0.00011793241861916423, 0.00011792453664775137, 0.00011791658322027268, 0.00011790867330633856, 0.00011790071307690601, 0.00011789274232459015, 0.00011788473649599653, 0.00011787674178279093, 0.00011786875458254482, 0.00011786078558404283, 0.00011785272894224686, 0.00011784470920164308, 0.00011783664485728611, 0.00011782859534924662, 0.00011782055216915723, 0.00011781249156942991, 0.00011780440321678284, 0.0001177963428540574, 0.00011778823163072921, 0.00011778012957937367, 0.00011777203338196447, 0.00011776390251118072, 0.00011775575839199204, 0.00011774762029265105, 0.00011773944815984057, 0.00011773129588778735, 0.00011772307825061653, 0.00011771488369742851, 0.00011770666070401529, 0.00011769844704847601, 0.00011769021165986464, 0.00011768192775696911, 0.00011767366971097832, 0.0001176654019242104, 0.00011765710188148707, 0.0001176488174808876, 0.00011764045524887182, 0.00011763212437220434, 0.00011762373993311271, 0.0001176153308695261, 0.00011760700736361699, 0.00011759873704092899, 0.0001175903475299972, 0.00011758192763542484, 0.00011757355805635088, 0.00011756518997038875, 0.0001175568165518844, 0.0001175484639895452, 0.00011754005234263813, 0.0001175316398662245, 0.00011752320286011664, 0.0001175147796238178, 0.00011750629704224904, 0.00011749786508428115, 0.0001174894234329365, 0.00011748097573804399, 0.00011747248071387671, 0.00011746405927879215, 0.00011745560191422306, 0.0001174471356857838, 0.00011743866173108336, 0.00011743023325704305, 0.00011742177667458017, 0.00011741329046688271, 0.00011740480501759123, 0.00011739633115769155, 0.00011738789294287411, 0.00011737945005911969, 0.00011737089938691177, 0.00011736244069513218, 0.00011735397759511769, 0.0001173455396647025, 0.00011733706684761107, 0.00011732858782107046, 0.00011732005110827309, 0.0001173114724935439, 0.00011730295853292667, 0.00011729445523739389, 0.00011728586681078701, 0.00011727729158518466, 0.00011726871953540746, 0.00011726019389059771, 0.00011725165127645359, 0.00011724310489397963, 0.00011723456479205541, 0.00011722605464716839, 0.00011721747591393826, 0.00011720888547281544, 0.00011720034958952461, 0.0001171918161947535, 0.00011718331808956182, 0.00011717477542801733, 0.00011716619963361047, 0.00011715761504643399, 0.00011714904264115398, 0.00011714050166232281, 0.00011713197679961926, 0.00011712344513496183, 0.00011711492863842453, 0.00011710641553101406, 0.00011709788474326359, 0.00011708928636257789, 0.00011708072905431742, 0.00011707218961599845, 0.00011706364081610534, 0.00011705510583342165, 0.00011704658407544268], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329]}
[2017-12-15 05:52:34,065 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:92]: done!
[2017-12-15 05:52:34,066 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:152]: >> Executing classifier part ... 
[2017-12-15 05:52:34,066 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:97]: =======================================
[2017-12-15 05:52:34,066 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f2ee0804400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-15 05:52:34,434 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:110]: training ... 
[2017-12-15 10:22:12,003 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:122]: trained!
[2017-12-15 10:22:12,004 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:125]: Training history: 
{'val_loss': [0.00011784389298063124, 0.0001178379564785458, 0.00011783194934425267, 0.00011782593681109172, 0.00011781990507798366, 0.00011781385659408373, 0.00011780776511588882, 0.00011780165711930365, 0.00011779554072050698, 0.00011778931795328819, 0.00011778307214255744, 0.00011777680058888084, 0.00011777050599169229, 0.00011776413536342822, 0.00011775777642675207, 0.00011775141454036336, 0.00011774505476346605, 0.00011773861845493596, 0.00011773215132637902, 0.00011772570684803902, 0.00011771925655965915, 0.00011771279320315886, 0.00011770628120321711, 0.00011769976895299674, 0.00011769314911871512, 0.00011768652839058122, 0.00011767993832158076, 0.00011767335438440698, 0.0001176666578754762, 0.00011765995246377666, 0.00011765319547680017, 0.00011764648468410986, 0.00011763968836763274, 0.00011763295133143929, 0.00011762620120924836, 0.00011761945861329368, 0.00011761273105193447, 0.00011760592402710696, 0.00011759911537546829, 0.00011759232413607217, 0.00011758542972824511, 0.00011757856794602652, 0.00011757174311566146, 0.00011756486048880754, 0.00011755794205423101, 0.00011755095813603596, 0.00011754400805908848, 0.00011753702446268025, 0.00011753001430098205, 0.00011752302857720537, 0.00011751601333842619, 0.0001175090147074225, 0.00011750195917378223, 0.00011749485067045545, 0.00011748776310114923, 0.00011748057064721567, 0.0001174733996099829, 0.00011746616775504055, 0.00011745895626205328, 0.00011745174820145881, 0.00011744450090074893, 0.0001174372664357579, 0.00011743002371157173, 0.00011742276185894665, 0.00011741552158391575, 0.00011740827698263978, 0.00011740103418694544, 0.0001173937387433514, 0.00011738639776692188, 0.00011737910101830351, 0.00011737174246873803, 0.00011736434589469626, 0.00011735701938079675, 0.00011734962951064713, 0.00011734225533654369, 0.00011733486628873818, 0.00011732746630018069, 0.00011732009827578097, 0.00011731273368377403, 0.00011730533843263366, 0.00011729789453805184, 0.00011729050354164836, 0.00011728304509515131, 0.0001172755143896354, 0.00011726797640816187, 0.00011726046025456119, 0.00011725281804991104, 0.00011724523389202838, 0.00011723758408963379, 0.00011722996519665127, 0.000117222316699281, 0.00011721457624238749, 0.00011720676565384355, 0.00011719904344941373, 0.00011719123475583663, 0.00011718347372246348, 0.00011717567058864762, 0.00011716787317548638, 0.00011716004281492986, 0.00011715223599844257, 0.00011714442272834178, 0.00011713659451303075, 0.00011712868569012049, 0.0001171207546460424, 0.00011711279392606837, 0.00011710484890214051, 0.00011709691197651436, 0.00011708896679169309, 0.00011708100108402331, 0.00011707307740528805, 0.00011706511316353601, 0.00011705716102454395, 0.000117049227692204, 0.0001170412565856664, 0.00011703328932269364, 0.00011702532819154757, 0.0001170173402448329, 0.00011700937158745057, 0.00011700132627329613, 0.000116993308829456, 0.00011698526253206403, 0.0001169772272648093, 0.00011696917099202582, 0.00011696105447359822, 0.00011695297277965571, 0.00011694488978068886, 0.00011693678177173503, 0.00011692869566216222, 0.00011692054317511861, 0.00011691242780082193, 0.00011690426574955884, 0.00011689606290287739, 0.00011688793421018163, 0.00011687986315308133, 0.00011687168368957571, 0.00011686347219040411, 0.00011685530148665089, 0.00011684713952477303, 0.00011683897355843692, 0.0001168308403786027, 0.0001168226387654374, 0.00011681442392325826, 0.00011680618766437832, 0.00011679797069483072, 0.0001167896987891216, 0.00011678147559836209, 0.00011677323345793411, 0.0001167649803588771, 0.00011675668401524644, 0.00011674846974513277, 0.00011674020234443916, 0.00011673191206112701, 0.00011672362333311784, 0.00011671536689105327, 0.00011670707571388883, 0.0001166987629591302, 0.00011669045519206733, 0.00011668213989876819, 0.00011667385462102885, 0.00011666555797348843, 0.00011665716527258126, 0.00011664886118818981, 0.00011664055440436445, 0.00011663226397803593, 0.00011662393872722232, 0.00011661560816692614, 0.0001166072145006585, 0.0001165987727630149, 0.00011659040836147112, 0.00011658204576550898, 0.00011657360377758672, 0.00011656516915500731, 0.00011655673281623151, 0.00011654834937563404, 0.00011653995506579276, 0.00011653155698389484, 0.00011652316471203677, 0.00011651479728927227, 0.00011650637419738736, 0.00011649794275692208, 0.00011648956029956213, 0.0001164811825796193, 0.00011647283510763783, 0.00011646443269949485, 0.00011645600503108622, 0.00011644757393028481, 0.00011643915353783382, 0.00011643076453747511, 0.00011642238862311389, 0.00011641399627974765, 0.00011640562640782789, 0.00011639724918844233, 0.00011638885798920701, 0.00011638040228959067, 0.00011637198713511408, 0.00011636358824874908, 0.00011635519336684233, 0.00011634679930727971, 0.00011633842600296717, 0.00011633002418476668], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011857953340179934, 0.00011857367981094806, 0.00011856778258805129, 0.00011856178319364549, 0.00011855579098039656, 0.00011854979928855175, 0.0001185437999652465, 0.00011853775364446912, 0.00011853171320133827, 0.00011852565932020102, 0.00011851947994657007, 0.00011851329275187718, 0.00011850709124227094, 0.00011850087634205867, 0.00011849459294830403, 0.00011848830559661806, 0.000118482015329809, 0.00011847570430163555, 0.00011846929738250279, 0.00011846284789783298, 0.00011845642197114971, 0.00011844996281680333, 0.00011844350482376614, 0.00011843699040058212, 0.00011843045796525547, 0.0001184238104418175, 0.00011841718225773266, 0.00011841059249165204, 0.00011840401123393879, 0.0001183973078254583, 0.00011839064155517189, 0.00011838392091665495, 0.00011837724651720417, 0.00011837047935521886, 0.00011836377044829482, 0.00011835704310262479, 0.00011835030677458362, 0.00011834358177523217, 0.00011833678553311658, 0.00011832999666175936, 0.00011832321153503179, 0.00011831632542180456, 0.0001183094612786513, 0.00011830264766429815, 0.00011829578624666648, 0.00011828888231089812, 0.00011828189369435923, 0.00011827492342176559, 0.00011826790956452683, 0.00011826086167381858, 0.00011825382617830847, 0.000118246771438246, 0.00011823971961330658, 0.00011823260358455879, 0.00011822544292835762, 0.00011821830267801802, 0.00011821107871861558, 0.00011820386002065481, 0.00011819657553097306, 0.00011818927731888265, 0.00011818199465411536, 0.00011817465999113632, 0.00011816733812625862, 0.00011816000870102106, 0.00011815264917654516, 0.0001181453184240971, 0.00011813796989650827, 0.00011813062373893821, 0.00011812321968180968, 0.00011811578282362141, 0.00011810838435973717, 0.00011810089848956081, 0.00011809338628847595, 0.00011808593836230004, 0.00011807841639673787, 0.00011807089454967662, 0.00011806335895650652, 0.00011805584229978639, 0.00011804836183325282, 0.00011804087155484155, 0.00011803335963815893, 0.00011802577007966152, 0.00011801822792153945, 0.00011801058041608319, 0.00011800287697818403, 0.00011799514844178614, 0.00011798743222948583, 0.00011797961081205246, 0.00011797184321774527, 0.00011796399399999177, 0.00011795618234703573, 0.00011794833260787809, 0.00011794039069869062, 0.00011793241861916423, 0.00011792453664775137, 0.00011791658322027268, 0.00011790867330633856, 0.00011790071307690601, 0.00011789274232459015, 0.00011788473649599653, 0.00011787674178279093, 0.00011786875458254482, 0.00011786078558404283, 0.00011785272894224686, 0.00011784470920164308, 0.00011783664485728611, 0.00011782859534924662, 0.00011782055216915723, 0.00011781249156942991, 0.00011780440321678284, 0.0001177963428540574, 0.00011778823163072921, 0.00011778012957937367, 0.00011777203338196447, 0.00011776390251118072, 0.00011775575839199204, 0.00011774762029265105, 0.00011773944815984057, 0.00011773129588778735, 0.00011772307825061653, 0.00011771488369742851, 0.00011770666070401529, 0.00011769844704847601, 0.00011769021165986464, 0.00011768192775696911, 0.00011767366971097832, 0.0001176654019242104, 0.00011765710188148707, 0.0001176488174808876, 0.00011764045524887182, 0.00011763212437220434, 0.00011762373993311271, 0.0001176153308695261, 0.00011760700736361699, 0.00011759873704092899, 0.0001175903475299972, 0.00011758192763542484, 0.00011757355805635088, 0.00011756518997038875, 0.0001175568165518844, 0.0001175484639895452, 0.00011754005234263813, 0.0001175316398662245, 0.00011752320286011664, 0.0001175147796238178, 0.00011750629704224904, 0.00011749786508428115, 0.0001174894234329365, 0.00011748097573804399, 0.00011747248071387671, 0.00011746405927879215, 0.00011745560191422306, 0.0001174471356857838, 0.00011743866173108336, 0.00011743023325704305, 0.00011742177667458017, 0.00011741329046688271, 0.00011740480501759123, 0.00011739633115769155, 0.00011738789294287411, 0.00011737945005911969, 0.00011737089938691177, 0.00011736244069513218, 0.00011735397759511769, 0.0001173455396647025, 0.00011733706684761107, 0.00011732858782107046, 0.00011732005110827309, 0.0001173114724935439, 0.00011730295853292667, 0.00011729445523739389, 0.00011728586681078701, 0.00011727729158518466, 0.00011726871953540746, 0.00011726019389059771, 0.00011725165127645359, 0.00011724310489397963, 0.00011723456479205541, 0.00011722605464716839, 0.00011721747591393826, 0.00011720888547281544, 0.00011720034958952461, 0.0001171918161947535, 0.00011718331808956182, 0.00011717477542801733, 0.00011716619963361047, 0.00011715761504643399, 0.00011714904264115398, 0.00011714050166232281, 0.00011713197679961926, 0.00011712344513496183, 0.00011711492863842453, 0.00011710641553101406, 0.00011709788474326359, 0.00011708928636257789, 0.00011708072905431742, 0.00011707218961599845, 0.00011706364081610534, 0.00011705510583342165, 0.00011704658407544268], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329]}
[2017-12-15 10:22:12,004 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:129]: evaluating model ... 
[2017-12-15 10:22:13,330 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:133]: evaluated! 
[2017-12-15 10:22:13,331 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:135]: generating reports ... 
[2017-12-15 10:22:13,757 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:138]: done!
[2017-12-15 10:22:13,757 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_9 finished!
[2018-04-29 11:38:17,414 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_9 was already executed!
[2018-04-29 13:12:02,673 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_9
[2018-04-29 13:12:02,673 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:146]: >> Printing header log
[2018-04-29 13:12:02,673 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_9
	layers = 9216,17510
	using GLOBAL obj = 
		{'mlp_configs': {'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7f24d1aca898>, 'activation': 'sigmoid', 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy'}, 'store_history': True, 'autoencoder_configs': {'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f24d1aca828>, 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse'}, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'shuffle_batches': True, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'batch': 32, 'numpy_seed': 666, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'data_dir': '/home/dhiego/malware_dataset/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:12:02,673 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,672 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,672 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,672 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:57]: =======================================
[2018-04-29 13:12:20,672 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f24d1aca828>, 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse'}
[2018-04-29 13:12:20,719 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,855 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_9
[2018-04-29 13:14:12,855 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:146]: >> Printing header log
[2018-04-29 13:14:12,856 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_9
	layers = 9216,17510
	using GLOBAL obj = 
		{'fullds_data_dir': '/home/dhiego/malware_dataset/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'batch': 32, 'store_history': True, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'numpy_seed': 666, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'autoencoder_configs': {'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f0069daf828>, 'output_layer_activation': 'relu', 'loss_function': 'mse', 'hidden_layer_activation': 'relu'}, 'shuffle_batches': True, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'epochs': 200, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'mlp_configs': {'use_last_dim_as_classifier': False, 'activation': 'sigmoid', 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f0069daf898>}}
	=======================================
	
[2018-04-29 13:14:12,856 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:148]: >> Loading dataset... 
[2018-04-29 13:14:30,818 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:30,818 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:30,819 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:57]: =======================================
[2018-04-29 13:14:30,819 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f0069daf828>, 'output_layer_activation': 'relu', 'loss_function': 'mse', 'hidden_layer_activation': 'relu'}
[2018-04-29 13:14:30,866 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,690 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_9
[2018-04-29 13:16:33,690 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:146]: >> Printing header log
[2018-04-29 13:16:33,690 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_9
	layers = 9216,17510
	using GLOBAL obj = 
		{'fullds_data_dir': '/home/dhiego/malware_dataset/', 'epochs': 200, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'autoencoder_configs': {'output_layer_activation': 'relu', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f18464d7828>}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'store_history': True, 'shuffle_batches': True, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'batch': 32, 'mlp_configs': {'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f18464d7898>}, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'data_dir': '/home/dhiego/malware_dataset/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'numpy_seed': 666}
	=======================================
	
[2018-04-29 13:16:33,690 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:148]: >> Loading dataset... 
[2018-04-29 13:16:57,900 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:57,901 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:57,901 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:57]: =======================================
[2018-04-29 13:16:57,902 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:62]: setting configurations for autoencoder: 
	 {'output_layer_activation': 'relu', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f18464d7828>}
[2018-04-29 13:16:57,974 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:21,132 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_9
[2018-04-29 14:30:21,132 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:146]: >> Printing header log
[2018-04-29 14:30:21,132 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_9
	layers = 9216,17510
	using GLOBAL obj = 
		{'epochs': 200, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'batch': 32, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f53d600d898>}, 'store_history': True, 'mlp_configs': {'activation': 'sigmoid', 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f53d600d908>, 'use_last_dim_as_classifier': False}, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'numpy_seed': 666}
	=======================================
	
[2018-04-29 14:30:21,132 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:148]: >> Loading dataset... 
[2018-04-29 14:30:39,191 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:39,191 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:39,191 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:57]: =======================================
[2018-04-29 14:30:39,191 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f53d600d898>}
[2018-04-29 14:30:39,242 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:73]: training and evaluate autoencoder
[2018-04-29 23:30:13,308 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:85]: trained and evaluated!
[2018-04-29 23:30:13,310 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:88]: Training history: 
{'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011862806880282366, 0.00011862209694803592, 0.00011861613310391162, 0.00011861010109804758, 0.00011860400612078491, 0.00011859778512962442, 0.00011859151157144767, 0.000118585152905897, 0.00011857879876708217, 0.00011857239270115617, 0.00011856594568130587, 0.00011855942068783815, 0.00011855286317771294, 0.00011854624613271631, 0.0001185395141418095, 0.0001185327027315738, 0.00011852589044443116, 0.00011851899949636566, 0.00011851212912006304, 0.00011850517214327468, 0.00011849814439772594, 0.00011849113103819114, 0.00011848410104112456, 0.00011847703753199265, 0.00011846989384512584, 0.00011846273600924699, 0.0001184555207952138, 0.00011844828612332654, 0.0001184410260448381, 0.00011843376821786748, 0.00011842650124262441, 0.00011841918620340077, 0.00011841176949037203, 0.00011840432189599875, 0.0001183968600104123, 0.00011838930228126693, 0.00011838176886851411, 0.00011837418905079383, 0.0001183665895382176, 0.00011835896407393587, 0.00011835133275570779, 0.00011834368477624778, 0.00011833601032367814, 0.00011832830240644351, 0.00011832054329680352, 0.00011831277745631023, 0.00011830489069745946, 0.00011829698936299327, 0.00011828897853366006, 0.00011828103944479491, 0.00011827302197940915, 0.00011826501015466805, 0.00011825693090289303, 0.00011824886316940922, 0.00011824077775558542, 0.0001182326385423356, 0.00011822449975568917, 0.0001182163683161009, 0.00011820821414803267, 0.00011819992564730074, 0.00011819171834341174, 0.00011818344832882619, 0.00011817520296243581, 0.00011816692842111441, 0.00011815853888648244, 0.00011815021997840971, 0.00011814182134290568, 0.00011813344038774166, 0.00011812503175075842, 0.00011811656981205312, 0.00011810811382209494, 0.00011809961905862973, 0.00011809117337825317, 0.00011808262054932816, 0.00011807402107843383, 0.00011806545116167352, 0.00011805686567388991, 0.00011804826532608863, 0.00011803965049747269, 0.00011803106290037237, 0.00011802243477595115, 0.0001180137791830124, 0.00011800507232656776, 0.00011799643538567672, 0.00011798775936317621, 0.00011797900825848117, 0.00011797030251594533, 0.00011796155553508295, 0.00011795276397416756, 0.00011794392261915789, 0.00011793510930147022, 0.00011792628273537765, 0.00011791742742095744, 0.00011790853669845685, 0.00011789968109963438, 0.00011789075219613146, 0.00011788187296822187, 0.00011787298764936408, 0.00011786402435688884, 0.00011785512680873421, 0.00011784617688316482], 'val_loss': [0.00011782049941824616, 0.00011781440622385485, 0.00011780821656492477, 0.00011780198603906812, 0.00011779562937277677, 0.00011778927655005027, 0.00011778284303033931, 0.0001177763956201638, 0.00011776991150841342, 0.00011776338824593286, 0.00011775674996254004, 0.00011775009533952743, 0.00011774339543395797, 0.00011773658762254046, 0.00011772972560792026, 0.00011772286711507809, 0.0001177159516302141, 0.00011770907719104713, 0.00011770214822689066, 0.0001176951245143918, 0.00011768812574037174, 0.0001176811191183286, 0.00011767407481147306, 0.00011766697254723523, 0.00011765987525281613, 0.00011765274903333703, 0.0001176455950150518, 0.00011763841484264866, 0.00011763122903897613, 0.0001176240299884127, 0.00011761680993232052, 0.0001176094945558205, 0.00011760215220285845, 0.00011759479849797236, 0.00011758731175211192, 0.00011757983278276636, 0.00011757227747843552, 0.00011756468890492257, 0.0001175570733549476, 0.0001175494313290679, 0.00011754177532333844, 0.00011753409430762199, 0.00011752640919806207, 0.00011751869180255755, 0.00011751102010078193, 0.00011750322658481667, 0.00011749541100857697, 0.00011748750656554293, 0.00011747966785640605, 0.00011747179047921925, 0.00011746388482054608, 0.00011745590028834715, 0.00011744791902764758, 0.00011743995641270671, 0.00011743193517893288, 0.0001174238902401964, 0.0001174158724566924, 0.00011740782499729246, 0.00011739964167234494, 0.00011739153209021109, 0.00011738337541993877, 0.00011737522725914021, 0.00011736706871177809, 0.0001173588217981789, 0.00011735062426098002, 0.0001173423780803397, 0.00011733414981249921, 0.00011732588459280518, 0.00011731757130173518, 0.00011730929546307599, 0.00011730096844243485, 0.00011729267159824692, 0.00011728429849058189, 0.0001172758614546013, 0.00011726744150907644, 0.00011725898403963257, 0.00011725050663728272, 0.0001172420101780021, 0.00011723353055889858, 0.00011722499030085591, 0.0001172164406394872, 0.0001172078229738365, 0.00011719927978395834, 0.00011719070030367735, 0.00011718206678108708, 0.00011717348018574187, 0.00011716486775806558, 0.00011715621634055254, 0.00011714751488518849, 0.00011713881655830744, 0.00011713009992533156, 0.0001171213640745315, 0.00011711260083609739, 0.00011710387914391756, 0.0001170950901625376, 0.00011708635284581979, 0.00011707760891459507, 0.00011706878773665573, 0.00011706004037303823, 0.00011705125850672246, 0.00011704241778917212]}
[2018-04-29 23:30:13,311 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:92]: done!
[2018-04-29 23:30:13,311 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:152]: >> Executing classifier part ... 
[2018-04-29 23:30:13,311 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:97]: =======================================
[2018-04-29 23:30:13,311 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f53d600d908>, 'use_last_dim_as_classifier': False}
[2018-04-29 23:30:13,569 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:110]: training ... 
[2018-04-30 04:10:55,236 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:122]: trained!
[2018-04-30 04:10:55,237 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:125]: Training history: 
{'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011862806880282366, 0.00011862209694803592, 0.00011861613310391162, 0.00011861010109804758, 0.00011860400612078491, 0.00011859778512962442, 0.00011859151157144767, 0.000118585152905897, 0.00011857879876708217, 0.00011857239270115617, 0.00011856594568130587, 0.00011855942068783815, 0.00011855286317771294, 0.00011854624613271631, 0.0001185395141418095, 0.0001185327027315738, 0.00011852589044443116, 0.00011851899949636566, 0.00011851212912006304, 0.00011850517214327468, 0.00011849814439772594, 0.00011849113103819114, 0.00011848410104112456, 0.00011847703753199265, 0.00011846989384512584, 0.00011846273600924699, 0.0001184555207952138, 0.00011844828612332654, 0.0001184410260448381, 0.00011843376821786748, 0.00011842650124262441, 0.00011841918620340077, 0.00011841176949037203, 0.00011840432189599875, 0.0001183968600104123, 0.00011838930228126693, 0.00011838176886851411, 0.00011837418905079383, 0.0001183665895382176, 0.00011835896407393587, 0.00011835133275570779, 0.00011834368477624778, 0.00011833601032367814, 0.00011832830240644351, 0.00011832054329680352, 0.00011831277745631023, 0.00011830489069745946, 0.00011829698936299327, 0.00011828897853366006, 0.00011828103944479491, 0.00011827302197940915, 0.00011826501015466805, 0.00011825693090289303, 0.00011824886316940922, 0.00011824077775558542, 0.0001182326385423356, 0.00011822449975568917, 0.0001182163683161009, 0.00011820821414803267, 0.00011819992564730074, 0.00011819171834341174, 0.00011818344832882619, 0.00011817520296243581, 0.00011816692842111441, 0.00011815853888648244, 0.00011815021997840971, 0.00011814182134290568, 0.00011813344038774166, 0.00011812503175075842, 0.00011811656981205312, 0.00011810811382209494, 0.00011809961905862973, 0.00011809117337825317, 0.00011808262054932816, 0.00011807402107843383, 0.00011806545116167352, 0.00011805686567388991, 0.00011804826532608863, 0.00011803965049747269, 0.00011803106290037237, 0.00011802243477595115, 0.0001180137791830124, 0.00011800507232656776, 0.00011799643538567672, 0.00011798775936317621, 0.00011797900825848117, 0.00011797030251594533, 0.00011796155553508295, 0.00011795276397416756, 0.00011794392261915789, 0.00011793510930147022, 0.00011792628273537765, 0.00011791742742095744, 0.00011790853669845685, 0.00011789968109963438, 0.00011789075219613146, 0.00011788187296822187, 0.00011787298764936408, 0.00011786402435688884, 0.00011785512680873421, 0.00011784617688316482], 'val_loss': [0.00011782049941824616, 0.00011781440622385485, 0.00011780821656492477, 0.00011780198603906812, 0.00011779562937277677, 0.00011778927655005027, 0.00011778284303033931, 0.0001177763956201638, 0.00011776991150841342, 0.00011776338824593286, 0.00011775674996254004, 0.00011775009533952743, 0.00011774339543395797, 0.00011773658762254046, 0.00011772972560792026, 0.00011772286711507809, 0.0001177159516302141, 0.00011770907719104713, 0.00011770214822689066, 0.0001176951245143918, 0.00011768812574037174, 0.0001176811191183286, 0.00011767407481147306, 0.00011766697254723523, 0.00011765987525281613, 0.00011765274903333703, 0.0001176455950150518, 0.00011763841484264866, 0.00011763122903897613, 0.0001176240299884127, 0.00011761680993232052, 0.0001176094945558205, 0.00011760215220285845, 0.00011759479849797236, 0.00011758731175211192, 0.00011757983278276636, 0.00011757227747843552, 0.00011756468890492257, 0.0001175570733549476, 0.0001175494313290679, 0.00011754177532333844, 0.00011753409430762199, 0.00011752640919806207, 0.00011751869180255755, 0.00011751102010078193, 0.00011750322658481667, 0.00011749541100857697, 0.00011748750656554293, 0.00011747966785640605, 0.00011747179047921925, 0.00011746388482054608, 0.00011745590028834715, 0.00011744791902764758, 0.00011743995641270671, 0.00011743193517893288, 0.0001174238902401964, 0.0001174158724566924, 0.00011740782499729246, 0.00011739964167234494, 0.00011739153209021109, 0.00011738337541993877, 0.00011737522725914021, 0.00011736706871177809, 0.0001173588217981789, 0.00011735062426098002, 0.0001173423780803397, 0.00011733414981249921, 0.00011732588459280518, 0.00011731757130173518, 0.00011730929546307599, 0.00011730096844243485, 0.00011729267159824692, 0.00011728429849058189, 0.0001172758614546013, 0.00011726744150907644, 0.00011725898403963257, 0.00011725050663728272, 0.0001172420101780021, 0.00011723353055889858, 0.00011722499030085591, 0.0001172164406394872, 0.0001172078229738365, 0.00011719927978395834, 0.00011719070030367735, 0.00011718206678108708, 0.00011717348018574187, 0.00011716486775806558, 0.00011715621634055254, 0.00011714751488518849, 0.00011713881655830744, 0.00011713009992533156, 0.0001171213640745315, 0.00011711260083609739, 0.00011710387914391756, 0.0001170950901625376, 0.00011708635284581979, 0.00011707760891459507, 0.00011706878773665573, 0.00011706004037303823, 0.00011705125850672246, 0.00011704241778917212]}
[2018-04-30 04:10:55,237 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:129]: evaluating model ... 
[2018-04-30 04:10:56,084 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:133]: evaluated! 
[2018-04-30 04:10:56,084 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:135]: generating reports ... 
[2018-04-30 04:10:56,677 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:138]: done!
[2018-04-30 04:10:56,677 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_9 finished!
