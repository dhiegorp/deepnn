[2017-12-14 09:31:57,885 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_9
[2017-12-14 09:31:57,885 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:146]: >> Printing header log
[2017-12-14 09:31:57,885 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_9
	layers = 9216,17510
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f81233745c0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f8123374ac8>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,886 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:148]: >> Loading dataset... 
[2017-12-14 09:32:19,827 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:19,828 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:19,828 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:57]: =======================================
[2017-12-14 09:32:19,828 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f81233745c0>, 'discard_decoder_function': True}
[2017-12-14 09:32:19,878 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:55,056 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_9
[2017-12-14 10:18:55,057 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:146]: >> Printing header log
[2017-12-14 10:18:55,057 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_9
	layers = 9216,17510
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f2ee0821eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f2ee0804400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:55,057 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:148]: >> Loading dataset... 
[2017-12-14 10:19:17,121 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:17,122 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:17,122 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:57]: =======================================
[2017-12-14 10:19:17,122 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f2ee0821eb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:17,169 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:73]: training and evaluate autoencoder
[2017-12-15 05:52:34,062 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:85]: trained and evaluated!
[2017-12-15 05:52:34,065 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:88]: Training history: 
{'val_loss': [0.00011784389298063124, 0.0001178379564785458, 0.00011783194934425267, 0.00011782593681109172, 0.00011781990507798366, 0.00011781385659408373, 0.00011780776511588882, 0.00011780165711930365, 0.00011779554072050698, 0.00011778931795328819, 0.00011778307214255744, 0.00011777680058888084, 0.00011777050599169229, 0.00011776413536342822, 0.00011775777642675207, 0.00011775141454036336, 0.00011774505476346605, 0.00011773861845493596, 0.00011773215132637902, 0.00011772570684803902, 0.00011771925655965915, 0.00011771279320315886, 0.00011770628120321711, 0.00011769976895299674, 0.00011769314911871512, 0.00011768652839058122, 0.00011767993832158076, 0.00011767335438440698, 0.0001176666578754762, 0.00011765995246377666, 0.00011765319547680017, 0.00011764648468410986, 0.00011763968836763274, 0.00011763295133143929, 0.00011762620120924836, 0.00011761945861329368, 0.00011761273105193447, 0.00011760592402710696, 0.00011759911537546829, 0.00011759232413607217, 0.00011758542972824511, 0.00011757856794602652, 0.00011757174311566146, 0.00011756486048880754, 0.00011755794205423101, 0.00011755095813603596, 0.00011754400805908848, 0.00011753702446268025, 0.00011753001430098205, 0.00011752302857720537, 0.00011751601333842619, 0.0001175090147074225, 0.00011750195917378223, 0.00011749485067045545, 0.00011748776310114923, 0.00011748057064721567, 0.0001174733996099829, 0.00011746616775504055, 0.00011745895626205328, 0.00011745174820145881, 0.00011744450090074893, 0.0001174372664357579, 0.00011743002371157173, 0.00011742276185894665, 0.00011741552158391575, 0.00011740827698263978, 0.00011740103418694544, 0.0001173937387433514, 0.00011738639776692188, 0.00011737910101830351, 0.00011737174246873803, 0.00011736434589469626, 0.00011735701938079675, 0.00011734962951064713, 0.00011734225533654369, 0.00011733486628873818, 0.00011732746630018069, 0.00011732009827578097, 0.00011731273368377403, 0.00011730533843263366, 0.00011729789453805184, 0.00011729050354164836, 0.00011728304509515131, 0.0001172755143896354, 0.00011726797640816187, 0.00011726046025456119, 0.00011725281804991104, 0.00011724523389202838, 0.00011723758408963379, 0.00011722996519665127, 0.000117222316699281, 0.00011721457624238749, 0.00011720676565384355, 0.00011719904344941373, 0.00011719123475583663, 0.00011718347372246348, 0.00011717567058864762, 0.00011716787317548638, 0.00011716004281492986, 0.00011715223599844257, 0.00011714442272834178, 0.00011713659451303075, 0.00011712868569012049, 0.0001171207546460424, 0.00011711279392606837, 0.00011710484890214051, 0.00011709691197651436, 0.00011708896679169309, 0.00011708100108402331, 0.00011707307740528805, 0.00011706511316353601, 0.00011705716102454395, 0.000117049227692204, 0.0001170412565856664, 0.00011703328932269364, 0.00011702532819154757, 0.0001170173402448329, 0.00011700937158745057, 0.00011700132627329613, 0.000116993308829456, 0.00011698526253206403, 0.0001169772272648093, 0.00011696917099202582, 0.00011696105447359822, 0.00011695297277965571, 0.00011694488978068886, 0.00011693678177173503, 0.00011692869566216222, 0.00011692054317511861, 0.00011691242780082193, 0.00011690426574955884, 0.00011689606290287739, 0.00011688793421018163, 0.00011687986315308133, 0.00011687168368957571, 0.00011686347219040411, 0.00011685530148665089, 0.00011684713952477303, 0.00011683897355843692, 0.0001168308403786027, 0.0001168226387654374, 0.00011681442392325826, 0.00011680618766437832, 0.00011679797069483072, 0.0001167896987891216, 0.00011678147559836209, 0.00011677323345793411, 0.0001167649803588771, 0.00011675668401524644, 0.00011674846974513277, 0.00011674020234443916, 0.00011673191206112701, 0.00011672362333311784, 0.00011671536689105327, 0.00011670707571388883, 0.0001166987629591302, 0.00011669045519206733, 0.00011668213989876819, 0.00011667385462102885, 0.00011666555797348843, 0.00011665716527258126, 0.00011664886118818981, 0.00011664055440436445, 0.00011663226397803593, 0.00011662393872722232, 0.00011661560816692614, 0.0001166072145006585, 0.0001165987727630149, 0.00011659040836147112, 0.00011658204576550898, 0.00011657360377758672, 0.00011656516915500731, 0.00011655673281623151, 0.00011654834937563404, 0.00011653995506579276, 0.00011653155698389484, 0.00011652316471203677, 0.00011651479728927227, 0.00011650637419738736, 0.00011649794275692208, 0.00011648956029956213, 0.0001164811825796193, 0.00011647283510763783, 0.00011646443269949485, 0.00011645600503108622, 0.00011644757393028481, 0.00011643915353783382, 0.00011643076453747511, 0.00011642238862311389, 0.00011641399627974765, 0.00011640562640782789, 0.00011639724918844233, 0.00011638885798920701, 0.00011638040228959067, 0.00011637198713511408, 0.00011636358824874908, 0.00011635519336684233, 0.00011634679930727971, 0.00011633842600296717, 0.00011633002418476668], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011857953340179934, 0.00011857367981094806, 0.00011856778258805129, 0.00011856178319364549, 0.00011855579098039656, 0.00011854979928855175, 0.0001185437999652465, 0.00011853775364446912, 0.00011853171320133827, 0.00011852565932020102, 0.00011851947994657007, 0.00011851329275187718, 0.00011850709124227094, 0.00011850087634205867, 0.00011849459294830403, 0.00011848830559661806, 0.000118482015329809, 0.00011847570430163555, 0.00011846929738250279, 0.00011846284789783298, 0.00011845642197114971, 0.00011844996281680333, 0.00011844350482376614, 0.00011843699040058212, 0.00011843045796525547, 0.0001184238104418175, 0.00011841718225773266, 0.00011841059249165204, 0.00011840401123393879, 0.0001183973078254583, 0.00011839064155517189, 0.00011838392091665495, 0.00011837724651720417, 0.00011837047935521886, 0.00011836377044829482, 0.00011835704310262479, 0.00011835030677458362, 0.00011834358177523217, 0.00011833678553311658, 0.00011832999666175936, 0.00011832321153503179, 0.00011831632542180456, 0.0001183094612786513, 0.00011830264766429815, 0.00011829578624666648, 0.00011828888231089812, 0.00011828189369435923, 0.00011827492342176559, 0.00011826790956452683, 0.00011826086167381858, 0.00011825382617830847, 0.000118246771438246, 0.00011823971961330658, 0.00011823260358455879, 0.00011822544292835762, 0.00011821830267801802, 0.00011821107871861558, 0.00011820386002065481, 0.00011819657553097306, 0.00011818927731888265, 0.00011818199465411536, 0.00011817465999113632, 0.00011816733812625862, 0.00011816000870102106, 0.00011815264917654516, 0.0001181453184240971, 0.00011813796989650827, 0.00011813062373893821, 0.00011812321968180968, 0.00011811578282362141, 0.00011810838435973717, 0.00011810089848956081, 0.00011809338628847595, 0.00011808593836230004, 0.00011807841639673787, 0.00011807089454967662, 0.00011806335895650652, 0.00011805584229978639, 0.00011804836183325282, 0.00011804087155484155, 0.00011803335963815893, 0.00011802577007966152, 0.00011801822792153945, 0.00011801058041608319, 0.00011800287697818403, 0.00011799514844178614, 0.00011798743222948583, 0.00011797961081205246, 0.00011797184321774527, 0.00011796399399999177, 0.00011795618234703573, 0.00011794833260787809, 0.00011794039069869062, 0.00011793241861916423, 0.00011792453664775137, 0.00011791658322027268, 0.00011790867330633856, 0.00011790071307690601, 0.00011789274232459015, 0.00011788473649599653, 0.00011787674178279093, 0.00011786875458254482, 0.00011786078558404283, 0.00011785272894224686, 0.00011784470920164308, 0.00011783664485728611, 0.00011782859534924662, 0.00011782055216915723, 0.00011781249156942991, 0.00011780440321678284, 0.0001177963428540574, 0.00011778823163072921, 0.00011778012957937367, 0.00011777203338196447, 0.00011776390251118072, 0.00011775575839199204, 0.00011774762029265105, 0.00011773944815984057, 0.00011773129588778735, 0.00011772307825061653, 0.00011771488369742851, 0.00011770666070401529, 0.00011769844704847601, 0.00011769021165986464, 0.00011768192775696911, 0.00011767366971097832, 0.0001176654019242104, 0.00011765710188148707, 0.0001176488174808876, 0.00011764045524887182, 0.00011763212437220434, 0.00011762373993311271, 0.0001176153308695261, 0.00011760700736361699, 0.00011759873704092899, 0.0001175903475299972, 0.00011758192763542484, 0.00011757355805635088, 0.00011756518997038875, 0.0001175568165518844, 0.0001175484639895452, 0.00011754005234263813, 0.0001175316398662245, 0.00011752320286011664, 0.0001175147796238178, 0.00011750629704224904, 0.00011749786508428115, 0.0001174894234329365, 0.00011748097573804399, 0.00011747248071387671, 0.00011746405927879215, 0.00011745560191422306, 0.0001174471356857838, 0.00011743866173108336, 0.00011743023325704305, 0.00011742177667458017, 0.00011741329046688271, 0.00011740480501759123, 0.00011739633115769155, 0.00011738789294287411, 0.00011737945005911969, 0.00011737089938691177, 0.00011736244069513218, 0.00011735397759511769, 0.0001173455396647025, 0.00011733706684761107, 0.00011732858782107046, 0.00011732005110827309, 0.0001173114724935439, 0.00011730295853292667, 0.00011729445523739389, 0.00011728586681078701, 0.00011727729158518466, 0.00011726871953540746, 0.00011726019389059771, 0.00011725165127645359, 0.00011724310489397963, 0.00011723456479205541, 0.00011722605464716839, 0.00011721747591393826, 0.00011720888547281544, 0.00011720034958952461, 0.0001171918161947535, 0.00011718331808956182, 0.00011717477542801733, 0.00011716619963361047, 0.00011715761504643399, 0.00011714904264115398, 0.00011714050166232281, 0.00011713197679961926, 0.00011712344513496183, 0.00011711492863842453, 0.00011710641553101406, 0.00011709788474326359, 0.00011708928636257789, 0.00011708072905431742, 0.00011707218961599845, 0.00011706364081610534, 0.00011705510583342165, 0.00011704658407544268], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329]}
[2017-12-15 05:52:34,065 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:92]: done!
[2017-12-15 05:52:34,066 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:152]: >> Executing classifier part ... 
[2017-12-15 05:52:34,066 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:97]: =======================================
[2017-12-15 05:52:34,066 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f2ee0804400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-15 05:52:34,434 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:110]: training ... 
[2017-12-15 10:22:12,003 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:122]: trained!
[2017-12-15 10:22:12,004 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:125]: Training history: 
{'val_loss': [0.00011784389298063124, 0.0001178379564785458, 0.00011783194934425267, 0.00011782593681109172, 0.00011781990507798366, 0.00011781385659408373, 0.00011780776511588882, 0.00011780165711930365, 0.00011779554072050698, 0.00011778931795328819, 0.00011778307214255744, 0.00011777680058888084, 0.00011777050599169229, 0.00011776413536342822, 0.00011775777642675207, 0.00011775141454036336, 0.00011774505476346605, 0.00011773861845493596, 0.00011773215132637902, 0.00011772570684803902, 0.00011771925655965915, 0.00011771279320315886, 0.00011770628120321711, 0.00011769976895299674, 0.00011769314911871512, 0.00011768652839058122, 0.00011767993832158076, 0.00011767335438440698, 0.0001176666578754762, 0.00011765995246377666, 0.00011765319547680017, 0.00011764648468410986, 0.00011763968836763274, 0.00011763295133143929, 0.00011762620120924836, 0.00011761945861329368, 0.00011761273105193447, 0.00011760592402710696, 0.00011759911537546829, 0.00011759232413607217, 0.00011758542972824511, 0.00011757856794602652, 0.00011757174311566146, 0.00011756486048880754, 0.00011755794205423101, 0.00011755095813603596, 0.00011754400805908848, 0.00011753702446268025, 0.00011753001430098205, 0.00011752302857720537, 0.00011751601333842619, 0.0001175090147074225, 0.00011750195917378223, 0.00011749485067045545, 0.00011748776310114923, 0.00011748057064721567, 0.0001174733996099829, 0.00011746616775504055, 0.00011745895626205328, 0.00011745174820145881, 0.00011744450090074893, 0.0001174372664357579, 0.00011743002371157173, 0.00011742276185894665, 0.00011741552158391575, 0.00011740827698263978, 0.00011740103418694544, 0.0001173937387433514, 0.00011738639776692188, 0.00011737910101830351, 0.00011737174246873803, 0.00011736434589469626, 0.00011735701938079675, 0.00011734962951064713, 0.00011734225533654369, 0.00011733486628873818, 0.00011732746630018069, 0.00011732009827578097, 0.00011731273368377403, 0.00011730533843263366, 0.00011729789453805184, 0.00011729050354164836, 0.00011728304509515131, 0.0001172755143896354, 0.00011726797640816187, 0.00011726046025456119, 0.00011725281804991104, 0.00011724523389202838, 0.00011723758408963379, 0.00011722996519665127, 0.000117222316699281, 0.00011721457624238749, 0.00011720676565384355, 0.00011719904344941373, 0.00011719123475583663, 0.00011718347372246348, 0.00011717567058864762, 0.00011716787317548638, 0.00011716004281492986, 0.00011715223599844257, 0.00011714442272834178, 0.00011713659451303075, 0.00011712868569012049, 0.0001171207546460424, 0.00011711279392606837, 0.00011710484890214051, 0.00011709691197651436, 0.00011708896679169309, 0.00011708100108402331, 0.00011707307740528805, 0.00011706511316353601, 0.00011705716102454395, 0.000117049227692204, 0.0001170412565856664, 0.00011703328932269364, 0.00011702532819154757, 0.0001170173402448329, 0.00011700937158745057, 0.00011700132627329613, 0.000116993308829456, 0.00011698526253206403, 0.0001169772272648093, 0.00011696917099202582, 0.00011696105447359822, 0.00011695297277965571, 0.00011694488978068886, 0.00011693678177173503, 0.00011692869566216222, 0.00011692054317511861, 0.00011691242780082193, 0.00011690426574955884, 0.00011689606290287739, 0.00011688793421018163, 0.00011687986315308133, 0.00011687168368957571, 0.00011686347219040411, 0.00011685530148665089, 0.00011684713952477303, 0.00011683897355843692, 0.0001168308403786027, 0.0001168226387654374, 0.00011681442392325826, 0.00011680618766437832, 0.00011679797069483072, 0.0001167896987891216, 0.00011678147559836209, 0.00011677323345793411, 0.0001167649803588771, 0.00011675668401524644, 0.00011674846974513277, 0.00011674020234443916, 0.00011673191206112701, 0.00011672362333311784, 0.00011671536689105327, 0.00011670707571388883, 0.0001166987629591302, 0.00011669045519206733, 0.00011668213989876819, 0.00011667385462102885, 0.00011666555797348843, 0.00011665716527258126, 0.00011664886118818981, 0.00011664055440436445, 0.00011663226397803593, 0.00011662393872722232, 0.00011661560816692614, 0.0001166072145006585, 0.0001165987727630149, 0.00011659040836147112, 0.00011658204576550898, 0.00011657360377758672, 0.00011656516915500731, 0.00011655673281623151, 0.00011654834937563404, 0.00011653995506579276, 0.00011653155698389484, 0.00011652316471203677, 0.00011651479728927227, 0.00011650637419738736, 0.00011649794275692208, 0.00011648956029956213, 0.0001164811825796193, 0.00011647283510763783, 0.00011646443269949485, 0.00011645600503108622, 0.00011644757393028481, 0.00011643915353783382, 0.00011643076453747511, 0.00011642238862311389, 0.00011641399627974765, 0.00011640562640782789, 0.00011639724918844233, 0.00011638885798920701, 0.00011638040228959067, 0.00011637198713511408, 0.00011636358824874908, 0.00011635519336684233, 0.00011634679930727971, 0.00011633842600296717, 0.00011633002418476668], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011857953340179934, 0.00011857367981094806, 0.00011856778258805129, 0.00011856178319364549, 0.00011855579098039656, 0.00011854979928855175, 0.0001185437999652465, 0.00011853775364446912, 0.00011853171320133827, 0.00011852565932020102, 0.00011851947994657007, 0.00011851329275187718, 0.00011850709124227094, 0.00011850087634205867, 0.00011849459294830403, 0.00011848830559661806, 0.000118482015329809, 0.00011847570430163555, 0.00011846929738250279, 0.00011846284789783298, 0.00011845642197114971, 0.00011844996281680333, 0.00011844350482376614, 0.00011843699040058212, 0.00011843045796525547, 0.0001184238104418175, 0.00011841718225773266, 0.00011841059249165204, 0.00011840401123393879, 0.0001183973078254583, 0.00011839064155517189, 0.00011838392091665495, 0.00011837724651720417, 0.00011837047935521886, 0.00011836377044829482, 0.00011835704310262479, 0.00011835030677458362, 0.00011834358177523217, 0.00011833678553311658, 0.00011832999666175936, 0.00011832321153503179, 0.00011831632542180456, 0.0001183094612786513, 0.00011830264766429815, 0.00011829578624666648, 0.00011828888231089812, 0.00011828189369435923, 0.00011827492342176559, 0.00011826790956452683, 0.00011826086167381858, 0.00011825382617830847, 0.000118246771438246, 0.00011823971961330658, 0.00011823260358455879, 0.00011822544292835762, 0.00011821830267801802, 0.00011821107871861558, 0.00011820386002065481, 0.00011819657553097306, 0.00011818927731888265, 0.00011818199465411536, 0.00011817465999113632, 0.00011816733812625862, 0.00011816000870102106, 0.00011815264917654516, 0.0001181453184240971, 0.00011813796989650827, 0.00011813062373893821, 0.00011812321968180968, 0.00011811578282362141, 0.00011810838435973717, 0.00011810089848956081, 0.00011809338628847595, 0.00011808593836230004, 0.00011807841639673787, 0.00011807089454967662, 0.00011806335895650652, 0.00011805584229978639, 0.00011804836183325282, 0.00011804087155484155, 0.00011803335963815893, 0.00011802577007966152, 0.00011801822792153945, 0.00011801058041608319, 0.00011800287697818403, 0.00011799514844178614, 0.00011798743222948583, 0.00011797961081205246, 0.00011797184321774527, 0.00011796399399999177, 0.00011795618234703573, 0.00011794833260787809, 0.00011794039069869062, 0.00011793241861916423, 0.00011792453664775137, 0.00011791658322027268, 0.00011790867330633856, 0.00011790071307690601, 0.00011789274232459015, 0.00011788473649599653, 0.00011787674178279093, 0.00011786875458254482, 0.00011786078558404283, 0.00011785272894224686, 0.00011784470920164308, 0.00011783664485728611, 0.00011782859534924662, 0.00011782055216915723, 0.00011781249156942991, 0.00011780440321678284, 0.0001177963428540574, 0.00011778823163072921, 0.00011778012957937367, 0.00011777203338196447, 0.00011776390251118072, 0.00011775575839199204, 0.00011774762029265105, 0.00011773944815984057, 0.00011773129588778735, 0.00011772307825061653, 0.00011771488369742851, 0.00011770666070401529, 0.00011769844704847601, 0.00011769021165986464, 0.00011768192775696911, 0.00011767366971097832, 0.0001176654019242104, 0.00011765710188148707, 0.0001176488174808876, 0.00011764045524887182, 0.00011763212437220434, 0.00011762373993311271, 0.0001176153308695261, 0.00011760700736361699, 0.00011759873704092899, 0.0001175903475299972, 0.00011758192763542484, 0.00011757355805635088, 0.00011756518997038875, 0.0001175568165518844, 0.0001175484639895452, 0.00011754005234263813, 0.0001175316398662245, 0.00011752320286011664, 0.0001175147796238178, 0.00011750629704224904, 0.00011749786508428115, 0.0001174894234329365, 0.00011748097573804399, 0.00011747248071387671, 0.00011746405927879215, 0.00011745560191422306, 0.0001174471356857838, 0.00011743866173108336, 0.00011743023325704305, 0.00011742177667458017, 0.00011741329046688271, 0.00011740480501759123, 0.00011739633115769155, 0.00011738789294287411, 0.00011737945005911969, 0.00011737089938691177, 0.00011736244069513218, 0.00011735397759511769, 0.0001173455396647025, 0.00011733706684761107, 0.00011732858782107046, 0.00011732005110827309, 0.0001173114724935439, 0.00011730295853292667, 0.00011729445523739389, 0.00011728586681078701, 0.00011727729158518466, 0.00011726871953540746, 0.00011726019389059771, 0.00011725165127645359, 0.00011724310489397963, 0.00011723456479205541, 0.00011722605464716839, 0.00011721747591393826, 0.00011720888547281544, 0.00011720034958952461, 0.0001171918161947535, 0.00011718331808956182, 0.00011717477542801733, 0.00011716619963361047, 0.00011715761504643399, 0.00011714904264115398, 0.00011714050166232281, 0.00011713197679961926, 0.00011712344513496183, 0.00011711492863842453, 0.00011710641553101406, 0.00011709788474326359, 0.00011708928636257789, 0.00011708072905431742, 0.00011707218961599845, 0.00011706364081610534, 0.00011705510583342165, 0.00011704658407544268], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329]}
[2017-12-15 10:22:12,004 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:129]: evaluating model ... 
[2017-12-15 10:22:13,330 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:133]: evaluated! 
[2017-12-15 10:22:13,331 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:135]: generating reports ... 
[2017-12-15 10:22:13,757 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:138]: done!
[2017-12-15 10:22:13,757 AE_BIGRAMA_1L_MINIDS_OVER_F1_9.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_9 finished!
