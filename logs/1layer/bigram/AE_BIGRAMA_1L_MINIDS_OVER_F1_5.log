[2017-12-14 09:31:58,013 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_5
[2017-12-14 09:31:58,013 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:146]: >> Printing header log
[2017-12-14 09:31:58,013 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_5
	layers = 9216,13824
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fe13f5b5eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fe13f598400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:58,013 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:148]: >> Loading dataset... 
[2017-12-14 09:32:19,993 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:19,994 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:19,994 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:57]: =======================================
[2017-12-14 09:32:19,994 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fe13f5b5eb8>, 'discard_decoder_function': True}
[2017-12-14 09:32:20,044 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:54,917 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_5
[2017-12-14 10:18:54,917 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:146]: >> Printing header log
[2017-12-14 10:18:54,917 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_5
	layers = 9216,13824
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f5919ebfeb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f5919ea2400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:54,917 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:148]: >> Loading dataset... 
[2017-12-14 10:19:16,494 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:16,495 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:16,495 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:57]: =======================================
[2017-12-14 10:19:16,495 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f5919ebfeb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:16,537 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:73]: training and evaluate autoencoder
[2017-12-15 01:53:30,488 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:85]: trained and evaluated!
[2017-12-15 01:53:30,496 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:88]: Training history: 
{'val_loss': [0.00011929411640094995, 0.00011928960171394297, 0.00011928507297557803, 0.00011928053785510778, 0.00011927601337147973, 0.00011927149664648952, 0.00011926697362877922, 0.00011926240745588053, 0.00011925783686735153, 0.00011925328025867229, 0.00011924871522990451, 0.00011924412424366633, 0.00011923947462071814, 0.00011923484312509431, 0.00011923017004746211, 0.00011922547449838343, 0.00011922075883762829, 0.00011921602340486056, 0.00011921127309839079, 0.00011920653954271287, 0.00011920176397597746, 0.00011919699545279809, 0.00011919222217432454, 0.00011918741825459461, 0.00011918261522871696, 0.00011917780403302941, 0.00011917294526652317, 0.00011916807432574877, 0.00011916317878615944, 0.00011915825276620713, 0.00011915332910602486, 0.00011914838052524084, 0.0001191433840518513, 0.00011913834301098673, 0.00011913328563050236, 0.00011912823543659038, 0.00011912315721147069, 0.00011911809582652808, 0.00011911299128639706, 0.00011910785315529711, 0.00011910271486330375, 0.00011909751543181397, 0.00011909235025274382, 0.00011908716235194854, 0.0001190819218926388, 0.00011907666058869373, 0.00011907139356409403, 0.00011906609977755686, 0.00011906078298326183, 0.00011905537987859, 0.00011904999263085772, 0.00011904453762912212, 0.00011903906130007095, 0.00011903356689732655, 0.00011902805272256956, 0.00011902252440706939, 0.00011901696847153356, 0.00011901137762212742, 0.00011900577681520679, 0.00011900015948989591, 0.00011899451372220527, 0.00011898888520586375, 0.0001189832311074698, 0.00011897752241257816, 0.00011897174669664207, 0.00011896595251371773, 0.00011896020139659657, 0.00011895440762484428, 0.00011894859881849653, 0.00011894266020691969, 0.00011893675463212338, 0.00011893076069108999, 0.0001189247259725153, 0.00011891871585275552, 0.00011891262212205285, 0.00011890649317357007, 0.00011890029743644443, 0.00011889407881670025, 0.00011888778598933922, 0.00011888142919790855, 0.00011887512618063146, 0.00011886875678588355, 0.0001188623708012372, 0.00011885585224041967, 0.00011884934177790386, 0.00011884279280823154, 0.00011883619757276486, 0.00011882955231732421, 0.00011882295701034934, 0.00011881629843650962, 0.00011880963095990113, 0.00011880288721981552, 0.00011879611087199849, 0.00011878925146742699, 0.00011878241896780931, 0.00011877556594534312, 0.00011876862768512288, 0.00011876160727987748, 0.00011875442070749196, 0.00011874721287929909, 0.00011873995404789471, 0.00011873265949815296, 0.00011872528655756567, 0.00011871785052888397, 0.00011871034592385483, 0.00011870276546652064, 0.00011869519278570136, 0.00011868758145470918, 0.00011867992026463645, 0.0001186722497606229, 0.00011866454720306633, 0.00011865682928912749, 0.00011864904626698808, 0.00011864121140141603, 0.0001186333534029468, 0.00011862552549154555, 0.00011861761699042212, 0.00011860972148591095, 0.00011860176475251812, 0.00011859382609281854, 0.00011858589225992131, 0.00011857794337455156, 0.00011856997602219356, 0.00011856198848665095, 0.00011855403838564209, 0.00011854599673628204, 0.00011853793048810704, 0.00011852988531696896, 0.00011852184658156735, 0.00011851375512675787, 0.00011850571074220981, 0.00011849766262135919, 0.0001184895421163504, 0.00011848142904819261, 0.00011847332055655855, 0.00011846518632197862, 0.00011845708478451535, 0.00011844894572313308, 0.00011844080135226822, 0.00011843267251655611, 0.00011842447371008699, 0.00011841630139739965, 0.00011840809254403082, 0.00011839990265820751, 0.0001183916489334539, 0.00011838342136281819, 0.00011837516265036882, 0.00011836688938600421, 0.00011835862813501433, 0.00011835034711201188, 0.00011834206925324652, 0.00011833382761337581, 0.00011832553656135071, 0.0001183172228054775, 0.00011830886131789219, 0.00011830050244035556, 0.00011829213653713995, 0.00011828373584519335, 0.00011827534464595803, 0.00011826694111156115, 0.00011825851787665987, 0.0001182500417614573, 0.00011824152427877092, 0.00011823304619709332, 0.00011822463620908293, 0.00011821615583914348, 0.00011820763900003073, 0.00011819916283119704, 0.00011819062854408765, 0.00011818213791272395, 0.00011817361385128473, 0.00011816505879104819, 0.00011815649761686202, 0.00011814793088291463, 0.00011813936709867978, 0.00011813076954470554, 0.00011812214542544133, 0.00011811355466474447, 0.00011810499104140303, 0.00011809638900029032, 0.00011808776570337021, 0.00011807910153952357, 0.00011807041179362447, 0.00011806172270917605, 0.00011805308847150397, 0.00011804438295805054, 0.00011803568280771082, 0.00011802702082486375, 0.00011801829253605405, 0.00011800956818019441, 0.00011800080867806285, 0.00011799210034003618, 0.00011798337770037293, 0.000117974695444956, 0.00011796599609908334, 0.00011795724975445745, 0.00011794853333600611, 0.00011793986080570207, 0.00011793113677162924, 0.00011792249400660633], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.0001200987147677353, 0.00012009395844817271, 0.00012008915911276949, 0.00012008435169560229, 0.0001200795405575056, 0.00012007472422906783, 0.00012006990308949196, 0.00012006507806308532, 0.00012006020824332396, 0.00012005532451155246, 0.00012005045217957123, 0.00012004557686136635, 0.00012004069348509766, 0.00012003575574059846, 0.00012003083122080398, 0.00012002586738239816, 0.00012002087984380467, 0.00012001585433751054, 0.00012001081340239422, 0.00012000576211029591, 0.00012000071444432631, 0.00011999560646137909, 0.00011999052644465331, 0.00011998543834616354, 0.00011998031957963093, 0.00011997519166482589, 0.00011997005080971836, 0.00011996485928360962, 0.00011995966872920856, 0.00011995444257712562, 0.00011994918151466625, 0.00011994391443235921, 0.00011993861696641158, 0.00011993329990040875, 0.00011992795200046176, 0.00011992259843616992, 0.00011991724051104355, 0.00011991187059362222, 0.0001199065369611882, 0.00011990115690008655, 0.00011989573683306811, 0.00011989032321250073, 0.00011988484635983263, 0.00011987941385021566, 0.00011987393943866691, 0.00011986841663133493, 0.0001198628827797155, 0.00011985731560563219, 0.00011985172160293646, 0.00011984609458587931, 0.00011984040090019423, 0.00011983472382834072, 0.00011982898143876999, 0.00011982322850261575, 0.00011981745795722208, 0.0001198116626214321, 0.00011980584375135577, 0.00011979999828966886, 0.00011979409658743663, 0.00011978819393719689, 0.00011978226393694058, 0.00011977630480915363, 0.00011977037487999788, 0.00011976442532708674, 0.00011975842067123927, 0.00011975234849355715, 0.00011974626771270689, 0.00011974022499435803, 0.00011973410555850168, 0.00011972798318382206, 0.00011972174429901988, 0.00011971550968025147, 0.00011970918154054252, 0.00011970282140558022, 0.00011969645273855034, 0.00011969004252509151, 0.00011968359953427311, 0.00011967711731964412, 0.00011967064541459678, 0.00011966408849697627, 0.00011965752318948931, 0.00011965097153331047, 0.00011964434128730931, 0.00011963764553398943, 0.00011963083428965505, 0.0001196240047250756, 0.00011961709498276127, 0.00011961010385400248, 0.0001196030581437115, 0.00011959609917610737, 0.00011958907626539693, 0.00011958202998630144, 0.00011957492682675555, 0.00011956776197562115, 0.00011956050490705654, 0.00011955324471006716, 0.00011954595638095502, 0.00011953862811702665, 0.00011953121353701479, 0.00011952365568936848, 0.00011951607655895365, 0.0001195084121078632, 0.00011950069506605632, 0.00011949291604825869, 0.00011948509200010449, 0.00011947723192766503, 0.00011946932763067539, 0.00011946146544891922, 0.00011945360729619496, 0.00011944570809474567, 0.00011943777258460887, 0.00011942976824912707, 0.00011942172210651423, 0.00011941364676527018, 0.00011940553499683768, 0.00011939741308472487, 0.0001193893283582065, 0.00011938117933307899, 0.00011937303490578788, 0.00011936483992599649, 0.00011935667433443779, 0.00011934847546781561, 0.00011934026761882232, 0.00011933205533789392, 0.00011932380186603936, 0.00011931560994357216, 0.00011930732907430066, 0.00011929902900787714, 0.00011929073759202212, 0.00011928244089102524, 0.00011927408977439748, 0.00011926578892586777, 0.00011925748625242361, 0.00011924911866416542, 0.00011924076302080182, 0.00011923242219005551, 0.00011922405024096279, 0.00011921568812744796, 0.00011920728529701071, 0.00011919888602160161, 0.00011919049786158052, 0.0001191820582721522, 0.00011917366831091684, 0.00011916523886516884, 0.0001191568158421717, 0.00011914832916047048, 0.00011913985804979255, 0.00011913134956391869, 0.00011912281934497272, 0.00011911429621238285, 0.00011910575340863724, 0.00011909720065081279, 0.00011908871337660689, 0.00011908016751553705, 0.00011907162539909687, 0.0001190630504341966, 0.00011905446307409815, 0.00011904586964675167, 0.00011903724365534734, 0.0001190286378565029, 0.00011902001243390306, 0.000119011372838591, 0.00011900268084216402, 0.00011899394483448852, 0.00011898524847722701, 0.00011897660061054945, 0.0001189678806716012, 0.00011895914025569079, 0.00011895043586406566, 0.00011894167044445726, 0.00011893294538626847, 0.00011892418925713365, 0.00011891540288655937, 0.00011890660478439218, 0.000118897806303022, 0.00011888901282239142, 0.00011888018080525567, 0.00011887133629812103, 0.00011886251899880184, 0.00011885373066111197, 0.00011884490703384268, 0.00011883605065291402, 0.00011882717642574404, 0.00011881830051586074, 0.00011880942640719171, 0.00011880060195041584, 0.00011879170572947171, 0.00011878283749844921, 0.00011877399801575436, 0.00011876509598826426, 0.00011875620206623832, 0.00011874728394632079, 0.00011873841443548817, 0.00011872953840710393, 0.00011872068979983683, 0.00011871183100148903, 0.00011870292541897076, 0.00011869405090739853, 0.00011868521220680987, 0.00011867632008599821], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 01:53:30,496 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:92]: done!
[2017-12-15 01:53:30,497 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:152]: >> Executing classifier part ... 
[2017-12-15 01:53:30,497 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:97]: =======================================
[2017-12-15 01:53:30,497 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f5919ea2400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-15 01:53:30,630 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:110]: training ... 
[2017-12-15 07:52:45,969 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:122]: trained!
[2017-12-15 07:52:45,970 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:125]: Training history: 
{'val_loss': [0.00011929411640094995, 0.00011928960171394297, 0.00011928507297557803, 0.00011928053785510778, 0.00011927601337147973, 0.00011927149664648952, 0.00011926697362877922, 0.00011926240745588053, 0.00011925783686735153, 0.00011925328025867229, 0.00011924871522990451, 0.00011924412424366633, 0.00011923947462071814, 0.00011923484312509431, 0.00011923017004746211, 0.00011922547449838343, 0.00011922075883762829, 0.00011921602340486056, 0.00011921127309839079, 0.00011920653954271287, 0.00011920176397597746, 0.00011919699545279809, 0.00011919222217432454, 0.00011918741825459461, 0.00011918261522871696, 0.00011917780403302941, 0.00011917294526652317, 0.00011916807432574877, 0.00011916317878615944, 0.00011915825276620713, 0.00011915332910602486, 0.00011914838052524084, 0.0001191433840518513, 0.00011913834301098673, 0.00011913328563050236, 0.00011912823543659038, 0.00011912315721147069, 0.00011911809582652808, 0.00011911299128639706, 0.00011910785315529711, 0.00011910271486330375, 0.00011909751543181397, 0.00011909235025274382, 0.00011908716235194854, 0.0001190819218926388, 0.00011907666058869373, 0.00011907139356409403, 0.00011906609977755686, 0.00011906078298326183, 0.00011905537987859, 0.00011904999263085772, 0.00011904453762912212, 0.00011903906130007095, 0.00011903356689732655, 0.00011902805272256956, 0.00011902252440706939, 0.00011901696847153356, 0.00011901137762212742, 0.00011900577681520679, 0.00011900015948989591, 0.00011899451372220527, 0.00011898888520586375, 0.0001189832311074698, 0.00011897752241257816, 0.00011897174669664207, 0.00011896595251371773, 0.00011896020139659657, 0.00011895440762484428, 0.00011894859881849653, 0.00011894266020691969, 0.00011893675463212338, 0.00011893076069108999, 0.0001189247259725153, 0.00011891871585275552, 0.00011891262212205285, 0.00011890649317357007, 0.00011890029743644443, 0.00011889407881670025, 0.00011888778598933922, 0.00011888142919790855, 0.00011887512618063146, 0.00011886875678588355, 0.0001188623708012372, 0.00011885585224041967, 0.00011884934177790386, 0.00011884279280823154, 0.00011883619757276486, 0.00011882955231732421, 0.00011882295701034934, 0.00011881629843650962, 0.00011880963095990113, 0.00011880288721981552, 0.00011879611087199849, 0.00011878925146742699, 0.00011878241896780931, 0.00011877556594534312, 0.00011876862768512288, 0.00011876160727987748, 0.00011875442070749196, 0.00011874721287929909, 0.00011873995404789471, 0.00011873265949815296, 0.00011872528655756567, 0.00011871785052888397, 0.00011871034592385483, 0.00011870276546652064, 0.00011869519278570136, 0.00011868758145470918, 0.00011867992026463645, 0.0001186722497606229, 0.00011866454720306633, 0.00011865682928912749, 0.00011864904626698808, 0.00011864121140141603, 0.0001186333534029468, 0.00011862552549154555, 0.00011861761699042212, 0.00011860972148591095, 0.00011860176475251812, 0.00011859382609281854, 0.00011858589225992131, 0.00011857794337455156, 0.00011856997602219356, 0.00011856198848665095, 0.00011855403838564209, 0.00011854599673628204, 0.00011853793048810704, 0.00011852988531696896, 0.00011852184658156735, 0.00011851375512675787, 0.00011850571074220981, 0.00011849766262135919, 0.0001184895421163504, 0.00011848142904819261, 0.00011847332055655855, 0.00011846518632197862, 0.00011845708478451535, 0.00011844894572313308, 0.00011844080135226822, 0.00011843267251655611, 0.00011842447371008699, 0.00011841630139739965, 0.00011840809254403082, 0.00011839990265820751, 0.0001183916489334539, 0.00011838342136281819, 0.00011837516265036882, 0.00011836688938600421, 0.00011835862813501433, 0.00011835034711201188, 0.00011834206925324652, 0.00011833382761337581, 0.00011832553656135071, 0.0001183172228054775, 0.00011830886131789219, 0.00011830050244035556, 0.00011829213653713995, 0.00011828373584519335, 0.00011827534464595803, 0.00011826694111156115, 0.00011825851787665987, 0.0001182500417614573, 0.00011824152427877092, 0.00011823304619709332, 0.00011822463620908293, 0.00011821615583914348, 0.00011820763900003073, 0.00011819916283119704, 0.00011819062854408765, 0.00011818213791272395, 0.00011817361385128473, 0.00011816505879104819, 0.00011815649761686202, 0.00011814793088291463, 0.00011813936709867978, 0.00011813076954470554, 0.00011812214542544133, 0.00011811355466474447, 0.00011810499104140303, 0.00011809638900029032, 0.00011808776570337021, 0.00011807910153952357, 0.00011807041179362447, 0.00011806172270917605, 0.00011805308847150397, 0.00011804438295805054, 0.00011803568280771082, 0.00011802702082486375, 0.00011801829253605405, 0.00011800956818019441, 0.00011800080867806285, 0.00011799210034003618, 0.00011798337770037293, 0.000117974695444956, 0.00011796599609908334, 0.00011795724975445745, 0.00011794853333600611, 0.00011793986080570207, 0.00011793113677162924, 0.00011792249400660633], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.0001200987147677353, 0.00012009395844817271, 0.00012008915911276949, 0.00012008435169560229, 0.0001200795405575056, 0.00012007472422906783, 0.00012006990308949196, 0.00012006507806308532, 0.00012006020824332396, 0.00012005532451155246, 0.00012005045217957123, 0.00012004557686136635, 0.00012004069348509766, 0.00012003575574059846, 0.00012003083122080398, 0.00012002586738239816, 0.00012002087984380467, 0.00012001585433751054, 0.00012001081340239422, 0.00012000576211029591, 0.00012000071444432631, 0.00011999560646137909, 0.00011999052644465331, 0.00011998543834616354, 0.00011998031957963093, 0.00011997519166482589, 0.00011997005080971836, 0.00011996485928360962, 0.00011995966872920856, 0.00011995444257712562, 0.00011994918151466625, 0.00011994391443235921, 0.00011993861696641158, 0.00011993329990040875, 0.00011992795200046176, 0.00011992259843616992, 0.00011991724051104355, 0.00011991187059362222, 0.0001199065369611882, 0.00011990115690008655, 0.00011989573683306811, 0.00011989032321250073, 0.00011988484635983263, 0.00011987941385021566, 0.00011987393943866691, 0.00011986841663133493, 0.0001198628827797155, 0.00011985731560563219, 0.00011985172160293646, 0.00011984609458587931, 0.00011984040090019423, 0.00011983472382834072, 0.00011982898143876999, 0.00011982322850261575, 0.00011981745795722208, 0.0001198116626214321, 0.00011980584375135577, 0.00011979999828966886, 0.00011979409658743663, 0.00011978819393719689, 0.00011978226393694058, 0.00011977630480915363, 0.00011977037487999788, 0.00011976442532708674, 0.00011975842067123927, 0.00011975234849355715, 0.00011974626771270689, 0.00011974022499435803, 0.00011973410555850168, 0.00011972798318382206, 0.00011972174429901988, 0.00011971550968025147, 0.00011970918154054252, 0.00011970282140558022, 0.00011969645273855034, 0.00011969004252509151, 0.00011968359953427311, 0.00011967711731964412, 0.00011967064541459678, 0.00011966408849697627, 0.00011965752318948931, 0.00011965097153331047, 0.00011964434128730931, 0.00011963764553398943, 0.00011963083428965505, 0.0001196240047250756, 0.00011961709498276127, 0.00011961010385400248, 0.0001196030581437115, 0.00011959609917610737, 0.00011958907626539693, 0.00011958202998630144, 0.00011957492682675555, 0.00011956776197562115, 0.00011956050490705654, 0.00011955324471006716, 0.00011954595638095502, 0.00011953862811702665, 0.00011953121353701479, 0.00011952365568936848, 0.00011951607655895365, 0.0001195084121078632, 0.00011950069506605632, 0.00011949291604825869, 0.00011948509200010449, 0.00011947723192766503, 0.00011946932763067539, 0.00011946146544891922, 0.00011945360729619496, 0.00011944570809474567, 0.00011943777258460887, 0.00011942976824912707, 0.00011942172210651423, 0.00011941364676527018, 0.00011940553499683768, 0.00011939741308472487, 0.0001193893283582065, 0.00011938117933307899, 0.00011937303490578788, 0.00011936483992599649, 0.00011935667433443779, 0.00011934847546781561, 0.00011934026761882232, 0.00011933205533789392, 0.00011932380186603936, 0.00011931560994357216, 0.00011930732907430066, 0.00011929902900787714, 0.00011929073759202212, 0.00011928244089102524, 0.00011927408977439748, 0.00011926578892586777, 0.00011925748625242361, 0.00011924911866416542, 0.00011924076302080182, 0.00011923242219005551, 0.00011922405024096279, 0.00011921568812744796, 0.00011920728529701071, 0.00011919888602160161, 0.00011919049786158052, 0.0001191820582721522, 0.00011917366831091684, 0.00011916523886516884, 0.0001191568158421717, 0.00011914832916047048, 0.00011913985804979255, 0.00011913134956391869, 0.00011912281934497272, 0.00011911429621238285, 0.00011910575340863724, 0.00011909720065081279, 0.00011908871337660689, 0.00011908016751553705, 0.00011907162539909687, 0.0001190630504341966, 0.00011905446307409815, 0.00011904586964675167, 0.00011903724365534734, 0.0001190286378565029, 0.00011902001243390306, 0.000119011372838591, 0.00011900268084216402, 0.00011899394483448852, 0.00011898524847722701, 0.00011897660061054945, 0.0001189678806716012, 0.00011895914025569079, 0.00011895043586406566, 0.00011894167044445726, 0.00011893294538626847, 0.00011892418925713365, 0.00011891540288655937, 0.00011890660478439218, 0.000118897806303022, 0.00011888901282239142, 0.00011888018080525567, 0.00011887133629812103, 0.00011886251899880184, 0.00011885373066111197, 0.00011884490703384268, 0.00011883605065291402, 0.00011882717642574404, 0.00011881830051586074, 0.00011880942640719171, 0.00011880060195041584, 0.00011879170572947171, 0.00011878283749844921, 0.00011877399801575436, 0.00011876509598826426, 0.00011875620206623832, 0.00011874728394632079, 0.00011873841443548817, 0.00011872953840710393, 0.00011872068979983683, 0.00011871183100148903, 0.00011870292541897076, 0.00011869405090739853, 0.00011868521220680987, 0.00011867632008599821], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 07:52:45,970 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:129]: evaluating model ... 
[2017-12-15 07:52:48,637 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:133]: evaluated! 
[2017-12-15 07:52:48,638 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:135]: generating reports ... 
[2017-12-15 07:52:49,932 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:138]: done!
[2017-12-15 07:52:49,932 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_5 finished!
[2018-04-29 11:38:17,403 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_5 was already executed!
[2018-04-29 13:12:02,667 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_5
[2018-04-29 13:12:02,667 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:146]: >> Printing header log
[2018-04-29 13:12:02,667 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_5
	layers = 9216,13824
	using GLOBAL obj = 
		{'epochs': 200, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'shuffle_batches': True, 'data_dir': '/home/dhiego/malware_dataset/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'store_history': True, 'autoencoder_configs': {'optimizer': <keras.optimizers.SGD object at 0x7ff95528a828>, 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'output_layer_activation': 'relu', 'discard_decoder_function': True}, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'numpy_seed': 666, 'batch': 32, 'mlp_configs': {'optimizer': <keras.optimizers.SGD object at 0x7ff95528a898>, 'use_last_dim_as_classifier': False, 'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'classifier_dim': 9}, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:12:02,667 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,718 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,718 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,719 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:57]: =======================================
[2018-04-29 13:12:20,719 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:62]: setting configurations for autoencoder: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7ff95528a828>, 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'output_layer_activation': 'relu', 'discard_decoder_function': True}
[2018-04-29 13:12:20,764 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,883 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_5
[2018-04-29 13:14:12,883 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:146]: >> Printing header log
[2018-04-29 13:14:12,884 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_5
	layers = 9216,13824
	using GLOBAL obj = 
		{'numpy_seed': 666, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'store_history': True, 'autoencoder_configs': {'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f9d9f836828>, 'output_layer_activation': 'relu', 'loss_function': 'mse', 'hidden_layer_activation': 'relu'}, 'epochs': 200, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'mlp_configs': {'activation': 'sigmoid', 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f9d9f836898>, 'classifier_dim': 9}, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'batch': 32, 'data_dir': '/home/dhiego/malware_dataset/', 'shuffle_batches': True, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'fullds_data_dir': '/home/dhiego/malware_dataset/'}
	=======================================
	
[2018-04-29 13:14:12,884 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:148]: >> Loading dataset... 
[2018-04-29 13:14:30,910 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:30,911 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:30,911 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:57]: =======================================
[2018-04-29 13:14:30,911 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f9d9f836828>, 'output_layer_activation': 'relu', 'loss_function': 'mse', 'hidden_layer_activation': 'relu'}
[2018-04-29 13:14:30,959 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,703 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_5
[2018-04-29 13:16:33,704 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:146]: >> Printing header log
[2018-04-29 13:16:33,704 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_5
	layers = 9216,13824
	using GLOBAL obj = 
		{'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'numpy_seed': 666, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'store_history': True, 'epochs': 200, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'batch': 32, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'autoencoder_configs': {'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f5a38be5828>}, 'data_dir': '/home/dhiego/malware_dataset/', 'mlp_configs': {'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f5a38be5898>, 'activation': 'sigmoid', 'use_last_dim_as_classifier': False}, 'shuffle_batches': True, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:16:33,704 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:148]: >> Loading dataset... 
[2018-04-29 13:16:58,195 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:58,196 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:58,196 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:57]: =======================================
[2018-04-29 13:16:58,196 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:62]: setting configurations for autoencoder: 
	 {'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f5a38be5828>}
[2018-04-29 13:16:58,279 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:21,105 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_5
[2018-04-29 14:30:21,105 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:146]: >> Printing header log
[2018-04-29 14:30:21,105 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_5
	layers = 9216,13824
	using GLOBAL obj = 
		{'mlp_configs': {'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7f74d6338908>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy'}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'numpy_seed': 666, 'shuffle_batches': True, 'epochs': 200, 'batch': 32, 'autoencoder_configs': {'optimizer': <keras.optimizers.SGD object at 0x7f74d6338898>, 'output_layer_activation': 'relu', 'loss_function': 'mse', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu'}, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'store_history': True, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/'}
	=======================================
	
[2018-04-29 14:30:21,105 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:148]: >> Loading dataset... 
[2018-04-29 14:30:39,273 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:39,274 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:39,274 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:57]: =======================================
[2018-04-29 14:30:39,274 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:62]: setting configurations for autoencoder: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7f74d6338898>, 'output_layer_activation': 'relu', 'loss_function': 'mse', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu'}
[2018-04-29 14:30:39,330 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:73]: training and evaluate autoencoder
[2018-04-29 22:33:35,941 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:85]: trained and evaluated!
[2018-04-29 22:33:35,942 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:88]: Training history: 
{'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322718259954, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.003257328990228013, 0.003257328990228013, 0.003257328990228013, 0.003257328990228013, 0.003257328990228013, 0.003257328990228013, 0.003257328990228013], 'loss': [0.00011923767965008636, 0.00011922929400228516, 0.00011922089216725579, 0.00011921247696532058, 0.00011920404282693542, 0.0001191955951557431, 0.00011918712494567231, 0.00011917872725817578, 0.0001191702931434908, 0.00011916177501164055, 0.00011915331404464295, 0.00011914481304802838, 0.00011913636577973925, 0.00011912789478756227, 0.00011911936098988796, 0.00011911083202705194, 0.00011910234181402275, 0.00011909383586406895, 0.00011908528924459312, 0.0001190767418193109, 0.00011906819325641969, 0.00011905962762939334, 0.00011905104380062287, 0.00011904245907124527, 0.00011903384251251561, 0.00011902517457177912, 0.00011901651622961861, 0.0001190078406100213, 0.00011899909957790601, 0.00011899042632832747, 0.00011898172034878975, 0.00011897305667408703, 0.00011896439176697454, 0.00011895568574003645, 0.00011894698988047886, 0.00011893831115615697, 0.00011892961875682678, 0.0001189209363589758, 0.00011891219648816972, 0.0001189034270869298, 0.00011889470006162544, 0.00011888595708609477, 0.00011887719733083123, 0.00011886850756222188, 0.00011885975555691972, 0.00011885099298133384, 0.00011884226960585838, 0.00011883351883296598, 0.00011882475104333882, 0.00011881594445650445, 0.00011880716827701086, 0.00011879839354322871, 0.00011878959864058687, 0.00011878081843206138, 0.0001187720916200587, 0.00011876336492655695, 0.00011875459476691104, 0.00011874582138403959, 0.00011873709476163841, 0.00011872828701349484, 0.00011871950602286316, 0.00011871075510776963, 0.00011870191894310104, 0.00011869310391899987, 0.00011868429290023041, 0.00011867552378339274, 0.00011866670254984239, 0.0001186578825487018, 0.00011864911366886602, 0.00011864032734569211, 0.00011863155114249834, 0.00011862277640871619, 0.0001186139458372919, 0.00011860514069616898, 0.00011859634562762582, 0.00011858754119750853, 0.00011857868028984402, 0.00011856987491171923, 0.00011856112802565759, 0.00011855234822003528, 0.00011854353713016526, 0.00011853475869915385, 0.00011852596965045835, 0.00011851719460857377, 0.00011850838667082871, 0.00011849961776729273, 0.0001184907841859446, 0.00011848196942254549, 0.00011847316646183984, 0.00011846433937434313, 0.00011845552570115266, 0.00011844662601998112, 0.00011843777622770464, 0.00011842895160502746, 0.00011842010543887968, 0.00011841123424533372, 0.0001184023887664914, 0.00011839352631831469, 0.0001183847045633602, 0.0001183758859368305, 0.00011836705692961861], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457], 'val_loss': [0.00011837413112695286, 0.00011836580802138474, 0.00011835745720639573, 0.00011834909568305633, 0.00011834072638320204, 0.00011833233053593482, 0.00011832401193538224, 0.00011831564796288756, 0.00011830721228556245, 0.00011829883090082523, 0.00011829040757653873, 0.00011828204884201846, 0.00011827367988182804, 0.0001182652500145428, 0.00011825683870363101, 0.00011824846786635079, 0.00011824008410396649, 0.00011823167075507151, 0.00011822326779274009, 0.00011821487046167808, 0.00011820646928705124, 0.00011819804202981467, 0.00011818963838815552, 0.00011818119525610234, 0.00011817270537557455, 0.00011816424778099138, 0.00011815576648144554, 0.00011814722876194338, 0.00011813874984004464, 0.0001181302208087867, 0.00011812173829360177, 0.00011811324882424604, 0.00011810473164546942, 0.00011809622615828072, 0.00011808772083198542, 0.00011807920758615887, 0.00011807070374365836, 0.00011806213359519523, 0.0001180535435853343, 0.00011804499109939234, 0.00011803642259561743, 0.00011802784231086937, 0.00011801932146729837, 0.00011801074944174544, 0.00011800218036590506, 0.00011799363801624805, 0.00011798509591686968, 0.00011797652356952992, 0.00011796790527818262, 0.0001179593189509931, 0.0001179507381835648, 0.00011794213927093509, 0.00011793354910018074, 0.00011792501869239028, 0.00011791649196727123, 0.00011790791888697261, 0.00011789933363240583, 0.0001178907873676758, 0.00011788215486407715, 0.00011787352890347724, 0.00011786492870370023, 0.0001178562426583496, 0.00011784757681406064, 0.00011783891350831219, 0.00011783030047281633, 0.00011782163078496256, 0.00011781296542335384, 0.00011780434838339975, 0.00011779571933007092, 0.00011778709762420789, 0.0001177784742915337, 0.00011776980018804961, 0.00011776115216717526, 0.00011775250888371804, 0.00011774385971871275, 0.00011773515921083212, 0.00011772650718549954, 0.00011771791728290087, 0.00011770929550552965, 0.00011770064871817145, 0.00011769202758437388, 0.00011768339992545463, 0.00011767479130558907, 0.00011766614745006638, 0.0001176575528100506, 0.00011764889179256399, 0.00011764025921745716, 0.00011763163817304482, 0.00011762299710634125, 0.00011761437327310978, 0.00011760567203227028, 0.00011759703218120583, 0.00011758840337815564, 0.0001175797580567152, 0.0001175710855085341, 0.00011756243789883179, 0.00011755378088580344, 0.0001175451598413911, 0.00011753653536458598, 0.00011752790287886438, 0.000117519247421139]}
[2018-04-29 22:33:35,942 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:92]: done!
[2018-04-29 22:33:35,943 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:152]: >> Executing classifier part ... 
[2018-04-29 22:33:35,943 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:97]: =======================================
[2018-04-29 22:33:35,943 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7f74d6338908>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy'}
[2018-04-29 22:33:36,075 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:110]: training ... 
[2018-04-30 03:26:56,241 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:122]: trained!
[2018-04-30 03:26:56,242 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:125]: Training history: 
{'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322718259954, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.003257328990228013, 0.003257328990228013, 0.003257328990228013, 0.003257328990228013, 0.003257328990228013, 0.003257328990228013, 0.003257328990228013], 'loss': [0.00011923767965008636, 0.00011922929400228516, 0.00011922089216725579, 0.00011921247696532058, 0.00011920404282693542, 0.0001191955951557431, 0.00011918712494567231, 0.00011917872725817578, 0.0001191702931434908, 0.00011916177501164055, 0.00011915331404464295, 0.00011914481304802838, 0.00011913636577973925, 0.00011912789478756227, 0.00011911936098988796, 0.00011911083202705194, 0.00011910234181402275, 0.00011909383586406895, 0.00011908528924459312, 0.0001190767418193109, 0.00011906819325641969, 0.00011905962762939334, 0.00011905104380062287, 0.00011904245907124527, 0.00011903384251251561, 0.00011902517457177912, 0.00011901651622961861, 0.0001190078406100213, 0.00011899909957790601, 0.00011899042632832747, 0.00011898172034878975, 0.00011897305667408703, 0.00011896439176697454, 0.00011895568574003645, 0.00011894698988047886, 0.00011893831115615697, 0.00011892961875682678, 0.0001189209363589758, 0.00011891219648816972, 0.0001189034270869298, 0.00011889470006162544, 0.00011888595708609477, 0.00011887719733083123, 0.00011886850756222188, 0.00011885975555691972, 0.00011885099298133384, 0.00011884226960585838, 0.00011883351883296598, 0.00011882475104333882, 0.00011881594445650445, 0.00011880716827701086, 0.00011879839354322871, 0.00011878959864058687, 0.00011878081843206138, 0.0001187720916200587, 0.00011876336492655695, 0.00011875459476691104, 0.00011874582138403959, 0.00011873709476163841, 0.00011872828701349484, 0.00011871950602286316, 0.00011871075510776963, 0.00011870191894310104, 0.00011869310391899987, 0.00011868429290023041, 0.00011867552378339274, 0.00011866670254984239, 0.0001186578825487018, 0.00011864911366886602, 0.00011864032734569211, 0.00011863155114249834, 0.00011862277640871619, 0.0001186139458372919, 0.00011860514069616898, 0.00011859634562762582, 0.00011858754119750853, 0.00011857868028984402, 0.00011856987491171923, 0.00011856112802565759, 0.00011855234822003528, 0.00011854353713016526, 0.00011853475869915385, 0.00011852596965045835, 0.00011851719460857377, 0.00011850838667082871, 0.00011849961776729273, 0.0001184907841859446, 0.00011848196942254549, 0.00011847316646183984, 0.00011846433937434313, 0.00011845552570115266, 0.00011844662601998112, 0.00011843777622770464, 0.00011842895160502746, 0.00011842010543887968, 0.00011841123424533372, 0.0001184023887664914, 0.00011839352631831469, 0.0001183847045633602, 0.0001183758859368305, 0.00011836705692961861], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457, 0.002457002457002457], 'val_loss': [0.00011837413112695286, 0.00011836580802138474, 0.00011835745720639573, 0.00011834909568305633, 0.00011834072638320204, 0.00011833233053593482, 0.00011832401193538224, 0.00011831564796288756, 0.00011830721228556245, 0.00011829883090082523, 0.00011829040757653873, 0.00011828204884201846, 0.00011827367988182804, 0.0001182652500145428, 0.00011825683870363101, 0.00011824846786635079, 0.00011824008410396649, 0.00011823167075507151, 0.00011822326779274009, 0.00011821487046167808, 0.00011820646928705124, 0.00011819804202981467, 0.00011818963838815552, 0.00011818119525610234, 0.00011817270537557455, 0.00011816424778099138, 0.00011815576648144554, 0.00011814722876194338, 0.00011813874984004464, 0.0001181302208087867, 0.00011812173829360177, 0.00011811324882424604, 0.00011810473164546942, 0.00011809622615828072, 0.00011808772083198542, 0.00011807920758615887, 0.00011807070374365836, 0.00011806213359519523, 0.0001180535435853343, 0.00011804499109939234, 0.00011803642259561743, 0.00011802784231086937, 0.00011801932146729837, 0.00011801074944174544, 0.00011800218036590506, 0.00011799363801624805, 0.00011798509591686968, 0.00011797652356952992, 0.00011796790527818262, 0.0001179593189509931, 0.0001179507381835648, 0.00011794213927093509, 0.00011793354910018074, 0.00011792501869239028, 0.00011791649196727123, 0.00011790791888697261, 0.00011789933363240583, 0.0001178907873676758, 0.00011788215486407715, 0.00011787352890347724, 0.00011786492870370023, 0.0001178562426583496, 0.00011784757681406064, 0.00011783891350831219, 0.00011783030047281633, 0.00011782163078496256, 0.00011781296542335384, 0.00011780434838339975, 0.00011779571933007092, 0.00011778709762420789, 0.0001177784742915337, 0.00011776980018804961, 0.00011776115216717526, 0.00011775250888371804, 0.00011774385971871275, 0.00011773515921083212, 0.00011772650718549954, 0.00011771791728290087, 0.00011770929550552965, 0.00011770064871817145, 0.00011769202758437388, 0.00011768339992545463, 0.00011767479130558907, 0.00011766614745006638, 0.0001176575528100506, 0.00011764889179256399, 0.00011764025921745716, 0.00011763163817304482, 0.00011762299710634125, 0.00011761437327310978, 0.00011760567203227028, 0.00011759703218120583, 0.00011758840337815564, 0.0001175797580567152, 0.0001175710855085341, 0.00011756243789883179, 0.00011755378088580344, 0.0001175451598413911, 0.00011753653536458598, 0.00011752790287886438, 0.000117519247421139]}
[2018-04-30 03:26:56,242 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:129]: evaluating model ... 
[2018-04-30 03:26:58,774 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:133]: evaluated! 
[2018-04-30 03:26:58,775 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:135]: generating reports ... 
[2018-04-30 03:27:00,105 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:138]: done!
[2018-04-30 03:27:00,105 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_5 finished!
