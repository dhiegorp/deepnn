[2017-12-14 09:31:58,013 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_5
[2017-12-14 09:31:58,013 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:146]: >> Printing header log
[2017-12-14 09:31:58,013 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_5
	layers = 9216,13824
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fe13f5b5eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fe13f598400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:58,013 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:148]: >> Loading dataset... 
[2017-12-14 09:32:19,993 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:19,994 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:19,994 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:57]: =======================================
[2017-12-14 09:32:19,994 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fe13f5b5eb8>, 'discard_decoder_function': True}
[2017-12-14 09:32:20,044 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:54,917 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_5
[2017-12-14 10:18:54,917 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:146]: >> Printing header log
[2017-12-14 10:18:54,917 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_5
	layers = 9216,13824
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f5919ebfeb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f5919ea2400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:54,917 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:148]: >> Loading dataset... 
[2017-12-14 10:19:16,494 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:16,495 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:16,495 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:57]: =======================================
[2017-12-14 10:19:16,495 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f5919ebfeb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:16,537 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:73]: training and evaluate autoencoder
[2017-12-15 01:53:30,488 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:85]: trained and evaluated!
[2017-12-15 01:53:30,496 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:88]: Training history: 
{'val_loss': [0.00011929411640094995, 0.00011928960171394297, 0.00011928507297557803, 0.00011928053785510778, 0.00011927601337147973, 0.00011927149664648952, 0.00011926697362877922, 0.00011926240745588053, 0.00011925783686735153, 0.00011925328025867229, 0.00011924871522990451, 0.00011924412424366633, 0.00011923947462071814, 0.00011923484312509431, 0.00011923017004746211, 0.00011922547449838343, 0.00011922075883762829, 0.00011921602340486056, 0.00011921127309839079, 0.00011920653954271287, 0.00011920176397597746, 0.00011919699545279809, 0.00011919222217432454, 0.00011918741825459461, 0.00011918261522871696, 0.00011917780403302941, 0.00011917294526652317, 0.00011916807432574877, 0.00011916317878615944, 0.00011915825276620713, 0.00011915332910602486, 0.00011914838052524084, 0.0001191433840518513, 0.00011913834301098673, 0.00011913328563050236, 0.00011912823543659038, 0.00011912315721147069, 0.00011911809582652808, 0.00011911299128639706, 0.00011910785315529711, 0.00011910271486330375, 0.00011909751543181397, 0.00011909235025274382, 0.00011908716235194854, 0.0001190819218926388, 0.00011907666058869373, 0.00011907139356409403, 0.00011906609977755686, 0.00011906078298326183, 0.00011905537987859, 0.00011904999263085772, 0.00011904453762912212, 0.00011903906130007095, 0.00011903356689732655, 0.00011902805272256956, 0.00011902252440706939, 0.00011901696847153356, 0.00011901137762212742, 0.00011900577681520679, 0.00011900015948989591, 0.00011899451372220527, 0.00011898888520586375, 0.0001189832311074698, 0.00011897752241257816, 0.00011897174669664207, 0.00011896595251371773, 0.00011896020139659657, 0.00011895440762484428, 0.00011894859881849653, 0.00011894266020691969, 0.00011893675463212338, 0.00011893076069108999, 0.0001189247259725153, 0.00011891871585275552, 0.00011891262212205285, 0.00011890649317357007, 0.00011890029743644443, 0.00011889407881670025, 0.00011888778598933922, 0.00011888142919790855, 0.00011887512618063146, 0.00011886875678588355, 0.0001188623708012372, 0.00011885585224041967, 0.00011884934177790386, 0.00011884279280823154, 0.00011883619757276486, 0.00011882955231732421, 0.00011882295701034934, 0.00011881629843650962, 0.00011880963095990113, 0.00011880288721981552, 0.00011879611087199849, 0.00011878925146742699, 0.00011878241896780931, 0.00011877556594534312, 0.00011876862768512288, 0.00011876160727987748, 0.00011875442070749196, 0.00011874721287929909, 0.00011873995404789471, 0.00011873265949815296, 0.00011872528655756567, 0.00011871785052888397, 0.00011871034592385483, 0.00011870276546652064, 0.00011869519278570136, 0.00011868758145470918, 0.00011867992026463645, 0.0001186722497606229, 0.00011866454720306633, 0.00011865682928912749, 0.00011864904626698808, 0.00011864121140141603, 0.0001186333534029468, 0.00011862552549154555, 0.00011861761699042212, 0.00011860972148591095, 0.00011860176475251812, 0.00011859382609281854, 0.00011858589225992131, 0.00011857794337455156, 0.00011856997602219356, 0.00011856198848665095, 0.00011855403838564209, 0.00011854599673628204, 0.00011853793048810704, 0.00011852988531696896, 0.00011852184658156735, 0.00011851375512675787, 0.00011850571074220981, 0.00011849766262135919, 0.0001184895421163504, 0.00011848142904819261, 0.00011847332055655855, 0.00011846518632197862, 0.00011845708478451535, 0.00011844894572313308, 0.00011844080135226822, 0.00011843267251655611, 0.00011842447371008699, 0.00011841630139739965, 0.00011840809254403082, 0.00011839990265820751, 0.0001183916489334539, 0.00011838342136281819, 0.00011837516265036882, 0.00011836688938600421, 0.00011835862813501433, 0.00011835034711201188, 0.00011834206925324652, 0.00011833382761337581, 0.00011832553656135071, 0.0001183172228054775, 0.00011830886131789219, 0.00011830050244035556, 0.00011829213653713995, 0.00011828373584519335, 0.00011827534464595803, 0.00011826694111156115, 0.00011825851787665987, 0.0001182500417614573, 0.00011824152427877092, 0.00011823304619709332, 0.00011822463620908293, 0.00011821615583914348, 0.00011820763900003073, 0.00011819916283119704, 0.00011819062854408765, 0.00011818213791272395, 0.00011817361385128473, 0.00011816505879104819, 0.00011815649761686202, 0.00011814793088291463, 0.00011813936709867978, 0.00011813076954470554, 0.00011812214542544133, 0.00011811355466474447, 0.00011810499104140303, 0.00011809638900029032, 0.00011808776570337021, 0.00011807910153952357, 0.00011807041179362447, 0.00011806172270917605, 0.00011805308847150397, 0.00011804438295805054, 0.00011803568280771082, 0.00011802702082486375, 0.00011801829253605405, 0.00011800956818019441, 0.00011800080867806285, 0.00011799210034003618, 0.00011798337770037293, 0.000117974695444956, 0.00011796599609908334, 0.00011795724975445745, 0.00011794853333600611, 0.00011793986080570207, 0.00011793113677162924, 0.00011792249400660633], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.0001200987147677353, 0.00012009395844817271, 0.00012008915911276949, 0.00012008435169560229, 0.0001200795405575056, 0.00012007472422906783, 0.00012006990308949196, 0.00012006507806308532, 0.00012006020824332396, 0.00012005532451155246, 0.00012005045217957123, 0.00012004557686136635, 0.00012004069348509766, 0.00012003575574059846, 0.00012003083122080398, 0.00012002586738239816, 0.00012002087984380467, 0.00012001585433751054, 0.00012001081340239422, 0.00012000576211029591, 0.00012000071444432631, 0.00011999560646137909, 0.00011999052644465331, 0.00011998543834616354, 0.00011998031957963093, 0.00011997519166482589, 0.00011997005080971836, 0.00011996485928360962, 0.00011995966872920856, 0.00011995444257712562, 0.00011994918151466625, 0.00011994391443235921, 0.00011993861696641158, 0.00011993329990040875, 0.00011992795200046176, 0.00011992259843616992, 0.00011991724051104355, 0.00011991187059362222, 0.0001199065369611882, 0.00011990115690008655, 0.00011989573683306811, 0.00011989032321250073, 0.00011988484635983263, 0.00011987941385021566, 0.00011987393943866691, 0.00011986841663133493, 0.0001198628827797155, 0.00011985731560563219, 0.00011985172160293646, 0.00011984609458587931, 0.00011984040090019423, 0.00011983472382834072, 0.00011982898143876999, 0.00011982322850261575, 0.00011981745795722208, 0.0001198116626214321, 0.00011980584375135577, 0.00011979999828966886, 0.00011979409658743663, 0.00011978819393719689, 0.00011978226393694058, 0.00011977630480915363, 0.00011977037487999788, 0.00011976442532708674, 0.00011975842067123927, 0.00011975234849355715, 0.00011974626771270689, 0.00011974022499435803, 0.00011973410555850168, 0.00011972798318382206, 0.00011972174429901988, 0.00011971550968025147, 0.00011970918154054252, 0.00011970282140558022, 0.00011969645273855034, 0.00011969004252509151, 0.00011968359953427311, 0.00011967711731964412, 0.00011967064541459678, 0.00011966408849697627, 0.00011965752318948931, 0.00011965097153331047, 0.00011964434128730931, 0.00011963764553398943, 0.00011963083428965505, 0.0001196240047250756, 0.00011961709498276127, 0.00011961010385400248, 0.0001196030581437115, 0.00011959609917610737, 0.00011958907626539693, 0.00011958202998630144, 0.00011957492682675555, 0.00011956776197562115, 0.00011956050490705654, 0.00011955324471006716, 0.00011954595638095502, 0.00011953862811702665, 0.00011953121353701479, 0.00011952365568936848, 0.00011951607655895365, 0.0001195084121078632, 0.00011950069506605632, 0.00011949291604825869, 0.00011948509200010449, 0.00011947723192766503, 0.00011946932763067539, 0.00011946146544891922, 0.00011945360729619496, 0.00011944570809474567, 0.00011943777258460887, 0.00011942976824912707, 0.00011942172210651423, 0.00011941364676527018, 0.00011940553499683768, 0.00011939741308472487, 0.0001193893283582065, 0.00011938117933307899, 0.00011937303490578788, 0.00011936483992599649, 0.00011935667433443779, 0.00011934847546781561, 0.00011934026761882232, 0.00011933205533789392, 0.00011932380186603936, 0.00011931560994357216, 0.00011930732907430066, 0.00011929902900787714, 0.00011929073759202212, 0.00011928244089102524, 0.00011927408977439748, 0.00011926578892586777, 0.00011925748625242361, 0.00011924911866416542, 0.00011924076302080182, 0.00011923242219005551, 0.00011922405024096279, 0.00011921568812744796, 0.00011920728529701071, 0.00011919888602160161, 0.00011919049786158052, 0.0001191820582721522, 0.00011917366831091684, 0.00011916523886516884, 0.0001191568158421717, 0.00011914832916047048, 0.00011913985804979255, 0.00011913134956391869, 0.00011912281934497272, 0.00011911429621238285, 0.00011910575340863724, 0.00011909720065081279, 0.00011908871337660689, 0.00011908016751553705, 0.00011907162539909687, 0.0001190630504341966, 0.00011905446307409815, 0.00011904586964675167, 0.00011903724365534734, 0.0001190286378565029, 0.00011902001243390306, 0.000119011372838591, 0.00011900268084216402, 0.00011899394483448852, 0.00011898524847722701, 0.00011897660061054945, 0.0001189678806716012, 0.00011895914025569079, 0.00011895043586406566, 0.00011894167044445726, 0.00011893294538626847, 0.00011892418925713365, 0.00011891540288655937, 0.00011890660478439218, 0.000118897806303022, 0.00011888901282239142, 0.00011888018080525567, 0.00011887133629812103, 0.00011886251899880184, 0.00011885373066111197, 0.00011884490703384268, 0.00011883605065291402, 0.00011882717642574404, 0.00011881830051586074, 0.00011880942640719171, 0.00011880060195041584, 0.00011879170572947171, 0.00011878283749844921, 0.00011877399801575436, 0.00011876509598826426, 0.00011875620206623832, 0.00011874728394632079, 0.00011873841443548817, 0.00011872953840710393, 0.00011872068979983683, 0.00011871183100148903, 0.00011870292541897076, 0.00011869405090739853, 0.00011868521220680987, 0.00011867632008599821], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 01:53:30,496 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:92]: done!
[2017-12-15 01:53:30,497 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:152]: >> Executing classifier part ... 
[2017-12-15 01:53:30,497 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:97]: =======================================
[2017-12-15 01:53:30,497 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f5919ea2400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-15 01:53:30,630 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:110]: training ... 
[2017-12-15 07:52:45,969 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:122]: trained!
[2017-12-15 07:52:45,970 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:125]: Training history: 
{'val_loss': [0.00011929411640094995, 0.00011928960171394297, 0.00011928507297557803, 0.00011928053785510778, 0.00011927601337147973, 0.00011927149664648952, 0.00011926697362877922, 0.00011926240745588053, 0.00011925783686735153, 0.00011925328025867229, 0.00011924871522990451, 0.00011924412424366633, 0.00011923947462071814, 0.00011923484312509431, 0.00011923017004746211, 0.00011922547449838343, 0.00011922075883762829, 0.00011921602340486056, 0.00011921127309839079, 0.00011920653954271287, 0.00011920176397597746, 0.00011919699545279809, 0.00011919222217432454, 0.00011918741825459461, 0.00011918261522871696, 0.00011917780403302941, 0.00011917294526652317, 0.00011916807432574877, 0.00011916317878615944, 0.00011915825276620713, 0.00011915332910602486, 0.00011914838052524084, 0.0001191433840518513, 0.00011913834301098673, 0.00011913328563050236, 0.00011912823543659038, 0.00011912315721147069, 0.00011911809582652808, 0.00011911299128639706, 0.00011910785315529711, 0.00011910271486330375, 0.00011909751543181397, 0.00011909235025274382, 0.00011908716235194854, 0.0001190819218926388, 0.00011907666058869373, 0.00011907139356409403, 0.00011906609977755686, 0.00011906078298326183, 0.00011905537987859, 0.00011904999263085772, 0.00011904453762912212, 0.00011903906130007095, 0.00011903356689732655, 0.00011902805272256956, 0.00011902252440706939, 0.00011901696847153356, 0.00011901137762212742, 0.00011900577681520679, 0.00011900015948989591, 0.00011899451372220527, 0.00011898888520586375, 0.0001189832311074698, 0.00011897752241257816, 0.00011897174669664207, 0.00011896595251371773, 0.00011896020139659657, 0.00011895440762484428, 0.00011894859881849653, 0.00011894266020691969, 0.00011893675463212338, 0.00011893076069108999, 0.0001189247259725153, 0.00011891871585275552, 0.00011891262212205285, 0.00011890649317357007, 0.00011890029743644443, 0.00011889407881670025, 0.00011888778598933922, 0.00011888142919790855, 0.00011887512618063146, 0.00011886875678588355, 0.0001188623708012372, 0.00011885585224041967, 0.00011884934177790386, 0.00011884279280823154, 0.00011883619757276486, 0.00011882955231732421, 0.00011882295701034934, 0.00011881629843650962, 0.00011880963095990113, 0.00011880288721981552, 0.00011879611087199849, 0.00011878925146742699, 0.00011878241896780931, 0.00011877556594534312, 0.00011876862768512288, 0.00011876160727987748, 0.00011875442070749196, 0.00011874721287929909, 0.00011873995404789471, 0.00011873265949815296, 0.00011872528655756567, 0.00011871785052888397, 0.00011871034592385483, 0.00011870276546652064, 0.00011869519278570136, 0.00011868758145470918, 0.00011867992026463645, 0.0001186722497606229, 0.00011866454720306633, 0.00011865682928912749, 0.00011864904626698808, 0.00011864121140141603, 0.0001186333534029468, 0.00011862552549154555, 0.00011861761699042212, 0.00011860972148591095, 0.00011860176475251812, 0.00011859382609281854, 0.00011858589225992131, 0.00011857794337455156, 0.00011856997602219356, 0.00011856198848665095, 0.00011855403838564209, 0.00011854599673628204, 0.00011853793048810704, 0.00011852988531696896, 0.00011852184658156735, 0.00011851375512675787, 0.00011850571074220981, 0.00011849766262135919, 0.0001184895421163504, 0.00011848142904819261, 0.00011847332055655855, 0.00011846518632197862, 0.00011845708478451535, 0.00011844894572313308, 0.00011844080135226822, 0.00011843267251655611, 0.00011842447371008699, 0.00011841630139739965, 0.00011840809254403082, 0.00011839990265820751, 0.0001183916489334539, 0.00011838342136281819, 0.00011837516265036882, 0.00011836688938600421, 0.00011835862813501433, 0.00011835034711201188, 0.00011834206925324652, 0.00011833382761337581, 0.00011832553656135071, 0.0001183172228054775, 0.00011830886131789219, 0.00011830050244035556, 0.00011829213653713995, 0.00011828373584519335, 0.00011827534464595803, 0.00011826694111156115, 0.00011825851787665987, 0.0001182500417614573, 0.00011824152427877092, 0.00011823304619709332, 0.00011822463620908293, 0.00011821615583914348, 0.00011820763900003073, 0.00011819916283119704, 0.00011819062854408765, 0.00011818213791272395, 0.00011817361385128473, 0.00011816505879104819, 0.00011815649761686202, 0.00011814793088291463, 0.00011813936709867978, 0.00011813076954470554, 0.00011812214542544133, 0.00011811355466474447, 0.00011810499104140303, 0.00011809638900029032, 0.00011808776570337021, 0.00011807910153952357, 0.00011807041179362447, 0.00011806172270917605, 0.00011805308847150397, 0.00011804438295805054, 0.00011803568280771082, 0.00011802702082486375, 0.00011801829253605405, 0.00011800956818019441, 0.00011800080867806285, 0.00011799210034003618, 0.00011798337770037293, 0.000117974695444956, 0.00011796599609908334, 0.00011795724975445745, 0.00011794853333600611, 0.00011793986080570207, 0.00011793113677162924, 0.00011792249400660633], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.0001200987147677353, 0.00012009395844817271, 0.00012008915911276949, 0.00012008435169560229, 0.0001200795405575056, 0.00012007472422906783, 0.00012006990308949196, 0.00012006507806308532, 0.00012006020824332396, 0.00012005532451155246, 0.00012005045217957123, 0.00012004557686136635, 0.00012004069348509766, 0.00012003575574059846, 0.00012003083122080398, 0.00012002586738239816, 0.00012002087984380467, 0.00012001585433751054, 0.00012001081340239422, 0.00012000576211029591, 0.00012000071444432631, 0.00011999560646137909, 0.00011999052644465331, 0.00011998543834616354, 0.00011998031957963093, 0.00011997519166482589, 0.00011997005080971836, 0.00011996485928360962, 0.00011995966872920856, 0.00011995444257712562, 0.00011994918151466625, 0.00011994391443235921, 0.00011993861696641158, 0.00011993329990040875, 0.00011992795200046176, 0.00011992259843616992, 0.00011991724051104355, 0.00011991187059362222, 0.0001199065369611882, 0.00011990115690008655, 0.00011989573683306811, 0.00011989032321250073, 0.00011988484635983263, 0.00011987941385021566, 0.00011987393943866691, 0.00011986841663133493, 0.0001198628827797155, 0.00011985731560563219, 0.00011985172160293646, 0.00011984609458587931, 0.00011984040090019423, 0.00011983472382834072, 0.00011982898143876999, 0.00011982322850261575, 0.00011981745795722208, 0.0001198116626214321, 0.00011980584375135577, 0.00011979999828966886, 0.00011979409658743663, 0.00011978819393719689, 0.00011978226393694058, 0.00011977630480915363, 0.00011977037487999788, 0.00011976442532708674, 0.00011975842067123927, 0.00011975234849355715, 0.00011974626771270689, 0.00011974022499435803, 0.00011973410555850168, 0.00011972798318382206, 0.00011972174429901988, 0.00011971550968025147, 0.00011970918154054252, 0.00011970282140558022, 0.00011969645273855034, 0.00011969004252509151, 0.00011968359953427311, 0.00011967711731964412, 0.00011967064541459678, 0.00011966408849697627, 0.00011965752318948931, 0.00011965097153331047, 0.00011964434128730931, 0.00011963764553398943, 0.00011963083428965505, 0.0001196240047250756, 0.00011961709498276127, 0.00011961010385400248, 0.0001196030581437115, 0.00011959609917610737, 0.00011958907626539693, 0.00011958202998630144, 0.00011957492682675555, 0.00011956776197562115, 0.00011956050490705654, 0.00011955324471006716, 0.00011954595638095502, 0.00011953862811702665, 0.00011953121353701479, 0.00011952365568936848, 0.00011951607655895365, 0.0001195084121078632, 0.00011950069506605632, 0.00011949291604825869, 0.00011948509200010449, 0.00011947723192766503, 0.00011946932763067539, 0.00011946146544891922, 0.00011945360729619496, 0.00011944570809474567, 0.00011943777258460887, 0.00011942976824912707, 0.00011942172210651423, 0.00011941364676527018, 0.00011940553499683768, 0.00011939741308472487, 0.0001193893283582065, 0.00011938117933307899, 0.00011937303490578788, 0.00011936483992599649, 0.00011935667433443779, 0.00011934847546781561, 0.00011934026761882232, 0.00011933205533789392, 0.00011932380186603936, 0.00011931560994357216, 0.00011930732907430066, 0.00011929902900787714, 0.00011929073759202212, 0.00011928244089102524, 0.00011927408977439748, 0.00011926578892586777, 0.00011925748625242361, 0.00011924911866416542, 0.00011924076302080182, 0.00011923242219005551, 0.00011922405024096279, 0.00011921568812744796, 0.00011920728529701071, 0.00011919888602160161, 0.00011919049786158052, 0.0001191820582721522, 0.00011917366831091684, 0.00011916523886516884, 0.0001191568158421717, 0.00011914832916047048, 0.00011913985804979255, 0.00011913134956391869, 0.00011912281934497272, 0.00011911429621238285, 0.00011910575340863724, 0.00011909720065081279, 0.00011908871337660689, 0.00011908016751553705, 0.00011907162539909687, 0.0001190630504341966, 0.00011905446307409815, 0.00011904586964675167, 0.00011903724365534734, 0.0001190286378565029, 0.00011902001243390306, 0.000119011372838591, 0.00011900268084216402, 0.00011899394483448852, 0.00011898524847722701, 0.00011897660061054945, 0.0001189678806716012, 0.00011895914025569079, 0.00011895043586406566, 0.00011894167044445726, 0.00011893294538626847, 0.00011892418925713365, 0.00011891540288655937, 0.00011890660478439218, 0.000118897806303022, 0.00011888901282239142, 0.00011888018080525567, 0.00011887133629812103, 0.00011886251899880184, 0.00011885373066111197, 0.00011884490703384268, 0.00011883605065291402, 0.00011882717642574404, 0.00011881830051586074, 0.00011880942640719171, 0.00011880060195041584, 0.00011879170572947171, 0.00011878283749844921, 0.00011877399801575436, 0.00011876509598826426, 0.00011875620206623832, 0.00011874728394632079, 0.00011873841443548817, 0.00011872953840710393, 0.00011872068979983683, 0.00011871183100148903, 0.00011870292541897076, 0.00011869405090739853, 0.00011868521220680987, 0.00011867632008599821], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 07:52:45,970 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:129]: evaluating model ... 
[2017-12-15 07:52:48,637 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:133]: evaluated! 
[2017-12-15 07:52:48,638 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:135]: generating reports ... 
[2017-12-15 07:52:49,932 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:138]: done!
[2017-12-15 07:52:49,932 AE_BIGRAMA_1L_MINIDS_OVER_F1_5.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_5 finished!
