[2017-12-14 09:31:58,028 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_7
[2017-12-14 09:31:58,028 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:146]: >> Printing header log
[2017-12-14 09:31:58,028 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_7
	layers = 9216,6451
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f113b20deb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f113b1f0400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:58,028 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:148]: >> Loading dataset... 
[2017-12-14 09:32:19,951 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:19,952 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:19,952 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:57]: =======================================
[2017-12-14 09:32:19,952 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f113b20deb8>, 'discard_decoder_function': True}
[2017-12-14 09:32:19,999 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:54,965 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_7
[2017-12-14 10:18:54,966 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:146]: >> Printing header log
[2017-12-14 10:18:54,966 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_7
	layers = 9216,6451
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fa86e0dbeb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fa86e0be400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:54,966 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:148]: >> Loading dataset... 
[2017-12-14 10:19:19,563 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:19,563 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:19,563 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:57]: =======================================
[2017-12-14 10:19:19,564 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fa86e0dbeb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:19,609 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:73]: training and evaluate autoencoder
[2017-12-14 20:12:06,023 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:85]: trained and evaluated!
[2017-12-14 20:12:06,025 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:88]: Training history: 
{'val_loss': [0.00011945505591746676, 0.00011944899826264237, 0.00011944296872841094, 0.0001194368622692517, 0.0001194307854144802, 0.0001194247093820528, 0.00011941857669726747, 0.00011941238761040284, 0.00011940622092347652, 0.00011940005350359132, 0.000119393872354135, 0.00011938762187749531, 0.00011938135087800714, 0.00011937503287976571, 0.00011936875437191831, 0.00011936245771886951, 0.00011935615591723152, 0.00011934978723756543, 0.00011934342395676717, 0.00011933701735988707, 0.00011933059204574011, 0.00011932414924784241, 0.00011931765666237234, 0.00011931117158526147, 0.00011930464040324966, 0.00011929814520773091, 0.0001192916326714778, 0.00011928507966159314, 0.00011927851022270346, 0.00011927192501625944, 0.00011926531749926233, 0.0001192586650214952, 0.00011925198467341377, 0.00011924526826733111, 0.000119238585702496, 0.00011923184534117205, 0.00011922511740439487, 0.00011921835512581284, 0.00011921161034885858, 0.00011920481024244777, 0.0001191979523395481, 0.00011919107048140715, 0.00011918424511473072, 0.00011917734389574924, 0.00011917043203992554, 0.00011916351610813542, 0.00011915658465906959, 0.00011914963097095888, 0.00011914266151529382, 0.00011913569794117681, 0.00011912864167457766, 0.00011912160698557273, 0.00011911452308106087, 0.00011910746255972484, 0.00011910042247185209, 0.00011909331813387697, 0.00011908620775346038, 0.00011907908764793091, 0.00011907190688558526, 0.00011906474698575199, 0.00011905758683564008, 0.00011905039103869897, 0.00011904318919931642, 0.00011903594182709835, 0.0001190286628303864, 0.00011902130583612391, 0.00011901399626966374, 0.00011900666986302649, 0.00011899934377817607, 0.00011899198801742972, 0.00011898461198411352, 0.00011897723350164204, 0.00011896982200026709, 0.00011896237910679983, 0.00011895497840316051, 0.00011894752971753043, 0.00011894005797051136, 0.00011893257249392116, 0.00011892512911777367, 0.00011891758781116964, 0.000118910062683292, 0.00011890253723362753, 0.00011889493609255143, 0.00011888740091777409, 0.00011887983191962622, 0.00011887229079179265, 0.00011886470209314037, 0.00011885712042016707, 0.00011884948969258028, 0.00011884186665212315, 0.00011883425095913181, 0.00011882667223587106, 0.00011881903276640889, 0.00011881139780196227, 0.00011880381679043965, 0.00011879617413886336, 0.00011878848391646835, 0.00011878076090757147, 0.0001187731246381005, 0.00011876545068381711, 0.00011875774403241707, 0.00011875002340116727, 0.00011874235640105469, 0.00011873461419221069, 0.00011872687246604692, 0.00011871912307063054, 0.00011871138340032703, 0.00011870361578820103, 0.00011869583156829956, 0.00011868802062221471, 0.00011868023706376392, 0.00011867247132872773, 0.00011866473657461181, 0.00011865693617598394, 0.00011864920640956377, 0.0001186414338812502, 0.00011863368924112798, 0.00011862592563346023, 0.00011861813292196203, 0.000118610359428288, 0.00011860253761295935, 0.00011859481177948923, 0.00011858703002662006, 0.0001185792474514068, 0.00011857146112201392, 0.00011856366227868903, 0.00011855591134584671, 0.00011854806207137581, 0.00011854028521681716, 0.00011853251099018426, 0.00011852468894245401, 0.00011851689991362722, 0.00011850912486465021, 0.00011850132071184274, 0.00011849352113555897, 0.00011848573841732933, 0.00011847794227343835, 0.00011847016788591202, 0.00011846235835211379, 0.0001184545733456223, 0.00011844676584980727, 0.00011843897585562002, 0.00011843115691849573, 0.0001184233169758427, 0.00011841550131021777, 0.0001184076495865916, 0.00011839987414431957, 0.00011839203042960988, 0.00011838423544772688, 0.00011837642502007635, 0.0001183686383867737, 0.00011836075357273587, 0.00011835291289712396, 0.00011834505178804878, 0.000118337158393029, 0.00011832930202137094, 0.00011832150557357017, 0.00011831366572030236, 0.00011830580870507066, 0.00011829792061953346, 0.00011829003482225811, 0.00011828218556566425, 0.00011827432633367887, 0.00011826645492742534, 0.00011825859953900479, 0.00011825075257067279, 0.00011824292809166431, 0.00011823506968202303, 0.00011822713876308425, 0.00011821923008319036, 0.00011821134551943118, 0.00011820344125516758, 0.00011819555529699883, 0.00011818768430191735, 0.00011817978566892317, 0.00011817187527283289, 0.00011816399029790165, 0.0001181561382703657, 0.00011814825116806602, 0.00011814040886564295, 0.00011813250345724842, 0.00011812458978965878, 0.00011811672909175566, 0.00011810884388442282, 0.00011810097272844793, 0.00011809312184504291, 0.0001180852255896958, 0.00011807733146171715, 0.00011806947028113378, 0.0001180616062402231, 0.00011805374366523016, 0.0001180458761919267, 0.00011803804770845998, 0.00011803020549542213, 0.00011802236875276029, 0.00011801450217330911, 0.00011800662790672826, 0.00011799878585458383, 0.00011799091666508397, 0.00011798303855493829], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012031076430473538, 0.00012030448617242241, 0.00012029814404960272, 0.00012029183501224504, 0.00012028545823975099, 0.000120279105973251, 0.0001202727830949837, 0.0001202663989280311, 0.00012025997037062701, 0.00012025356873663609, 0.00012024716722114613, 0.00012024074105746097, 0.00012023423336513025, 0.0001202277155291192, 0.00012022116527125142, 0.00012021464935495557, 0.00012020811981105183, 0.00012020158313339157, 0.00012019494753114801, 0.00012018833766730824, 0.00012018171585857388, 0.00012017507525559072, 0.00012016838831874065, 0.00012016166298758657, 0.00012015494118776045, 0.00012014816371619349, 0.00012014139079506257, 0.00012013460941296466, 0.00012012778214730341, 0.00012012094492756334, 0.00012011408481344198, 0.00012010721159311686, 0.00012010030099759576, 0.00012009335802761831, 0.00012008637114119312, 0.00012007941174698563, 0.00012007236942582147, 0.00012006532791046371, 0.00012005824788230098, 0.00012005118072334016, 0.00012004409562333727, 0.00012003695542039804, 0.00012002979495379837, 0.00012002269899910953, 0.00012001552675351659, 0.00012000835351251575, 0.00012000118520115396, 0.00011999401847770473, 0.00011998683753414292, 0.00011997963857843847, 0.00011997242608992686, 0.00011996516072629656, 0.00011995791598182955, 0.00011995059752977887, 0.00011994329543085769, 0.00011993601075157445, 0.00011992863916370302, 0.00011992129061241401, 0.00011991391983034896, 0.00011990651034587746, 0.00011989911482081648, 0.00011989171557482605, 0.00011988427940394323, 0.00011987681747095641, 0.00011986933115047648, 0.0001198618591686101, 0.00011985429470861143, 0.00011984677207944399, 0.00011983922795420634, 0.00011983167069906473, 0.00011982408023996018, 0.00011981647278782108, 0.00011980887095262645, 0.0001198012415067132, 0.00011979359338505206, 0.00011978599053074936, 0.00011977833145960152, 0.00011977064653154893, 0.00011976295198122016, 0.00011975527366551993, 0.00011974750521800598, 0.00011973972987373744, 0.00011973197135660215, 0.0001197241433505166, 0.000119716377344122, 0.00011970860150214952, 0.00011970085751322926, 0.00011969305896647699, 0.00011968527217501781, 0.00011967743201073599, 0.00011966957369211041, 0.00011966172916699407, 0.0001196539348388752, 0.00011964609171206993, 0.00011963825026797798, 0.00011963045086801895, 0.00011962258702724966, 0.00011961469462775423, 0.00011960676994860319, 0.00011959893824528836, 0.00011959105790918845, 0.00011958314294711435, 0.00011957520321834414, 0.00011956732548926487, 0.00011955936729804846, 0.00011955141673829248, 0.00011954347584821308, 0.00011953554775623501, 0.00011952761795784343, 0.00011951966910450098, 0.00011951169804408266, 0.00011950374656001937, 0.000119495810599579, 0.00011948791149293045, 0.00011947994097761646, 0.00011947203952464934, 0.00011946410230809901, 0.00011945619348437352, 0.00011944825968065023, 0.00011944030191603721, 0.00011943235709172665, 0.00011942436289992517, 0.00011941647104553405, 0.00011940851721515218, 0.0001194005614650551, 0.00011939258834272046, 0.00011938462541146653, 0.00011937671018869037, 0.00011936869378981305, 0.00011936074240055051, 0.00011935279968555664, 0.00011934480248383134, 0.00011933683324832748, 0.00011932888053185443, 0.00011932089259690251, 0.00011931292644242305, 0.00011930497040792372, 0.00011929699882610127, 0.00011928906004533858, 0.00011928110225702536, 0.00011927316304965927, 0.00011926520234622298, 0.00011925725550739646, 0.00011924927897223481, 0.00011924127861838454, 0.00011923330198842213, 0.00011922528141831179, 0.00011921734100223613, 0.00011920933107351004, 0.00011920138053745426, 0.0001191934207109249, 0.00011918547896763874, 0.00011917744967585933, 0.00011916946832955958, 0.00011916146849711344, 0.0001191534313842721, 0.00011914543368484285, 0.00011913748575580771, 0.00011912949504793382, 0.00011912148161157996, 0.00011911344390623392, 0.00011910540615348752, 0.00011909739719646913, 0.00011908937861717454, 0.00011908135513194111, 0.00011907333553353846, 0.00011906533280966943, 0.00011905734648633026, 0.00011904933757671225, 0.00011904125806423517, 0.0001190331933406752, 0.00011902515620413367, 0.00011901709861433018, 0.00011900905517353874, 0.00011900103666534472, 0.00011899297779573109, 0.00011898490719452458, 0.00011897686339823032, 0.00011896885072028246, 0.0001189608028712561, 0.00011895280346541335, 0.00011894473094449162, 0.0001189366611490915, 0.00011892863965467383, 0.00011892059017033452, 0.00011891255936174311, 0.00011890454357907067, 0.0001188964884540867, 0.00011888842562654173, 0.00011888040429802538, 0.00011887237306283058, 0.0001188643475630812, 0.00011885631756029616, 0.00011884832242048718, 0.00011884031358196974, 0.00011883231232751234, 0.00011882427696848489, 0.00011881623900243679, 0.00011880823059052271, 0.00011880019662980634], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-14 20:12:06,026 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:92]: done!
[2017-12-14 20:12:06,026 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:152]: >> Executing classifier part ... 
[2017-12-14 20:12:06,026 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:97]: =======================================
[2017-12-14 20:12:06,026 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fa86e0be400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-14 20:12:06,097 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:110]: training ... 
[2017-12-15 00:44:27,292 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:122]: trained!
[2017-12-15 00:44:27,294 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:125]: Training history: 
{'val_loss': [0.00011945505591746676, 0.00011944899826264237, 0.00011944296872841094, 0.0001194368622692517, 0.0001194307854144802, 0.0001194247093820528, 0.00011941857669726747, 0.00011941238761040284, 0.00011940622092347652, 0.00011940005350359132, 0.000119393872354135, 0.00011938762187749531, 0.00011938135087800714, 0.00011937503287976571, 0.00011936875437191831, 0.00011936245771886951, 0.00011935615591723152, 0.00011934978723756543, 0.00011934342395676717, 0.00011933701735988707, 0.00011933059204574011, 0.00011932414924784241, 0.00011931765666237234, 0.00011931117158526147, 0.00011930464040324966, 0.00011929814520773091, 0.0001192916326714778, 0.00011928507966159314, 0.00011927851022270346, 0.00011927192501625944, 0.00011926531749926233, 0.0001192586650214952, 0.00011925198467341377, 0.00011924526826733111, 0.000119238585702496, 0.00011923184534117205, 0.00011922511740439487, 0.00011921835512581284, 0.00011921161034885858, 0.00011920481024244777, 0.0001191979523395481, 0.00011919107048140715, 0.00011918424511473072, 0.00011917734389574924, 0.00011917043203992554, 0.00011916351610813542, 0.00011915658465906959, 0.00011914963097095888, 0.00011914266151529382, 0.00011913569794117681, 0.00011912864167457766, 0.00011912160698557273, 0.00011911452308106087, 0.00011910746255972484, 0.00011910042247185209, 0.00011909331813387697, 0.00011908620775346038, 0.00011907908764793091, 0.00011907190688558526, 0.00011906474698575199, 0.00011905758683564008, 0.00011905039103869897, 0.00011904318919931642, 0.00011903594182709835, 0.0001190286628303864, 0.00011902130583612391, 0.00011901399626966374, 0.00011900666986302649, 0.00011899934377817607, 0.00011899198801742972, 0.00011898461198411352, 0.00011897723350164204, 0.00011896982200026709, 0.00011896237910679983, 0.00011895497840316051, 0.00011894752971753043, 0.00011894005797051136, 0.00011893257249392116, 0.00011892512911777367, 0.00011891758781116964, 0.000118910062683292, 0.00011890253723362753, 0.00011889493609255143, 0.00011888740091777409, 0.00011887983191962622, 0.00011887229079179265, 0.00011886470209314037, 0.00011885712042016707, 0.00011884948969258028, 0.00011884186665212315, 0.00011883425095913181, 0.00011882667223587106, 0.00011881903276640889, 0.00011881139780196227, 0.00011880381679043965, 0.00011879617413886336, 0.00011878848391646835, 0.00011878076090757147, 0.0001187731246381005, 0.00011876545068381711, 0.00011875774403241707, 0.00011875002340116727, 0.00011874235640105469, 0.00011873461419221069, 0.00011872687246604692, 0.00011871912307063054, 0.00011871138340032703, 0.00011870361578820103, 0.00011869583156829956, 0.00011868802062221471, 0.00011868023706376392, 0.00011867247132872773, 0.00011866473657461181, 0.00011865693617598394, 0.00011864920640956377, 0.0001186414338812502, 0.00011863368924112798, 0.00011862592563346023, 0.00011861813292196203, 0.000118610359428288, 0.00011860253761295935, 0.00011859481177948923, 0.00011858703002662006, 0.0001185792474514068, 0.00011857146112201392, 0.00011856366227868903, 0.00011855591134584671, 0.00011854806207137581, 0.00011854028521681716, 0.00011853251099018426, 0.00011852468894245401, 0.00011851689991362722, 0.00011850912486465021, 0.00011850132071184274, 0.00011849352113555897, 0.00011848573841732933, 0.00011847794227343835, 0.00011847016788591202, 0.00011846235835211379, 0.0001184545733456223, 0.00011844676584980727, 0.00011843897585562002, 0.00011843115691849573, 0.0001184233169758427, 0.00011841550131021777, 0.0001184076495865916, 0.00011839987414431957, 0.00011839203042960988, 0.00011838423544772688, 0.00011837642502007635, 0.0001183686383867737, 0.00011836075357273587, 0.00011835291289712396, 0.00011834505178804878, 0.000118337158393029, 0.00011832930202137094, 0.00011832150557357017, 0.00011831366572030236, 0.00011830580870507066, 0.00011829792061953346, 0.00011829003482225811, 0.00011828218556566425, 0.00011827432633367887, 0.00011826645492742534, 0.00011825859953900479, 0.00011825075257067279, 0.00011824292809166431, 0.00011823506968202303, 0.00011822713876308425, 0.00011821923008319036, 0.00011821134551943118, 0.00011820344125516758, 0.00011819555529699883, 0.00011818768430191735, 0.00011817978566892317, 0.00011817187527283289, 0.00011816399029790165, 0.0001181561382703657, 0.00011814825116806602, 0.00011814040886564295, 0.00011813250345724842, 0.00011812458978965878, 0.00011811672909175566, 0.00011810884388442282, 0.00011810097272844793, 0.00011809312184504291, 0.0001180852255896958, 0.00011807733146171715, 0.00011806947028113378, 0.0001180616062402231, 0.00011805374366523016, 0.0001180458761919267, 0.00011803804770845998, 0.00011803020549542213, 0.00011802236875276029, 0.00011801450217330911, 0.00011800662790672826, 0.00011799878585458383, 0.00011799091666508397, 0.00011798303855493829], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012031076430473538, 0.00012030448617242241, 0.00012029814404960272, 0.00012029183501224504, 0.00012028545823975099, 0.000120279105973251, 0.0001202727830949837, 0.0001202663989280311, 0.00012025997037062701, 0.00012025356873663609, 0.00012024716722114613, 0.00012024074105746097, 0.00012023423336513025, 0.0001202277155291192, 0.00012022116527125142, 0.00012021464935495557, 0.00012020811981105183, 0.00012020158313339157, 0.00012019494753114801, 0.00012018833766730824, 0.00012018171585857388, 0.00012017507525559072, 0.00012016838831874065, 0.00012016166298758657, 0.00012015494118776045, 0.00012014816371619349, 0.00012014139079506257, 0.00012013460941296466, 0.00012012778214730341, 0.00012012094492756334, 0.00012011408481344198, 0.00012010721159311686, 0.00012010030099759576, 0.00012009335802761831, 0.00012008637114119312, 0.00012007941174698563, 0.00012007236942582147, 0.00012006532791046371, 0.00012005824788230098, 0.00012005118072334016, 0.00012004409562333727, 0.00012003695542039804, 0.00012002979495379837, 0.00012002269899910953, 0.00012001552675351659, 0.00012000835351251575, 0.00012000118520115396, 0.00011999401847770473, 0.00011998683753414292, 0.00011997963857843847, 0.00011997242608992686, 0.00011996516072629656, 0.00011995791598182955, 0.00011995059752977887, 0.00011994329543085769, 0.00011993601075157445, 0.00011992863916370302, 0.00011992129061241401, 0.00011991391983034896, 0.00011990651034587746, 0.00011989911482081648, 0.00011989171557482605, 0.00011988427940394323, 0.00011987681747095641, 0.00011986933115047648, 0.0001198618591686101, 0.00011985429470861143, 0.00011984677207944399, 0.00011983922795420634, 0.00011983167069906473, 0.00011982408023996018, 0.00011981647278782108, 0.00011980887095262645, 0.0001198012415067132, 0.00011979359338505206, 0.00011978599053074936, 0.00011977833145960152, 0.00011977064653154893, 0.00011976295198122016, 0.00011975527366551993, 0.00011974750521800598, 0.00011973972987373744, 0.00011973197135660215, 0.0001197241433505166, 0.000119716377344122, 0.00011970860150214952, 0.00011970085751322926, 0.00011969305896647699, 0.00011968527217501781, 0.00011967743201073599, 0.00011966957369211041, 0.00011966172916699407, 0.0001196539348388752, 0.00011964609171206993, 0.00011963825026797798, 0.00011963045086801895, 0.00011962258702724966, 0.00011961469462775423, 0.00011960676994860319, 0.00011959893824528836, 0.00011959105790918845, 0.00011958314294711435, 0.00011957520321834414, 0.00011956732548926487, 0.00011955936729804846, 0.00011955141673829248, 0.00011954347584821308, 0.00011953554775623501, 0.00011952761795784343, 0.00011951966910450098, 0.00011951169804408266, 0.00011950374656001937, 0.000119495810599579, 0.00011948791149293045, 0.00011947994097761646, 0.00011947203952464934, 0.00011946410230809901, 0.00011945619348437352, 0.00011944825968065023, 0.00011944030191603721, 0.00011943235709172665, 0.00011942436289992517, 0.00011941647104553405, 0.00011940851721515218, 0.0001194005614650551, 0.00011939258834272046, 0.00011938462541146653, 0.00011937671018869037, 0.00011936869378981305, 0.00011936074240055051, 0.00011935279968555664, 0.00011934480248383134, 0.00011933683324832748, 0.00011932888053185443, 0.00011932089259690251, 0.00011931292644242305, 0.00011930497040792372, 0.00011929699882610127, 0.00011928906004533858, 0.00011928110225702536, 0.00011927316304965927, 0.00011926520234622298, 0.00011925725550739646, 0.00011924927897223481, 0.00011924127861838454, 0.00011923330198842213, 0.00011922528141831179, 0.00011921734100223613, 0.00011920933107351004, 0.00011920138053745426, 0.0001191934207109249, 0.00011918547896763874, 0.00011917744967585933, 0.00011916946832955958, 0.00011916146849711344, 0.0001191534313842721, 0.00011914543368484285, 0.00011913748575580771, 0.00011912949504793382, 0.00011912148161157996, 0.00011911344390623392, 0.00011910540615348752, 0.00011909739719646913, 0.00011908937861717454, 0.00011908135513194111, 0.00011907333553353846, 0.00011906533280966943, 0.00011905734648633026, 0.00011904933757671225, 0.00011904125806423517, 0.0001190331933406752, 0.00011902515620413367, 0.00011901709861433018, 0.00011900905517353874, 0.00011900103666534472, 0.00011899297779573109, 0.00011898490719452458, 0.00011897686339823032, 0.00011896885072028246, 0.0001189608028712561, 0.00011895280346541335, 0.00011894473094449162, 0.0001189366611490915, 0.00011892863965467383, 0.00011892059017033452, 0.00011891255936174311, 0.00011890454357907067, 0.0001188964884540867, 0.00011888842562654173, 0.00011888040429802538, 0.00011887237306283058, 0.0001188643475630812, 0.00011885631756029616, 0.00011884832242048718, 0.00011884031358196974, 0.00011883231232751234, 0.00011882427696848489, 0.00011881623900243679, 0.00011880823059052271, 0.00011880019662980634], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 00:44:27,295 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:129]: evaluating model ... 
[2017-12-15 00:44:34,862 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:133]: evaluated! 
[2017-12-15 00:44:34,862 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:135]: generating reports ... 
[2017-12-15 00:44:37,805 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:138]: done!
[2017-12-15 00:44:37,822 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_7 finished!
[2018-04-29 11:38:17,427 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_7 was already executed!
[2018-04-29 11:42:07,354 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_7 was already executed!
[2018-04-29 13:12:02,700 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_7
[2018-04-29 13:12:02,700 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:146]: >> Printing header log
[2018-04-29 13:12:02,700 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_7
	layers = 9216,6451
	using GLOBAL obj = 
		{'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'store_history': True, 'numpy_seed': 666, 'mlp_configs': {'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid', 'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'optimizer': <keras.optimizers.SGD object at 0x7fafc29b0898>}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'autoencoder_configs': {'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fafc29b0828>, 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'discard_decoder_function': True}, 'shuffle_batches': True, 'batch': 32}
	=======================================
	
[2018-04-29 13:12:02,700 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,777 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,777 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,777 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:57]: =======================================
[2018-04-29 13:12:20,778 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:62]: setting configurations for autoencoder: 
	 {'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fafc29b0828>, 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'discard_decoder_function': True}
[2018-04-29 13:12:20,825 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,908 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_7
[2018-04-29 13:14:12,908 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:146]: >> Printing header log
[2018-04-29 13:14:12,908 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_7
	layers = 9216,6451
	using GLOBAL obj = 
		{'autoencoder_configs': {'discard_decoder_function': True, 'loss_function': 'mse', 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f700105c828>, 'hidden_layer_activation': 'relu'}, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'mlp_configs': {'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7f700105c898>}, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'epochs': 200, 'store_history': True, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'shuffle_batches': True, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'batch': 32, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'numpy_seed': 666, 'data_dir': '/home/dhiego/malware_dataset/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/'}
	=======================================
	
[2018-04-29 13:14:12,908 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:148]: >> Loading dataset... 
[2018-04-29 13:14:31,086 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:31,087 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:31,087 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:57]: =======================================
[2018-04-29 13:14:31,087 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'loss_function': 'mse', 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f700105c828>, 'hidden_layer_activation': 'relu'}
[2018-04-29 13:14:31,134 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,665 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_7
[2018-04-29 13:16:33,665 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:146]: >> Printing header log
[2018-04-29 13:16:33,665 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_7
	layers = 9216,6451
	using GLOBAL obj = 
		{'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'batch': 32, 'shuffle_batches': True, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'epochs': 200, 'numpy_seed': 666, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'mlp_configs': {'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7f1ae7b3a898>, 'classifier_dim': 9, 'use_last_dim_as_classifier': False}, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'autoencoder_configs': {'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f1ae7b3a828>, 'discard_decoder_function': True, 'output_layer_activation': 'relu'}, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'store_history': True}
	=======================================
	
[2018-04-29 13:16:33,665 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:148]: >> Loading dataset... 
[2018-04-29 13:16:57,044 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:57,044 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:57,044 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:57]: =======================================
[2018-04-29 13:16:57,044 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:62]: setting configurations for autoencoder: 
	 {'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f1ae7b3a828>, 'discard_decoder_function': True, 'output_layer_activation': 'relu'}
[2018-04-29 13:16:57,094 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:21,104 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_7
[2018-04-29 14:30:21,104 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:146]: >> Printing header log
[2018-04-29 14:30:21,104 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_7
	layers = 9216,6451
	using GLOBAL obj = 
		{'epochs': 200, 'mlp_configs': {'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7fa7da19fb38>, 'activation': 'sigmoid'}, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'store_history': True, 'numpy_seed': 666, 'shuffle_batches': True, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'data_dir': '/home/dhiego/malware_dataset/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'autoencoder_configs': {'loss_function': 'mse', 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7fa7da19fac8>, 'hidden_layer_activation': 'relu'}, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'batch': 32, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/'}
	=======================================
	
[2018-04-29 14:30:21,104 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:148]: >> Loading dataset... 
[2018-04-29 14:30:39,859 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:39,859 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:39,859 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:57]: =======================================
[2018-04-29 14:30:39,860 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:62]: setting configurations for autoencoder: 
	 {'loss_function': 'mse', 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7fa7da19fac8>, 'hidden_layer_activation': 'relu'}
[2018-04-29 14:30:39,908 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:73]: training and evaluate autoencoder
[2018-04-29 19:03:56,928 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:85]: trained and evaluated!
[2018-04-29 19:03:56,946 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:88]: Training history: 
{'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011941346308881575, 0.00011940742117627326, 0.00011940139322314132, 0.00011939530556923664, 0.00011938923045273124, 0.0001193831285076134, 0.00011937698695948196, 0.00011937087423077873, 0.00011936472448238235, 0.00011935856129597958, 0.00011935237485969269, 0.00011934619214433528, 0.00011933993903942049, 0.00011933366586044674, 0.00011932740893980173, 0.00011932114799012481, 0.00011931493434602248, 0.00011930863768016276, 0.00011930237651718416, 0.00011929607996982538, 0.00011928976116799036, 0.00011928342278980034, 0.00011927710602618148, 0.00011927074586751898, 0.00011926432769079708, 0.00011925788631159145, 0.00011925141315043415, 0.00011924495710081235, 0.00011923848211474061, 0.00011923197387730556, 0.00011922541861869819, 0.00011921887596859066, 0.0001192123256633225, 0.00011920572677266963, 0.00011919913812049782, 0.00011919254714570764, 0.00011918591680490572, 0.00011917932175368326, 0.00011917263085890185, 0.00011916594885169082, 0.00011915924707852328, 0.00011915252748281462, 0.00011914579518380536, 0.00011913904131762532, 0.00011913235921561354, 0.0001191256234326767, 0.00011911883562782791, 0.00011911207045665836, 0.00011910525387978175, 0.0001190983895233268, 0.00011909152580677692, 0.00011908469470168527, 0.00011907785665243864, 0.00011907099099247337, 0.00011906410234332606, 0.00011905722239214763, 0.00011905033341119769, 0.00011904341039677827, 0.00011903648809336448, 0.00011902957278150604, 0.0001190226081021567, 0.00011901562441525683, 0.00011900858873014524, 0.00011900157774062918, 0.00011899451939813817, 0.00011898752032981651, 0.00011898051237392449, 0.00011897348981871685, 0.00011896642934320895, 0.0001189593472531299, 0.00011895229180206177, 0.00011894515854327754, 0.0001189380827811486, 0.00011893098282112803, 0.00011892385849731455, 0.00011891667340621989, 0.00011890948758041941, 0.00011890222477640938, 0.00011889500176501445, 0.00011888777882472008, 0.00011888048933429874, 0.00011887327492607194, 0.00011886596725760665, 0.00011885870215467841, 0.00011885141939511037, 0.00011884413632743989, 0.00011883679929444209, 0.0001188295146862594, 0.00011882219135197007, 0.00011881489453819073, 0.00011880755345246084, 0.00011880023689642518, 0.00011879290548037186, 0.00011878552254011054, 0.00011877814369998169, 0.00011877073239059572, 0.0001187633212234109, 0.00011875589543321027, 0.00011874851595317635, 0.00011874110040145681, 0.00011873367143543103], 'val_loss': [0.00011856635894165295, 0.00011856053762137318, 0.00011855466578056217, 0.00011854881953968064, 0.00011854293863520745, 0.00011853704178440945, 0.0001185311745022451, 0.00011852526556656419, 0.0001185193613683004, 0.00011851343133770552, 0.0001185075077786012, 0.00011850151651912467, 0.00011849549828318614, 0.00011848949656563786, 0.00011848349967489193, 0.00011847754062985182, 0.00011847151431348862, 0.00011846554030536123, 0.00011845953099006851, 0.00011845349730836246, 0.00011844741900555025, 0.00011844132527484757, 0.0001184351894437022, 0.00011842899238367517, 0.00011842279230242741, 0.00011841657137654433, 0.00011841033623840989, 0.00011840408279418062, 0.00011839778421041087, 0.0001183914437586, 0.00011838511596373751, 0.00011837877581583643, 0.00011837238228707678, 0.00011836600347112576, 0.0001183596060272931, 0.00011835317408076216, 0.00011834677736988836, 0.00011834026385039773, 0.00011833373583262302, 0.00011832718182162379, 0.00011832062069556037, 0.00011831404943321202, 0.0001183074570759497, 0.00011830093429614939, 0.00011829436426731719, 0.0001182877514050834, 0.0001182811644466889, 0.00011827455190624195, 0.00011826790495248195, 0.00011826125651492716, 0.00011825462452425445, 0.0001182479796978629, 0.00011824131860335973, 0.0001182346280117311, 0.0001182279454111419, 0.00011822125015360434, 0.00011821452840228498, 0.0001182077872901251, 0.00011820106359020777, 0.00011819427106366435, 0.0001181874540098142, 0.0001181805637315847, 0.00011817369249240891, 0.00011816676899862844, 0.00011815990289016478, 0.0001181529899438413, 0.00011814606180202894, 0.00011813909874634625, 0.000118132119833724, 0.00011812516700371148, 0.00011811813928675437, 0.0001181111734601296, 0.00011810418219446875, 0.00011809717156796737, 0.00011809011049244292, 0.00011808308842463227, 0.0001180759752732736, 0.00011806889008161445, 0.00011806179950896453, 0.00011805464631302342, 0.00011804755345211166, 0.00011804036657581636, 0.00011803321400557185, 0.00011802605784204113, 0.00011801891190418058, 0.00011801170668603637, 0.0001180045551884146, 0.00011799736428978402, 0.00011799019611287857, 0.00011798297553835208, 0.00011797577557605932, 0.0001179685636719, 0.00011796129438242388, 0.0001179540217499402, 0.00011794671763597899, 0.00011793941523821416, 0.00011793211620133394, 0.00011792486024813392, 0.00011791757780115214, 0.00011791029354858873, 0.00011790299762231447], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-04-29 19:03:56,947 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:92]: done!
[2018-04-29 19:03:56,947 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:152]: >> Executing classifier part ... 
[2018-04-29 19:03:56,947 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:97]: =======================================
[2018-04-29 19:03:56,948 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:101]: setting configurations for classifier: 
	 {'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7fa7da19fb38>, 'activation': 'sigmoid'}
[2018-04-29 19:03:57,190 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:110]: training ... 
[2018-04-29 23:17:29,379 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:122]: trained!
[2018-04-29 23:17:29,380 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:125]: Training history: 
{'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011941346308881575, 0.00011940742117627326, 0.00011940139322314132, 0.00011939530556923664, 0.00011938923045273124, 0.0001193831285076134, 0.00011937698695948196, 0.00011937087423077873, 0.00011936472448238235, 0.00011935856129597958, 0.00011935237485969269, 0.00011934619214433528, 0.00011933993903942049, 0.00011933366586044674, 0.00011932740893980173, 0.00011932114799012481, 0.00011931493434602248, 0.00011930863768016276, 0.00011930237651718416, 0.00011929607996982538, 0.00011928976116799036, 0.00011928342278980034, 0.00011927710602618148, 0.00011927074586751898, 0.00011926432769079708, 0.00011925788631159145, 0.00011925141315043415, 0.00011924495710081235, 0.00011923848211474061, 0.00011923197387730556, 0.00011922541861869819, 0.00011921887596859066, 0.0001192123256633225, 0.00011920572677266963, 0.00011919913812049782, 0.00011919254714570764, 0.00011918591680490572, 0.00011917932175368326, 0.00011917263085890185, 0.00011916594885169082, 0.00011915924707852328, 0.00011915252748281462, 0.00011914579518380536, 0.00011913904131762532, 0.00011913235921561354, 0.0001191256234326767, 0.00011911883562782791, 0.00011911207045665836, 0.00011910525387978175, 0.0001190983895233268, 0.00011909152580677692, 0.00011908469470168527, 0.00011907785665243864, 0.00011907099099247337, 0.00011906410234332606, 0.00011905722239214763, 0.00011905033341119769, 0.00011904341039677827, 0.00011903648809336448, 0.00011902957278150604, 0.0001190226081021567, 0.00011901562441525683, 0.00011900858873014524, 0.00011900157774062918, 0.00011899451939813817, 0.00011898752032981651, 0.00011898051237392449, 0.00011897348981871685, 0.00011896642934320895, 0.0001189593472531299, 0.00011895229180206177, 0.00011894515854327754, 0.0001189380827811486, 0.00011893098282112803, 0.00011892385849731455, 0.00011891667340621989, 0.00011890948758041941, 0.00011890222477640938, 0.00011889500176501445, 0.00011888777882472008, 0.00011888048933429874, 0.00011887327492607194, 0.00011886596725760665, 0.00011885870215467841, 0.00011885141939511037, 0.00011884413632743989, 0.00011883679929444209, 0.0001188295146862594, 0.00011882219135197007, 0.00011881489453819073, 0.00011880755345246084, 0.00011880023689642518, 0.00011879290548037186, 0.00011878552254011054, 0.00011877814369998169, 0.00011877073239059572, 0.0001187633212234109, 0.00011875589543321027, 0.00011874851595317635, 0.00011874110040145681, 0.00011873367143543103], 'val_loss': [0.00011856635894165295, 0.00011856053762137318, 0.00011855466578056217, 0.00011854881953968064, 0.00011854293863520745, 0.00011853704178440945, 0.0001185311745022451, 0.00011852526556656419, 0.0001185193613683004, 0.00011851343133770552, 0.0001185075077786012, 0.00011850151651912467, 0.00011849549828318614, 0.00011848949656563786, 0.00011848349967489193, 0.00011847754062985182, 0.00011847151431348862, 0.00011846554030536123, 0.00011845953099006851, 0.00011845349730836246, 0.00011844741900555025, 0.00011844132527484757, 0.0001184351894437022, 0.00011842899238367517, 0.00011842279230242741, 0.00011841657137654433, 0.00011841033623840989, 0.00011840408279418062, 0.00011839778421041087, 0.0001183914437586, 0.00011838511596373751, 0.00011837877581583643, 0.00011837238228707678, 0.00011836600347112576, 0.0001183596060272931, 0.00011835317408076216, 0.00011834677736988836, 0.00011834026385039773, 0.00011833373583262302, 0.00011832718182162379, 0.00011832062069556037, 0.00011831404943321202, 0.0001183074570759497, 0.00011830093429614939, 0.00011829436426731719, 0.0001182877514050834, 0.0001182811644466889, 0.00011827455190624195, 0.00011826790495248195, 0.00011826125651492716, 0.00011825462452425445, 0.0001182479796978629, 0.00011824131860335973, 0.0001182346280117311, 0.0001182279454111419, 0.00011822125015360434, 0.00011821452840228498, 0.0001182077872901251, 0.00011820106359020777, 0.00011819427106366435, 0.0001181874540098142, 0.0001181805637315847, 0.00011817369249240891, 0.00011816676899862844, 0.00011815990289016478, 0.0001181529899438413, 0.00011814606180202894, 0.00011813909874634625, 0.000118132119833724, 0.00011812516700371148, 0.00011811813928675437, 0.0001181111734601296, 0.00011810418219446875, 0.00011809717156796737, 0.00011809011049244292, 0.00011808308842463227, 0.0001180759752732736, 0.00011806889008161445, 0.00011806179950896453, 0.00011805464631302342, 0.00011804755345211166, 0.00011804036657581636, 0.00011803321400557185, 0.00011802605784204113, 0.00011801891190418058, 0.00011801170668603637, 0.0001180045551884146, 0.00011799736428978402, 0.00011799019611287857, 0.00011798297553835208, 0.00011797577557605932, 0.0001179685636719, 0.00011796129438242388, 0.0001179540217499402, 0.00011794671763597899, 0.00011793941523821416, 0.00011793211620133394, 0.00011792486024813392, 0.00011791757780115214, 0.00011791029354858873, 0.00011790299762231447], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-04-29 23:17:29,380 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:129]: evaluating model ... 
[2018-04-29 23:17:36,097 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:133]: evaluated! 
[2018-04-29 23:17:36,099 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:135]: generating reports ... 
[2018-04-29 23:17:39,654 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:138]: done!
[2018-04-29 23:17:39,655 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_7 finished!
