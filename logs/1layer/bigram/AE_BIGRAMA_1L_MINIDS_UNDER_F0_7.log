[2017-12-14 09:31:58,028 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_7
[2017-12-14 09:31:58,028 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:146]: >> Printing header log
[2017-12-14 09:31:58,028 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_7
	layers = 9216,6451
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f113b20deb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f113b1f0400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:58,028 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:148]: >> Loading dataset... 
[2017-12-14 09:32:19,951 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:19,952 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:19,952 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:57]: =======================================
[2017-12-14 09:32:19,952 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f113b20deb8>, 'discard_decoder_function': True}
[2017-12-14 09:32:19,999 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:54,965 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_7
[2017-12-14 10:18:54,966 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:146]: >> Printing header log
[2017-12-14 10:18:54,966 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_7
	layers = 9216,6451
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fa86e0dbeb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fa86e0be400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:54,966 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:148]: >> Loading dataset... 
[2017-12-14 10:19:19,563 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:19,563 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:19,563 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:57]: =======================================
[2017-12-14 10:19:19,564 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fa86e0dbeb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:19,609 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:73]: training and evaluate autoencoder
[2017-12-14 20:12:06,023 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:85]: trained and evaluated!
[2017-12-14 20:12:06,025 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:88]: Training history: 
{'val_loss': [0.00011945505591746676, 0.00011944899826264237, 0.00011944296872841094, 0.0001194368622692517, 0.0001194307854144802, 0.0001194247093820528, 0.00011941857669726747, 0.00011941238761040284, 0.00011940622092347652, 0.00011940005350359132, 0.000119393872354135, 0.00011938762187749531, 0.00011938135087800714, 0.00011937503287976571, 0.00011936875437191831, 0.00011936245771886951, 0.00011935615591723152, 0.00011934978723756543, 0.00011934342395676717, 0.00011933701735988707, 0.00011933059204574011, 0.00011932414924784241, 0.00011931765666237234, 0.00011931117158526147, 0.00011930464040324966, 0.00011929814520773091, 0.0001192916326714778, 0.00011928507966159314, 0.00011927851022270346, 0.00011927192501625944, 0.00011926531749926233, 0.0001192586650214952, 0.00011925198467341377, 0.00011924526826733111, 0.000119238585702496, 0.00011923184534117205, 0.00011922511740439487, 0.00011921835512581284, 0.00011921161034885858, 0.00011920481024244777, 0.0001191979523395481, 0.00011919107048140715, 0.00011918424511473072, 0.00011917734389574924, 0.00011917043203992554, 0.00011916351610813542, 0.00011915658465906959, 0.00011914963097095888, 0.00011914266151529382, 0.00011913569794117681, 0.00011912864167457766, 0.00011912160698557273, 0.00011911452308106087, 0.00011910746255972484, 0.00011910042247185209, 0.00011909331813387697, 0.00011908620775346038, 0.00011907908764793091, 0.00011907190688558526, 0.00011906474698575199, 0.00011905758683564008, 0.00011905039103869897, 0.00011904318919931642, 0.00011903594182709835, 0.0001190286628303864, 0.00011902130583612391, 0.00011901399626966374, 0.00011900666986302649, 0.00011899934377817607, 0.00011899198801742972, 0.00011898461198411352, 0.00011897723350164204, 0.00011896982200026709, 0.00011896237910679983, 0.00011895497840316051, 0.00011894752971753043, 0.00011894005797051136, 0.00011893257249392116, 0.00011892512911777367, 0.00011891758781116964, 0.000118910062683292, 0.00011890253723362753, 0.00011889493609255143, 0.00011888740091777409, 0.00011887983191962622, 0.00011887229079179265, 0.00011886470209314037, 0.00011885712042016707, 0.00011884948969258028, 0.00011884186665212315, 0.00011883425095913181, 0.00011882667223587106, 0.00011881903276640889, 0.00011881139780196227, 0.00011880381679043965, 0.00011879617413886336, 0.00011878848391646835, 0.00011878076090757147, 0.0001187731246381005, 0.00011876545068381711, 0.00011875774403241707, 0.00011875002340116727, 0.00011874235640105469, 0.00011873461419221069, 0.00011872687246604692, 0.00011871912307063054, 0.00011871138340032703, 0.00011870361578820103, 0.00011869583156829956, 0.00011868802062221471, 0.00011868023706376392, 0.00011867247132872773, 0.00011866473657461181, 0.00011865693617598394, 0.00011864920640956377, 0.0001186414338812502, 0.00011863368924112798, 0.00011862592563346023, 0.00011861813292196203, 0.000118610359428288, 0.00011860253761295935, 0.00011859481177948923, 0.00011858703002662006, 0.0001185792474514068, 0.00011857146112201392, 0.00011856366227868903, 0.00011855591134584671, 0.00011854806207137581, 0.00011854028521681716, 0.00011853251099018426, 0.00011852468894245401, 0.00011851689991362722, 0.00011850912486465021, 0.00011850132071184274, 0.00011849352113555897, 0.00011848573841732933, 0.00011847794227343835, 0.00011847016788591202, 0.00011846235835211379, 0.0001184545733456223, 0.00011844676584980727, 0.00011843897585562002, 0.00011843115691849573, 0.0001184233169758427, 0.00011841550131021777, 0.0001184076495865916, 0.00011839987414431957, 0.00011839203042960988, 0.00011838423544772688, 0.00011837642502007635, 0.0001183686383867737, 0.00011836075357273587, 0.00011835291289712396, 0.00011834505178804878, 0.000118337158393029, 0.00011832930202137094, 0.00011832150557357017, 0.00011831366572030236, 0.00011830580870507066, 0.00011829792061953346, 0.00011829003482225811, 0.00011828218556566425, 0.00011827432633367887, 0.00011826645492742534, 0.00011825859953900479, 0.00011825075257067279, 0.00011824292809166431, 0.00011823506968202303, 0.00011822713876308425, 0.00011821923008319036, 0.00011821134551943118, 0.00011820344125516758, 0.00011819555529699883, 0.00011818768430191735, 0.00011817978566892317, 0.00011817187527283289, 0.00011816399029790165, 0.0001181561382703657, 0.00011814825116806602, 0.00011814040886564295, 0.00011813250345724842, 0.00011812458978965878, 0.00011811672909175566, 0.00011810884388442282, 0.00011810097272844793, 0.00011809312184504291, 0.0001180852255896958, 0.00011807733146171715, 0.00011806947028113378, 0.0001180616062402231, 0.00011805374366523016, 0.0001180458761919267, 0.00011803804770845998, 0.00011803020549542213, 0.00011802236875276029, 0.00011801450217330911, 0.00011800662790672826, 0.00011799878585458383, 0.00011799091666508397, 0.00011798303855493829], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012031076430473538, 0.00012030448617242241, 0.00012029814404960272, 0.00012029183501224504, 0.00012028545823975099, 0.000120279105973251, 0.0001202727830949837, 0.0001202663989280311, 0.00012025997037062701, 0.00012025356873663609, 0.00012024716722114613, 0.00012024074105746097, 0.00012023423336513025, 0.0001202277155291192, 0.00012022116527125142, 0.00012021464935495557, 0.00012020811981105183, 0.00012020158313339157, 0.00012019494753114801, 0.00012018833766730824, 0.00012018171585857388, 0.00012017507525559072, 0.00012016838831874065, 0.00012016166298758657, 0.00012015494118776045, 0.00012014816371619349, 0.00012014139079506257, 0.00012013460941296466, 0.00012012778214730341, 0.00012012094492756334, 0.00012011408481344198, 0.00012010721159311686, 0.00012010030099759576, 0.00012009335802761831, 0.00012008637114119312, 0.00012007941174698563, 0.00012007236942582147, 0.00012006532791046371, 0.00012005824788230098, 0.00012005118072334016, 0.00012004409562333727, 0.00012003695542039804, 0.00012002979495379837, 0.00012002269899910953, 0.00012001552675351659, 0.00012000835351251575, 0.00012000118520115396, 0.00011999401847770473, 0.00011998683753414292, 0.00011997963857843847, 0.00011997242608992686, 0.00011996516072629656, 0.00011995791598182955, 0.00011995059752977887, 0.00011994329543085769, 0.00011993601075157445, 0.00011992863916370302, 0.00011992129061241401, 0.00011991391983034896, 0.00011990651034587746, 0.00011989911482081648, 0.00011989171557482605, 0.00011988427940394323, 0.00011987681747095641, 0.00011986933115047648, 0.0001198618591686101, 0.00011985429470861143, 0.00011984677207944399, 0.00011983922795420634, 0.00011983167069906473, 0.00011982408023996018, 0.00011981647278782108, 0.00011980887095262645, 0.0001198012415067132, 0.00011979359338505206, 0.00011978599053074936, 0.00011977833145960152, 0.00011977064653154893, 0.00011976295198122016, 0.00011975527366551993, 0.00011974750521800598, 0.00011973972987373744, 0.00011973197135660215, 0.0001197241433505166, 0.000119716377344122, 0.00011970860150214952, 0.00011970085751322926, 0.00011969305896647699, 0.00011968527217501781, 0.00011967743201073599, 0.00011966957369211041, 0.00011966172916699407, 0.0001196539348388752, 0.00011964609171206993, 0.00011963825026797798, 0.00011963045086801895, 0.00011962258702724966, 0.00011961469462775423, 0.00011960676994860319, 0.00011959893824528836, 0.00011959105790918845, 0.00011958314294711435, 0.00011957520321834414, 0.00011956732548926487, 0.00011955936729804846, 0.00011955141673829248, 0.00011954347584821308, 0.00011953554775623501, 0.00011952761795784343, 0.00011951966910450098, 0.00011951169804408266, 0.00011950374656001937, 0.000119495810599579, 0.00011948791149293045, 0.00011947994097761646, 0.00011947203952464934, 0.00011946410230809901, 0.00011945619348437352, 0.00011944825968065023, 0.00011944030191603721, 0.00011943235709172665, 0.00011942436289992517, 0.00011941647104553405, 0.00011940851721515218, 0.0001194005614650551, 0.00011939258834272046, 0.00011938462541146653, 0.00011937671018869037, 0.00011936869378981305, 0.00011936074240055051, 0.00011935279968555664, 0.00011934480248383134, 0.00011933683324832748, 0.00011932888053185443, 0.00011932089259690251, 0.00011931292644242305, 0.00011930497040792372, 0.00011929699882610127, 0.00011928906004533858, 0.00011928110225702536, 0.00011927316304965927, 0.00011926520234622298, 0.00011925725550739646, 0.00011924927897223481, 0.00011924127861838454, 0.00011923330198842213, 0.00011922528141831179, 0.00011921734100223613, 0.00011920933107351004, 0.00011920138053745426, 0.0001191934207109249, 0.00011918547896763874, 0.00011917744967585933, 0.00011916946832955958, 0.00011916146849711344, 0.0001191534313842721, 0.00011914543368484285, 0.00011913748575580771, 0.00011912949504793382, 0.00011912148161157996, 0.00011911344390623392, 0.00011910540615348752, 0.00011909739719646913, 0.00011908937861717454, 0.00011908135513194111, 0.00011907333553353846, 0.00011906533280966943, 0.00011905734648633026, 0.00011904933757671225, 0.00011904125806423517, 0.0001190331933406752, 0.00011902515620413367, 0.00011901709861433018, 0.00011900905517353874, 0.00011900103666534472, 0.00011899297779573109, 0.00011898490719452458, 0.00011897686339823032, 0.00011896885072028246, 0.0001189608028712561, 0.00011895280346541335, 0.00011894473094449162, 0.0001189366611490915, 0.00011892863965467383, 0.00011892059017033452, 0.00011891255936174311, 0.00011890454357907067, 0.0001188964884540867, 0.00011888842562654173, 0.00011888040429802538, 0.00011887237306283058, 0.0001188643475630812, 0.00011885631756029616, 0.00011884832242048718, 0.00011884031358196974, 0.00011883231232751234, 0.00011882427696848489, 0.00011881623900243679, 0.00011880823059052271, 0.00011880019662980634], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-14 20:12:06,026 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:92]: done!
[2017-12-14 20:12:06,026 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:152]: >> Executing classifier part ... 
[2017-12-14 20:12:06,026 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:97]: =======================================
[2017-12-14 20:12:06,026 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fa86e0be400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-14 20:12:06,097 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:110]: training ... 
[2017-12-15 00:44:27,292 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:122]: trained!
[2017-12-15 00:44:27,294 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:125]: Training history: 
{'val_loss': [0.00011945505591746676, 0.00011944899826264237, 0.00011944296872841094, 0.0001194368622692517, 0.0001194307854144802, 0.0001194247093820528, 0.00011941857669726747, 0.00011941238761040284, 0.00011940622092347652, 0.00011940005350359132, 0.000119393872354135, 0.00011938762187749531, 0.00011938135087800714, 0.00011937503287976571, 0.00011936875437191831, 0.00011936245771886951, 0.00011935615591723152, 0.00011934978723756543, 0.00011934342395676717, 0.00011933701735988707, 0.00011933059204574011, 0.00011932414924784241, 0.00011931765666237234, 0.00011931117158526147, 0.00011930464040324966, 0.00011929814520773091, 0.0001192916326714778, 0.00011928507966159314, 0.00011927851022270346, 0.00011927192501625944, 0.00011926531749926233, 0.0001192586650214952, 0.00011925198467341377, 0.00011924526826733111, 0.000119238585702496, 0.00011923184534117205, 0.00011922511740439487, 0.00011921835512581284, 0.00011921161034885858, 0.00011920481024244777, 0.0001191979523395481, 0.00011919107048140715, 0.00011918424511473072, 0.00011917734389574924, 0.00011917043203992554, 0.00011916351610813542, 0.00011915658465906959, 0.00011914963097095888, 0.00011914266151529382, 0.00011913569794117681, 0.00011912864167457766, 0.00011912160698557273, 0.00011911452308106087, 0.00011910746255972484, 0.00011910042247185209, 0.00011909331813387697, 0.00011908620775346038, 0.00011907908764793091, 0.00011907190688558526, 0.00011906474698575199, 0.00011905758683564008, 0.00011905039103869897, 0.00011904318919931642, 0.00011903594182709835, 0.0001190286628303864, 0.00011902130583612391, 0.00011901399626966374, 0.00011900666986302649, 0.00011899934377817607, 0.00011899198801742972, 0.00011898461198411352, 0.00011897723350164204, 0.00011896982200026709, 0.00011896237910679983, 0.00011895497840316051, 0.00011894752971753043, 0.00011894005797051136, 0.00011893257249392116, 0.00011892512911777367, 0.00011891758781116964, 0.000118910062683292, 0.00011890253723362753, 0.00011889493609255143, 0.00011888740091777409, 0.00011887983191962622, 0.00011887229079179265, 0.00011886470209314037, 0.00011885712042016707, 0.00011884948969258028, 0.00011884186665212315, 0.00011883425095913181, 0.00011882667223587106, 0.00011881903276640889, 0.00011881139780196227, 0.00011880381679043965, 0.00011879617413886336, 0.00011878848391646835, 0.00011878076090757147, 0.0001187731246381005, 0.00011876545068381711, 0.00011875774403241707, 0.00011875002340116727, 0.00011874235640105469, 0.00011873461419221069, 0.00011872687246604692, 0.00011871912307063054, 0.00011871138340032703, 0.00011870361578820103, 0.00011869583156829956, 0.00011868802062221471, 0.00011868023706376392, 0.00011867247132872773, 0.00011866473657461181, 0.00011865693617598394, 0.00011864920640956377, 0.0001186414338812502, 0.00011863368924112798, 0.00011862592563346023, 0.00011861813292196203, 0.000118610359428288, 0.00011860253761295935, 0.00011859481177948923, 0.00011858703002662006, 0.0001185792474514068, 0.00011857146112201392, 0.00011856366227868903, 0.00011855591134584671, 0.00011854806207137581, 0.00011854028521681716, 0.00011853251099018426, 0.00011852468894245401, 0.00011851689991362722, 0.00011850912486465021, 0.00011850132071184274, 0.00011849352113555897, 0.00011848573841732933, 0.00011847794227343835, 0.00011847016788591202, 0.00011846235835211379, 0.0001184545733456223, 0.00011844676584980727, 0.00011843897585562002, 0.00011843115691849573, 0.0001184233169758427, 0.00011841550131021777, 0.0001184076495865916, 0.00011839987414431957, 0.00011839203042960988, 0.00011838423544772688, 0.00011837642502007635, 0.0001183686383867737, 0.00011836075357273587, 0.00011835291289712396, 0.00011834505178804878, 0.000118337158393029, 0.00011832930202137094, 0.00011832150557357017, 0.00011831366572030236, 0.00011830580870507066, 0.00011829792061953346, 0.00011829003482225811, 0.00011828218556566425, 0.00011827432633367887, 0.00011826645492742534, 0.00011825859953900479, 0.00011825075257067279, 0.00011824292809166431, 0.00011823506968202303, 0.00011822713876308425, 0.00011821923008319036, 0.00011821134551943118, 0.00011820344125516758, 0.00011819555529699883, 0.00011818768430191735, 0.00011817978566892317, 0.00011817187527283289, 0.00011816399029790165, 0.0001181561382703657, 0.00011814825116806602, 0.00011814040886564295, 0.00011813250345724842, 0.00011812458978965878, 0.00011811672909175566, 0.00011810884388442282, 0.00011810097272844793, 0.00011809312184504291, 0.0001180852255896958, 0.00011807733146171715, 0.00011806947028113378, 0.0001180616062402231, 0.00011805374366523016, 0.0001180458761919267, 0.00011803804770845998, 0.00011803020549542213, 0.00011802236875276029, 0.00011801450217330911, 0.00011800662790672826, 0.00011799878585458383, 0.00011799091666508397, 0.00011798303855493829], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012031076430473538, 0.00012030448617242241, 0.00012029814404960272, 0.00012029183501224504, 0.00012028545823975099, 0.000120279105973251, 0.0001202727830949837, 0.0001202663989280311, 0.00012025997037062701, 0.00012025356873663609, 0.00012024716722114613, 0.00012024074105746097, 0.00012023423336513025, 0.0001202277155291192, 0.00012022116527125142, 0.00012021464935495557, 0.00012020811981105183, 0.00012020158313339157, 0.00012019494753114801, 0.00012018833766730824, 0.00012018171585857388, 0.00012017507525559072, 0.00012016838831874065, 0.00012016166298758657, 0.00012015494118776045, 0.00012014816371619349, 0.00012014139079506257, 0.00012013460941296466, 0.00012012778214730341, 0.00012012094492756334, 0.00012011408481344198, 0.00012010721159311686, 0.00012010030099759576, 0.00012009335802761831, 0.00012008637114119312, 0.00012007941174698563, 0.00012007236942582147, 0.00012006532791046371, 0.00012005824788230098, 0.00012005118072334016, 0.00012004409562333727, 0.00012003695542039804, 0.00012002979495379837, 0.00012002269899910953, 0.00012001552675351659, 0.00012000835351251575, 0.00012000118520115396, 0.00011999401847770473, 0.00011998683753414292, 0.00011997963857843847, 0.00011997242608992686, 0.00011996516072629656, 0.00011995791598182955, 0.00011995059752977887, 0.00011994329543085769, 0.00011993601075157445, 0.00011992863916370302, 0.00011992129061241401, 0.00011991391983034896, 0.00011990651034587746, 0.00011989911482081648, 0.00011989171557482605, 0.00011988427940394323, 0.00011987681747095641, 0.00011986933115047648, 0.0001198618591686101, 0.00011985429470861143, 0.00011984677207944399, 0.00011983922795420634, 0.00011983167069906473, 0.00011982408023996018, 0.00011981647278782108, 0.00011980887095262645, 0.0001198012415067132, 0.00011979359338505206, 0.00011978599053074936, 0.00011977833145960152, 0.00011977064653154893, 0.00011976295198122016, 0.00011975527366551993, 0.00011974750521800598, 0.00011973972987373744, 0.00011973197135660215, 0.0001197241433505166, 0.000119716377344122, 0.00011970860150214952, 0.00011970085751322926, 0.00011969305896647699, 0.00011968527217501781, 0.00011967743201073599, 0.00011966957369211041, 0.00011966172916699407, 0.0001196539348388752, 0.00011964609171206993, 0.00011963825026797798, 0.00011963045086801895, 0.00011962258702724966, 0.00011961469462775423, 0.00011960676994860319, 0.00011959893824528836, 0.00011959105790918845, 0.00011958314294711435, 0.00011957520321834414, 0.00011956732548926487, 0.00011955936729804846, 0.00011955141673829248, 0.00011954347584821308, 0.00011953554775623501, 0.00011952761795784343, 0.00011951966910450098, 0.00011951169804408266, 0.00011950374656001937, 0.000119495810599579, 0.00011948791149293045, 0.00011947994097761646, 0.00011947203952464934, 0.00011946410230809901, 0.00011945619348437352, 0.00011944825968065023, 0.00011944030191603721, 0.00011943235709172665, 0.00011942436289992517, 0.00011941647104553405, 0.00011940851721515218, 0.0001194005614650551, 0.00011939258834272046, 0.00011938462541146653, 0.00011937671018869037, 0.00011936869378981305, 0.00011936074240055051, 0.00011935279968555664, 0.00011934480248383134, 0.00011933683324832748, 0.00011932888053185443, 0.00011932089259690251, 0.00011931292644242305, 0.00011930497040792372, 0.00011929699882610127, 0.00011928906004533858, 0.00011928110225702536, 0.00011927316304965927, 0.00011926520234622298, 0.00011925725550739646, 0.00011924927897223481, 0.00011924127861838454, 0.00011923330198842213, 0.00011922528141831179, 0.00011921734100223613, 0.00011920933107351004, 0.00011920138053745426, 0.0001191934207109249, 0.00011918547896763874, 0.00011917744967585933, 0.00011916946832955958, 0.00011916146849711344, 0.0001191534313842721, 0.00011914543368484285, 0.00011913748575580771, 0.00011912949504793382, 0.00011912148161157996, 0.00011911344390623392, 0.00011910540615348752, 0.00011909739719646913, 0.00011908937861717454, 0.00011908135513194111, 0.00011907333553353846, 0.00011906533280966943, 0.00011905734648633026, 0.00011904933757671225, 0.00011904125806423517, 0.0001190331933406752, 0.00011902515620413367, 0.00011901709861433018, 0.00011900905517353874, 0.00011900103666534472, 0.00011899297779573109, 0.00011898490719452458, 0.00011897686339823032, 0.00011896885072028246, 0.0001189608028712561, 0.00011895280346541335, 0.00011894473094449162, 0.0001189366611490915, 0.00011892863965467383, 0.00011892059017033452, 0.00011891255936174311, 0.00011890454357907067, 0.0001188964884540867, 0.00011888842562654173, 0.00011888040429802538, 0.00011887237306283058, 0.0001188643475630812, 0.00011885631756029616, 0.00011884832242048718, 0.00011884031358196974, 0.00011883231232751234, 0.00011882427696848489, 0.00011881623900243679, 0.00011880823059052271, 0.00011880019662980634], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 00:44:27,295 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:129]: evaluating model ... 
[2017-12-15 00:44:34,862 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:133]: evaluated! 
[2017-12-15 00:44:34,862 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:135]: generating reports ... 
[2017-12-15 00:44:37,805 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:138]: done!
[2017-12-15 00:44:37,822 AE_BIGRAMA_1L_MINIDS_UNDER_F0_7.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_7 finished!
