[2017-12-14 09:31:57,965 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_7
[2017-12-14 09:31:57,965 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:146]: >> Printing header log
[2017-12-14 09:31:57,965 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_7
	layers = 9216,15667
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f59f30eeeb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f59f30d1400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,965 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:148]: >> Loading dataset... 
[2017-12-14 09:32:19,797 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:19,798 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:19,798 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:57]: =======================================
[2017-12-14 09:32:19,798 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f59f30eeeb8>, 'discard_decoder_function': True}
[2017-12-14 09:32:19,846 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:55,075 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_7
[2017-12-14 10:18:55,075 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:146]: >> Printing header log
[2017-12-14 10:18:55,075 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_7
	layers = 9216,15667
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fa14905ee80>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fa1490413c8>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:55,075 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:148]: >> Loading dataset... 
[2017-12-14 10:19:18,103 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:18,103 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:18,103 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:57]: =======================================
[2017-12-14 10:19:18,104 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fa14905ee80>, 'discard_decoder_function': True}
[2017-12-14 10:19:18,150 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:73]: training and evaluate autoencoder
[2017-12-15 03:26:39,036 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:85]: trained and evaluated!
[2017-12-15 03:26:39,039 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:88]: Training history: 
{'val_loss': [0.00011902249743060738, 0.00011901771408735708, 0.00011901290046039131, 0.00011900809296525223, 0.00011900327901649964, 0.00011899847422079446, 0.00011899365928880435, 0.0001189888275702683, 0.00011898395457363365, 0.0001189790727457384, 0.00011897420734684818, 0.00011896931627652029, 0.00011896445520387514, 0.00011895957385866013, 0.00011895467592354667, 0.00011894973644217893, 0.00011894477105697191, 0.00011893984171188911, 0.00011893487019485541, 0.00011892989204543773, 0.00011892491471836417, 0.0001189199141690082, 0.00011891490986547262, 0.00011890990009156104, 0.00011890486391325292, 0.00011889978481215798, 0.00011889471799259346, 0.00011888964323562064, 0.0001188845662797712, 0.00011887940503365111, 0.00011887426643774796, 0.00011886910756927496, 0.00011886394698460556, 0.00011885874633747668, 0.00011885354054175862, 0.00011884829060761464, 0.00011884302301094947, 0.00011883775754165275, 0.00011883246754504928, 0.00011882717656520829, 0.000118821862792134, 0.00011881654387047052, 0.00011881122346501225, 0.00011880589006294174, 0.00011880054890223337, 0.00011879515296625688, 0.00011878974435545495, 0.00011878434693568365, 0.00011877889506243105, 0.00011877340959820951, 0.00011876792175634089, 0.00011876242704968671, 0.00011875684510304933, 0.0001187512711832055, 0.00011874567550699701, 0.00011874003781973104, 0.00011873437528337148, 0.00011872871340846263, 0.00011872307572119666, 0.00011871735335036504, 0.00011871164823088256, 0.00011870588163223978, 0.00011870007359460499, 0.00011869421169343142, 0.00011868837578548233, 0.00011868254667081063, 0.00011867672279411333, 0.00011867084936224524, 0.00011866496021645396, 0.00011865910500129548, 0.00011865320598737495, 0.00011864724499373685, 0.00011864123194214158, 0.00011863522014193951, 0.0001186290841856548, 0.00011862296001034324, 0.00011861681057476606, 0.00011861061371138654, 0.0001186044399809042, 0.00011859822920918309, 0.00011859194160191942, 0.00011858569479007411, 0.00011857936663767071, 0.00011857302100151658, 0.00011856665975870153, 0.00011856023827024234, 0.0001185537705159888, 0.0001185472279641759, 0.00011854063825271634, 0.00011853407249649808, 0.00011852738190486945, 0.00011852072279471835, 0.00011851401822323996, 0.0001185072655088774, 0.00011850047714768563, 0.0001184937238612576, 0.00011848689200521358, 0.00011848003637269872, 0.0001184731621123022, 0.00011846624387437317, 0.00011845928314270644, 0.00011845222855654959, 0.00011844517870780987, 0.00011843811620212176, 0.00011843102868644667, 0.00011842387005588365, 0.0001184166014993664, 0.00011840936580085921, 0.00011840205775394793, 0.00011839478142091579, 0.00011838739603790466, 0.0001183800304626602, 0.00011837257750441619, 0.00011836507034296952, 0.00011835759734455725, 0.00011835008077978452, 0.00011834259592889092, 0.00011833506652839935, 0.00011832747173367448, 0.00011831984971220895, 0.00011831225706272957, 0.00011830456932524393, 0.0001182968718626454, 0.00011828916794643336, 0.0001182814914178554, 0.00011827381358425309, 0.00011826610215968185, 0.00011825836096982946, 0.00011825061929729682, 0.00011824284655445871, 0.00011823504847984679, 0.0001182272435404493, 0.0001182193973944614, 0.00011821151626309499, 0.00011820365467133957, 0.00011819579651197693, 0.00011818788380974776, 0.00011817993024059203, 0.00011817197232731419, 0.00011816401949111734, 0.00011815600199364606, 0.00011814802913515798, 0.0001181400831637467, 0.00011813203116357716, 0.00011812397172655658, 0.0001181159247140828, 0.00011810782022690698, 0.00011809971665146049, 0.00011809159623583691, 0.00011808342234996955, 0.00011807524578254533, 0.00011806701903425373, 0.00011805876359330371, 0.00011805051705512248, 0.00011804228017054595, 0.00011803406175295765, 0.000118025843585648, 0.0001180175409671743, 0.00011800926884694061, 0.00011800092634477787, 0.00011799262136653413, 0.00011798429897604784, 0.00011797590595335386, 0.00011796748066259232, 0.00011795896570056938, 0.00011795043414864799, 0.00011794185225496581, 0.00011793327617132349, 0.00011792462675603958, 0.00011791597097652738, 0.00011790734880586116, 0.00011789866696161628, 0.00011789001339885775, 0.0001178813234741882, 0.00011787267531029747, 0.00011786400712411553, 0.00011785531130002088, 0.0001178466183541306, 0.00011783786335701457, 0.00011782906397119398, 0.00011782030547017698, 0.00011781154729094682, 0.00011780284706909891, 0.00011779405962514487, 0.00011778526744377371, 0.00011777644803566189, 0.00011776758497180441, 0.00011775873629896871, 0.00011774988524848595, 0.00011774104163485421, 0.00011773218952962575, 0.00011772325043469274, 0.00011771437380215753, 0.00011770556052587239, 0.00011769666165429229, 0.00011768775575703321, 0.00011767889490992938, 0.00011766996244738034, 0.00011766107151320855, 0.00011765211438033638], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011986194228516825, 0.00011985714612559019, 0.00011985236724344893, 0.00011984756022918489, 0.00011984276184178919, 0.0001198379534055139, 0.00011983315188969343, 0.00011982834300311458, 0.00011982350878103512, 0.000119818643156207, 0.00011981376435407453, 0.00011980889370480661, 0.00011980398226751573, 0.00011979909517031757, 0.00011979417700217339, 0.00011978922733647979, 0.00011978421813591477, 0.00011977918613576921, 0.00011977418030063085, 0.00011976911348490961, 0.00011976402794604011, 0.00011975894766861227, 0.00011975384290889057, 0.00011974875414679556, 0.00011974364741995827, 0.00011973852744471609, 0.00011973337801014065, 0.00011972824504719562, 0.0001197230995705515, 0.00011971794449533138, 0.00011971271625763194, 0.00011970750503666723, 0.00011970226539917752, 0.00011969701398269452, 0.00011969172962295067, 0.00011968643884045596, 0.00011968109219661893, 0.0001196757117089139, 0.00011967032574646551, 0.00011966490397303357, 0.00011965948824314947, 0.00011965405277100904, 0.00011964861357793915, 0.00011964316488109401, 0.00011963769765070212, 0.00011963221501518824, 0.00011962669294256208, 0.00011962114361472009, 0.00011961563239677988, 0.00011961006012715624, 0.00011960447759535133, 0.00011959890051458959, 0.00011959330727029987, 0.00011958761149899828, 0.0001195819266771834, 0.00011957621033411892, 0.00011957045097521382, 0.0001195646680109218, 0.00011955888758254987, 0.00011955312983525753, 0.00011954726261679836, 0.00011954141163296773, 0.00011953550988333512, 0.00011952956862548967, 0.0001195235856790141, 0.0001195176335664827, 0.00011951168972531679, 0.0001195057416418173, 0.00011949975016327417, 0.00011949374150209498, 0.00011948776559457517, 0.00011948174501220159, 0.0001194756956341, 0.00011946961693886625, 0.00011946354227266441, 0.00011945735429586533, 0.00011945118710413083, 0.00011944498360370883, 0.00011943868800435755, 0.00011943239143329858, 0.00011942604340913217, 0.00011941962530351084, 0.00011941323502190981, 0.00011940677352124485, 0.00011940030536082716, 0.00011939385201302675, 0.00011938734633521197, 0.00011938076678391224, 0.00011937412286290281, 0.00011936744602233269, 0.00011936077657622112, 0.00011935399979195961, 0.00011934723201376941, 0.0001193404133275761, 0.00011933353131448135, 0.00011932659455395307, 0.00011931969336740649, 0.00011931273710162376, 0.00011930573755929836, 0.00011929872192454552, 0.0001192916822341022, 0.0001192846079176847, 0.00011927742439080242, 0.00011927025989517084, 0.00011926307662899063, 0.00011925588810136875, 0.00011924862693267168, 0.00011924134358059894, 0.00011923406700677988, 0.00011922671847919106, 0.00011921941512415993, 0.00011921202379403217, 0.00011920467595374879, 0.00011919723975916579, 0.00011918974080648583, 0.00011918225661902278, 0.00011917470251600612, 0.00011916717666361317, 0.0001191596143129312, 0.00011915195730369969, 0.00011914429529372858, 0.0001191366627904911, 0.00011912893605530747, 0.00011912118820325662, 0.00011911345352851013, 0.00011910573255247208, 0.00011909802420863408, 0.00011909026727941136, 0.00011908246806905384, 0.00011907466205674244, 0.00011906682829151131, 0.00011905897284060844, 0.0001190511162046962, 0.00011904322036867355, 0.00011903529481261556, 0.00011902736567782924, 0.00011901944287099302, 0.00011901144495826208, 0.00011900338248621994, 0.00011899532726643522, 0.00011898727669188729, 0.0001189791501108375, 0.00011897106818094128, 0.00011896298762565594, 0.00011895479032324615, 0.00011894658700098869, 0.00011893836907941564, 0.00011893009541500116, 0.00011892180914208688, 0.00011891350348241906, 0.00011890515528091439, 0.00011889681608548103, 0.00011888844238257442, 0.00011888001092231047, 0.0001188715846760878, 0.00011886316414161037, 0.00011885474875007365, 0.00011884632783639321, 0.00011883782816698914, 0.00011882934477961401, 0.00011882077258763568, 0.00011881222077781874, 0.00011880362879618371, 0.00011879496265666147, 0.00011878629611423603, 0.00011877757698109416, 0.00011876883846119877, 0.00011876005009980872, 0.0001187512847513009, 0.00011874245088555052, 0.00011873361957942042, 0.00011872483479675871, 0.00011871601811364439, 0.00011870722740593576, 0.00011869838970075498, 0.00011868958849386321, 0.00011868076458219166, 0.00011867192739841501, 0.00011866306886076927, 0.00011865414875003598, 0.00011864517891630895, 0.00011863626470692238, 0.00011862733999835266, 0.00011861846652958869, 0.00011860949117371794, 0.00011860049560158711, 0.0001185914636496682, 0.00011858237958103662, 0.00011857331923629288, 0.00011856425853604633, 0.00011855519641378852, 0.00011854611440707325, 0.00011853694821729139, 0.00011852783954786501, 0.00011851880579473184, 0.00011850967520263186, 0.00011850053612586468, 0.0001184914366758113, 0.00011848228814266925, 0.00011847317011166873], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 03:26:39,039 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:92]: done!
[2017-12-15 03:26:39,039 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:152]: >> Executing classifier part ... 
[2017-12-15 03:26:39,040 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:97]: =======================================
[2017-12-15 03:26:39,040 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fa1490413c8>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-15 03:26:39,187 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:110]: training ... 
[2017-12-15 09:08:21,649 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:122]: trained!
[2017-12-15 09:08:21,654 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:125]: Training history: 
{'val_loss': [0.00011902249743060738, 0.00011901771408735708, 0.00011901290046039131, 0.00011900809296525223, 0.00011900327901649964, 0.00011899847422079446, 0.00011899365928880435, 0.0001189888275702683, 0.00011898395457363365, 0.0001189790727457384, 0.00011897420734684818, 0.00011896931627652029, 0.00011896445520387514, 0.00011895957385866013, 0.00011895467592354667, 0.00011894973644217893, 0.00011894477105697191, 0.00011893984171188911, 0.00011893487019485541, 0.00011892989204543773, 0.00011892491471836417, 0.0001189199141690082, 0.00011891490986547262, 0.00011890990009156104, 0.00011890486391325292, 0.00011889978481215798, 0.00011889471799259346, 0.00011888964323562064, 0.0001188845662797712, 0.00011887940503365111, 0.00011887426643774796, 0.00011886910756927496, 0.00011886394698460556, 0.00011885874633747668, 0.00011885354054175862, 0.00011884829060761464, 0.00011884302301094947, 0.00011883775754165275, 0.00011883246754504928, 0.00011882717656520829, 0.000118821862792134, 0.00011881654387047052, 0.00011881122346501225, 0.00011880589006294174, 0.00011880054890223337, 0.00011879515296625688, 0.00011878974435545495, 0.00011878434693568365, 0.00011877889506243105, 0.00011877340959820951, 0.00011876792175634089, 0.00011876242704968671, 0.00011875684510304933, 0.0001187512711832055, 0.00011874567550699701, 0.00011874003781973104, 0.00011873437528337148, 0.00011872871340846263, 0.00011872307572119666, 0.00011871735335036504, 0.00011871164823088256, 0.00011870588163223978, 0.00011870007359460499, 0.00011869421169343142, 0.00011868837578548233, 0.00011868254667081063, 0.00011867672279411333, 0.00011867084936224524, 0.00011866496021645396, 0.00011865910500129548, 0.00011865320598737495, 0.00011864724499373685, 0.00011864123194214158, 0.00011863522014193951, 0.0001186290841856548, 0.00011862296001034324, 0.00011861681057476606, 0.00011861061371138654, 0.0001186044399809042, 0.00011859822920918309, 0.00011859194160191942, 0.00011858569479007411, 0.00011857936663767071, 0.00011857302100151658, 0.00011856665975870153, 0.00011856023827024234, 0.0001185537705159888, 0.0001185472279641759, 0.00011854063825271634, 0.00011853407249649808, 0.00011852738190486945, 0.00011852072279471835, 0.00011851401822323996, 0.0001185072655088774, 0.00011850047714768563, 0.0001184937238612576, 0.00011848689200521358, 0.00011848003637269872, 0.0001184731621123022, 0.00011846624387437317, 0.00011845928314270644, 0.00011845222855654959, 0.00011844517870780987, 0.00011843811620212176, 0.00011843102868644667, 0.00011842387005588365, 0.0001184166014993664, 0.00011840936580085921, 0.00011840205775394793, 0.00011839478142091579, 0.00011838739603790466, 0.0001183800304626602, 0.00011837257750441619, 0.00011836507034296952, 0.00011835759734455725, 0.00011835008077978452, 0.00011834259592889092, 0.00011833506652839935, 0.00011832747173367448, 0.00011831984971220895, 0.00011831225706272957, 0.00011830456932524393, 0.0001182968718626454, 0.00011828916794643336, 0.0001182814914178554, 0.00011827381358425309, 0.00011826610215968185, 0.00011825836096982946, 0.00011825061929729682, 0.00011824284655445871, 0.00011823504847984679, 0.0001182272435404493, 0.0001182193973944614, 0.00011821151626309499, 0.00011820365467133957, 0.00011819579651197693, 0.00011818788380974776, 0.00011817993024059203, 0.00011817197232731419, 0.00011816401949111734, 0.00011815600199364606, 0.00011814802913515798, 0.0001181400831637467, 0.00011813203116357716, 0.00011812397172655658, 0.0001181159247140828, 0.00011810782022690698, 0.00011809971665146049, 0.00011809159623583691, 0.00011808342234996955, 0.00011807524578254533, 0.00011806701903425373, 0.00011805876359330371, 0.00011805051705512248, 0.00011804228017054595, 0.00011803406175295765, 0.000118025843585648, 0.0001180175409671743, 0.00011800926884694061, 0.00011800092634477787, 0.00011799262136653413, 0.00011798429897604784, 0.00011797590595335386, 0.00011796748066259232, 0.00011795896570056938, 0.00011795043414864799, 0.00011794185225496581, 0.00011793327617132349, 0.00011792462675603958, 0.00011791597097652738, 0.00011790734880586116, 0.00011789866696161628, 0.00011789001339885775, 0.0001178813234741882, 0.00011787267531029747, 0.00011786400712411553, 0.00011785531130002088, 0.0001178466183541306, 0.00011783786335701457, 0.00011782906397119398, 0.00011782030547017698, 0.00011781154729094682, 0.00011780284706909891, 0.00011779405962514487, 0.00011778526744377371, 0.00011777644803566189, 0.00011776758497180441, 0.00011775873629896871, 0.00011774988524848595, 0.00011774104163485421, 0.00011773218952962575, 0.00011772325043469274, 0.00011771437380215753, 0.00011770556052587239, 0.00011769666165429229, 0.00011768775575703321, 0.00011767889490992938, 0.00011766996244738034, 0.00011766107151320855, 0.00011765211438033638], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011986194228516825, 0.00011985714612559019, 0.00011985236724344893, 0.00011984756022918489, 0.00011984276184178919, 0.0001198379534055139, 0.00011983315188969343, 0.00011982834300311458, 0.00011982350878103512, 0.000119818643156207, 0.00011981376435407453, 0.00011980889370480661, 0.00011980398226751573, 0.00011979909517031757, 0.00011979417700217339, 0.00011978922733647979, 0.00011978421813591477, 0.00011977918613576921, 0.00011977418030063085, 0.00011976911348490961, 0.00011976402794604011, 0.00011975894766861227, 0.00011975384290889057, 0.00011974875414679556, 0.00011974364741995827, 0.00011973852744471609, 0.00011973337801014065, 0.00011972824504719562, 0.0001197230995705515, 0.00011971794449533138, 0.00011971271625763194, 0.00011970750503666723, 0.00011970226539917752, 0.00011969701398269452, 0.00011969172962295067, 0.00011968643884045596, 0.00011968109219661893, 0.0001196757117089139, 0.00011967032574646551, 0.00011966490397303357, 0.00011965948824314947, 0.00011965405277100904, 0.00011964861357793915, 0.00011964316488109401, 0.00011963769765070212, 0.00011963221501518824, 0.00011962669294256208, 0.00011962114361472009, 0.00011961563239677988, 0.00011961006012715624, 0.00011960447759535133, 0.00011959890051458959, 0.00011959330727029987, 0.00011958761149899828, 0.0001195819266771834, 0.00011957621033411892, 0.00011957045097521382, 0.0001195646680109218, 0.00011955888758254987, 0.00011955312983525753, 0.00011954726261679836, 0.00011954141163296773, 0.00011953550988333512, 0.00011952956862548967, 0.0001195235856790141, 0.0001195176335664827, 0.00011951168972531679, 0.0001195057416418173, 0.00011949975016327417, 0.00011949374150209498, 0.00011948776559457517, 0.00011948174501220159, 0.0001194756956341, 0.00011946961693886625, 0.00011946354227266441, 0.00011945735429586533, 0.00011945118710413083, 0.00011944498360370883, 0.00011943868800435755, 0.00011943239143329858, 0.00011942604340913217, 0.00011941962530351084, 0.00011941323502190981, 0.00011940677352124485, 0.00011940030536082716, 0.00011939385201302675, 0.00011938734633521197, 0.00011938076678391224, 0.00011937412286290281, 0.00011936744602233269, 0.00011936077657622112, 0.00011935399979195961, 0.00011934723201376941, 0.0001193404133275761, 0.00011933353131448135, 0.00011932659455395307, 0.00011931969336740649, 0.00011931273710162376, 0.00011930573755929836, 0.00011929872192454552, 0.0001192916822341022, 0.0001192846079176847, 0.00011927742439080242, 0.00011927025989517084, 0.00011926307662899063, 0.00011925588810136875, 0.00011924862693267168, 0.00011924134358059894, 0.00011923406700677988, 0.00011922671847919106, 0.00011921941512415993, 0.00011921202379403217, 0.00011920467595374879, 0.00011919723975916579, 0.00011918974080648583, 0.00011918225661902278, 0.00011917470251600612, 0.00011916717666361317, 0.0001191596143129312, 0.00011915195730369969, 0.00011914429529372858, 0.0001191366627904911, 0.00011912893605530747, 0.00011912118820325662, 0.00011911345352851013, 0.00011910573255247208, 0.00011909802420863408, 0.00011909026727941136, 0.00011908246806905384, 0.00011907466205674244, 0.00011906682829151131, 0.00011905897284060844, 0.0001190511162046962, 0.00011904322036867355, 0.00011903529481261556, 0.00011902736567782924, 0.00011901944287099302, 0.00011901144495826208, 0.00011900338248621994, 0.00011899532726643522, 0.00011898727669188729, 0.0001189791501108375, 0.00011897106818094128, 0.00011896298762565594, 0.00011895479032324615, 0.00011894658700098869, 0.00011893836907941564, 0.00011893009541500116, 0.00011892180914208688, 0.00011891350348241906, 0.00011890515528091439, 0.00011889681608548103, 0.00011888844238257442, 0.00011888001092231047, 0.0001188715846760878, 0.00011886316414161037, 0.00011885474875007365, 0.00011884632783639321, 0.00011883782816698914, 0.00011882934477961401, 0.00011882077258763568, 0.00011881222077781874, 0.00011880362879618371, 0.00011879496265666147, 0.00011878629611423603, 0.00011877757698109416, 0.00011876883846119877, 0.00011876005009980872, 0.0001187512847513009, 0.00011874245088555052, 0.00011873361957942042, 0.00011872483479675871, 0.00011871601811364439, 0.00011870722740593576, 0.00011869838970075498, 0.00011868958849386321, 0.00011868076458219166, 0.00011867192739841501, 0.00011866306886076927, 0.00011865414875003598, 0.00011864517891630895, 0.00011863626470692238, 0.00011862733999835266, 0.00011861846652958869, 0.00011860949117371794, 0.00011860049560158711, 0.0001185914636496682, 0.00011858237958103662, 0.00011857331923629288, 0.00011856425853604633, 0.00011855519641378852, 0.00011854611440707325, 0.00011853694821729139, 0.00011852783954786501, 0.00011851880579473184, 0.00011850967520263186, 0.00011850053612586468, 0.0001184914366758113, 0.00011848228814266925, 0.00011847317011166873], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 09:08:21,655 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:129]: evaluating model ... 
[2017-12-15 09:08:24,839 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:133]: evaluated! 
[2017-12-15 09:08:24,839 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:135]: generating reports ... 
[2017-12-15 09:08:26,049 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:138]: done!
[2017-12-15 09:08:26,050 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_7 finished!
[2018-04-29 11:38:17,567 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_7 was already executed!
[2018-04-29 13:12:02,671 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_7
[2018-04-29 13:12:02,671 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:146]: >> Printing header log
[2018-04-29 13:12:02,671 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_7
	layers = 9216,15667
	using GLOBAL obj = 
		{'mlp_configs': {'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'optimizer': <keras.optimizers.SGD object at 0x7f5e63317898>, 'activation': 'sigmoid'}, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'numpy_seed': 666, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'autoencoder_configs': {'loss_function': 'mse', 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f5e63317828>, 'hidden_layer_activation': 'relu'}, 'epochs': 200, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'store_history': True, 'shuffle_batches': True, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'batch': 32}
	=======================================
	
[2018-04-29 13:12:02,671 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,666 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,666 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,666 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:57]: =======================================
[2018-04-29 13:12:20,666 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:62]: setting configurations for autoencoder: 
	 {'loss_function': 'mse', 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f5e63317828>, 'hidden_layer_activation': 'relu'}
[2018-04-29 13:12:20,713 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,901 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_7
[2018-04-29 13:14:12,901 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:146]: >> Printing header log
[2018-04-29 13:14:12,901 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_7
	layers = 9216,15667
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'epochs': 200, 'autoencoder_configs': {'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7fdb111aa828>, 'loss_function': 'mse', 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu'}, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'shuffle_batches': True, 'data_dir': '/home/dhiego/malware_dataset/', 'mlp_configs': {'optimizer': <keras.optimizers.SGD object at 0x7fdb111aa898>, 'activation': 'sigmoid', 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy', 'classifier_dim': 9}, 'store_history': True, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'batch': 32, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'fullds_data_dir': '/home/dhiego/malware_dataset/'}
	=======================================
	
[2018-04-29 13:14:12,901 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:148]: >> Loading dataset... 
[2018-04-29 13:14:30,887 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:30,887 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:30,887 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:57]: =======================================
[2018-04-29 13:14:30,887 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7fdb111aa828>, 'loss_function': 'mse', 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu'}
[2018-04-29 13:14:30,936 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,619 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_7
[2018-04-29 13:16:33,619 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:146]: >> Printing header log
[2018-04-29 13:16:33,619 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_7
	layers = 9216,15667
	using GLOBAL obj = 
		{'mlp_configs': {'optimizer': <keras.optimizers.SGD object at 0x7f7a46f54898>, 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid', 'classifier_dim': 9}, 'numpy_seed': 666, 'epochs': 200, 'batch': 32, 'store_history': True, 'autoencoder_configs': {'output_layer_activation': 'relu', 'discard_decoder_function': True, 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f7a46f54828>, 'hidden_layer_activation': 'relu'}, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'shuffle_batches': True, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'data_dir': '/home/dhiego/malware_dataset/'}
	=======================================
	
[2018-04-29 13:16:33,619 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:148]: >> Loading dataset... 
[2018-04-29 13:16:57,254 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:57,254 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:57,255 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:57]: =======================================
[2018-04-29 13:16:57,255 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:62]: setting configurations for autoencoder: 
	 {'output_layer_activation': 'relu', 'discard_decoder_function': True, 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f7a46f54828>, 'hidden_layer_activation': 'relu'}
[2018-04-29 13:16:57,334 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:21,217 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_7
[2018-04-29 14:30:21,218 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:146]: >> Printing header log
[2018-04-29 14:30:21,218 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_7
	layers = 9216,15667
	using GLOBAL obj = 
		{'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'numpy_seed': 666, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'batch': 32, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'data_dir': '/home/dhiego/malware_dataset/', 'store_history': True, 'shuffle_batches': True, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'mlp_configs': {'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'optimizer': <keras.optimizers.SGD object at 0x7f771c6a2908>, 'use_last_dim_as_classifier': False, 'activation': 'sigmoid'}, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'autoencoder_configs': {'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f771c6a2898>, 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu'}, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/'}
	=======================================
	
[2018-04-29 14:30:21,218 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:148]: >> Loading dataset... 
[2018-04-29 14:30:39,279 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:39,279 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:39,280 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:57]: =======================================
[2018-04-29 14:30:39,280 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:62]: setting configurations for autoencoder: 
	 {'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f771c6a2898>, 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu'}
[2018-04-29 14:30:39,378 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:73]: training and evaluate autoencoder
[2018-04-29 23:18:09,374 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:85]: trained and evaluated!
[2018-04-29 23:18:09,375 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:88]: Training history: 
{'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0008143322718259954, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322718259954, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.003257328990228013, 0.003257328990228013, 0.003257328990228013], 'loss': [0.00011788317353972036, 0.00011787446215653986, 0.00011786577580075754, 0.00011785707738157968, 0.00011784833473785164, 0.00011783966893013202, 0.00011783091941334956, 0.00011782217897373898, 0.00011781347301790145, 0.00011780469525049529, 0.00011779593416802124, 0.00011778720607620843, 0.0001177784999307694, 0.00011776976425489654, 0.00011776098094164646, 0.00011775223251507263, 0.00011774349506168568, 0.00011773473689433472, 0.00011772600098145998, 0.00011771725831403176, 0.00011770850974525679, 0.00011769976771773364, 0.00011769101827205174, 0.00011768223299168609, 0.00011767344683441349, 0.00011766468672364714, 0.0001176559367802613, 0.00011764712505048621, 0.00011763833666539596, 0.00011762959449567168, 0.00011762086389163897, 0.00011761205946152168, 0.0001176032761719718, 0.00011759455874524343, 0.00011758579353893673, 0.000117577014183618, 0.00011756822921135478, 0.00011755943433241312, 0.00011755061696199336, 0.00011754184959896958, 0.00011753308581467415, 0.00011752432264658359, 0.0001175155097554993, 0.00011750668141189265, 0.00011749791857560472, 0.00011748914730204998, 0.00011748035555153309, 0.0001174715466894807, 0.00011746276809256796, 0.00011745396465785856, 0.00011744522997739357, 0.00011743643249143128, 0.00011742763946110425, 0.00011741886572272999, 0.00011741011708285447, 0.0001174013118469308, 0.00011739252964758954, 0.00011738376088625469, 0.00011737502746189965, 0.00011736624106762519, 0.00011735746562283741, 0.00011734868309169352, 0.00011733992189071854, 0.00011733115900703023, 0.00011732243449394576, 0.0001173136922768211, 0.00011730497918722708, 0.00011729623801291067, 0.00011728744628609398, 0.00011727871248253594, 0.00011726996652078163, 0.00011726123688845662, 0.00011725251934322731, 0.0001172437976030648, 0.00011723509283223666, 0.0001172263427229495, 0.00011721758533770474, 0.00011720883707703222, 0.00011720014256838534, 0.00011719140968913461, 0.00011718264325041816, 0.00011717390880695505, 0.0001171651556166435, 0.00011715644311955417, 0.00011714772206669709, 0.00011713897238401332, 0.00011713021080383534, 0.00011712152487465638, 0.00011711278689986532, 0.00011710404425613728, 0.0001170953741586837, 0.00011708665019070353, 0.00011707790396824716, 0.00011706915826719491, 0.0001170604584260058, 0.00011705172687396559, 0.0001170429403137898, 0.00011703422096734623, 0.00011702556527960676, 0.00011701687619830284, 0.00011700817289688633], 'val_loss': [0.00011708754783694195, 0.00011707905497094753, 0.00011707055917311762, 0.00011706202232959069, 0.00011705354957527273, 0.00011704501199878692, 0.00011703647867703801, 0.00011702798736634658, 0.00011701943834855151, 0.00011701090379328643, 0.0001170024009519005, 0.00011699392444340293, 0.00011698541784783738, 0.00011697686702446069, 0.00011696833370271178, 0.00011695980552955203, 0.00011695126019442838, 0.00011694272197436894, 0.0001169341749230489, 0.00011692562132873012, 0.00011691707256121369, 0.00011690849697812865, 0.0001168998837102312, 0.00011689126538312982, 0.00011688266936658151, 0.0001168740784271142, 0.00011686543417829649, 0.0001168568139741053, 0.00011684824445133877, 0.00011683968465368513, 0.00011683105619029881, 0.00011682244816037577, 0.00011681391435594662, 0.00011680532913713393, 0.0001167967327987988, 0.00011678812371413007, 0.00011677950538702867, 0.00011677085992257187, 0.00011676225900771303, 0.00011675367175091712, 0.0001167450779511225, 0.00011673644090675421, 0.00011672778384009473, 0.00011671919331179946, 0.00011671060303378283, 0.0001167019922329177, 0.0001166933559215083, 0.00011668475738429654, 0.00011667613317564711, 0.0001166675780439024, 0.00011665895980618624, 0.00011665034156847008, 0.00011664173829384121, 0.00011663316485600166, 0.00011662453524848441, 0.00011661592927442163, 0.00011660733556401222, 0.00011659877594512903, 0.00011659016498337049, 0.00011658156367521664, 0.0001165729572899818, 0.00011656437330468528, 0.00011655578858642988, 0.00011654723599322566, 0.00011653865349172392, 0.00011653010327616679, 0.00011652152468973807, 0.00011651289804981042, 0.00011650432273488108, 0.0001164957345842329, 0.0001164871667955398, 0.00011647861184256554, 0.0001164700518125103, 0.0001164615171321059, 0.00011645292840939225, 0.00011644434574699711, 0.00011643576070695489, 0.00011642723378518834, 0.00011641867089480578, 0.00011641007425256088, 0.00011640151046832603, 0.000116392923551194, 0.00011638438404398726, 0.00011637582991335711, 0.00011636725132692839, 0.00011635866317628021, 0.00011635014508577426, 0.00011634158139092464, 0.00011633300673744598, 0.00011632450942006717, 0.00011631596092070642, 0.00011630738356779387, 0.00011629881152436389, 0.00011629028199254867, 0.00011628172401835369, 0.0001162731079973912, 0.00011626456727454535, 0.00011625608622527816, 0.00011624756495265807, 0.000116239042124735, 0.00011623054872242922], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002457002457002457, 0.002457002457002457, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371]}
[2018-04-29 23:18:09,376 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:92]: done!
[2018-04-29 23:18:09,376 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:152]: >> Executing classifier part ... 
[2018-04-29 23:18:09,376 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:97]: =======================================
[2018-04-29 23:18:09,376 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:101]: setting configurations for classifier: 
	 {'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'optimizer': <keras.optimizers.SGD object at 0x7f771c6a2908>, 'use_last_dim_as_classifier': False, 'activation': 'sigmoid'}
[2018-04-29 23:18:09,805 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:110]: training ... 
[2018-04-30 04:00:40,600 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:122]: trained!
[2018-04-30 04:00:40,601 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:125]: Training history: 
{'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0008143322718259954, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322718259954, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0008143322475570033, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.003257328990228013, 0.003257328990228013, 0.003257328990228013], 'loss': [0.00011788317353972036, 0.00011787446215653986, 0.00011786577580075754, 0.00011785707738157968, 0.00011784833473785164, 0.00011783966893013202, 0.00011783091941334956, 0.00011782217897373898, 0.00011781347301790145, 0.00011780469525049529, 0.00011779593416802124, 0.00011778720607620843, 0.0001177784999307694, 0.00011776976425489654, 0.00011776098094164646, 0.00011775223251507263, 0.00011774349506168568, 0.00011773473689433472, 0.00011772600098145998, 0.00011771725831403176, 0.00011770850974525679, 0.00011769976771773364, 0.00011769101827205174, 0.00011768223299168609, 0.00011767344683441349, 0.00011766468672364714, 0.0001176559367802613, 0.00011764712505048621, 0.00011763833666539596, 0.00011762959449567168, 0.00011762086389163897, 0.00011761205946152168, 0.0001176032761719718, 0.00011759455874524343, 0.00011758579353893673, 0.000117577014183618, 0.00011756822921135478, 0.00011755943433241312, 0.00011755061696199336, 0.00011754184959896958, 0.00011753308581467415, 0.00011752432264658359, 0.0001175155097554993, 0.00011750668141189265, 0.00011749791857560472, 0.00011748914730204998, 0.00011748035555153309, 0.0001174715466894807, 0.00011746276809256796, 0.00011745396465785856, 0.00011744522997739357, 0.00011743643249143128, 0.00011742763946110425, 0.00011741886572272999, 0.00011741011708285447, 0.0001174013118469308, 0.00011739252964758954, 0.00011738376088625469, 0.00011737502746189965, 0.00011736624106762519, 0.00011735746562283741, 0.00011734868309169352, 0.00011733992189071854, 0.00011733115900703023, 0.00011732243449394576, 0.0001173136922768211, 0.00011730497918722708, 0.00011729623801291067, 0.00011728744628609398, 0.00011727871248253594, 0.00011726996652078163, 0.00011726123688845662, 0.00011725251934322731, 0.0001172437976030648, 0.00011723509283223666, 0.0001172263427229495, 0.00011721758533770474, 0.00011720883707703222, 0.00011720014256838534, 0.00011719140968913461, 0.00011718264325041816, 0.00011717390880695505, 0.0001171651556166435, 0.00011715644311955417, 0.00011714772206669709, 0.00011713897238401332, 0.00011713021080383534, 0.00011712152487465638, 0.00011711278689986532, 0.00011710404425613728, 0.0001170953741586837, 0.00011708665019070353, 0.00011707790396824716, 0.00011706915826719491, 0.0001170604584260058, 0.00011705172687396559, 0.0001170429403137898, 0.00011703422096734623, 0.00011702556527960676, 0.00011701687619830284, 0.00011700817289688633], 'val_loss': [0.00011708754783694195, 0.00011707905497094753, 0.00011707055917311762, 0.00011706202232959069, 0.00011705354957527273, 0.00011704501199878692, 0.00011703647867703801, 0.00011702798736634658, 0.00011701943834855151, 0.00011701090379328643, 0.0001170024009519005, 0.00011699392444340293, 0.00011698541784783738, 0.00011697686702446069, 0.00011696833370271178, 0.00011695980552955203, 0.00011695126019442838, 0.00011694272197436894, 0.0001169341749230489, 0.00011692562132873012, 0.00011691707256121369, 0.00011690849697812865, 0.0001168998837102312, 0.00011689126538312982, 0.00011688266936658151, 0.0001168740784271142, 0.00011686543417829649, 0.0001168568139741053, 0.00011684824445133877, 0.00011683968465368513, 0.00011683105619029881, 0.00011682244816037577, 0.00011681391435594662, 0.00011680532913713393, 0.0001167967327987988, 0.00011678812371413007, 0.00011677950538702867, 0.00011677085992257187, 0.00011676225900771303, 0.00011675367175091712, 0.0001167450779511225, 0.00011673644090675421, 0.00011672778384009473, 0.00011671919331179946, 0.00011671060303378283, 0.0001167019922329177, 0.0001166933559215083, 0.00011668475738429654, 0.00011667613317564711, 0.0001166675780439024, 0.00011665895980618624, 0.00011665034156847008, 0.00011664173829384121, 0.00011663316485600166, 0.00011662453524848441, 0.00011661592927442163, 0.00011660733556401222, 0.00011659877594512903, 0.00011659016498337049, 0.00011658156367521664, 0.0001165729572899818, 0.00011656437330468528, 0.00011655578858642988, 0.00011654723599322566, 0.00011653865349172392, 0.00011653010327616679, 0.00011652152468973807, 0.00011651289804981042, 0.00011650432273488108, 0.0001164957345842329, 0.0001164871667955398, 0.00011647861184256554, 0.0001164700518125103, 0.0001164615171321059, 0.00011645292840939225, 0.00011644434574699711, 0.00011643576070695489, 0.00011642723378518834, 0.00011641867089480578, 0.00011641007425256088, 0.00011640151046832603, 0.000116392923551194, 0.00011638438404398726, 0.00011637582991335711, 0.00011636725132692839, 0.00011635866317628021, 0.00011635014508577426, 0.00011634158139092464, 0.00011633300673744598, 0.00011632450942006717, 0.00011631596092070642, 0.00011630738356779387, 0.00011629881152436389, 0.00011629028199254867, 0.00011628172401835369, 0.0001162731079973912, 0.00011626456727454535, 0.00011625608622527816, 0.00011624756495265807, 0.000116239042124735, 0.00011623054872242922], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002457002457002457, 0.002457002457002457, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.004914004914004914, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371, 0.007371007371007371]}
[2018-04-30 04:00:40,601 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:129]: evaluating model ... 
[2018-04-30 04:00:42,588 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:133]: evaluated! 
[2018-04-30 04:00:42,589 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:135]: generating reports ... 
[2018-04-30 04:00:43,609 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:138]: done!
[2018-04-30 04:00:43,609 AE_BIGRAMA_1L_MINIDS_OVER_F1_7.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_7 finished!
