[2017-12-14 09:31:57,885 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_3
[2017-12-14 09:31:57,886 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:146]: >> Printing header log
[2017-12-14 09:31:57,886 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_3
	layers = 9216,11981
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f5b4c05be48>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f5b4c0bd390>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,886 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:148]: >> Loading dataset... 
[2017-12-14 09:32:22,146 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:22,146 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:22,146 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:57]: =======================================
[2017-12-14 09:32:22,146 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f5b4c05be48>, 'discard_decoder_function': True}
[2017-12-14 09:32:22,193 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:54,983 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_3
[2017-12-14 10:18:54,983 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:146]: >> Printing header log
[2017-12-14 10:18:54,983 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_3
	layers = 9216,11981
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fd3fb65eeb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fd3fb641400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:54,983 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:148]: >> Loading dataset... 
[2017-12-14 10:19:16,994 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:16,994 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:16,995 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:57]: =======================================
[2017-12-14 10:19:16,995 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fd3fb65eeb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:17,037 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:73]: training and evaluate autoencoder
[2017-12-15 01:40:42,226 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:85]: trained and evaluated!
[2017-12-15 01:40:42,229 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:88]: Training history: 
{'val_loss': [0.00011911663146408078, 0.00011910823490173173, 0.00011909988313925931, 0.00011909146827081544, 0.00011908306271631238, 0.00011907465217411357, 0.00011906623419506375, 0.00011905784417571345, 0.00011904939317776014, 0.00011904102114271784, 0.0001190326055413151, 0.00011902425492297359, 0.0001190158371942024, 0.00011900742175369307, 0.00011899903148406411, 0.00011899063256194504, 0.0001189821955438415, 0.00011897378671783908, 0.00011896536031870071, 0.00011895692959331727, 0.00011894851971256915, 0.0001189400762408521, 0.00011893162261497308, 0.00011892317006171678, 0.0001189147014012423, 0.00011890625456864065, 0.00011889779759975406, 0.00011888936785760814, 0.00011888096680812062, 0.00011887257246252524, 0.000118864177383971, 0.00011885574510328457, 0.00011884732778568544, 0.00011883890531949713, 0.00011883051644427775, 0.00011882209143954895, 0.00011881367077894225, 0.00011880523163347027, 0.00011879682232478762, 0.00011878839585414105, 0.00011877998940578573, 0.00011877159064456005, 0.00011876319588779264, 0.00011875478870647843, 0.00011874637180005134, 0.00011873797639971027, 0.0001187295497681703, 0.00011872111055119014, 0.00011871267762693007, 0.00011870424833171028, 0.00011869581029461514, 0.00011868742865959928, 0.0001186790655809569, 0.00011867066138510933, 0.00011866226128310523, 0.0001186538539766517, 0.00011864546220535092, 0.00011863702048558435, 0.00011862862528189079, 0.00011862020513971842, 0.00011861183691248687, 0.00011860343042837745, 0.00011859506286259658, 0.0001185866826219903, 0.00011857828929538653, 0.00011856991715308197, 0.00011856151819520879, 0.00011855312601273596, 0.00011854471520238146, 0.00011853628647922715, 0.00011852795259380044, 0.00011851954693203512, 0.00011851113227811581, 0.00011850273879061863, 0.00011849439467952177, 0.00011848600986239178, 0.00011847764286867638, 0.00011846923263038735, 0.00011846087478971937, 0.00011845246658941356, 0.00011844411062583538, 0.00011843573676733442, 0.00011842737279483975, 0.00011841898454531698, 0.00011841062825995198, 0.00011840226151651523, 0.00011839396007792653, 0.00011838559267303909, 0.00011837723597650203, 0.00011836892204185838, 0.00011836054981016858, 0.0001183521793840604, 0.00011834381525067232, 0.00011833547081778864, 0.0001183271589211282, 0.00011831884686357436, 0.00011831047161066384, 0.00011830213053866475, 0.00011829377596949614, 0.00011828543750754573, 0.00011827706724233097, 0.00011826876834228278, 0.00011826043610154429, 0.00011825214603275669, 0.00011824381770709122, 0.00011823550155569389, 0.00011822718345569858, 0.00011821887670762731, 0.00011821058974944567, 0.00011820229993093671, 0.00011819392067356795, 0.00011818564973321929, 0.00011817737192808507, 0.00011816908979670577, 0.00011816078739275662, 0.00011815248391618471, 0.00011814425075003369, 0.00011813599452249367, 0.00011812766759123777, 0.00011811935422865957, 0.00011811103203482079, 0.00011810275946766096, 0.00011809449237087714, 0.00011808618717810885, 0.00011807786523454871, 0.00011806956740712326, 0.00011806128861875152, 0.00011805302929848259, 0.00011804476448996062, 0.00011803651203447724, 0.00011802820153222636, 0.00011801992471032965, 0.00011801161747957815, 0.00011800328441649557, 0.00011799497073213055, 0.00011798669546553682, 0.00011797846614295063, 0.00011797020101264183, 0.00011796198255929944, 0.00011795371571279426, 0.00011794545728637761, 0.00011793721553924463, 0.00011792897550830806, 0.00011792071513329343, 0.00011791243945552764, 0.00011790417899112777, 0.0001178959491679843, 0.00011788767545669355, 0.00011787939618564159, 0.0001178711624474251, 0.00011786296305101347, 0.00011785473919880327, 0.00011784651731306811, 0.00011783828325306479, 0.00011783007699186761, 0.00011782186949715426, 0.00011781360722717278, 0.00011780532134161388, 0.00011779704346497147, 0.00011778880302286284, 0.00011778065970674367, 0.00011777246152597115, 0.00011776423670840049, 0.00011775601687852558, 0.0001177478332496683, 0.00011773961889016938, 0.00011773140485245729, 0.00011772323325485179, 0.00011771501571323874, 0.00011770679792134704, 0.00011769857113730135, 0.00011769031026172944, 0.00011768212884962582, 0.00011767391375716803, 0.00011766573561656378, 0.00011765752468945766, 0.00011764933812876485, 0.00011764110759053955, 0.00011763294759513673, 0.00011762476135623076, 0.00011761655697212338, 0.00011760834947741003, 0.00011760017394685445, 0.00011759204216142979, 0.0001175838815939615, 0.00011757570745781549, 0.0001175675298892767, 0.00011755940529042443, 0.0001175512962267249, 0.00011754315176647482, 0.00011753501343805142, 0.00011752690167491798, 0.00011751874666721091, 0.00011751059779133053, 0.00011750244637690965, 0.00011749430182727434, 0.00011748616365974435, 0.00011747803244638515, 0.00011746989713918248, 0.00011746172071477462], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.0073710074167726083, 0.0073710074167726083, 0.0073710074167726083, 0.0073710074167726083, 0.0073710074167726083, 0.0073710074167726083, 0.0073710074167726083, 0.0073710074167726083, 0.0073710074167726083, 0.0073710074167726083, 0.0098280098737750649, 0.0098280098737750649, 0.0098280098737750649, 0.012285012330777521, 0.012285012330777521, 0.01474201478777998, 0.01474201478777998, 0.01474201478777998, 0.01474201478777998, 0.01474201478777998, 0.01474201478777998, 0.01474201478777998, 0.01474201478777998, 0.01474201478777998, 0.019656019701784894, 0.019656019701784894, 0.019656019701784894, 0.019656019701784894, 0.022113022158787351, 0.024570024615789807, 0.024570024615789807, 0.027027027072792264, 0.02948402952979472, 0.034398034489564869, 0.034398034489564869, 0.034398034489564869, 0.039312039238814933, 0.041769041952102716, 0.041769041952102716, 0.041769041952102716, 0.041769041952102716, 0.041769041952102716, 0.041769041952102716, 0.041769041952102716, 0.044226044409105172, 0.046683046866107629, 0.049140049323110085, 0.051597051780112542, 0.051597051780112542, 0.054054054237115005, 0.058968059151119918, 0.063882064065124824, 0.06633906593633225], 'loss': [0.00011997465198785249, 0.00011996608925224903, 0.00011995754031015482, 0.00011994904876991512, 0.00011994049767110381, 0.00011993196095830643, 0.00011992340985949512, 0.00011991484755049506, 0.00011990630979488941, 0.00011989769629348396, 0.00011988916628783969, 0.00011988060047121184, 0.00011987208767190381, 0.00011986351239890109, 0.00011985494215033815, 0.00011984640027089984, 0.00011983785374622477, 0.00011982926162238861, 0.00011982069146862642, 0.00011981210178590959, 0.00011980350534863928, 0.00011979492559630109, 0.00011978630598024722, 0.00011977769435115662, 0.00011976906677183971, 0.00011976043070785562, 0.0001197518292698457, 0.0001197431953388785, 0.00011973460250403671, 0.00011972604152224715, 0.00011971748231797166, 0.00011970892183388604, 0.00011970031992187238, 0.00011969174329795895, 0.00011968314866190291, 0.00011967459495607096, 0.00011966600197902804, 0.00011965740862278213, 0.00011964879590348289, 0.0001196402097994944, 0.00011963160227053625, 0.00011962302448531364, 0.00011961444861980622, 0.00011960587732843503, 0.00011959729458987318, 0.00011958871317852185, 0.00011958014705379156, 0.00011957155483515467, 0.00011956295228323593, 0.00011955435537196187, 0.0001195457730600034, 0.00011953717415791357, 0.00011952863308428165, 0.00011952011106560061, 0.00011951154529637315, 0.00011950297637502072, 0.0001194944164597396, 0.00011948585502764648, 0.00011947724934730296, 0.00011946868869731603, 0.00011946011335321275, 0.00011945159498436061, 0.00011944303066084459, 0.000119434501626908, 0.00011942596787663407, 0.0001194174234375755, 0.0001194088915596164, 0.00011940033690577695, 0.00011939179094990637, 0.0001193832193978331, 0.00011937463021282022, 0.00011936613606555988, 0.00011935757311665474, 0.00011934900602021677, 0.00011934045110567524, 0.00011933195342708694, 0.00011932341237715522, 0.00011931488270331356, 0.00011930631577277689, 0.00011929779797272927, 0.00011928922608885337, 0.00011928070655869204, 0.00011927216716777346, 0.00011926364296867515, 0.00011925509063745409, 0.0001192465735247119, 0.00011923803854202821, 0.00011922957766983137, 0.00011922104633697658, 0.00011921251957825801, 0.00011920403725739131, 0.00011919550139780068, 0.0001191869614143774, 0.0001191784389453928, 0.00011916992484257444, 0.00011916145155147925, 0.00011915298041710114, 0.00011914443725785271, 0.00011913593386751918, 0.00011912741938549782, 0.00011911891616106561, 0.00011911037942456803, 0.00011910191942927812, 0.00011909342668032887, 0.00011908497673391853, 0.0001190764860231854, 0.00011906801000656863, 0.00011905953126443029, 0.00011905106321107657, 0.00011904261300396418, 0.00011903415786721274, 0.00011902561312005172, 0.00011901718234709321, 0.000119008739487039, 0.00011900029385406281, 0.00011899182973494025, 0.00011898336286659592, 0.00011897496515539921, 0.00011896654850775255, 0.00011895806180235114, 0.00011894959000436777, 0.00011894110848930746, 0.00011893267849845514, 0.00011892425746627377, 0.00011891579270724614, 0.0001189073124008954, 0.00011889885859135448, 0.00011889041900192615, 0.0001188819988703519, 0.00011887357582365457, 0.00011886516550395801, 0.00011885669785350749, 0.00011884825918838649, 0.00011883978753260425, 0.00011883129113382609, 0.00011882280916846221, 0.00011881437512487779, 0.00011880598575614715, 0.00011879755545719239, 0.00011878917206090902, 0.00011878074425047396, 0.00011877231689034248, 0.00011876391467611011, 0.00011875550959415504, 0.00011874709214070199, 0.00011873864778753595, 0.00011873021791518456, 0.0001187218278117481, 0.00011871338914662709, 0.00011870494026672519, 0.00011869654013810935, 0.00011868817676848456, 0.0001186797928033967, 0.00011867139935823375, 0.00011866299681219877, 0.00011865463152285878, 0.00011864626310509402, 0.00011863783692997192, 0.00011862939326411132, 0.00011862094779703645, 0.00011861254999103898, 0.00011860424191395204, 0.00011859588551218242, 0.00011858749547984652, 0.00011857912514236655, 0.00011857078277110805, 0.00011856240828609525, 0.00011855403553119615, 0.00011854570740375043, 0.00011853733690036915, 0.00011852895537640062, 0.00011852057032110413, 0.00011851215002362856, 0.00011850381175250252, 0.00011849543833399816, 0.00011848709520433365, 0.0001184787240847475, 0.00011847037524333775, 0.00011846198641971142, 0.0001184536615628916, 0.00011844532068474491, 0.00011843695428149611, 0.00011842858704874074, 0.00011842024659719742, 0.00011841195966067787, 0.00011840363501715974, 0.00011839529916345285, 0.00011838696430515382, 0.00011837868094736261, 0.00011837041313689451, 0.00011836211115075578, 0.00011835381229304184, 0.00011834554405597035, 0.00011833723683209016, 0.00011832892863650227, 0.00011832061980100931, 0.00011831231762526909, 0.00011830401459632211, 0.00011829573052752526, 0.00011828743584104434], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433227182599538, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0032573289902280132, 0.0032573289902280132, 0.0040716612377850164, 0.0040716612377850164, 0.0040716612377850164, 0.0040716612377850164, 0.0040716612377850164, 0.0040716612620540086, 0.0040716612377850164, 0.0048859934853420191, 0.0048859934853420191, 0.0048859934853420191, 0.0048859934853420191, 0.0057003257328990227, 0.0057003257571680149, 0.0057003257328990227, 0.0057003257328990227, 0.0065146579804560263, 0.0065146580047250185, 0.0065146579804560263, 0.0073289902522820213, 0.0073289902280130291, 0.0073289902280130291, 0.0073289902280130291, 0.0073289902280130291, 0.0073289902280130291, 0.0073289902280130291, 0.0073289902280130291, 0.0089576547231270363, 0.0089576547231270363, 0.0089576547231270363, 0.0089576547231270363, 0.011400651465798045, 0.013029315960912053, 0.013843648208469055, 0.013843648208469055, 0.016286644951140065, 0.017915309446254073, 0.018729641693811076, 0.018729641693811076, 0.02035830618892508, 0.021172638436482084, 0.021172638460751077, 0.02198697070830808, 0.023615635203422088, 0.025244299674267102, 0.026872964169381109, 0.029315960912052116, 0.029315960912052116, 0.029315960936321109, 0.031758957654723127, 0.035830618892508145, 0.037459283411891142, 0.038273615635179156, 0.038273615659448146, 0.038273615635179156, 0.039087947882736153, 0.039087947882736153, 0.039087947882736153, 0.04071661237785016, 0.042345276872964167, 0.043973941368078175, 0.048045602605863193, 0.048859934853420196, 0.050488599348534204, 0.051302931620360197, 0.052931596115474204, 0.054560260586319222, 0.057817589625085215, 0.060260586367756226, 0.061889250838601237, 0.065146579853098241, 0.066775244299674269]}
[2017-12-15 01:40:42,229 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:92]: done!
[2017-12-15 01:40:42,229 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:152]: >> Executing classifier part ... 
[2017-12-15 01:40:42,229 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:97]: =======================================
[2017-12-15 01:40:42,230 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fd3fb641400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-15 01:40:42,599 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:110]: training ... 
[2017-12-15 06:46:38,286 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:122]: trained!
[2017-12-15 06:46:38,287 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:125]: Training history: 
{'val_loss': [0.00011911663146408078, 0.00011910823490173173, 0.00011909988313925931, 0.00011909146827081544, 0.00011908306271631238, 0.00011907465217411357, 0.00011906623419506375, 0.00011905784417571345, 0.00011904939317776014, 0.00011904102114271784, 0.0001190326055413151, 0.00011902425492297359, 0.0001190158371942024, 0.00011900742175369307, 0.00011899903148406411, 0.00011899063256194504, 0.0001189821955438415, 0.00011897378671783908, 0.00011896536031870071, 0.00011895692959331727, 0.00011894851971256915, 0.0001189400762408521, 0.00011893162261497308, 0.00011892317006171678, 0.0001189147014012423, 0.00011890625456864065, 0.00011889779759975406, 0.00011888936785760814, 0.00011888096680812062, 0.00011887257246252524, 0.000118864177383971, 0.00011885574510328457, 0.00011884732778568544, 0.00011883890531949713, 0.00011883051644427775, 0.00011882209143954895, 0.00011881367077894225, 0.00011880523163347027, 0.00011879682232478762, 0.00011878839585414105, 0.00011877998940578573, 0.00011877159064456005, 0.00011876319588779264, 0.00011875478870647843, 0.00011874637180005134, 0.00011873797639971027, 0.0001187295497681703, 0.00011872111055119014, 0.00011871267762693007, 0.00011870424833171028, 0.00011869581029461514, 0.00011868742865959928, 0.0001186790655809569, 0.00011867066138510933, 0.00011866226128310523, 0.0001186538539766517, 0.00011864546220535092, 0.00011863702048558435, 0.00011862862528189079, 0.00011862020513971842, 0.00011861183691248687, 0.00011860343042837745, 0.00011859506286259658, 0.0001185866826219903, 0.00011857828929538653, 0.00011856991715308197, 0.00011856151819520879, 0.00011855312601273596, 0.00011854471520238146, 0.00011853628647922715, 0.00011852795259380044, 0.00011851954693203512, 0.00011851113227811581, 0.00011850273879061863, 0.00011849439467952177, 0.00011848600986239178, 0.00011847764286867638, 0.00011846923263038735, 0.00011846087478971937, 0.00011845246658941356, 0.00011844411062583538, 0.00011843573676733442, 0.00011842737279483975, 0.00011841898454531698, 0.00011841062825995198, 0.00011840226151651523, 0.00011839396007792653, 0.00011838559267303909, 0.00011837723597650203, 0.00011836892204185838, 0.00011836054981016858, 0.0001183521793840604, 0.00011834381525067232, 0.00011833547081778864, 0.0001183271589211282, 0.00011831884686357436, 0.00011831047161066384, 0.00011830213053866475, 0.00011829377596949614, 0.00011828543750754573, 0.00011827706724233097, 0.00011826876834228278, 0.00011826043610154429, 0.00011825214603275669, 0.00011824381770709122, 0.00011823550155569389, 0.00011822718345569858, 0.00011821887670762731, 0.00011821058974944567, 0.00011820229993093671, 0.00011819392067356795, 0.00011818564973321929, 0.00011817737192808507, 0.00011816908979670577, 0.00011816078739275662, 0.00011815248391618471, 0.00011814425075003369, 0.00011813599452249367, 0.00011812766759123777, 0.00011811935422865957, 0.00011811103203482079, 0.00011810275946766096, 0.00011809449237087714, 0.00011808618717810885, 0.00011807786523454871, 0.00011806956740712326, 0.00011806128861875152, 0.00011805302929848259, 0.00011804476448996062, 0.00011803651203447724, 0.00011802820153222636, 0.00011801992471032965, 0.00011801161747957815, 0.00011800328441649557, 0.00011799497073213055, 0.00011798669546553682, 0.00011797846614295063, 0.00011797020101264183, 0.00011796198255929944, 0.00011795371571279426, 0.00011794545728637761, 0.00011793721553924463, 0.00011792897550830806, 0.00011792071513329343, 0.00011791243945552764, 0.00011790417899112777, 0.0001178959491679843, 0.00011788767545669355, 0.00011787939618564159, 0.0001178711624474251, 0.00011786296305101347, 0.00011785473919880327, 0.00011784651731306811, 0.00011783828325306479, 0.00011783007699186761, 0.00011782186949715426, 0.00011781360722717278, 0.00011780532134161388, 0.00011779704346497147, 0.00011778880302286284, 0.00011778065970674367, 0.00011777246152597115, 0.00011776423670840049, 0.00011775601687852558, 0.0001177478332496683, 0.00011773961889016938, 0.00011773140485245729, 0.00011772323325485179, 0.00011771501571323874, 0.00011770679792134704, 0.00011769857113730135, 0.00011769031026172944, 0.00011768212884962582, 0.00011767391375716803, 0.00011766573561656378, 0.00011765752468945766, 0.00011764933812876485, 0.00011764110759053955, 0.00011763294759513673, 0.00011762476135623076, 0.00011761655697212338, 0.00011760834947741003, 0.00011760017394685445, 0.00011759204216142979, 0.0001175838815939615, 0.00011757570745781549, 0.0001175675298892767, 0.00011755940529042443, 0.0001175512962267249, 0.00011754315176647482, 0.00011753501343805142, 0.00011752690167491798, 0.00011751874666721091, 0.00011751059779133053, 0.00011750244637690965, 0.00011749430182727434, 0.00011748616365974435, 0.00011747803244638515, 0.00011746989713918248, 0.00011746172071477462], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.004914004959770151, 0.0073710074167726083, 0.0073710074167726083, 0.0073710074167726083, 0.0073710074167726083, 0.0073710074167726083, 0.0073710074167726083, 0.0073710074167726083, 0.0073710074167726083, 0.0073710074167726083, 0.0073710074167726083, 0.0098280098737750649, 0.0098280098737750649, 0.0098280098737750649, 0.012285012330777521, 0.012285012330777521, 0.01474201478777998, 0.01474201478777998, 0.01474201478777998, 0.01474201478777998, 0.01474201478777998, 0.01474201478777998, 0.01474201478777998, 0.01474201478777998, 0.01474201478777998, 0.019656019701784894, 0.019656019701784894, 0.019656019701784894, 0.019656019701784894, 0.022113022158787351, 0.024570024615789807, 0.024570024615789807, 0.027027027072792264, 0.02948402952979472, 0.034398034489564869, 0.034398034489564869, 0.034398034489564869, 0.039312039238814933, 0.041769041952102716, 0.041769041952102716, 0.041769041952102716, 0.041769041952102716, 0.041769041952102716, 0.041769041952102716, 0.041769041952102716, 0.044226044409105172, 0.046683046866107629, 0.049140049323110085, 0.051597051780112542, 0.051597051780112542, 0.054054054237115005, 0.058968059151119918, 0.063882064065124824, 0.06633906593633225], 'loss': [0.00011997465198785249, 0.00011996608925224903, 0.00011995754031015482, 0.00011994904876991512, 0.00011994049767110381, 0.00011993196095830643, 0.00011992340985949512, 0.00011991484755049506, 0.00011990630979488941, 0.00011989769629348396, 0.00011988916628783969, 0.00011988060047121184, 0.00011987208767190381, 0.00011986351239890109, 0.00011985494215033815, 0.00011984640027089984, 0.00011983785374622477, 0.00011982926162238861, 0.00011982069146862642, 0.00011981210178590959, 0.00011980350534863928, 0.00011979492559630109, 0.00011978630598024722, 0.00011977769435115662, 0.00011976906677183971, 0.00011976043070785562, 0.0001197518292698457, 0.0001197431953388785, 0.00011973460250403671, 0.00011972604152224715, 0.00011971748231797166, 0.00011970892183388604, 0.00011970031992187238, 0.00011969174329795895, 0.00011968314866190291, 0.00011967459495607096, 0.00011966600197902804, 0.00011965740862278213, 0.00011964879590348289, 0.0001196402097994944, 0.00011963160227053625, 0.00011962302448531364, 0.00011961444861980622, 0.00011960587732843503, 0.00011959729458987318, 0.00011958871317852185, 0.00011958014705379156, 0.00011957155483515467, 0.00011956295228323593, 0.00011955435537196187, 0.0001195457730600034, 0.00011953717415791357, 0.00011952863308428165, 0.00011952011106560061, 0.00011951154529637315, 0.00011950297637502072, 0.0001194944164597396, 0.00011948585502764648, 0.00011947724934730296, 0.00011946868869731603, 0.00011946011335321275, 0.00011945159498436061, 0.00011944303066084459, 0.000119434501626908, 0.00011942596787663407, 0.0001194174234375755, 0.0001194088915596164, 0.00011940033690577695, 0.00011939179094990637, 0.0001193832193978331, 0.00011937463021282022, 0.00011936613606555988, 0.00011935757311665474, 0.00011934900602021677, 0.00011934045110567524, 0.00011933195342708694, 0.00011932341237715522, 0.00011931488270331356, 0.00011930631577277689, 0.00011929779797272927, 0.00011928922608885337, 0.00011928070655869204, 0.00011927216716777346, 0.00011926364296867515, 0.00011925509063745409, 0.0001192465735247119, 0.00011923803854202821, 0.00011922957766983137, 0.00011922104633697658, 0.00011921251957825801, 0.00011920403725739131, 0.00011919550139780068, 0.0001191869614143774, 0.0001191784389453928, 0.00011916992484257444, 0.00011916145155147925, 0.00011915298041710114, 0.00011914443725785271, 0.00011913593386751918, 0.00011912741938549782, 0.00011911891616106561, 0.00011911037942456803, 0.00011910191942927812, 0.00011909342668032887, 0.00011908497673391853, 0.0001190764860231854, 0.00011906801000656863, 0.00011905953126443029, 0.00011905106321107657, 0.00011904261300396418, 0.00011903415786721274, 0.00011902561312005172, 0.00011901718234709321, 0.000119008739487039, 0.00011900029385406281, 0.00011899182973494025, 0.00011898336286659592, 0.00011897496515539921, 0.00011896654850775255, 0.00011895806180235114, 0.00011894959000436777, 0.00011894110848930746, 0.00011893267849845514, 0.00011892425746627377, 0.00011891579270724614, 0.0001189073124008954, 0.00011889885859135448, 0.00011889041900192615, 0.0001188819988703519, 0.00011887357582365457, 0.00011886516550395801, 0.00011885669785350749, 0.00011884825918838649, 0.00011883978753260425, 0.00011883129113382609, 0.00011882280916846221, 0.00011881437512487779, 0.00011880598575614715, 0.00011879755545719239, 0.00011878917206090902, 0.00011878074425047396, 0.00011877231689034248, 0.00011876391467611011, 0.00011875550959415504, 0.00011874709214070199, 0.00011873864778753595, 0.00011873021791518456, 0.0001187218278117481, 0.00011871338914662709, 0.00011870494026672519, 0.00011869654013810935, 0.00011868817676848456, 0.0001186797928033967, 0.00011867139935823375, 0.00011866299681219877, 0.00011865463152285878, 0.00011864626310509402, 0.00011863783692997192, 0.00011862939326411132, 0.00011862094779703645, 0.00011861254999103898, 0.00011860424191395204, 0.00011859588551218242, 0.00011858749547984652, 0.00011857912514236655, 0.00011857078277110805, 0.00011856240828609525, 0.00011855403553119615, 0.00011854570740375043, 0.00011853733690036915, 0.00011852895537640062, 0.00011852057032110413, 0.00011851215002362856, 0.00011850381175250252, 0.00011849543833399816, 0.00011848709520433365, 0.0001184787240847475, 0.00011847037524333775, 0.00011846198641971142, 0.0001184536615628916, 0.00011844532068474491, 0.00011843695428149611, 0.00011842858704874074, 0.00011842024659719742, 0.00011841195966067787, 0.00011840363501715974, 0.00011839529916345285, 0.00011838696430515382, 0.00011837868094736261, 0.00011837041313689451, 0.00011836211115075578, 0.00011835381229304184, 0.00011834554405597035, 0.00011833723683209016, 0.00011832892863650227, 0.00011832061980100931, 0.00011831231762526909, 0.00011830401459632211, 0.00011829573052752526, 0.00011828743584104434], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433227182599538, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0032573289902280132, 0.0032573289902280132, 0.0040716612377850164, 0.0040716612377850164, 0.0040716612377850164, 0.0040716612377850164, 0.0040716612377850164, 0.0040716612620540086, 0.0040716612377850164, 0.0048859934853420191, 0.0048859934853420191, 0.0048859934853420191, 0.0048859934853420191, 0.0057003257328990227, 0.0057003257571680149, 0.0057003257328990227, 0.0057003257328990227, 0.0065146579804560263, 0.0065146580047250185, 0.0065146579804560263, 0.0073289902522820213, 0.0073289902280130291, 0.0073289902280130291, 0.0073289902280130291, 0.0073289902280130291, 0.0073289902280130291, 0.0073289902280130291, 0.0073289902280130291, 0.0089576547231270363, 0.0089576547231270363, 0.0089576547231270363, 0.0089576547231270363, 0.011400651465798045, 0.013029315960912053, 0.013843648208469055, 0.013843648208469055, 0.016286644951140065, 0.017915309446254073, 0.018729641693811076, 0.018729641693811076, 0.02035830618892508, 0.021172638436482084, 0.021172638460751077, 0.02198697070830808, 0.023615635203422088, 0.025244299674267102, 0.026872964169381109, 0.029315960912052116, 0.029315960912052116, 0.029315960936321109, 0.031758957654723127, 0.035830618892508145, 0.037459283411891142, 0.038273615635179156, 0.038273615659448146, 0.038273615635179156, 0.039087947882736153, 0.039087947882736153, 0.039087947882736153, 0.04071661237785016, 0.042345276872964167, 0.043973941368078175, 0.048045602605863193, 0.048859934853420196, 0.050488599348534204, 0.051302931620360197, 0.052931596115474204, 0.054560260586319222, 0.057817589625085215, 0.060260586367756226, 0.061889250838601237, 0.065146579853098241, 0.066775244299674269]}
[2017-12-15 06:46:38,287 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:129]: evaluating model ... 
[2017-12-15 06:46:42,186 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:133]: evaluated! 
[2017-12-15 06:46:42,188 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:135]: generating reports ... 
[2017-12-15 06:46:43,949 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:138]: done!
[2017-12-15 06:46:43,949 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_3 finished!
[2018-04-29 11:38:17,517 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_3 was already executed!
[2018-04-29 13:12:02,720 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_3
[2018-04-29 13:12:02,720 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:146]: >> Printing header log
[2018-04-29 13:12:02,720 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_3
	layers = 9216,11981
	using GLOBAL obj = 
		{'data_dir': '/home/dhiego/malware_dataset/', 'autoencoder_configs': {'discard_decoder_function': True, 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f16eaf82828>, 'loss_function': 'mse', 'hidden_layer_activation': 'relu'}, 'epochs': 200, 'numpy_seed': 666, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'shuffle_batches': True, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'batch': 32, 'mlp_configs': {'activation': 'sigmoid', 'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7f16eaf82898>, 'loss_function': 'categorical_crossentropy'}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'store_history': True, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:12:02,720 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,654 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,655 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,655 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:57]: =======================================
[2018-04-29 13:12:20,655 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f16eaf82828>, 'loss_function': 'mse', 'hidden_layer_activation': 'relu'}
[2018-04-29 13:12:20,702 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,894 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_3
[2018-04-29 13:14:12,894 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:146]: >> Printing header log
[2018-04-29 13:14:12,894 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_3
	layers = 9216,11981
	using GLOBAL obj = 
		{'autoencoder_configs': {'output_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f0acb48d828>, 'loss_function': 'mse', 'hidden_layer_activation': 'relu'}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'store_history': True, 'shuffle_batches': True, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'batch': 32, 'mlp_configs': {'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f0acb48d898>, 'activation': 'sigmoid', 'use_last_dim_as_classifier': False}, 'epochs': 200, 'numpy_seed': 666, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:14:12,894 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:148]: >> Loading dataset... 
[2018-04-29 13:14:30,856 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:30,856 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:30,856 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:57]: =======================================
[2018-04-29 13:14:30,856 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:62]: setting configurations for autoencoder: 
	 {'output_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f0acb48d828>, 'loss_function': 'mse', 'hidden_layer_activation': 'relu'}
[2018-04-29 13:14:30,903 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:34,172 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_3
[2018-04-29 13:16:34,172 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:146]: >> Printing header log
[2018-04-29 13:16:34,173 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_3
	layers = 9216,11981
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'epochs': 200, 'batch': 32, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'store_history': True, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'mlp_configs': {'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7f45be170898>}, 'shuffle_batches': True, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'autoencoder_configs': {'output_layer_activation': 'relu', 'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f45be170828>}}
	=======================================
	
[2018-04-29 13:16:34,173 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:148]: >> Loading dataset... 
[2018-04-29 13:16:58,766 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:58,768 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:58,769 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:57]: =======================================
[2018-04-29 13:16:58,770 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:62]: setting configurations for autoencoder: 
	 {'output_layer_activation': 'relu', 'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7f45be170828>}
[2018-04-29 13:16:58,901 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:20,924 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_3
[2018-04-29 14:30:20,924 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:146]: >> Printing header log
[2018-04-29 14:30:20,924 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_3
	layers = 9216,11981
	using GLOBAL obj = 
		{'epochs': 200, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'batch': 32, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'numpy_seed': 666, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'mlp_configs': {'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid', 'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'optimizer': <keras.optimizers.SGD object at 0x7fa2158138d0>}, 'store_history': True, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'shuffle_batches': True, 'autoencoder_configs': {'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7fa215813860>}, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/'}
	=======================================
	
[2018-04-29 14:30:20,925 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:148]: >> Loading dataset... 
[2018-04-29 14:30:37,326 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:37,327 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:37,327 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:57]: =======================================
[2018-04-29 14:30:37,327 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:62]: setting configurations for autoencoder: 
	 {'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'discard_decoder_function': True, 'optimizer': <keras.optimizers.SGD object at 0x7fa215813860>}
[2018-04-29 14:30:37,370 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:73]: training and evaluate autoencoder
[2018-04-29 21:42:16,787 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:85]: trained and evaluated!
[2018-04-29 21:42:16,788 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:88]: Training history: 
{'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.00011859641999852356, 0.00011858832718505861, 0.00011858018933931545, 0.00011857206023544764, 0.00011856391044783793, 0.0001185557756233155, 0.00011854763352283544, 0.00011853948675644645, 0.00011853130655998194, 0.00011852318793206295, 0.00011851497439490814, 0.00011850674623432992, 0.000118498527959758, 0.00011849026039817099, 0.00011848202921637203, 0.00011847376655309555, 0.00011846549899150853, 0.00011845720232609106, 0.00011844889209199587, 0.00011844061700417259, 0.00011843232949180254, 0.00011842407027879587, 0.000118415773452485, 0.00011840747335467473, 0.00011839917620657703, 0.00011839083320385701, 0.00011838245226604594, 0.00011837418552680303, 0.00011836582813306119, 0.00011835751668332689, 0.00011834920294533074, 0.00011834089948663589, 0.00011833259045030276, 0.00011832423327108547, 0.00011831591361578718, 0.00011830757123876377, 0.0001182991898719036, 0.00011829081651395993, 0.00011828246049675061, 0.00011827404536015105, 0.00011826565223019479, 0.00011825721928805767, 0.00011824878532692896, 0.00011824033756472092, 0.0001182318898025129, 0.00011822343124256925, 0.00011821497048374898, 0.00011820652917515445, 0.00011819811411006307, 0.00011818969046398974, 0.00011818120076223243, 0.00011817276853517714, 0.00011816436802200099, 0.00011815590595815638, 0.00011814752628961554, 0.00011813905979226359, 0.00011813060638090912, 0.00011812216500080641, 0.00011811371502184471, 0.00011810525124180371, 0.00011809677535900274, 0.00011808830226502091, 0.00011807978615886703, 0.00011807132663356292, 0.00011806288646909932, 0.00011805443584656398, 0.00011804599723740337, 0.0001180375462930812, 0.00011802910123030707, 0.00011802065060777173, 0.00011801216916520954, 0.00011800371977619035, 0.0001179952968630759, 0.00011798684616903236, 0.00011797838238899136, 0.00011796995711610686, 0.0001179615231728552, 0.00011795307451679488, 0.00011794460231666533, 0.00011793615652093234, 0.00011792768914760514, 0.00011791921914635221, 0.00011791081300190666, 0.00011790238674578465, 0.00011789390285406719, 0.00011788548314094392, 0.00011787701085142915, 0.00011786857869588204, 0.00011786010002426194, 0.00011785169640047984, 0.00011784327234323445, 0.00011783482849609944, 0.00011782636879202487, 0.00011781794800627885, 0.00011780959245387273, 0.00011780113854196096, 0.00011779273377404792, 0.00011778431338159693, 0.0001177758846584426, 0.00011776753375406837, 0.00011775913779953888], 'loss': [0.00011938128012997715, 0.00011937298539609585, 0.0001193647163058176, 0.00011935639614163494, 0.00011934807704396073, 0.00011933973251598514, 0.00011933138092535362, 0.00011932302651439979, 0.0001193146604666538, 0.0001193062865504455, 0.00011929798228908877, 0.00011928959424756862, 0.00011928117494550094, 0.00011927278782828811, 0.00011926435331069995, 0.00011925595616830773, 0.00011924753219730309, 0.00011923908964535131, 0.00011923061685196006, 0.00011922211905487082, 0.00011921366140589949, 0.00011920520276152028, 0.00011919676338539365, 0.0001191882754238823, 0.00011917979021159271, 0.00011917130248708323, 0.00011916277994699807, 0.00011915421635818785, 0.00011914577288192876, 0.00011913723275630435, 0.00011912873659452806, 0.0001191202407645544, 0.00011911174680689556, 0.00011910325671236732, 0.000119094730238051, 0.00011908623251206233, 0.00011907771554152127, 0.00011906916795033774, 0.00011906062815651595, 0.00011905211066457075, 0.0001190435446820416, 0.00011903500386911174, 0.00011902642873831015, 0.0001190178613811701, 0.00011900929655995014, 0.00011900073278153843, 0.00011899215938085053, 0.00011898357865680464, 0.00011897501518649538, 0.0001189664872901678, 0.00011895794363321542, 0.00011894932811729404, 0.00011894077135413788, 0.00011893224061378778, 0.0001189236541068961, 0.00011891513704155429, 0.00011890653557984419, 0.00011889794549422417, 0.000118889368017104, 0.00011888078383283071, 0.00011887218514404256, 0.00011886357503176396, 0.00011885496510908687, 0.00011884630932654664, 0.00011883771383728383, 0.00011882913351614113, 0.00011882054049169784, 0.00011881195934104857, 0.00011880336506049534, 0.000118794774429771, 0.00011878617846650444, 0.00011877754491474025, 0.00011876895335970859, 0.0001187603824475404, 0.00011875178572586783, 0.00011874317191635996, 0.00011873460199959964, 0.0001187260225316637, 0.00011871742853551272, 0.00011870881320919283, 0.00011870021482850713, 0.0001186915947621497, 0.000118682976805109, 0.0001186744203263551, 0.00011866584481635049, 0.00011865720742515588, 0.00011864863959401209, 0.00011864001789234172, 0.00011863143361326768, 0.00011862280605765095, 0.00011861424972109818, 0.00011860567873782943, 0.00011859708422027431, 0.00011858847389469403, 0.00011857990926307556, 0.00011857139582386247, 0.0001185627941014503, 0.00011855423987421423, 0.00011854566687642951, 0.00011853708077244103, 0.00011852858088973527], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-04-29 21:42:16,788 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:92]: done!
[2018-04-29 21:42:16,788 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:152]: >> Executing classifier part ... 
[2018-04-29 21:42:16,788 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:97]: =======================================
[2018-04-29 21:42:16,788 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:101]: setting configurations for classifier: 
	 {'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid', 'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'optimizer': <keras.optimizers.SGD object at 0x7fa2158138d0>}
[2018-04-29 21:42:16,941 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:110]: training ... 
[2018-04-30 02:44:28,221 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:122]: trained!
[2018-04-30 02:44:28,223 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:125]: Training history: 
{'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.00011859641999852356, 0.00011858832718505861, 0.00011858018933931545, 0.00011857206023544764, 0.00011856391044783793, 0.0001185557756233155, 0.00011854763352283544, 0.00011853948675644645, 0.00011853130655998194, 0.00011852318793206295, 0.00011851497439490814, 0.00011850674623432992, 0.000118498527959758, 0.00011849026039817099, 0.00011848202921637203, 0.00011847376655309555, 0.00011846549899150853, 0.00011845720232609106, 0.00011844889209199587, 0.00011844061700417259, 0.00011843232949180254, 0.00011842407027879587, 0.000118415773452485, 0.00011840747335467473, 0.00011839917620657703, 0.00011839083320385701, 0.00011838245226604594, 0.00011837418552680303, 0.00011836582813306119, 0.00011835751668332689, 0.00011834920294533074, 0.00011834089948663589, 0.00011833259045030276, 0.00011832423327108547, 0.00011831591361578718, 0.00011830757123876377, 0.0001182991898719036, 0.00011829081651395993, 0.00011828246049675061, 0.00011827404536015105, 0.00011826565223019479, 0.00011825721928805767, 0.00011824878532692896, 0.00011824033756472092, 0.0001182318898025129, 0.00011822343124256925, 0.00011821497048374898, 0.00011820652917515445, 0.00011819811411006307, 0.00011818969046398974, 0.00011818120076223243, 0.00011817276853517714, 0.00011816436802200099, 0.00011815590595815638, 0.00011814752628961554, 0.00011813905979226359, 0.00011813060638090912, 0.00011812216500080641, 0.00011811371502184471, 0.00011810525124180371, 0.00011809677535900274, 0.00011808830226502091, 0.00011807978615886703, 0.00011807132663356292, 0.00011806288646909932, 0.00011805443584656398, 0.00011804599723740337, 0.0001180375462930812, 0.00011802910123030707, 0.00011802065060777173, 0.00011801216916520954, 0.00011800371977619035, 0.0001179952968630759, 0.00011798684616903236, 0.00011797838238899136, 0.00011796995711610686, 0.0001179615231728552, 0.00011795307451679488, 0.00011794460231666533, 0.00011793615652093234, 0.00011792768914760514, 0.00011791921914635221, 0.00011791081300190666, 0.00011790238674578465, 0.00011789390285406719, 0.00011788548314094392, 0.00011787701085142915, 0.00011786857869588204, 0.00011786010002426194, 0.00011785169640047984, 0.00011784327234323445, 0.00011783482849609944, 0.00011782636879202487, 0.00011781794800627885, 0.00011780959245387273, 0.00011780113854196096, 0.00011779273377404792, 0.00011778431338159693, 0.0001177758846584426, 0.00011776753375406837, 0.00011775913779953888], 'loss': [0.00011938128012997715, 0.00011937298539609585, 0.0001193647163058176, 0.00011935639614163494, 0.00011934807704396073, 0.00011933973251598514, 0.00011933138092535362, 0.00011932302651439979, 0.0001193146604666538, 0.0001193062865504455, 0.00011929798228908877, 0.00011928959424756862, 0.00011928117494550094, 0.00011927278782828811, 0.00011926435331069995, 0.00011925595616830773, 0.00011924753219730309, 0.00011923908964535131, 0.00011923061685196006, 0.00011922211905487082, 0.00011921366140589949, 0.00011920520276152028, 0.00011919676338539365, 0.0001191882754238823, 0.00011917979021159271, 0.00011917130248708323, 0.00011916277994699807, 0.00011915421635818785, 0.00011914577288192876, 0.00011913723275630435, 0.00011912873659452806, 0.0001191202407645544, 0.00011911174680689556, 0.00011910325671236732, 0.000119094730238051, 0.00011908623251206233, 0.00011907771554152127, 0.00011906916795033774, 0.00011906062815651595, 0.00011905211066457075, 0.0001190435446820416, 0.00011903500386911174, 0.00011902642873831015, 0.0001190178613811701, 0.00011900929655995014, 0.00011900073278153843, 0.00011899215938085053, 0.00011898357865680464, 0.00011897501518649538, 0.0001189664872901678, 0.00011895794363321542, 0.00011894932811729404, 0.00011894077135413788, 0.00011893224061378778, 0.0001189236541068961, 0.00011891513704155429, 0.00011890653557984419, 0.00011889794549422417, 0.000118889368017104, 0.00011888078383283071, 0.00011887218514404256, 0.00011886357503176396, 0.00011885496510908687, 0.00011884630932654664, 0.00011883771383728383, 0.00011882913351614113, 0.00011882054049169784, 0.00011881195934104857, 0.00011880336506049534, 0.000118794774429771, 0.00011878617846650444, 0.00011877754491474025, 0.00011876895335970859, 0.0001187603824475404, 0.00011875178572586783, 0.00011874317191635996, 0.00011873460199959964, 0.0001187260225316637, 0.00011871742853551272, 0.00011870881320919283, 0.00011870021482850713, 0.0001186915947621497, 0.000118682976805109, 0.0001186744203263551, 0.00011866584481635049, 0.00011865720742515588, 0.00011864863959401209, 0.00011864001789234172, 0.00011863143361326768, 0.00011862280605765095, 0.00011861424972109818, 0.00011860567873782943, 0.00011859708422027431, 0.00011858847389469403, 0.00011857990926307556, 0.00011857139582386247, 0.0001185627941014503, 0.00011855423987421423, 0.00011854566687642951, 0.00011853708077244103, 0.00011852858088973527], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-04-30 02:44:28,223 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:129]: evaluating model ... 
[2018-04-30 02:44:31,926 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:133]: evaluated! 
[2018-04-30 02:44:31,927 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:135]: generating reports ... 
[2018-04-30 02:44:33,382 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:138]: done!
[2018-04-30 02:44:33,382 AE_BIGRAMA_1L_MINIDS_OVER_F1_3.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_3 finished!
