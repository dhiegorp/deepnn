[2017-11-12 23:49:04,589 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_0
[2017-11-12 23:49:04,590 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:146]: >> Printing header log
[2017-11-12 23:49:04,590 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_0
	layers = 9216,9216
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fb8aedfbbe0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fb8aedfb470>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-11-12 23:49:04,590 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:148]: >> Loading dataset... 
[2017-11-12 23:49:05,175 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1627, 96)
	trainy shape = (1627, 9)
	valx shape = (1076, 96)
	valy shape = (1076, 9)
	=======================================
	
[2017-11-12 23:49:05,175 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:150]: >> Executing autoencoder part ... 
[2017-11-12 23:49:05,175 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:57]: =======================================
[2017-11-12 23:49:05,175 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fb8aedfbbe0>, 'discard_decoder_function': True}
[2017-11-12 23:49:05,217 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:73]: training and evaluate autoencoder
[2017-11-13 00:02:43,487 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_0
[2017-11-13 00:02:43,488 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:146]: >> Printing header log
[2017-11-13 00:02:43,488 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_0
	layers = 9216,9216
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f984bdbdeb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f984bdc2400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-11-13 00:02:43,488 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:148]: >> Loading dataset... 
[2017-11-13 00:03:33,591 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1627, 9216)
	trainy shape = (1627, 9)
	valx shape = (1076, 9216)
	valy shape = (1076, 9)
	=======================================
	
[2017-11-13 00:03:33,591 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:150]: >> Executing autoencoder part ... 
[2017-11-13 00:03:33,592 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:57]: =======================================
[2017-11-13 00:03:33,592 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f984bdbdeb8>, 'discard_decoder_function': True}
[2017-11-13 00:03:33,654 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:73]: training and evaluate autoencoder
[2017-11-13 06:14:05,982 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:85]: trained and evaluated!
[2017-11-13 06:14:05,983 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:88]: Training history: 
{'val_loss': [0.00012042978951589768, 0.00012042496458226538, 0.00012042013699791245, 0.00012041531179379845, 0.0001204104920534147, 0.00012040566084460705, 0.00012040082914893235, 0.00012039599880566613, 0.00012039116759685848, 0.00012038633560365391, 0.00012038150634231447, 0.00012037667356471298, 0.0001203718447902406, 0.00012036701444697439, 0.00012036218375208196, 0.00012035734973026467, 0.00012035251981976917, 0.00012034768731264937, 0.00012034286186510185, 0.00012033803087267956, 0.00012033320715621487, 0.00012032837962595829, 0.0001203235557472046, 0.00012031872821694801, 0.00012031390090307677, 0.00012030908135203022, 0.00012030425547171197, 0.00012029943297241493, 0.00012029460947233561, 0.00012028978900165127, 0.00012028496122796116, 0.0001202801326157778, 0.00012027530424702794, 0.0001202704733898465, 0.00012026564461537411, 0.00012026081245988053, 0.00012025598225185516, 0.00012025115023160242, 0.0001202463214030337, 0.00012024149076223761, 0.00012023666315083652, 0.0001202318304002832, 0.00012022700265364126, 0.00012022217185055615, 0.00012021733671976392, 0.00012021250221107959, 0.00012020767162437983, 0.0001202028395229826, 0.00012019801139766628, 0.00012019318162241162, 0.00012018835368643249, 0.00012018352894213736, 0.00012017870549615438, 0.00012017388140101533, 0.00012016905963201885, 0.00012016423864741931, 0.00012015941360559433, 0.00012015459232346491, 0.00012014976779555514, 0.00012014494940757986, 0.00012014012725990903, 0.00012013531157675072, 0.00012013049440594307, 0.00012012567780314699, 0.0001201208611192064, 0.00012011603983707698, 0.00012011122242283582, 0.00012010640481925747, 0.00012010158878447294, 0.00012009677304721829, 0.00012009195685014474, 0.00012008714054487853, 0.00012008232139955452, 0.00012007750999000699, 0.00012007269352245176, 0.00012006788065230308, 0.00012006306916161104, 0.00012005826099784387, 0.00012005345056203044, 0.00012004863755664092, 0.00012004383290913579, 0.00012003902068814318, 0.0001200342078179945, 0.00012002939789609631, 0.00012002458610787441, 0.00012001977783591455, 0.00012001496986148456, 0.0001200101642672935, 0.00012000536048532979, 0.00012000055367397109, 0.00011999574843140624, 0.00011999094491992423, 0.00011998614798114742, 0.00011998135161038218, 0.0001199765517233549, 0.00011997174999705207, 0.00011996695449182826, 0.00011996215668751003, 0.00011995736426577754, 0.00011995256481152096, 0.00011994777420201585], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012098346980165985, 0.00012097852759291638, 0.00012097358783930552, 0.00012096864486584862, 0.00012096370384665938, 0.00012095876519763459, 0.0001209538217948649, 0.0001209488775379416, 0.00012094393227928843, 0.00012093898783454078, 0.00012093404567946143, 0.0001209290948352698, 0.00012092413589227104, 0.00012091919058889778, 0.00012091423957029783, 0.00012090928805977696, 0.00012090433637037577, 0.00012089938780243641, 0.00012089444050454743, 0.00012088949407422806, 0.00012088454435698255, 0.00012087960288612047, 0.00012087465588785605, 0.00012086971061578686, 0.00012086475757136712, 0.00012085980708046416, 0.00012085485917885404, 0.00012084991038731428, 0.00012084496483350855, 0.00012084001189641701, 0.00012083506393219879, 0.00012083010934493616, 0.00012082515263794158, 0.00012082019198216359, 0.00012081523072713649, 0.00012081027417219019, 0.00012080531622644929, 0.00012080035959100683, 0.00012079540274985198, 0.00012079044956232798, 0.00012078549489009721, 0.00012078054439472223, 0.00012077558768772765, 0.00012077063652602343, 0.00012076568477848613, 0.00012076073209182705, 0.00012075577987025687, 0.00012075082866383256, 0.0001207458796621084, 0.0001207409349222082, 0.00012073598599650817, 0.00012073104241038612, 0.00012072610009878647, 0.00012072115704483343, 0.00012071621738960675, 0.00012071128222427646, 0.00012070634026596548, 0.0001207013992825523, 0.00012069645804423466, 0.00012069152023147543, 0.00012068657933750243, 0.00012068164316149824, 0.00012067671029925227, 0.00012067177550062667, 0.00012066684475364067, 0.00012066190706609766, 0.00012065696819794444, 0.00012065203287162187, 0.00012064709463849384, 0.0001206421585877059, 0.00012063722165593231, 0.00012063228032370249, 0.00012062733913904895, 0.00012062239879066097, 0.00012061745868376144, 0.00012061251550459217, 0.0001206075771373039, 0.00012060264037546662, 0.00012059770659645894, 0.00012059277126566434, 0.00012058783461115527, 0.00012058290300107164, 0.00012057796633314655, 0.00012057303331438028, 0.00012056809796569766, 0.00012056316630642194, 0.00012055823606924487, 0.00012055331007600376, 0.00012054837982093866, 0.00012054345326869649, 0.0001205385289971786, 0.00012053360011502008, 0.00012052867255210401, 0.00012052375239483381, 0.00012051883413816716, 0.00012051391203110132, 0.00012050898909671392, 0.00012050407310288351, 0.00012049915386237503, 0.00012049423954107575, 0.00012048932290327613], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-11-13 06:14:05,983 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:92]: done!
[2017-11-13 06:14:05,983 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:152]: >> Executing classifier part ... 
[2017-11-13 06:14:05,983 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:97]: =======================================
[2017-11-13 06:14:05,984 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f984bdc2400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-11-13 06:14:06,031 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:110]: training ... 
[2017-11-13 16:59:39,752 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:122]: trained!
[2017-11-13 16:59:39,753 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:125]: Training history: 
{'val_loss': [0.00012042978951589768, 0.00012042496458226538, 0.00012042013699791245, 0.00012041531179379845, 0.0001204104920534147, 0.00012040566084460705, 0.00012040082914893235, 0.00012039599880566613, 0.00012039116759685848, 0.00012038633560365391, 0.00012038150634231447, 0.00012037667356471298, 0.0001203718447902406, 0.00012036701444697439, 0.00012036218375208196, 0.00012035734973026467, 0.00012035251981976917, 0.00012034768731264937, 0.00012034286186510185, 0.00012033803087267956, 0.00012033320715621487, 0.00012032837962595829, 0.0001203235557472046, 0.00012031872821694801, 0.00012031390090307677, 0.00012030908135203022, 0.00012030425547171197, 0.00012029943297241493, 0.00012029460947233561, 0.00012028978900165127, 0.00012028496122796116, 0.0001202801326157778, 0.00012027530424702794, 0.0001202704733898465, 0.00012026564461537411, 0.00012026081245988053, 0.00012025598225185516, 0.00012025115023160242, 0.0001202463214030337, 0.00012024149076223761, 0.00012023666315083652, 0.0001202318304002832, 0.00012022700265364126, 0.00012022217185055615, 0.00012021733671976392, 0.00012021250221107959, 0.00012020767162437983, 0.0001202028395229826, 0.00012019801139766628, 0.00012019318162241162, 0.00012018835368643249, 0.00012018352894213736, 0.00012017870549615438, 0.00012017388140101533, 0.00012016905963201885, 0.00012016423864741931, 0.00012015941360559433, 0.00012015459232346491, 0.00012014976779555514, 0.00012014494940757986, 0.00012014012725990903, 0.00012013531157675072, 0.00012013049440594307, 0.00012012567780314699, 0.0001201208611192064, 0.00012011603983707698, 0.00012011122242283582, 0.00012010640481925747, 0.00012010158878447294, 0.00012009677304721829, 0.00012009195685014474, 0.00012008714054487853, 0.00012008232139955452, 0.00012007750999000699, 0.00012007269352245176, 0.00012006788065230308, 0.00012006306916161104, 0.00012005826099784387, 0.00012005345056203044, 0.00012004863755664092, 0.00012004383290913579, 0.00012003902068814318, 0.0001200342078179945, 0.00012002939789609631, 0.00012002458610787441, 0.00012001977783591455, 0.00012001496986148456, 0.0001200101642672935, 0.00012000536048532979, 0.00012000055367397109, 0.00011999574843140624, 0.00011999094491992423, 0.00011998614798114742, 0.00011998135161038218, 0.0001199765517233549, 0.00011997174999705207, 0.00011996695449182826, 0.00011996215668751003, 0.00011995736426577754, 0.00011995256481152096, 0.00011994777420201585], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012098346980165985, 0.00012097852759291638, 0.00012097358783930552, 0.00012096864486584862, 0.00012096370384665938, 0.00012095876519763459, 0.0001209538217948649, 0.0001209488775379416, 0.00012094393227928843, 0.00012093898783454078, 0.00012093404567946143, 0.0001209290948352698, 0.00012092413589227104, 0.00012091919058889778, 0.00012091423957029783, 0.00012090928805977696, 0.00012090433637037577, 0.00012089938780243641, 0.00012089444050454743, 0.00012088949407422806, 0.00012088454435698255, 0.00012087960288612047, 0.00012087465588785605, 0.00012086971061578686, 0.00012086475757136712, 0.00012085980708046416, 0.00012085485917885404, 0.00012084991038731428, 0.00012084496483350855, 0.00012084001189641701, 0.00012083506393219879, 0.00012083010934493616, 0.00012082515263794158, 0.00012082019198216359, 0.00012081523072713649, 0.00012081027417219019, 0.00012080531622644929, 0.00012080035959100683, 0.00012079540274985198, 0.00012079044956232798, 0.00012078549489009721, 0.00012078054439472223, 0.00012077558768772765, 0.00012077063652602343, 0.00012076568477848613, 0.00012076073209182705, 0.00012075577987025687, 0.00012075082866383256, 0.0001207458796621084, 0.0001207409349222082, 0.00012073598599650817, 0.00012073104241038612, 0.00012072610009878647, 0.00012072115704483343, 0.00012071621738960675, 0.00012071128222427646, 0.00012070634026596548, 0.0001207013992825523, 0.00012069645804423466, 0.00012069152023147543, 0.00012068657933750243, 0.00012068164316149824, 0.00012067671029925227, 0.00012067177550062667, 0.00012066684475364067, 0.00012066190706609766, 0.00012065696819794444, 0.00012065203287162187, 0.00012064709463849384, 0.0001206421585877059, 0.00012063722165593231, 0.00012063228032370249, 0.00012062733913904895, 0.00012062239879066097, 0.00012061745868376144, 0.00012061251550459217, 0.0001206075771373039, 0.00012060264037546662, 0.00012059770659645894, 0.00012059277126566434, 0.00012058783461115527, 0.00012058290300107164, 0.00012057796633314655, 0.00012057303331438028, 0.00012056809796569766, 0.00012056316630642194, 0.00012055823606924487, 0.00012055331007600376, 0.00012054837982093866, 0.00012054345326869649, 0.0001205385289971786, 0.00012053360011502008, 0.00012052867255210401, 0.00012052375239483381, 0.00012051883413816716, 0.00012051391203110132, 0.00012050898909671392, 0.00012050407310288351, 0.00012049915386237503, 0.00012049423954107575, 0.00012048932290327613], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-11-13 16:59:39,753 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:129]: evaluating model ... 
[2017-11-13 16:59:42,933 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:133]: evaluated! 
[2017-11-13 16:59:42,933 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:135]: generating reports ... 
[2017-11-13 16:59:43,585 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:138]: done!
[2017-11-13 16:59:43,585 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_0 finished!
