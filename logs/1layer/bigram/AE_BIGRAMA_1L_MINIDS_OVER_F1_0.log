[2017-12-14 09:31:58,067 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_0
[2017-12-14 09:31:58,067 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:146]: >> Printing header log
[2017-12-14 09:31:58,067 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_0
	layers = 9216,9216
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f87ea5105c0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f87ea510ac8>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:58,067 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:148]: >> Loading dataset... 
[2017-12-14 09:32:21,761 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:21,762 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:21,762 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:57]: =======================================
[2017-12-14 09:32:21,762 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f87ea5105c0>, 'discard_decoder_function': True}
[2017-12-14 09:32:21,810 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:55,030 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_0
[2017-12-14 10:18:55,031 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:146]: >> Printing header log
[2017-12-14 10:18:55,031 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_0
	layers = 9216,9216
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f5acd407eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f5acd3ea400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:55,031 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:148]: >> Loading dataset... 
[2017-12-14 10:19:16,876 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:16,876 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:16,876 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:57]: =======================================
[2017-12-14 10:19:16,877 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f5acd407eb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:16,923 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:73]: training and evaluate autoencoder
[2017-12-14 22:36:18,700 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:85]: trained and evaluated!
[2017-12-14 22:36:18,702 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:88]: Training history: 
{'val_loss': [0.00011976905694761696, 0.00011976240588213645, 0.00011975576829594841, 0.00011974912834999035, 0.0001197424242254381, 0.00011973577283817076, 0.00011972903750029664, 0.00011972230026745569, 0.00011971561773837468, 0.00011970885190226055, 0.00011970200826524338, 0.00011969518549073858, 0.0001196883150381528, 0.0001196814493408612, 0.00011967455396767367, 0.00011966757855895238, 0.00011966058566648038, 0.00011965362327224838, 0.000119646598576512, 0.00011963953719707778, 0.00011963250533264606, 0.00011962544836884213, 0.000119618351843136, 0.00011961127190732829, 0.00011960414275601369, 0.00011959702579684428, 0.00011958992011809071, 0.00011958279375559525, 0.00011957560977538137, 0.00011956840078518051, 0.00011956119726535565, 0.00011955389874690974, 0.00011954659189776052, 0.0001195392599492391, 0.00011953191427114655, 0.00011952458845445181, 0.00011951719634967149, 0.00011950981135995537, 0.00011950240854682465, 0.00011949498031253489, 0.00011948756850725016, 0.00011948012098362806, 0.00011947262211713061, 0.00011946518644598983, 0.0001194577656485511, 0.00011945030062330123, 0.00011944284893432747, 0.00011943541326318669, 0.00011942795567478783, 0.00011942046710546872, 0.00011941297984117395, 0.00011940552707957745, 0.0001193980642532042, 0.0001193906097754113, 0.0001193831615903385, 0.0001193756489048847, 0.00011936806755369822, 0.00011936049004607658, 0.00011935296330926483, 0.00011934543501715009, 0.00011933790313174915, 0.00011933031017836, 0.00011932274264612987, 0.00011931517422004747, 0.00011930755035724624, 0.00011929994285194185, 0.00011929234525052079, 0.00011928472058325251, 0.00011927710261987637, 0.00011926945269234245, 0.00011926184994233224, 0.00011925419454442231, 0.00011924656996653926, 0.00011923893067584756, 0.00011923123065683148, 0.00011922357420417587, 0.00011921586446004694, 0.00011920815462653276, 0.00011920046032817133, 0.00011919274068015904, 0.00011918503551255381, 0.0001191773627202784, 0.00011916969564865763, 0.00011916198606542208, 0.00011915426591685252, 0.00011914656677381169, 0.0001191388572442073, 0.00011913112089903431, 0.00011912340818731577, 0.00011911565378632658, 0.00011910791851377632, 0.00011910017424907206, 0.00011909238623923689, 0.00011908458550094515, 0.00011907681082738609, 0.00011906902290693615, 0.00011906125836966203, 0.00011905347168272824, 0.00011904566390088047, 0.00011903789021055893, 0.000119030073365049, 0.00011902230065796497, 0.00011901450735652426, 0.00011900670561711796, 0.0001189989223446999, 0.00011899112262539977, 0.00011898328041236193, 0.00011897545266185409, 0.00011896767763075412, 0.00011895985059532811, 0.00011895202845821263, 0.00011894420346076983, 0.00011893638557839122, 0.00011892855829268657, 0.00011892075079687155, 0.00011891288067776531, 0.00011890503884014543, 0.00011889721466504674, 0.00011888939588881585, 0.0001188815891259597, 0.0001188737540816172, 0.00011886593015679715, 0.00011885803837071148, 0.00011885017676107902, 0.0001188422862085095, 0.00011883449857409228, 0.00011882667105598604, 0.00011881888767630571, 0.00011881104730460358, 0.00011880324778195096, 0.00011879541581246031, 0.00011878757109663607, 0.00011877971574396961, 0.0001187718769812016, 0.00011876402121736309, 0.0001187561735518263, 0.00011874832188183126, 0.00011874050078158443, 0.00011873265679871906, 0.00011872480233990489, 0.00011871696309445664, 0.0001187091183786324, 0.00011870126181032682, 0.00011869337442199441, 0.00011868549970848742, 0.00011867757569008829, 0.00011866966132529387, 0.00011866176389006174, 0.00011865390649941208, 0.00011864598650334823, 0.00011863809666586056, 0.00011863020543396331, 0.00011862231200318944, 0.00011861440335904964, 0.00011860650199086746, 0.0001185986024282669, 0.00011859073000302176, 0.0001185828258638975, 0.0001185749027572277, 0.00011856704005709545, 0.00011855916305532661, 0.0001185513274210416, 0.00011854348597671672, 0.00011853556573037424, 0.00011852768211409848, 0.00011851980241289573, 0.00011851192794966737, 0.00011850403918480245, 0.00011849618572710283, 0.0001184882873086332, 0.00011848036788463483, 0.00011847245434218451, 0.00011846457301417059, 0.00011845670427159687, 0.00011844881550673194, 0.00011844096262109779, 0.00011843306552552955, 0.0001184251598311023, 0.0001184172786639818, 0.00011840940289572911, 0.00011840151552527374, 0.00011839369183285528, 0.00011838582464558455, 0.00011837793473658868, 0.00011837006551133474, 0.00011836221157095489, 0.00011835435248198587, 0.00011834645971266269, 0.00011833858721590938, 0.00011833067033045149, 0.00011832279497337086, 0.00011831485356060623, 0.0001183069576806771, 0.00011829907831913822, 0.00011829125332169542, 0.00011828332318934664, 0.00011827542289378721, 0.0001182675449266579, 0.00011825966089921008, 0.00011825181056999348], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012051484818495992, 0.00012050796110002499, 0.00012050104401065249, 0.00012049413405503647, 0.00012048721395574013, 0.00012048024527105906, 0.00012047331711369892, 0.00012046630344606165, 0.00012045928776390844, 0.00012045232554937861, 0.00012044527751646924, 0.0001204382121113223, 0.00012043118230385724, 0.00012042409779635905, 0.00012041699380730658, 0.00012040985476567655, 0.00012040264803631056, 0.00012039542177798991, 0.00012038823329776841, 0.00012038098701278919, 0.00012037372920951876, 0.00012036651975463117, 0.00012035926000794535, 0.00012035194691213709, 0.00012034464758613787, 0.00012033730135746724, 0.00012032998231291189, 0.00012032266291285371, 0.00012031533367721764, 0.00012030795052365463, 0.00012030055438238879, 0.00012029317274563779, 0.00012028570666511814, 0.00012027823795387765, 0.00012027073738958492, 0.00012026323047364192, 0.00012025573268227116, 0.00012024816680026123, 0.00012024061118043256, 0.00012023301240256214, 0.00012022540485562229, 0.00012021780693095863, 0.00012021016881077668, 0.00012020247909528619, 0.0001201948508580825, 0.0001201872322668552, 0.00012017956620415199, 0.00012017191379275689, 0.00012016427756858996, 0.00012015661790493743, 0.00012014893819092613, 0.00012014124425680222, 0.00012013358907248515, 0.00012012592424219171, 0.00012011825851129113, 0.00012011060064885285, 0.00012010286969503582, 0.00012009507975145167, 0.00012008728883615983, 0.00012007955522792177, 0.00012007182543541393, 0.00012006408706343815, 0.00012005628252053839, 0.00012004849333536026, 0.00012004069485970855, 0.00012003283409996365, 0.00012002501209002559, 0.00012001720415799899, 0.00012000936671923877, 0.00012000152399533668, 0.00011999364899177899, 0.00011998582188630058, 0.00011997795036667047, 0.00011997010927808134, 0.00011996226072393309, 0.00011995437377548082, 0.00011994652749655058, 0.00011993862407646789, 0.00011993071044160429, 0.00011992281062395012, 0.00011991489277045312, 0.00011990698968217305, 0.0001198991240164893, 0.00011989125617038828, 0.00011988335438561854, 0.00011987543909174181, 0.00011986754882526327, 0.00011985964727749539, 0.00011985172598747118, 0.00011984382396569955, 0.00011983588504273572, 0.00011982796337350851, 0.00011982002798187264, 0.00011981205685035377, 0.00011980408332511595, 0.00011979613352376598, 0.00011978816777218971, 0.00011978022306638009, 0.00011977225454188185, 0.00011976426866884626, 0.00011975632398673682, 0.00011974833645468808, 0.00011974039442699966, 0.00011973242483599299, 0.00011972446019832554, 0.00011971651063397745, 0.00011970855151845372, 0.00011970055730295207, 0.00011969257534044744, 0.00011968464056501645, 0.00011967668007488184, 0.00011966872503579039, 0.00011966076032702237, 0.00011965280962506527, 0.00011964484695451341, 0.00011963691089927227, 0.00011962891218073496, 0.00011962094052781196, 0.00011961298785873929, 0.00011960503955050114, 0.00011959710655258422, 0.00011958914020850326, 0.00011958118170918442, 0.0001195731568493401, 0.0001195651607852238, 0.00011955713647048381, 0.00011954921700537406, 0.00011954125393191899, 0.00011953333176498783, 0.00011952535203030084, 0.00011951741834507848, 0.00011950945297270521, 0.00011950147688784713, 0.00011949348826558976, 0.000119485507677696, 0.00011947752225496396, 0.00011946953545762105, 0.00011946154088661657, 0.00011945358103638702, 0.0001194455942627443, 0.00011943758942955856, 0.00011942960384092522, 0.00011942161296715001, 0.00011941362121646786, 0.00011940559877404269, 0.0001193975881106108, 0.00011938952739238254, 0.00011938148129717007, 0.00011937345164988784, 0.00011936545646267849, 0.00011935740152729602, 0.00011934937361012749, 0.00011934134588256048, 0.00011933330957552551, 0.00011932525757896632, 0.00011931721335606868, 0.00011930917008117854, 0.0001193011581616367, 0.00011929310953050414, 0.00011928504286352879, 0.00011927704063736369, 0.00011926901857414152, 0.00011926104109097236, 0.00011925305680584934, 0.0001192450010409603, 0.00011923697781642892, 0.0001192289643089745, 0.00011922095132292422, 0.00011921292667638158, 0.00011920493347998799, 0.00011919689809726035, 0.00011918884081555931, 0.00011918077877012054, 0.00011917276765638508, 0.00011916476751583649, 0.0001191567397408691, 0.00011914874853529126, 0.00011914071274966044, 0.00011913267101528251, 0.00011912466397797932, 0.00011911665940549565, 0.00011910863580176127, 0.00011910068334599029, 0.00011909267941341168, 0.00011908465635478163, 0.00011907666019586458, 0.0001190686714077059, 0.00011906068453926241, 0.00011905265531858356, 0.00011904464389674566, 0.00011903659325109716, 0.00011902857803722922, 0.00011902049195980016, 0.00011901245570016558, 0.0001190044321201314, 0.0001189964693073784, 0.00011898839645465405, 0.00011898035898630989, 0.00011897233981451062, 0.00011896431490726593], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433227182599538, 0.00081433227182599538, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066]}
[2017-12-14 22:36:18,702 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:92]: done!
[2017-12-14 22:36:18,702 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:152]: >> Executing classifier part ... 
[2017-12-14 22:36:18,703 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:97]: =======================================
[2017-12-14 22:36:18,703 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f5acd3ea400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-14 22:36:18,969 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:110]: training ... 
[2017-12-15 04:10:03,173 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:122]: trained!
[2017-12-15 04:10:03,176 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:125]: Training history: 
{'val_loss': [0.00011976905694761696, 0.00011976240588213645, 0.00011975576829594841, 0.00011974912834999035, 0.0001197424242254381, 0.00011973577283817076, 0.00011972903750029664, 0.00011972230026745569, 0.00011971561773837468, 0.00011970885190226055, 0.00011970200826524338, 0.00011969518549073858, 0.0001196883150381528, 0.0001196814493408612, 0.00011967455396767367, 0.00011966757855895238, 0.00011966058566648038, 0.00011965362327224838, 0.000119646598576512, 0.00011963953719707778, 0.00011963250533264606, 0.00011962544836884213, 0.000119618351843136, 0.00011961127190732829, 0.00011960414275601369, 0.00011959702579684428, 0.00011958992011809071, 0.00011958279375559525, 0.00011957560977538137, 0.00011956840078518051, 0.00011956119726535565, 0.00011955389874690974, 0.00011954659189776052, 0.0001195392599492391, 0.00011953191427114655, 0.00011952458845445181, 0.00011951719634967149, 0.00011950981135995537, 0.00011950240854682465, 0.00011949498031253489, 0.00011948756850725016, 0.00011948012098362806, 0.00011947262211713061, 0.00011946518644598983, 0.0001194577656485511, 0.00011945030062330123, 0.00011944284893432747, 0.00011943541326318669, 0.00011942795567478783, 0.00011942046710546872, 0.00011941297984117395, 0.00011940552707957745, 0.0001193980642532042, 0.0001193906097754113, 0.0001193831615903385, 0.0001193756489048847, 0.00011936806755369822, 0.00011936049004607658, 0.00011935296330926483, 0.00011934543501715009, 0.00011933790313174915, 0.00011933031017836, 0.00011932274264612987, 0.00011931517422004747, 0.00011930755035724624, 0.00011929994285194185, 0.00011929234525052079, 0.00011928472058325251, 0.00011927710261987637, 0.00011926945269234245, 0.00011926184994233224, 0.00011925419454442231, 0.00011924656996653926, 0.00011923893067584756, 0.00011923123065683148, 0.00011922357420417587, 0.00011921586446004694, 0.00011920815462653276, 0.00011920046032817133, 0.00011919274068015904, 0.00011918503551255381, 0.0001191773627202784, 0.00011916969564865763, 0.00011916198606542208, 0.00011915426591685252, 0.00011914656677381169, 0.0001191388572442073, 0.00011913112089903431, 0.00011912340818731577, 0.00011911565378632658, 0.00011910791851377632, 0.00011910017424907206, 0.00011909238623923689, 0.00011908458550094515, 0.00011907681082738609, 0.00011906902290693615, 0.00011906125836966203, 0.00011905347168272824, 0.00011904566390088047, 0.00011903789021055893, 0.000119030073365049, 0.00011902230065796497, 0.00011901450735652426, 0.00011900670561711796, 0.0001189989223446999, 0.00011899112262539977, 0.00011898328041236193, 0.00011897545266185409, 0.00011896767763075412, 0.00011895985059532811, 0.00011895202845821263, 0.00011894420346076983, 0.00011893638557839122, 0.00011892855829268657, 0.00011892075079687155, 0.00011891288067776531, 0.00011890503884014543, 0.00011889721466504674, 0.00011888939588881585, 0.0001188815891259597, 0.0001188737540816172, 0.00011886593015679715, 0.00011885803837071148, 0.00011885017676107902, 0.0001188422862085095, 0.00011883449857409228, 0.00011882667105598604, 0.00011881888767630571, 0.00011881104730460358, 0.00011880324778195096, 0.00011879541581246031, 0.00011878757109663607, 0.00011877971574396961, 0.0001187718769812016, 0.00011876402121736309, 0.0001187561735518263, 0.00011874832188183126, 0.00011874050078158443, 0.00011873265679871906, 0.00011872480233990489, 0.00011871696309445664, 0.0001187091183786324, 0.00011870126181032682, 0.00011869337442199441, 0.00011868549970848742, 0.00011867757569008829, 0.00011866966132529387, 0.00011866176389006174, 0.00011865390649941208, 0.00011864598650334823, 0.00011863809666586056, 0.00011863020543396331, 0.00011862231200318944, 0.00011861440335904964, 0.00011860650199086746, 0.0001185986024282669, 0.00011859073000302176, 0.0001185828258638975, 0.0001185749027572277, 0.00011856704005709545, 0.00011855916305532661, 0.0001185513274210416, 0.00011854348597671672, 0.00011853556573037424, 0.00011852768211409848, 0.00011851980241289573, 0.00011851192794966737, 0.00011850403918480245, 0.00011849618572710283, 0.0001184882873086332, 0.00011848036788463483, 0.00011847245434218451, 0.00011846457301417059, 0.00011845670427159687, 0.00011844881550673194, 0.00011844096262109779, 0.00011843306552552955, 0.0001184251598311023, 0.0001184172786639818, 0.00011840940289572911, 0.00011840151552527374, 0.00011839369183285528, 0.00011838582464558455, 0.00011837793473658868, 0.00011837006551133474, 0.00011836221157095489, 0.00011835435248198587, 0.00011834645971266269, 0.00011833858721590938, 0.00011833067033045149, 0.00011832279497337086, 0.00011831485356060623, 0.0001183069576806771, 0.00011829907831913822, 0.00011829125332169542, 0.00011828332318934664, 0.00011827542289378721, 0.0001182675449266579, 0.00011825966089921008, 0.00011825181056999348], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012051484818495992, 0.00012050796110002499, 0.00012050104401065249, 0.00012049413405503647, 0.00012048721395574013, 0.00012048024527105906, 0.00012047331711369892, 0.00012046630344606165, 0.00012045928776390844, 0.00012045232554937861, 0.00012044527751646924, 0.0001204382121113223, 0.00012043118230385724, 0.00012042409779635905, 0.00012041699380730658, 0.00012040985476567655, 0.00012040264803631056, 0.00012039542177798991, 0.00012038823329776841, 0.00012038098701278919, 0.00012037372920951876, 0.00012036651975463117, 0.00012035926000794535, 0.00012035194691213709, 0.00012034464758613787, 0.00012033730135746724, 0.00012032998231291189, 0.00012032266291285371, 0.00012031533367721764, 0.00012030795052365463, 0.00012030055438238879, 0.00012029317274563779, 0.00012028570666511814, 0.00012027823795387765, 0.00012027073738958492, 0.00012026323047364192, 0.00012025573268227116, 0.00012024816680026123, 0.00012024061118043256, 0.00012023301240256214, 0.00012022540485562229, 0.00012021780693095863, 0.00012021016881077668, 0.00012020247909528619, 0.0001201948508580825, 0.0001201872322668552, 0.00012017956620415199, 0.00012017191379275689, 0.00012016427756858996, 0.00012015661790493743, 0.00012014893819092613, 0.00012014124425680222, 0.00012013358907248515, 0.00012012592424219171, 0.00012011825851129113, 0.00012011060064885285, 0.00012010286969503582, 0.00012009507975145167, 0.00012008728883615983, 0.00012007955522792177, 0.00012007182543541393, 0.00012006408706343815, 0.00012005628252053839, 0.00012004849333536026, 0.00012004069485970855, 0.00012003283409996365, 0.00012002501209002559, 0.00012001720415799899, 0.00012000936671923877, 0.00012000152399533668, 0.00011999364899177899, 0.00011998582188630058, 0.00011997795036667047, 0.00011997010927808134, 0.00011996226072393309, 0.00011995437377548082, 0.00011994652749655058, 0.00011993862407646789, 0.00011993071044160429, 0.00011992281062395012, 0.00011991489277045312, 0.00011990698968217305, 0.0001198991240164893, 0.00011989125617038828, 0.00011988335438561854, 0.00011987543909174181, 0.00011986754882526327, 0.00011985964727749539, 0.00011985172598747118, 0.00011984382396569955, 0.00011983588504273572, 0.00011982796337350851, 0.00011982002798187264, 0.00011981205685035377, 0.00011980408332511595, 0.00011979613352376598, 0.00011978816777218971, 0.00011978022306638009, 0.00011977225454188185, 0.00011976426866884626, 0.00011975632398673682, 0.00011974833645468808, 0.00011974039442699966, 0.00011973242483599299, 0.00011972446019832554, 0.00011971651063397745, 0.00011970855151845372, 0.00011970055730295207, 0.00011969257534044744, 0.00011968464056501645, 0.00011967668007488184, 0.00011966872503579039, 0.00011966076032702237, 0.00011965280962506527, 0.00011964484695451341, 0.00011963691089927227, 0.00011962891218073496, 0.00011962094052781196, 0.00011961298785873929, 0.00011960503955050114, 0.00011959710655258422, 0.00011958914020850326, 0.00011958118170918442, 0.0001195731568493401, 0.0001195651607852238, 0.00011955713647048381, 0.00011954921700537406, 0.00011954125393191899, 0.00011953333176498783, 0.00011952535203030084, 0.00011951741834507848, 0.00011950945297270521, 0.00011950147688784713, 0.00011949348826558976, 0.000119485507677696, 0.00011947752225496396, 0.00011946953545762105, 0.00011946154088661657, 0.00011945358103638702, 0.0001194455942627443, 0.00011943758942955856, 0.00011942960384092522, 0.00011942161296715001, 0.00011941362121646786, 0.00011940559877404269, 0.0001193975881106108, 0.00011938952739238254, 0.00011938148129717007, 0.00011937345164988784, 0.00011936545646267849, 0.00011935740152729602, 0.00011934937361012749, 0.00011934134588256048, 0.00011933330957552551, 0.00011932525757896632, 0.00011931721335606868, 0.00011930917008117854, 0.0001193011581616367, 0.00011929310953050414, 0.00011928504286352879, 0.00011927704063736369, 0.00011926901857414152, 0.00011926104109097236, 0.00011925305680584934, 0.0001192450010409603, 0.00011923697781642892, 0.0001192289643089745, 0.00011922095132292422, 0.00011921292667638158, 0.00011920493347998799, 0.00011919689809726035, 0.00011918884081555931, 0.00011918077877012054, 0.00011917276765638508, 0.00011916476751583649, 0.0001191567397408691, 0.00011914874853529126, 0.00011914071274966044, 0.00011913267101528251, 0.00011912466397797932, 0.00011911665940549565, 0.00011910863580176127, 0.00011910068334599029, 0.00011909267941341168, 0.00011908465635478163, 0.00011907666019586458, 0.0001190686714077059, 0.00011906068453926241, 0.00011905265531858356, 0.00011904464389674566, 0.00011903659325109716, 0.00011902857803722922, 0.00011902049195980016, 0.00011901245570016558, 0.0001190044321201314, 0.0001189964693073784, 0.00011898839645465405, 0.00011898035898630989, 0.00011897233981451062, 0.00011896431490726593], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433227182599538, 0.00081433227182599538, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066]}
[2017-12-15 04:10:03,176 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:129]: evaluating model ... 
[2017-12-15 04:10:10,782 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:133]: evaluated! 
[2017-12-15 04:10:10,782 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:135]: generating reports ... 
[2017-12-15 04:10:14,637 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:138]: done!
[2017-12-15 04:10:14,638 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_0 finished!
