[2017-12-14 09:31:58,067 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_0
[2017-12-14 09:31:58,067 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:146]: >> Printing header log
[2017-12-14 09:31:58,067 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_0
	layers = 9216,9216
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f87ea5105c0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f87ea510ac8>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:58,067 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:148]: >> Loading dataset... 
[2017-12-14 09:32:21,761 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:21,762 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:21,762 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:57]: =======================================
[2017-12-14 09:32:21,762 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f87ea5105c0>, 'discard_decoder_function': True}
[2017-12-14 09:32:21,810 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:55,030 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_0
[2017-12-14 10:18:55,031 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:146]: >> Printing header log
[2017-12-14 10:18:55,031 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_0
	layers = 9216,9216
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f5acd407eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f5acd3ea400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:55,031 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:148]: >> Loading dataset... 
[2017-12-14 10:19:16,876 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:16,876 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:16,876 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:57]: =======================================
[2017-12-14 10:19:16,877 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f5acd407eb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:16,923 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:73]: training and evaluate autoencoder
[2017-12-14 22:36:18,700 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:85]: trained and evaluated!
[2017-12-14 22:36:18,702 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:88]: Training history: 
{'val_loss': [0.00011976905694761696, 0.00011976240588213645, 0.00011975576829594841, 0.00011974912834999035, 0.0001197424242254381, 0.00011973577283817076, 0.00011972903750029664, 0.00011972230026745569, 0.00011971561773837468, 0.00011970885190226055, 0.00011970200826524338, 0.00011969518549073858, 0.0001196883150381528, 0.0001196814493408612, 0.00011967455396767367, 0.00011966757855895238, 0.00011966058566648038, 0.00011965362327224838, 0.000119646598576512, 0.00011963953719707778, 0.00011963250533264606, 0.00011962544836884213, 0.000119618351843136, 0.00011961127190732829, 0.00011960414275601369, 0.00011959702579684428, 0.00011958992011809071, 0.00011958279375559525, 0.00011957560977538137, 0.00011956840078518051, 0.00011956119726535565, 0.00011955389874690974, 0.00011954659189776052, 0.0001195392599492391, 0.00011953191427114655, 0.00011952458845445181, 0.00011951719634967149, 0.00011950981135995537, 0.00011950240854682465, 0.00011949498031253489, 0.00011948756850725016, 0.00011948012098362806, 0.00011947262211713061, 0.00011946518644598983, 0.0001194577656485511, 0.00011945030062330123, 0.00011944284893432747, 0.00011943541326318669, 0.00011942795567478783, 0.00011942046710546872, 0.00011941297984117395, 0.00011940552707957745, 0.0001193980642532042, 0.0001193906097754113, 0.0001193831615903385, 0.0001193756489048847, 0.00011936806755369822, 0.00011936049004607658, 0.00011935296330926483, 0.00011934543501715009, 0.00011933790313174915, 0.00011933031017836, 0.00011932274264612987, 0.00011931517422004747, 0.00011930755035724624, 0.00011929994285194185, 0.00011929234525052079, 0.00011928472058325251, 0.00011927710261987637, 0.00011926945269234245, 0.00011926184994233224, 0.00011925419454442231, 0.00011924656996653926, 0.00011923893067584756, 0.00011923123065683148, 0.00011922357420417587, 0.00011921586446004694, 0.00011920815462653276, 0.00011920046032817133, 0.00011919274068015904, 0.00011918503551255381, 0.0001191773627202784, 0.00011916969564865763, 0.00011916198606542208, 0.00011915426591685252, 0.00011914656677381169, 0.0001191388572442073, 0.00011913112089903431, 0.00011912340818731577, 0.00011911565378632658, 0.00011910791851377632, 0.00011910017424907206, 0.00011909238623923689, 0.00011908458550094515, 0.00011907681082738609, 0.00011906902290693615, 0.00011906125836966203, 0.00011905347168272824, 0.00011904566390088047, 0.00011903789021055893, 0.000119030073365049, 0.00011902230065796497, 0.00011901450735652426, 0.00011900670561711796, 0.0001189989223446999, 0.00011899112262539977, 0.00011898328041236193, 0.00011897545266185409, 0.00011896767763075412, 0.00011895985059532811, 0.00011895202845821263, 0.00011894420346076983, 0.00011893638557839122, 0.00011892855829268657, 0.00011892075079687155, 0.00011891288067776531, 0.00011890503884014543, 0.00011889721466504674, 0.00011888939588881585, 0.0001188815891259597, 0.0001188737540816172, 0.00011886593015679715, 0.00011885803837071148, 0.00011885017676107902, 0.0001188422862085095, 0.00011883449857409228, 0.00011882667105598604, 0.00011881888767630571, 0.00011881104730460358, 0.00011880324778195096, 0.00011879541581246031, 0.00011878757109663607, 0.00011877971574396961, 0.0001187718769812016, 0.00011876402121736309, 0.0001187561735518263, 0.00011874832188183126, 0.00011874050078158443, 0.00011873265679871906, 0.00011872480233990489, 0.00011871696309445664, 0.0001187091183786324, 0.00011870126181032682, 0.00011869337442199441, 0.00011868549970848742, 0.00011867757569008829, 0.00011866966132529387, 0.00011866176389006174, 0.00011865390649941208, 0.00011864598650334823, 0.00011863809666586056, 0.00011863020543396331, 0.00011862231200318944, 0.00011861440335904964, 0.00011860650199086746, 0.0001185986024282669, 0.00011859073000302176, 0.0001185828258638975, 0.0001185749027572277, 0.00011856704005709545, 0.00011855916305532661, 0.0001185513274210416, 0.00011854348597671672, 0.00011853556573037424, 0.00011852768211409848, 0.00011851980241289573, 0.00011851192794966737, 0.00011850403918480245, 0.00011849618572710283, 0.0001184882873086332, 0.00011848036788463483, 0.00011847245434218451, 0.00011846457301417059, 0.00011845670427159687, 0.00011844881550673194, 0.00011844096262109779, 0.00011843306552552955, 0.0001184251598311023, 0.0001184172786639818, 0.00011840940289572911, 0.00011840151552527374, 0.00011839369183285528, 0.00011838582464558455, 0.00011837793473658868, 0.00011837006551133474, 0.00011836221157095489, 0.00011835435248198587, 0.00011834645971266269, 0.00011833858721590938, 0.00011833067033045149, 0.00011832279497337086, 0.00011831485356060623, 0.0001183069576806771, 0.00011829907831913822, 0.00011829125332169542, 0.00011828332318934664, 0.00011827542289378721, 0.0001182675449266579, 0.00011825966089921008, 0.00011825181056999348], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012051484818495992, 0.00012050796110002499, 0.00012050104401065249, 0.00012049413405503647, 0.00012048721395574013, 0.00012048024527105906, 0.00012047331711369892, 0.00012046630344606165, 0.00012045928776390844, 0.00012045232554937861, 0.00012044527751646924, 0.0001204382121113223, 0.00012043118230385724, 0.00012042409779635905, 0.00012041699380730658, 0.00012040985476567655, 0.00012040264803631056, 0.00012039542177798991, 0.00012038823329776841, 0.00012038098701278919, 0.00012037372920951876, 0.00012036651975463117, 0.00012035926000794535, 0.00012035194691213709, 0.00012034464758613787, 0.00012033730135746724, 0.00012032998231291189, 0.00012032266291285371, 0.00012031533367721764, 0.00012030795052365463, 0.00012030055438238879, 0.00012029317274563779, 0.00012028570666511814, 0.00012027823795387765, 0.00012027073738958492, 0.00012026323047364192, 0.00012025573268227116, 0.00012024816680026123, 0.00012024061118043256, 0.00012023301240256214, 0.00012022540485562229, 0.00012021780693095863, 0.00012021016881077668, 0.00012020247909528619, 0.0001201948508580825, 0.0001201872322668552, 0.00012017956620415199, 0.00012017191379275689, 0.00012016427756858996, 0.00012015661790493743, 0.00012014893819092613, 0.00012014124425680222, 0.00012013358907248515, 0.00012012592424219171, 0.00012011825851129113, 0.00012011060064885285, 0.00012010286969503582, 0.00012009507975145167, 0.00012008728883615983, 0.00012007955522792177, 0.00012007182543541393, 0.00012006408706343815, 0.00012005628252053839, 0.00012004849333536026, 0.00012004069485970855, 0.00012003283409996365, 0.00012002501209002559, 0.00012001720415799899, 0.00012000936671923877, 0.00012000152399533668, 0.00011999364899177899, 0.00011998582188630058, 0.00011997795036667047, 0.00011997010927808134, 0.00011996226072393309, 0.00011995437377548082, 0.00011994652749655058, 0.00011993862407646789, 0.00011993071044160429, 0.00011992281062395012, 0.00011991489277045312, 0.00011990698968217305, 0.0001198991240164893, 0.00011989125617038828, 0.00011988335438561854, 0.00011987543909174181, 0.00011986754882526327, 0.00011985964727749539, 0.00011985172598747118, 0.00011984382396569955, 0.00011983588504273572, 0.00011982796337350851, 0.00011982002798187264, 0.00011981205685035377, 0.00011980408332511595, 0.00011979613352376598, 0.00011978816777218971, 0.00011978022306638009, 0.00011977225454188185, 0.00011976426866884626, 0.00011975632398673682, 0.00011974833645468808, 0.00011974039442699966, 0.00011973242483599299, 0.00011972446019832554, 0.00011971651063397745, 0.00011970855151845372, 0.00011970055730295207, 0.00011969257534044744, 0.00011968464056501645, 0.00011967668007488184, 0.00011966872503579039, 0.00011966076032702237, 0.00011965280962506527, 0.00011964484695451341, 0.00011963691089927227, 0.00011962891218073496, 0.00011962094052781196, 0.00011961298785873929, 0.00011960503955050114, 0.00011959710655258422, 0.00011958914020850326, 0.00011958118170918442, 0.0001195731568493401, 0.0001195651607852238, 0.00011955713647048381, 0.00011954921700537406, 0.00011954125393191899, 0.00011953333176498783, 0.00011952535203030084, 0.00011951741834507848, 0.00011950945297270521, 0.00011950147688784713, 0.00011949348826558976, 0.000119485507677696, 0.00011947752225496396, 0.00011946953545762105, 0.00011946154088661657, 0.00011945358103638702, 0.0001194455942627443, 0.00011943758942955856, 0.00011942960384092522, 0.00011942161296715001, 0.00011941362121646786, 0.00011940559877404269, 0.0001193975881106108, 0.00011938952739238254, 0.00011938148129717007, 0.00011937345164988784, 0.00011936545646267849, 0.00011935740152729602, 0.00011934937361012749, 0.00011934134588256048, 0.00011933330957552551, 0.00011932525757896632, 0.00011931721335606868, 0.00011930917008117854, 0.0001193011581616367, 0.00011929310953050414, 0.00011928504286352879, 0.00011927704063736369, 0.00011926901857414152, 0.00011926104109097236, 0.00011925305680584934, 0.0001192450010409603, 0.00011923697781642892, 0.0001192289643089745, 0.00011922095132292422, 0.00011921292667638158, 0.00011920493347998799, 0.00011919689809726035, 0.00011918884081555931, 0.00011918077877012054, 0.00011917276765638508, 0.00011916476751583649, 0.0001191567397408691, 0.00011914874853529126, 0.00011914071274966044, 0.00011913267101528251, 0.00011912466397797932, 0.00011911665940549565, 0.00011910863580176127, 0.00011910068334599029, 0.00011909267941341168, 0.00011908465635478163, 0.00011907666019586458, 0.0001190686714077059, 0.00011906068453926241, 0.00011905265531858356, 0.00011904464389674566, 0.00011903659325109716, 0.00011902857803722922, 0.00011902049195980016, 0.00011901245570016558, 0.0001190044321201314, 0.0001189964693073784, 0.00011898839645465405, 0.00011898035898630989, 0.00011897233981451062, 0.00011896431490726593], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433227182599538, 0.00081433227182599538, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066]}
[2017-12-14 22:36:18,702 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:92]: done!
[2017-12-14 22:36:18,702 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:152]: >> Executing classifier part ... 
[2017-12-14 22:36:18,703 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:97]: =======================================
[2017-12-14 22:36:18,703 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f5acd3ea400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-14 22:36:18,969 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:110]: training ... 
[2017-12-15 04:10:03,173 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:122]: trained!
[2017-12-15 04:10:03,176 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:125]: Training history: 
{'val_loss': [0.00011976905694761696, 0.00011976240588213645, 0.00011975576829594841, 0.00011974912834999035, 0.0001197424242254381, 0.00011973577283817076, 0.00011972903750029664, 0.00011972230026745569, 0.00011971561773837468, 0.00011970885190226055, 0.00011970200826524338, 0.00011969518549073858, 0.0001196883150381528, 0.0001196814493408612, 0.00011967455396767367, 0.00011966757855895238, 0.00011966058566648038, 0.00011965362327224838, 0.000119646598576512, 0.00011963953719707778, 0.00011963250533264606, 0.00011962544836884213, 0.000119618351843136, 0.00011961127190732829, 0.00011960414275601369, 0.00011959702579684428, 0.00011958992011809071, 0.00011958279375559525, 0.00011957560977538137, 0.00011956840078518051, 0.00011956119726535565, 0.00011955389874690974, 0.00011954659189776052, 0.0001195392599492391, 0.00011953191427114655, 0.00011952458845445181, 0.00011951719634967149, 0.00011950981135995537, 0.00011950240854682465, 0.00011949498031253489, 0.00011948756850725016, 0.00011948012098362806, 0.00011947262211713061, 0.00011946518644598983, 0.0001194577656485511, 0.00011945030062330123, 0.00011944284893432747, 0.00011943541326318669, 0.00011942795567478783, 0.00011942046710546872, 0.00011941297984117395, 0.00011940552707957745, 0.0001193980642532042, 0.0001193906097754113, 0.0001193831615903385, 0.0001193756489048847, 0.00011936806755369822, 0.00011936049004607658, 0.00011935296330926483, 0.00011934543501715009, 0.00011933790313174915, 0.00011933031017836, 0.00011932274264612987, 0.00011931517422004747, 0.00011930755035724624, 0.00011929994285194185, 0.00011929234525052079, 0.00011928472058325251, 0.00011927710261987637, 0.00011926945269234245, 0.00011926184994233224, 0.00011925419454442231, 0.00011924656996653926, 0.00011923893067584756, 0.00011923123065683148, 0.00011922357420417587, 0.00011921586446004694, 0.00011920815462653276, 0.00011920046032817133, 0.00011919274068015904, 0.00011918503551255381, 0.0001191773627202784, 0.00011916969564865763, 0.00011916198606542208, 0.00011915426591685252, 0.00011914656677381169, 0.0001191388572442073, 0.00011913112089903431, 0.00011912340818731577, 0.00011911565378632658, 0.00011910791851377632, 0.00011910017424907206, 0.00011909238623923689, 0.00011908458550094515, 0.00011907681082738609, 0.00011906902290693615, 0.00011906125836966203, 0.00011905347168272824, 0.00011904566390088047, 0.00011903789021055893, 0.000119030073365049, 0.00011902230065796497, 0.00011901450735652426, 0.00011900670561711796, 0.0001189989223446999, 0.00011899112262539977, 0.00011898328041236193, 0.00011897545266185409, 0.00011896767763075412, 0.00011895985059532811, 0.00011895202845821263, 0.00011894420346076983, 0.00011893638557839122, 0.00011892855829268657, 0.00011892075079687155, 0.00011891288067776531, 0.00011890503884014543, 0.00011889721466504674, 0.00011888939588881585, 0.0001188815891259597, 0.0001188737540816172, 0.00011886593015679715, 0.00011885803837071148, 0.00011885017676107902, 0.0001188422862085095, 0.00011883449857409228, 0.00011882667105598604, 0.00011881888767630571, 0.00011881104730460358, 0.00011880324778195096, 0.00011879541581246031, 0.00011878757109663607, 0.00011877971574396961, 0.0001187718769812016, 0.00011876402121736309, 0.0001187561735518263, 0.00011874832188183126, 0.00011874050078158443, 0.00011873265679871906, 0.00011872480233990489, 0.00011871696309445664, 0.0001187091183786324, 0.00011870126181032682, 0.00011869337442199441, 0.00011868549970848742, 0.00011867757569008829, 0.00011866966132529387, 0.00011866176389006174, 0.00011865390649941208, 0.00011864598650334823, 0.00011863809666586056, 0.00011863020543396331, 0.00011862231200318944, 0.00011861440335904964, 0.00011860650199086746, 0.0001185986024282669, 0.00011859073000302176, 0.0001185828258638975, 0.0001185749027572277, 0.00011856704005709545, 0.00011855916305532661, 0.0001185513274210416, 0.00011854348597671672, 0.00011853556573037424, 0.00011852768211409848, 0.00011851980241289573, 0.00011851192794966737, 0.00011850403918480245, 0.00011849618572710283, 0.0001184882873086332, 0.00011848036788463483, 0.00011847245434218451, 0.00011846457301417059, 0.00011845670427159687, 0.00011844881550673194, 0.00011844096262109779, 0.00011843306552552955, 0.0001184251598311023, 0.0001184172786639818, 0.00011840940289572911, 0.00011840151552527374, 0.00011839369183285528, 0.00011838582464558455, 0.00011837793473658868, 0.00011837006551133474, 0.00011836221157095489, 0.00011835435248198587, 0.00011834645971266269, 0.00011833858721590938, 0.00011833067033045149, 0.00011832279497337086, 0.00011831485356060623, 0.0001183069576806771, 0.00011829907831913822, 0.00011829125332169542, 0.00011828332318934664, 0.00011827542289378721, 0.0001182675449266579, 0.00011825966089921008, 0.00011825181056999348], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012051484818495992, 0.00012050796110002499, 0.00012050104401065249, 0.00012049413405503647, 0.00012048721395574013, 0.00012048024527105906, 0.00012047331711369892, 0.00012046630344606165, 0.00012045928776390844, 0.00012045232554937861, 0.00012044527751646924, 0.0001204382121113223, 0.00012043118230385724, 0.00012042409779635905, 0.00012041699380730658, 0.00012040985476567655, 0.00012040264803631056, 0.00012039542177798991, 0.00012038823329776841, 0.00012038098701278919, 0.00012037372920951876, 0.00012036651975463117, 0.00012035926000794535, 0.00012035194691213709, 0.00012034464758613787, 0.00012033730135746724, 0.00012032998231291189, 0.00012032266291285371, 0.00012031533367721764, 0.00012030795052365463, 0.00012030055438238879, 0.00012029317274563779, 0.00012028570666511814, 0.00012027823795387765, 0.00012027073738958492, 0.00012026323047364192, 0.00012025573268227116, 0.00012024816680026123, 0.00012024061118043256, 0.00012023301240256214, 0.00012022540485562229, 0.00012021780693095863, 0.00012021016881077668, 0.00012020247909528619, 0.0001201948508580825, 0.0001201872322668552, 0.00012017956620415199, 0.00012017191379275689, 0.00012016427756858996, 0.00012015661790493743, 0.00012014893819092613, 0.00012014124425680222, 0.00012013358907248515, 0.00012012592424219171, 0.00012011825851129113, 0.00012011060064885285, 0.00012010286969503582, 0.00012009507975145167, 0.00012008728883615983, 0.00012007955522792177, 0.00012007182543541393, 0.00012006408706343815, 0.00012005628252053839, 0.00012004849333536026, 0.00012004069485970855, 0.00012003283409996365, 0.00012002501209002559, 0.00012001720415799899, 0.00012000936671923877, 0.00012000152399533668, 0.00011999364899177899, 0.00011998582188630058, 0.00011997795036667047, 0.00011997010927808134, 0.00011996226072393309, 0.00011995437377548082, 0.00011994652749655058, 0.00011993862407646789, 0.00011993071044160429, 0.00011992281062395012, 0.00011991489277045312, 0.00011990698968217305, 0.0001198991240164893, 0.00011989125617038828, 0.00011988335438561854, 0.00011987543909174181, 0.00011986754882526327, 0.00011985964727749539, 0.00011985172598747118, 0.00011984382396569955, 0.00011983588504273572, 0.00011982796337350851, 0.00011982002798187264, 0.00011981205685035377, 0.00011980408332511595, 0.00011979613352376598, 0.00011978816777218971, 0.00011978022306638009, 0.00011977225454188185, 0.00011976426866884626, 0.00011975632398673682, 0.00011974833645468808, 0.00011974039442699966, 0.00011973242483599299, 0.00011972446019832554, 0.00011971651063397745, 0.00011970855151845372, 0.00011970055730295207, 0.00011969257534044744, 0.00011968464056501645, 0.00011967668007488184, 0.00011966872503579039, 0.00011966076032702237, 0.00011965280962506527, 0.00011964484695451341, 0.00011963691089927227, 0.00011962891218073496, 0.00011962094052781196, 0.00011961298785873929, 0.00011960503955050114, 0.00011959710655258422, 0.00011958914020850326, 0.00011958118170918442, 0.0001195731568493401, 0.0001195651607852238, 0.00011955713647048381, 0.00011954921700537406, 0.00011954125393191899, 0.00011953333176498783, 0.00011952535203030084, 0.00011951741834507848, 0.00011950945297270521, 0.00011950147688784713, 0.00011949348826558976, 0.000119485507677696, 0.00011947752225496396, 0.00011946953545762105, 0.00011946154088661657, 0.00011945358103638702, 0.0001194455942627443, 0.00011943758942955856, 0.00011942960384092522, 0.00011942161296715001, 0.00011941362121646786, 0.00011940559877404269, 0.0001193975881106108, 0.00011938952739238254, 0.00011938148129717007, 0.00011937345164988784, 0.00011936545646267849, 0.00011935740152729602, 0.00011934937361012749, 0.00011934134588256048, 0.00011933330957552551, 0.00011932525757896632, 0.00011931721335606868, 0.00011930917008117854, 0.0001193011581616367, 0.00011929310953050414, 0.00011928504286352879, 0.00011927704063736369, 0.00011926901857414152, 0.00011926104109097236, 0.00011925305680584934, 0.0001192450010409603, 0.00011923697781642892, 0.0001192289643089745, 0.00011922095132292422, 0.00011921292667638158, 0.00011920493347998799, 0.00011919689809726035, 0.00011918884081555931, 0.00011918077877012054, 0.00011917276765638508, 0.00011916476751583649, 0.0001191567397408691, 0.00011914874853529126, 0.00011914071274966044, 0.00011913267101528251, 0.00011912466397797932, 0.00011911665940549565, 0.00011910863580176127, 0.00011910068334599029, 0.00011909267941341168, 0.00011908465635478163, 0.00011907666019586458, 0.0001190686714077059, 0.00011906068453926241, 0.00011905265531858356, 0.00011904464389674566, 0.00011903659325109716, 0.00011902857803722922, 0.00011902049195980016, 0.00011901245570016558, 0.0001190044321201314, 0.0001189964693073784, 0.00011898839645465405, 0.00011898035898630989, 0.00011897233981451062, 0.00011896431490726593], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433227182599538, 0.00081433227182599538, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066]}
[2017-12-15 04:10:03,176 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:129]: evaluating model ... 
[2017-12-15 04:10:10,782 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:133]: evaluated! 
[2017-12-15 04:10:10,782 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:135]: generating reports ... 
[2017-12-15 04:10:14,637 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:138]: done!
[2017-12-15 04:10:14,638 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_0 finished!
[2018-04-29 11:38:17,355 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_0 was already executed!
[2018-04-29 13:12:02,734 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_0
[2018-04-29 13:12:02,734 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:146]: >> Printing header log
[2018-04-29 13:12:02,734 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_0
	layers = 9216,9216
	using GLOBAL obj = 
		{'batch': 32, 'epochs': 200, 'shuffle_batches': True, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'store_history': True, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_dir': '/home/dhiego/malware_dataset/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'autoencoder_configs': {'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f08ae02f828>, 'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'loss_function': 'mse'}, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'numpy_seed': 666, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'mlp_configs': {'optimizer': <keras.optimizers.SGD object at 0x7f08ae02f898>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy'}}
	=======================================
	
[2018-04-29 13:12:02,734 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,700 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,700 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,700 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:57]: =======================================
[2018-04-29 13:12:20,701 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:62]: setting configurations for autoencoder: 
	 {'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f08ae02f828>, 'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'loss_function': 'mse'}
[2018-04-29 13:12:20,747 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,841 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_0
[2018-04-29 13:14:12,841 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:146]: >> Printing header log
[2018-04-29 13:14:12,842 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_0
	layers = 9216,9216
	using GLOBAL obj = 
		{'shuffle_batches': True, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'batch': 32, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f98906f1828>, 'loss_function': 'mse'}, 'epochs': 200, 'mlp_configs': {'activation': 'sigmoid', 'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'optimizer': <keras.optimizers.SGD object at 0x7f98906f1898>, 'loss_function': 'categorical_crossentropy'}, 'numpy_seed': 666, 'store_history': True, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:14:12,842 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:148]: >> Loading dataset... 
[2018-04-29 13:14:30,852 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:30,853 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:30,853 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:57]: =======================================
[2018-04-29 13:14:30,853 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f98906f1828>, 'loss_function': 'mse'}
[2018-04-29 13:14:30,900 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,700 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_0
[2018-04-29 13:16:33,700 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:146]: >> Printing header log
[2018-04-29 13:16:33,700 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_0
	layers = 9216,9216
	using GLOBAL obj = 
		{'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'mlp_configs': {'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7fe4bd772898>}, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'batch': 32, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'numpy_seed': 666, 'data_dir': '/home/dhiego/malware_dataset/', 'shuffle_batches': True, 'epochs': 200, 'store_history': True, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'autoencoder_configs': {'loss_function': 'mse', 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fe4bd772828>}, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:16:33,700 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:148]: >> Loading dataset... 
[2018-04-29 13:16:58,248 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:58,248 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:58,248 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:57]: =======================================
[2018-04-29 13:16:58,249 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:62]: setting configurations for autoencoder: 
	 {'loss_function': 'mse', 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fe4bd772828>}
[2018-04-29 13:16:58,357 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:21,166 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_0
[2018-04-29 14:30:21,166 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:146]: >> Printing header log
[2018-04-29 14:30:21,166 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_0
	layers = 9216,9216
	using GLOBAL obj = 
		{'mlp_configs': {'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7fdac2f51908>, 'classifier_dim': 9, 'activation': 'sigmoid'}, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'store_history': True, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'autoencoder_configs': {'loss_function': 'mse', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fdac2f51898>, 'output_layer_activation': 'relu'}, 'numpy_seed': 666, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'batch': 32, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'shuffle_batches': True, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'epochs': 200}
	=======================================
	
[2018-04-29 14:30:21,166 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:148]: >> Loading dataset... 
[2018-04-29 14:30:39,270 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:39,270 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:39,270 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:57]: =======================================
[2018-04-29 14:30:39,270 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:62]: setting configurations for autoencoder: 
	 {'loss_function': 'mse', 'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fdac2f51898>, 'output_layer_activation': 'relu'}
[2018-04-29 14:30:39,373 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:73]: training and evaluate autoencoder
[2018-04-29 20:40:39,877 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:85]: trained and evaluated!
[2018-04-29 20:40:39,879 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:88]: Training history: 
{'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.00012042240833698289, 0.00012041579967585486, 0.00012040919453650484, 0.00012040252652358498, 0.00012039573517692658, 0.00012038887042711839, 0.00012038203503141931, 0.00012037520585693213, 0.00012036826575537617, 0.00012036127490088737, 0.00012035432164182576, 0.00012034730182652286, 0.00012034024940348855, 0.00012033319894692927, 0.00012032614343116605, 0.00012031903256806922, 0.00012031194070827202, 0.00012030477701850506, 0.00012029760220921564, 0.00012029035816212809, 0.00012028305281465072, 0.00012027573046610286, 0.00012026834224064146, 0.0001202608888712254, 0.00012025335889866837, 0.0001202458390802733, 0.00012023828034354968, 0.00012023073288724192, 0.00012022314039865596, 0.00012021554717711112, 0.000120207844011735, 0.00012020012645533707, 0.00012019237162529878, 0.0001201846114857779, 0.00012017682617537663, 0.00012016903065718226, 0.00012016121633233576, 0.00012015333661325596, 0.0001201454850862773, 0.000120137616897892, 0.00012012976596085583, 0.00012012183368326158, 0.00012011388252750703, 0.00012010588894952295, 0.00012009787942521407, 0.00012008985045067942, 0.00012008179710973146, 0.00012007375136652792, 0.00012006571028923332, 0.00012005767362756901, 0.00012004961154474569, 0.00012004151683631387, 0.00012003347445399494, 0.00012002539994662481, 0.00012001729731866174, 0.00012000917670639067, 0.00012000106598012588, 0.00011999294257903568, 0.00011998484093431012, 0.00011997671018575412, 0.00011996856227523421, 0.00011996043669314442, 0.00011995227571450408, 0.00011994408432700893, 0.00011993588144457339, 0.00011992766923031996, 0.00011991943232786638, 0.00011991123336050385, 0.00011990300757757272, 0.0001198948391084502, 0.00011988659403618672, 0.00011987830588024302, 0.000119869994931066, 0.00011986173212477314, 0.00011985347226819285, 0.00011984517309998896, 0.00011983692665119296, 0.0001198286848683059, 0.00011982041398158836, 0.00011981208692519314, 0.00011980380516923182, 0.00011979551261553486, 0.00011978721770206786, 0.00011977893537404107, 0.00011977063849409905, 0.00011976230196286958, 0.00011975402510521879, 0.00011974573264090706, 0.00011973740887388825, 0.00011972907055495422, 0.00011972071841706382, 0.00011971234110829304, 0.00011970397819054407, 0.0001196956322738656, 0.00011968731202862479, 0.00011967891689643942, 0.00011967056100436942, 0.00011966224403062799, 0.00011965387726931419, 0.00011964550224880526, 0.00011963713500481122], 'loss': [0.00012124630231323048, 0.00012123950609481509, 0.0001212326627604266, 0.00012122580769444521, 0.00012121887828097512, 0.00012121185856979, 0.00012120480015619843, 0.00012119780149077997, 0.0001211907878468429, 0.00012118366658035363, 0.00012117649009242708, 0.00012116933282535275, 0.00012116209433773526, 0.00012115481577310044, 0.00012114754635673805, 0.00012114024987476134, 0.00012113287613017284, 0.0001211255303281056, 0.00012111810430090311, 0.00012111066874622516, 0.00012110316185398233, 0.00012109559613787372, 0.00012108804736739929, 0.00012108042714085903, 0.00012107273505534977, 0.00012106497836312892, 0.00012105721768927655, 0.00012104943357593857, 0.00012104165526914656, 0.00012103386297924383, 0.00012102600989835974, 0.00012101806711226532, 0.00012101009958317497, 0.00012100210598387819, 0.00012099411188687746, 0.00012098608769063841, 0.00012097806676502525, 0.00012097000535949154, 0.00012096186979607063, 0.00012095376120346328, 0.00012094562556894181, 0.00012093753548618103, 0.00012092937598557057, 0.00012092118448970676, 0.00012091295914997495, 0.00012090471077366073, 0.00012089644926744256, 0.00012088813332189329, 0.00012087981913015793, 0.000120871506004931, 0.00012086319643473222, 0.00012085486276144259, 0.00012084649093085081, 0.00012083815519564485, 0.00012082978857909435, 0.00012082138866378018, 0.00012081296301006222, 0.00012080453809105006, 0.00012079610466367053, 0.00012078770460615524, 0.00012077928414277836, 0.00012077086183078684, 0.00012076248210803257, 0.00012075406953681819, 0.00012074562582355721, 0.00012073718948105459, 0.00012072874946502289, 0.00012072026297292318, 0.000120711818098353, 0.00012070333760240076, 0.00012069492315887155, 0.00012068642687859432, 0.00012067790950515008, 0.00012066939727464655, 0.0001206609285102872, 0.00012065246154714212, 0.00012064397953437788, 0.00012063554103515818, 0.00012062710869798728, 0.00012061864187704333, 0.00012061011571082945, 0.00012060162855512447, 0.00012059313606687728, 0.000120584624120776, 0.00012057613326784174, 0.00012056762144024142, 0.0001205590745837637, 0.00012055058493953901, 0.00012054206647588614, 0.00012053352798557467, 0.00012052498518182905, 0.00012051643315871043, 0.00012050786208064093, 0.00012049931318594706, 0.00012049078171089116, 0.00012048227151860377, 0.0001204736799872723, 0.00012046511962168762, 0.00012045660859989367, 0.00012044803920453748, 0.00012043946428703757]}
[2018-04-29 20:40:39,880 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:92]: done!
[2018-04-29 20:40:39,880 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:152]: >> Executing classifier part ... 
[2018-04-29 20:40:39,880 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:97]: =======================================
[2018-04-29 20:40:39,880 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:101]: setting configurations for classifier: 
	 {'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False, 'optimizer': <keras.optimizers.SGD object at 0x7fdac2f51908>, 'classifier_dim': 9, 'activation': 'sigmoid'}
[2018-04-29 20:40:40,018 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:110]: training ... 
[2018-04-30 01:21:16,167 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:122]: trained!
[2018-04-30 01:21:16,169 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:125]: Training history: 
{'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.00012042240833698289, 0.00012041579967585486, 0.00012040919453650484, 0.00012040252652358498, 0.00012039573517692658, 0.00012038887042711839, 0.00012038203503141931, 0.00012037520585693213, 0.00012036826575537617, 0.00012036127490088737, 0.00012035432164182576, 0.00012034730182652286, 0.00012034024940348855, 0.00012033319894692927, 0.00012032614343116605, 0.00012031903256806922, 0.00012031194070827202, 0.00012030477701850506, 0.00012029760220921564, 0.00012029035816212809, 0.00012028305281465072, 0.00012027573046610286, 0.00012026834224064146, 0.0001202608888712254, 0.00012025335889866837, 0.0001202458390802733, 0.00012023828034354968, 0.00012023073288724192, 0.00012022314039865596, 0.00012021554717711112, 0.000120207844011735, 0.00012020012645533707, 0.00012019237162529878, 0.0001201846114857779, 0.00012017682617537663, 0.00012016903065718226, 0.00012016121633233576, 0.00012015333661325596, 0.0001201454850862773, 0.000120137616897892, 0.00012012976596085583, 0.00012012183368326158, 0.00012011388252750703, 0.00012010588894952295, 0.00012009787942521407, 0.00012008985045067942, 0.00012008179710973146, 0.00012007375136652792, 0.00012006571028923332, 0.00012005767362756901, 0.00012004961154474569, 0.00012004151683631387, 0.00012003347445399494, 0.00012002539994662481, 0.00012001729731866174, 0.00012000917670639067, 0.00012000106598012588, 0.00011999294257903568, 0.00011998484093431012, 0.00011997671018575412, 0.00011996856227523421, 0.00011996043669314442, 0.00011995227571450408, 0.00011994408432700893, 0.00011993588144457339, 0.00011992766923031996, 0.00011991943232786638, 0.00011991123336050385, 0.00011990300757757272, 0.0001198948391084502, 0.00011988659403618672, 0.00011987830588024302, 0.000119869994931066, 0.00011986173212477314, 0.00011985347226819285, 0.00011984517309998896, 0.00011983692665119296, 0.0001198286848683059, 0.00011982041398158836, 0.00011981208692519314, 0.00011980380516923182, 0.00011979551261553486, 0.00011978721770206786, 0.00011977893537404107, 0.00011977063849409905, 0.00011976230196286958, 0.00011975402510521879, 0.00011974573264090706, 0.00011973740887388825, 0.00011972907055495422, 0.00011972071841706382, 0.00011971234110829304, 0.00011970397819054407, 0.0001196956322738656, 0.00011968731202862479, 0.00011967891689643942, 0.00011967056100436942, 0.00011966224403062799, 0.00011965387726931419, 0.00011964550224880526, 0.00011963713500481122], 'loss': [0.00012124630231323048, 0.00012123950609481509, 0.0001212326627604266, 0.00012122580769444521, 0.00012121887828097512, 0.00012121185856979, 0.00012120480015619843, 0.00012119780149077997, 0.0001211907878468429, 0.00012118366658035363, 0.00012117649009242708, 0.00012116933282535275, 0.00012116209433773526, 0.00012115481577310044, 0.00012114754635673805, 0.00012114024987476134, 0.00012113287613017284, 0.0001211255303281056, 0.00012111810430090311, 0.00012111066874622516, 0.00012110316185398233, 0.00012109559613787372, 0.00012108804736739929, 0.00012108042714085903, 0.00012107273505534977, 0.00012106497836312892, 0.00012105721768927655, 0.00012104943357593857, 0.00012104165526914656, 0.00012103386297924383, 0.00012102600989835974, 0.00012101806711226532, 0.00012101009958317497, 0.00012100210598387819, 0.00012099411188687746, 0.00012098608769063841, 0.00012097806676502525, 0.00012097000535949154, 0.00012096186979607063, 0.00012095376120346328, 0.00012094562556894181, 0.00012093753548618103, 0.00012092937598557057, 0.00012092118448970676, 0.00012091295914997495, 0.00012090471077366073, 0.00012089644926744256, 0.00012088813332189329, 0.00012087981913015793, 0.000120871506004931, 0.00012086319643473222, 0.00012085486276144259, 0.00012084649093085081, 0.00012083815519564485, 0.00012082978857909435, 0.00012082138866378018, 0.00012081296301006222, 0.00012080453809105006, 0.00012079610466367053, 0.00012078770460615524, 0.00012077928414277836, 0.00012077086183078684, 0.00012076248210803257, 0.00012075406953681819, 0.00012074562582355721, 0.00012073718948105459, 0.00012072874946502289, 0.00012072026297292318, 0.000120711818098353, 0.00012070333760240076, 0.00012069492315887155, 0.00012068642687859432, 0.00012067790950515008, 0.00012066939727464655, 0.0001206609285102872, 0.00012065246154714212, 0.00012064397953437788, 0.00012063554103515818, 0.00012062710869798728, 0.00012061864187704333, 0.00012061011571082945, 0.00012060162855512447, 0.00012059313606687728, 0.000120584624120776, 0.00012057613326784174, 0.00012056762144024142, 0.0001205590745837637, 0.00012055058493953901, 0.00012054206647588614, 0.00012053352798557467, 0.00012052498518182905, 0.00012051643315871043, 0.00012050786208064093, 0.00012049931318594706, 0.00012049078171089116, 0.00012048227151860377, 0.0001204736799872723, 0.00012046511962168762, 0.00012045660859989367, 0.00012044803920453748, 0.00012043946428703757]}
[2018-04-30 01:21:16,170 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:129]: evaluating model ... 
[2018-04-30 01:21:22,429 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:133]: evaluated! 
[2018-04-30 01:21:22,433 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:135]: generating reports ... 
[2018-04-30 01:21:25,457 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:138]: done!
[2018-04-30 01:21:25,458 AE_BIGRAMA_1L_MINIDS_OVER_F1_0.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_0 finished!
