[2018-01-18 00:19:28,522 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_FULLDS_OVER_F2_0
[2018-01-18 00:19:28,522 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:146]: >> Printing header log
[2018-01-18 00:19:28,522 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_FULLDS_OVER_F2_0
	layers = 9216,18432
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f9577fcabe0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f9577fca470>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2018-01-18 00:19:28,522 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:148]: >> Loading dataset... 
[2018-01-18 00:21:59,650 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (8147, 9216)
	trainy shape = (8147, 9)
	valx shape = (2721, 9216)
	valy shape = (2721, 9)
	=======================================
	
[2018-01-18 00:21:59,651 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:150]: >> Executing autoencoder part ... 
[2018-01-18 00:21:59,651 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:57]: =======================================
[2018-01-18 00:21:59,651 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f9577fcabe0>, 'discard_decoder_function': True}
[2018-01-18 00:21:59,690 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:73]: training and evaluate autoencoder
[2018-01-21 01:34:13,971 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:85]: trained and evaluated!
[2018-01-21 01:34:13,972 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:88]: Training history: 
{'val_loss': [0.00011793092356418705, 0.00011790972983878149, 0.00011788853517747548, 0.0001178673287345196, 0.00011784612217925568, 0.00011782489579894591, 0.00011780367474524679, 0.00011778245965189663, 0.00011776123901801571, 0.00011774000051645791, 0.000117718775868901, 0.00011769753943434624, 0.00011767629487617544, 0.00011765505674630383, 0.00011763379208766505, 0.00011761253369688538, 0.00011759127399049704, 0.00011757000302388912, 0.00011754871708822178, 0.00011752740840749912, 0.00011750611008050962, 0.00011748478803512832, 0.00011746344949650684, 0.00011744210454295617, 0.00011742075953325148, 0.00011739935093579519, 0.00011737793038822696, 0.00011735647405984698, 0.0001173349964223504, 0.00011731347740200926, 0.00011729193244652925, 0.00011727034035107982, 0.00011724874072296864, 0.00011722711657426529, 0.00011720544533372454, 0.00011718376083013721, 0.00011716202570235672, 0.00011714022889873561, 0.00011711839474733794, 0.00011709648742189392, 0.00011707451931907372, 0.00011705251920043561, 0.00011703044921014278, 0.00011700828585709359, 0.00011698603864736274, 0.00011696371924494404, 0.00011694132487155009, 0.00011691883913287876, 0.00011689619782883215, 0.00011687340629136892, 0.00011685044823047297, 0.00011682732367021031, 0.00011680402078347309, 0.00011678045744633249, 0.00011675667434371871, 0.00011673255158678222, 0.00011670798736827044, 0.00011668294243116989, 0.00011665729692674101, 0.00011663094308738228, 0.00011660389923267679, 0.00011657609508986966, 0.00011654733272403453, 0.00011651771268831739, 0.00011648717547014465, 0.00011645558747640995, 0.00011642264564917277, 0.00011638837651185228, 0.00011635262580933385, 0.00011631534997411383, 0.00011627616506773354, 0.00011623490641182114, 0.00011619152633160697, 0.0001161458685638014, 0.00011609780606392734, 0.00011604733773831822, 0.00011599460788143375, 0.00011594024672329895, 0.00011588424699330423, 0.00011582710042959784, 0.0001157688038447702, 0.00011570958195119693, 0.00011564969707108235, 0.0001155894553722305, 0.00011552900285511396, 0.0001154684647646069, 0.00011540786109421725, 0.0001153472537444018, 0.00011528668199891401, 0.00011522615692811942, 0.00011516565811066987, 0.0001151052103880377, 0.00011504483903488346, 0.00011498455643450731, 0.00011492437825923103, 0.00011486429948728013, 0.00011480426987416964, 0.00011474435543029637, 0.00011468453225008842, 0.00011462477804039674, 0.0001145650797039628], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00073502388827636903, 0.002205071664829107], 'loss': [0.00011805013842967949, 0.00011802891975820176, 0.00011800769258902714, 0.00011798645298276086, 0.00011796518226322449, 0.00011794391520533366, 0.00011792263457792031, 0.00011790136659211492, 0.00011788010270913871, 0.00011785882723214239, 0.00011783753093467229, 0.00011781624901050057, 0.00011779494673472433, 0.0001177736274215793, 0.00011775231728576832, 0.00011773098068787017, 0.00011770964653077136, 0.00011768830228628562, 0.00011766693768662302, 0.00011764555651131621, 0.00011762414617323539, 0.00011760272066433194, 0.00011758127124666926, 0.00011755980563738863, 0.00011753832895654221, 0.00011751683389244899, 0.00011749530658890643, 0.00011747376573906153, 0.00011745219558980073, 0.00011743060890597759, 0.00011740897625867898, 0.00011738731559889892, 0.00011736563837776401, 0.00011734393668076136, 0.00011732219596758585, 0.00011730040269461441, 0.00011727856326436821, 0.00011725666737512012, 0.00011723470575308299, 0.0001172127126015985, 0.00011719064504547662, 0.00011716851753124866, 0.00011714635343718174, 0.00011712410974965778, 0.00011710177890246678, 0.0001170793583306707, 0.00011705685360890158, 0.0001170342663920446, 0.00011701157631688515, 0.0001169887230904105, 0.00011696570996434053, 0.00011694249833689347, 0.00011691911507650983, 0.00011689552598520644, 0.00011687169380529337, 0.00011684757761832825, 0.00011682303314743905, 0.00011679797845482247, 0.00011677244115821674, 0.00011674633536345525, 0.00011671959784285034, 0.00011669219929519994, 0.00011666400675186512, 0.00011663491979712891, 0.00011660490179310187, 0.00011657391164629712, 0.0001165416263861817, 0.00011650804930287812, 0.00011647304853695814, 0.00011643655328202089, 0.00011639831072734128, 0.00011635809721425119, 0.00011631561326212035, 0.00011627110975350496, 0.00011622433310248271, 0.00011617521664299152, 0.00011612380013425033, 0.00011607061105594714, 0.00011601572301966047, 0.00011595951353032852, 0.00011590207135606314, 0.00011584365303532323, 0.00011578451770544292, 0.00011572490420841617, 0.00011566496012820609, 0.00011560482552722004, 0.00011554457260685648, 0.00011548421141788588, 0.00011542384990026026, 0.00011536352260830343, 0.00011530321353080047, 0.00011524297629121067, 0.00011518278824448907, 0.00011512265903773206, 0.00011506262986355887, 0.00011500271767628146, 0.00011494289984157186, 0.00011488314595526891, 0.00011482350470386204, 0.00011476396515064118, 0.00011470449800601435], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00024548913710568308, 0.00085921197986989076, 0.0015956793911869401]}
[2018-01-21 01:34:13,973 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:92]: done!
[2018-01-21 01:34:13,973 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:152]: >> Executing classifier part ... 
[2018-01-21 01:34:13,973 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:97]: =======================================
[2018-01-21 01:34:13,973 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f9577fca470>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2018-01-21 01:34:14,279 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:110]: training ... 
[2018-01-24 11:39:32,182 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:122]: trained!
[2018-01-24 11:39:32,184 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:125]: Training history: 
{'val_loss': [0.00011793092356418705, 0.00011790972983878149, 0.00011788853517747548, 0.0001178673287345196, 0.00011784612217925568, 0.00011782489579894591, 0.00011780367474524679, 0.00011778245965189663, 0.00011776123901801571, 0.00011774000051645791, 0.000117718775868901, 0.00011769753943434624, 0.00011767629487617544, 0.00011765505674630383, 0.00011763379208766505, 0.00011761253369688538, 0.00011759127399049704, 0.00011757000302388912, 0.00011754871708822178, 0.00011752740840749912, 0.00011750611008050962, 0.00011748478803512832, 0.00011746344949650684, 0.00011744210454295617, 0.00011742075953325148, 0.00011739935093579519, 0.00011737793038822696, 0.00011735647405984698, 0.0001173349964223504, 0.00011731347740200926, 0.00011729193244652925, 0.00011727034035107982, 0.00011724874072296864, 0.00011722711657426529, 0.00011720544533372454, 0.00011718376083013721, 0.00011716202570235672, 0.00011714022889873561, 0.00011711839474733794, 0.00011709648742189392, 0.00011707451931907372, 0.00011705251920043561, 0.00011703044921014278, 0.00011700828585709359, 0.00011698603864736274, 0.00011696371924494404, 0.00011694132487155009, 0.00011691883913287876, 0.00011689619782883215, 0.00011687340629136892, 0.00011685044823047297, 0.00011682732367021031, 0.00011680402078347309, 0.00011678045744633249, 0.00011675667434371871, 0.00011673255158678222, 0.00011670798736827044, 0.00011668294243116989, 0.00011665729692674101, 0.00011663094308738228, 0.00011660389923267679, 0.00011657609508986966, 0.00011654733272403453, 0.00011651771268831739, 0.00011648717547014465, 0.00011645558747640995, 0.00011642264564917277, 0.00011638837651185228, 0.00011635262580933385, 0.00011631534997411383, 0.00011627616506773354, 0.00011623490641182114, 0.00011619152633160697, 0.0001161458685638014, 0.00011609780606392734, 0.00011604733773831822, 0.00011599460788143375, 0.00011594024672329895, 0.00011588424699330423, 0.00011582710042959784, 0.0001157688038447702, 0.00011570958195119693, 0.00011564969707108235, 0.0001155894553722305, 0.00011552900285511396, 0.0001154684647646069, 0.00011540786109421725, 0.0001153472537444018, 0.00011528668199891401, 0.00011522615692811942, 0.00011516565811066987, 0.0001151052103880377, 0.00011504483903488346, 0.00011498455643450731, 0.00011492437825923103, 0.00011486429948728013, 0.00011480426987416964, 0.00011474435543029637, 0.00011468453225008842, 0.00011462477804039674, 0.0001145650797039628], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00073502388827636903, 0.002205071664829107], 'loss': [0.00011805013842967949, 0.00011802891975820176, 0.00011800769258902714, 0.00011798645298276086, 0.00011796518226322449, 0.00011794391520533366, 0.00011792263457792031, 0.00011790136659211492, 0.00011788010270913871, 0.00011785882723214239, 0.00011783753093467229, 0.00011781624901050057, 0.00011779494673472433, 0.0001177736274215793, 0.00011775231728576832, 0.00011773098068787017, 0.00011770964653077136, 0.00011768830228628562, 0.00011766693768662302, 0.00011764555651131621, 0.00011762414617323539, 0.00011760272066433194, 0.00011758127124666926, 0.00011755980563738863, 0.00011753832895654221, 0.00011751683389244899, 0.00011749530658890643, 0.00011747376573906153, 0.00011745219558980073, 0.00011743060890597759, 0.00011740897625867898, 0.00011738731559889892, 0.00011736563837776401, 0.00011734393668076136, 0.00011732219596758585, 0.00011730040269461441, 0.00011727856326436821, 0.00011725666737512012, 0.00011723470575308299, 0.0001172127126015985, 0.00011719064504547662, 0.00011716851753124866, 0.00011714635343718174, 0.00011712410974965778, 0.00011710177890246678, 0.0001170793583306707, 0.00011705685360890158, 0.0001170342663920446, 0.00011701157631688515, 0.0001169887230904105, 0.00011696570996434053, 0.00011694249833689347, 0.00011691911507650983, 0.00011689552598520644, 0.00011687169380529337, 0.00011684757761832825, 0.00011682303314743905, 0.00011679797845482247, 0.00011677244115821674, 0.00011674633536345525, 0.00011671959784285034, 0.00011669219929519994, 0.00011666400675186512, 0.00011663491979712891, 0.00011660490179310187, 0.00011657391164629712, 0.0001165416263861817, 0.00011650804930287812, 0.00011647304853695814, 0.00011643655328202089, 0.00011639831072734128, 0.00011635809721425119, 0.00011631561326212035, 0.00011627110975350496, 0.00011622433310248271, 0.00011617521664299152, 0.00011612380013425033, 0.00011607061105594714, 0.00011601572301966047, 0.00011595951353032852, 0.00011590207135606314, 0.00011584365303532323, 0.00011578451770544292, 0.00011572490420841617, 0.00011566496012820609, 0.00011560482552722004, 0.00011554457260685648, 0.00011548421141788588, 0.00011542384990026026, 0.00011536352260830343, 0.00011530321353080047, 0.00011524297629121067, 0.00011518278824448907, 0.00011512265903773206, 0.00011506262986355887, 0.00011500271767628146, 0.00011494289984157186, 0.00011488314595526891, 0.00011482350470386204, 0.00011476396515064118, 0.00011470449800601435], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00024548913710568308, 0.00085921197986989076, 0.0015956793911869401]}
[2018-01-24 11:39:32,185 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:129]: evaluating model ... 
[2018-01-24 11:39:40,460 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:133]: evaluated! 
[2018-01-24 11:39:40,460 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:135]: generating reports ... 
[2018-01-24 11:39:41,330 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:138]: done!
[2018-01-24 11:39:41,330 AE_BIGRAMA_1L_FULLDS_OVER_F2_0.py:154]: >> experiment AE_BIGRAMA_1L_FULLDS_OVER_F2_0 finished!
