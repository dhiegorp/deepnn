[2017-12-14 09:31:57,879 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_2
[2017-12-14 09:31:57,879 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:146]: >> Printing header log
[2017-12-14 09:31:57,880 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_2
	layers = 9216,11059
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7ff3c4c02eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7ff3c4be5400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,880 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:148]: >> Loading dataset... 
[2017-12-14 09:32:21,252 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:21,253 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:21,253 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:57]: =======================================
[2017-12-14 09:32:21,253 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7ff3c4c02eb8>, 'discard_decoder_function': True}
[2017-12-14 09:32:21,305 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:54,917 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_2
[2017-12-14 10:18:54,917 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:146]: >> Printing header log
[2017-12-14 10:18:54,917 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_2
	layers = 9216,11059
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f60710abeb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f607108e400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:54,917 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:148]: >> Loading dataset... 
[2017-12-14 10:19:16,780 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:16,781 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:16,781 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:57]: =======================================
[2017-12-14 10:19:16,781 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f60710abeb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:16,822 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:73]: training and evaluate autoencoder
[2017-12-14 23:49:39,683 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:85]: trained and evaluated!
[2017-12-14 23:49:39,685 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:88]: Training history: 
{'val_loss': [0.0001191955853471846, 0.00011919123419939199, 0.00011918684332880377, 0.00011918242925381018, 0.00011917798664705159, 0.00011917355941455235, 0.00011916912376196455, 0.00011916468312168101, 0.00011916026545340122, 0.00011915584140301612, 0.00011915138335049001, 0.00011914687626122747, 0.00011914235773065565, 0.00011913780863033562, 0.00011913320134023172, 0.00011912858498646564, 0.00011912394549980238, 0.00011911928198638965, 0.00011911455913906212, 0.00011910981757446771, 0.00011910506612386701, 0.000119100292273328, 0.00011909547764524767, 0.00011909069594668559, 0.00011908589292080793, 0.00011908105303246199, 0.00011907621870387726, 0.00011907135552174071, 0.00011906650760660122, 0.00011906162617200097, 0.00011905671956652033, 0.00011905177699242368, 0.00011904681677368289, 0.00011904183007903735, 0.00011903680666493989, 0.00011903169309690077, 0.00011902657765177186, 0.00011902146481669162, 0.00011901632795486192, 0.0001190112079332093, 0.0001190060611853733, 0.00011900090363980168, 0.00011899570195580413, 0.0001189904732238364, 0.0001189852914906219, 0.00011898006912288244, 0.00011897479460780058, 0.00011896956276522688, 0.00011896431179421425, 0.00011895904013945971, 0.00011895375589926496, 0.00011894846733282515, 0.00011894314581899005, 0.00011893779185831693, 0.00011893237446988555, 0.00011892695994178149, 0.00011892153601035137, 0.0001189161437212922, 0.00011891072070159142, 0.00011890525434793177, 0.00011889973218213533, 0.00011889414674947404, 0.00011888857701285891, 0.00011888295840040074, 0.00011887730210313016, 0.00011887164752205596, 0.00011886589257924701, 0.00011886010004101087, 0.00011885432114296064, 0.00011884846428311396, 0.00011884254932286863, 0.00011883664367656413, 0.00011883060211108091, 0.00011882453414120114, 0.00011881839727318711, 0.00011881217132385418, 0.00011880591357125688, 0.00011879959934507211, 0.00011879330285291671, 0.00011878696663796567, 0.00011878065641623915, 0.00011877433570068677, 0.00011876797292044578, 0.00011876155114595386, 0.00011875514163511534, 0.00011874869222271071, 0.00011874230781124438, 0.00011873588015520442, 0.00011872941236519678, 0.00011872293141768348, 0.00011871640093287645, 0.00011870985278554824, 0.00011870331175328422, 0.00011869680227400593, 0.00011869028592994208, 0.0001186836814699198, 0.00011867706565797347, 0.00011867040783496967, 0.00011866369427133726, 0.00011865693501397596, 0.0001186501194439206, 0.00011864330746715143, 0.00011863645919997941, 0.00011862963832044146, 0.00011862280101189847, 0.0001186159433414004, 0.0001186090149493094, 0.00011860200044348909, 0.00011859501130519669, 0.00011858804587186692, 0.00011858103220626776, 0.00011857399950161489, 0.00011856699009075262, 0.00011855995811905863, 0.0001185529107731053, 0.00011854587615560855, 0.00011853880825105263, 0.00011853172726049923, 0.00011852467578494834, 0.00011851761563903028, 0.00011851047653020119, 0.00011850333291635659, 0.00011849617639528496, 0.00011848908485727458, 0.00011848194362107706, 0.00011847481318261516, 0.0001184676708022867, 0.00011846055269898636, 0.00011845337776455763, 0.0001184461887251398, 0.00011843895025569053, 0.00011843174720066886, 0.00011842455476461234, 0.00011841737770281517, 0.00011841012035520559, 0.00011840292514820699, 0.00011839572314793102, 0.00011838849957006085, 0.00011838126020675927, 0.00011837406694835867, 0.00011836685373917502, 0.00011835957685195445, 0.00011835229538821018, 0.00011834496111567281, 0.00011833771504847909, 0.00011833045149753465, 0.00011832314134113198, 0.00011831578679602474, 0.00011830846787986966, 0.00011830113258834067, 0.00011829379836943445, 0.00011828649488116983, 0.00011827914872039704, 0.00011827176864686851, 0.00011826444170391988, 0.00011825708320798554, 0.00011824967991217459, 0.00011824225389463851, 0.00011823485885802267, 0.00011822745686723606, 0.00011822004463290224, 0.0001182126295382411, 0.00011820523798764918, 0.00011819785510742446, 0.00011819043976248468, 0.00011818299450924739, 0.00011817555342136176, 0.00011816813800491378, 0.00011816070508683806, 0.00011815327168608208, 0.00011814585921934666, 0.00011813839090472036, 0.00011813091533201351, 0.00011812348968989538, 0.00011811602563000598, 0.00011810853881263734, 0.00011810105469470262, 0.00011809362935649428, 0.0001180861374799217, 0.00011807866260441963, 0.00011807117162169934, 0.00011806365922227827, 0.00011805618574118576, 0.00011804871470924851, 0.00011804120443719588, 0.00011803371738742565, 0.00011802625527613423, 0.00011801873610131282, 0.0001180111560193966, 0.00011800360668599903, 0.00011799604002974415, 0.00011798853950068144, 0.00011798103064091542, 0.00011797342842721657, 0.00011796581827610941, 0.00011795824859863381, 0.00011795067917143683, 0.00011794309583589828, 0.00011793550130932911, 0.00011792792746650183], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012002245095294539, 0.00012001792248698704, 0.00012001337581928455, 0.00012000881313025521, 0.00012000424743130202, 0.0001199996431958437, 0.00011999505701992836, 0.00011999045726380551, 0.00011998585196183873, 0.00011998127109476544, 0.00011997666432338704, 0.00011997200316007792, 0.00011996729805662087, 0.00011996258363899007, 0.000119957848602196, 0.00011995307083396357, 0.00011994830934776005, 0.00011994352954131148, 0.00011993874644053681, 0.00011993393454403414, 0.00011992910463538882, 0.00011992425654869957, 0.00011991938021138663, 0.00011991445621299627, 0.00011990955314187163, 0.00011990462459304525, 0.00011989964625012457, 0.00011989467110672923, 0.00011988969025158866, 0.00011988470811663796, 0.00011987968635497348, 0.00011987464295503765, 0.00011986957130447813, 0.00011986446583375079, 0.00011985934133177277, 0.00011985420839252793, 0.00011984898361505588, 0.00011984377165938537, 0.0001198385634483445, 0.00011983331937891968, 0.00011982809182852568, 0.00011982280851159008, 0.00011981750746691412, 0.00011981216489950935, 0.00011980676784537314, 0.00011980140823753345, 0.00011979599307645385, 0.00011979051548907995, 0.00011978508335866598, 0.00011977960942112097, 0.00011977411258919468, 0.00011976860696449876, 0.00011976307666790748, 0.00011975749055737423, 0.00011975186548373246, 0.00011974617134774383, 0.0001197404923798753, 0.00011973480706035648, 0.00011972915150827337, 0.0001197234355207117, 0.00011971767035526063, 0.00011971187753169053, 0.00011970601953260437, 0.00011970016956788181, 0.00011969426961946347, 0.00011968831907114445, 0.00011968234420643288, 0.00011967627572598002, 0.00011967019089239767, 0.00011966411909391855, 0.00011965796448698371, 0.00011965180608801884, 0.00011964562706989069, 0.00011963931300809322, 0.00011963293730210761, 0.0001196265085788022, 0.00011962004376011097, 0.00011961354952948684, 0.00011960700761408511, 0.00011960050653410675, 0.00011959394471054738, 0.00011958739905051603, 0.00011958083227361743, 0.00011957424077742311, 0.00011956759922643243, 0.0001195609804750223, 0.00011955431538974526, 0.00011954771827660647, 0.00011954107433189684, 0.0001195343979179301, 0.00011952768744679369, 0.00011952091881539673, 0.00011951414193633447, 0.00011950735991433147, 0.00011950061536232519, 0.00011949383504673571, 0.00011948696097320383, 0.00011948012242625325, 0.00011947324863712361, 0.0001194663332778648, 0.00011945938848297291, 0.0001194523773986561, 0.00011944536482122747, 0.00011943833932719658, 0.00011943134004557323, 0.00011942434085875064, 0.00011941731090908446, 0.0001194102620703687, 0.00011940312760287489, 0.00011939602306871811, 0.00011938893486399064, 0.00011938178124674519, 0.00011937460061128582, 0.0001193674488663552, 0.00011936026524467218, 0.00011935306358714634, 0.00011934586764136573, 0.00011933865446554866, 0.00011933141178299797, 0.00011932419993439143, 0.0001193169757616873, 0.00011930963955819607, 0.00011930231127056751, 0.00011929495605952576, 0.00011928767045593521, 0.00011928034430132353, 0.00011927302352665447, 0.00011926567798528929, 0.00011925835950953843, 0.0001192509833238306, 0.00011924361140415655, 0.00011923620007107041, 0.0001192288217049453, 0.00011922145153908514, 0.00011921410040447567, 0.00011920665930395382, 0.00011919927088894915, 0.00011919187884781576, 0.0001191844529391142, 0.00011917700897086963, 0.00011916960218821952, 0.00011916217438350295, 0.00011915467621292919, 0.00011914717662034417, 0.00011913961744548734, 0.00011913214254849788, 0.00011912464748264869, 0.00011911709648435663, 0.00011910949194734061, 0.0001191019237190121, 0.00011909434743261978, 0.00011908676507897942, 0.0001190792128245774, 0.00011907162203367023, 0.00011906399803880013, 0.00011905643495341235, 0.00011904883529863499, 0.00011904118928629055, 0.00011903350011590437, 0.00011902583424280267, 0.00011901816339266156, 0.00011901048460295758, 0.00011900280531554965, 0.00011899514667100518, 0.0001189874997817538, 0.00011897982902641345, 0.00011897212236528876, 0.00011896441639146953, 0.00011895673354903345, 0.0001189490366286859, 0.00011894133411509407, 0.00011893365617859685, 0.00011892591517590022, 0.00011891817851033796, 0.00011891047836676489, 0.00011890273997108891, 0.00011889497721162003, 0.00011888723035497707, 0.00011887953594684941, 0.00011887177783261729, 0.00011886403294308991, 0.00011885626366606941, 0.00011884847149466762, 0.00011884071707766479, 0.00011883297069502557, 0.00011882520201050976, 0.00011881745702618163, 0.00011880972156932893, 0.0001188019410110191, 0.00011879410790939321, 0.00011878631102165408, 0.00011877850545964626, 0.00011877077066639881, 0.00011876302937929995, 0.00011875519281744666, 0.00011874734952474006, 0.0001187395553862227, 0.00011873177079888097, 0.0001187239707116165, 0.00011871615502962855], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-14 23:49:39,685 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:92]: done!
[2017-12-14 23:49:39,685 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:152]: >> Executing classifier part ... 
[2017-12-14 23:49:39,685 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:97]: =======================================
[2017-12-14 23:49:39,686 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f607108e400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-14 23:49:40,213 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:110]: training ... 
[2017-12-15 05:46:01,927 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:122]: trained!
[2017-12-15 05:46:01,929 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:125]: Training history: 
{'val_loss': [0.0001191955853471846, 0.00011919123419939199, 0.00011918684332880377, 0.00011918242925381018, 0.00011917798664705159, 0.00011917355941455235, 0.00011916912376196455, 0.00011916468312168101, 0.00011916026545340122, 0.00011915584140301612, 0.00011915138335049001, 0.00011914687626122747, 0.00011914235773065565, 0.00011913780863033562, 0.00011913320134023172, 0.00011912858498646564, 0.00011912394549980238, 0.00011911928198638965, 0.00011911455913906212, 0.00011910981757446771, 0.00011910506612386701, 0.000119100292273328, 0.00011909547764524767, 0.00011909069594668559, 0.00011908589292080793, 0.00011908105303246199, 0.00011907621870387726, 0.00011907135552174071, 0.00011906650760660122, 0.00011906162617200097, 0.00011905671956652033, 0.00011905177699242368, 0.00011904681677368289, 0.00011904183007903735, 0.00011903680666493989, 0.00011903169309690077, 0.00011902657765177186, 0.00011902146481669162, 0.00011901632795486192, 0.0001190112079332093, 0.0001190060611853733, 0.00011900090363980168, 0.00011899570195580413, 0.0001189904732238364, 0.0001189852914906219, 0.00011898006912288244, 0.00011897479460780058, 0.00011896956276522688, 0.00011896431179421425, 0.00011895904013945971, 0.00011895375589926496, 0.00011894846733282515, 0.00011894314581899005, 0.00011893779185831693, 0.00011893237446988555, 0.00011892695994178149, 0.00011892153601035137, 0.0001189161437212922, 0.00011891072070159142, 0.00011890525434793177, 0.00011889973218213533, 0.00011889414674947404, 0.00011888857701285891, 0.00011888295840040074, 0.00011887730210313016, 0.00011887164752205596, 0.00011886589257924701, 0.00011886010004101087, 0.00011885432114296064, 0.00011884846428311396, 0.00011884254932286863, 0.00011883664367656413, 0.00011883060211108091, 0.00011882453414120114, 0.00011881839727318711, 0.00011881217132385418, 0.00011880591357125688, 0.00011879959934507211, 0.00011879330285291671, 0.00011878696663796567, 0.00011878065641623915, 0.00011877433570068677, 0.00011876797292044578, 0.00011876155114595386, 0.00011875514163511534, 0.00011874869222271071, 0.00011874230781124438, 0.00011873588015520442, 0.00011872941236519678, 0.00011872293141768348, 0.00011871640093287645, 0.00011870985278554824, 0.00011870331175328422, 0.00011869680227400593, 0.00011869028592994208, 0.0001186836814699198, 0.00011867706565797347, 0.00011867040783496967, 0.00011866369427133726, 0.00011865693501397596, 0.0001186501194439206, 0.00011864330746715143, 0.00011863645919997941, 0.00011862963832044146, 0.00011862280101189847, 0.0001186159433414004, 0.0001186090149493094, 0.00011860200044348909, 0.00011859501130519669, 0.00011858804587186692, 0.00011858103220626776, 0.00011857399950161489, 0.00011856699009075262, 0.00011855995811905863, 0.0001185529107731053, 0.00011854587615560855, 0.00011853880825105263, 0.00011853172726049923, 0.00011852467578494834, 0.00011851761563903028, 0.00011851047653020119, 0.00011850333291635659, 0.00011849617639528496, 0.00011848908485727458, 0.00011848194362107706, 0.00011847481318261516, 0.0001184676708022867, 0.00011846055269898636, 0.00011845337776455763, 0.0001184461887251398, 0.00011843895025569053, 0.00011843174720066886, 0.00011842455476461234, 0.00011841737770281517, 0.00011841012035520559, 0.00011840292514820699, 0.00011839572314793102, 0.00011838849957006085, 0.00011838126020675927, 0.00011837406694835867, 0.00011836685373917502, 0.00011835957685195445, 0.00011835229538821018, 0.00011834496111567281, 0.00011833771504847909, 0.00011833045149753465, 0.00011832314134113198, 0.00011831578679602474, 0.00011830846787986966, 0.00011830113258834067, 0.00011829379836943445, 0.00011828649488116983, 0.00011827914872039704, 0.00011827176864686851, 0.00011826444170391988, 0.00011825708320798554, 0.00011824967991217459, 0.00011824225389463851, 0.00011823485885802267, 0.00011822745686723606, 0.00011822004463290224, 0.0001182126295382411, 0.00011820523798764918, 0.00011819785510742446, 0.00011819043976248468, 0.00011818299450924739, 0.00011817555342136176, 0.00011816813800491378, 0.00011816070508683806, 0.00011815327168608208, 0.00011814585921934666, 0.00011813839090472036, 0.00011813091533201351, 0.00011812348968989538, 0.00011811602563000598, 0.00011810853881263734, 0.00011810105469470262, 0.00011809362935649428, 0.0001180861374799217, 0.00011807866260441963, 0.00011807117162169934, 0.00011806365922227827, 0.00011805618574118576, 0.00011804871470924851, 0.00011804120443719588, 0.00011803371738742565, 0.00011802625527613423, 0.00011801873610131282, 0.0001180111560193966, 0.00011800360668599903, 0.00011799604002974415, 0.00011798853950068144, 0.00011798103064091542, 0.00011797342842721657, 0.00011796581827610941, 0.00011795824859863381, 0.00011795067917143683, 0.00011794309583589828, 0.00011793550130932911, 0.00011792792746650183], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012002245095294539, 0.00012001792248698704, 0.00012001337581928455, 0.00012000881313025521, 0.00012000424743130202, 0.0001199996431958437, 0.00011999505701992836, 0.00011999045726380551, 0.00011998585196183873, 0.00011998127109476544, 0.00011997666432338704, 0.00011997200316007792, 0.00011996729805662087, 0.00011996258363899007, 0.000119957848602196, 0.00011995307083396357, 0.00011994830934776005, 0.00011994352954131148, 0.00011993874644053681, 0.00011993393454403414, 0.00011992910463538882, 0.00011992425654869957, 0.00011991938021138663, 0.00011991445621299627, 0.00011990955314187163, 0.00011990462459304525, 0.00011989964625012457, 0.00011989467110672923, 0.00011988969025158866, 0.00011988470811663796, 0.00011987968635497348, 0.00011987464295503765, 0.00011986957130447813, 0.00011986446583375079, 0.00011985934133177277, 0.00011985420839252793, 0.00011984898361505588, 0.00011984377165938537, 0.0001198385634483445, 0.00011983331937891968, 0.00011982809182852568, 0.00011982280851159008, 0.00011981750746691412, 0.00011981216489950935, 0.00011980676784537314, 0.00011980140823753345, 0.00011979599307645385, 0.00011979051548907995, 0.00011978508335866598, 0.00011977960942112097, 0.00011977411258919468, 0.00011976860696449876, 0.00011976307666790748, 0.00011975749055737423, 0.00011975186548373246, 0.00011974617134774383, 0.0001197404923798753, 0.00011973480706035648, 0.00011972915150827337, 0.0001197234355207117, 0.00011971767035526063, 0.00011971187753169053, 0.00011970601953260437, 0.00011970016956788181, 0.00011969426961946347, 0.00011968831907114445, 0.00011968234420643288, 0.00011967627572598002, 0.00011967019089239767, 0.00011966411909391855, 0.00011965796448698371, 0.00011965180608801884, 0.00011964562706989069, 0.00011963931300809322, 0.00011963293730210761, 0.0001196265085788022, 0.00011962004376011097, 0.00011961354952948684, 0.00011960700761408511, 0.00011960050653410675, 0.00011959394471054738, 0.00011958739905051603, 0.00011958083227361743, 0.00011957424077742311, 0.00011956759922643243, 0.0001195609804750223, 0.00011955431538974526, 0.00011954771827660647, 0.00011954107433189684, 0.0001195343979179301, 0.00011952768744679369, 0.00011952091881539673, 0.00011951414193633447, 0.00011950735991433147, 0.00011950061536232519, 0.00011949383504673571, 0.00011948696097320383, 0.00011948012242625325, 0.00011947324863712361, 0.0001194663332778648, 0.00011945938848297291, 0.0001194523773986561, 0.00011944536482122747, 0.00011943833932719658, 0.00011943134004557323, 0.00011942434085875064, 0.00011941731090908446, 0.0001194102620703687, 0.00011940312760287489, 0.00011939602306871811, 0.00011938893486399064, 0.00011938178124674519, 0.00011937460061128582, 0.0001193674488663552, 0.00011936026524467218, 0.00011935306358714634, 0.00011934586764136573, 0.00011933865446554866, 0.00011933141178299797, 0.00011932419993439143, 0.0001193169757616873, 0.00011930963955819607, 0.00011930231127056751, 0.00011929495605952576, 0.00011928767045593521, 0.00011928034430132353, 0.00011927302352665447, 0.00011926567798528929, 0.00011925835950953843, 0.0001192509833238306, 0.00011924361140415655, 0.00011923620007107041, 0.0001192288217049453, 0.00011922145153908514, 0.00011921410040447567, 0.00011920665930395382, 0.00011919927088894915, 0.00011919187884781576, 0.0001191844529391142, 0.00011917700897086963, 0.00011916960218821952, 0.00011916217438350295, 0.00011915467621292919, 0.00011914717662034417, 0.00011913961744548734, 0.00011913214254849788, 0.00011912464748264869, 0.00011911709648435663, 0.00011910949194734061, 0.0001191019237190121, 0.00011909434743261978, 0.00011908676507897942, 0.0001190792128245774, 0.00011907162203367023, 0.00011906399803880013, 0.00011905643495341235, 0.00011904883529863499, 0.00011904118928629055, 0.00011903350011590437, 0.00011902583424280267, 0.00011901816339266156, 0.00011901048460295758, 0.00011900280531554965, 0.00011899514667100518, 0.0001189874997817538, 0.00011897982902641345, 0.00011897212236528876, 0.00011896441639146953, 0.00011895673354903345, 0.0001189490366286859, 0.00011894133411509407, 0.00011893365617859685, 0.00011892591517590022, 0.00011891817851033796, 0.00011891047836676489, 0.00011890273997108891, 0.00011889497721162003, 0.00011888723035497707, 0.00011887953594684941, 0.00011887177783261729, 0.00011886403294308991, 0.00011885626366606941, 0.00011884847149466762, 0.00011884071707766479, 0.00011883297069502557, 0.00011882520201050976, 0.00011881745702618163, 0.00011880972156932893, 0.0001188019410110191, 0.00011879410790939321, 0.00011878631102165408, 0.00011877850545964626, 0.00011877077066639881, 0.00011876302937929995, 0.00011875519281744666, 0.00011874734952474006, 0.0001187395553862227, 0.00011873177079888097, 0.0001187239707116165, 0.00011871615502962855], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 05:46:01,929 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:129]: evaluating model ... 
[2017-12-15 05:46:06,748 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:133]: evaluated! 
[2017-12-15 05:46:06,749 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:135]: generating reports ... 
[2017-12-15 05:46:08,566 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:138]: done!
[2017-12-15 05:46:08,568 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_2 finished!
