[2017-12-14 09:31:57,879 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_2
[2017-12-14 09:31:57,879 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:146]: >> Printing header log
[2017-12-14 09:31:57,880 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_2
	layers = 9216,11059
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7ff3c4c02eb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7ff3c4be5400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,880 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:148]: >> Loading dataset... 
[2017-12-14 09:32:21,252 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:21,253 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:21,253 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:57]: =======================================
[2017-12-14 09:32:21,253 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7ff3c4c02eb8>, 'discard_decoder_function': True}
[2017-12-14 09:32:21,305 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:54,917 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_2
[2017-12-14 10:18:54,917 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:146]: >> Printing header log
[2017-12-14 10:18:54,917 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_2
	layers = 9216,11059
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f60710abeb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f607108e400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:54,917 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:148]: >> Loading dataset... 
[2017-12-14 10:19:16,780 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:16,781 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:16,781 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:57]: =======================================
[2017-12-14 10:19:16,781 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f60710abeb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:16,822 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:73]: training and evaluate autoencoder
[2017-12-14 23:49:39,683 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:85]: trained and evaluated!
[2017-12-14 23:49:39,685 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:88]: Training history: 
{'val_loss': [0.0001191955853471846, 0.00011919123419939199, 0.00011918684332880377, 0.00011918242925381018, 0.00011917798664705159, 0.00011917355941455235, 0.00011916912376196455, 0.00011916468312168101, 0.00011916026545340122, 0.00011915584140301612, 0.00011915138335049001, 0.00011914687626122747, 0.00011914235773065565, 0.00011913780863033562, 0.00011913320134023172, 0.00011912858498646564, 0.00011912394549980238, 0.00011911928198638965, 0.00011911455913906212, 0.00011910981757446771, 0.00011910506612386701, 0.000119100292273328, 0.00011909547764524767, 0.00011909069594668559, 0.00011908589292080793, 0.00011908105303246199, 0.00011907621870387726, 0.00011907135552174071, 0.00011906650760660122, 0.00011906162617200097, 0.00011905671956652033, 0.00011905177699242368, 0.00011904681677368289, 0.00011904183007903735, 0.00011903680666493989, 0.00011903169309690077, 0.00011902657765177186, 0.00011902146481669162, 0.00011901632795486192, 0.0001190112079332093, 0.0001190060611853733, 0.00011900090363980168, 0.00011899570195580413, 0.0001189904732238364, 0.0001189852914906219, 0.00011898006912288244, 0.00011897479460780058, 0.00011896956276522688, 0.00011896431179421425, 0.00011895904013945971, 0.00011895375589926496, 0.00011894846733282515, 0.00011894314581899005, 0.00011893779185831693, 0.00011893237446988555, 0.00011892695994178149, 0.00011892153601035137, 0.0001189161437212922, 0.00011891072070159142, 0.00011890525434793177, 0.00011889973218213533, 0.00011889414674947404, 0.00011888857701285891, 0.00011888295840040074, 0.00011887730210313016, 0.00011887164752205596, 0.00011886589257924701, 0.00011886010004101087, 0.00011885432114296064, 0.00011884846428311396, 0.00011884254932286863, 0.00011883664367656413, 0.00011883060211108091, 0.00011882453414120114, 0.00011881839727318711, 0.00011881217132385418, 0.00011880591357125688, 0.00011879959934507211, 0.00011879330285291671, 0.00011878696663796567, 0.00011878065641623915, 0.00011877433570068677, 0.00011876797292044578, 0.00011876155114595386, 0.00011875514163511534, 0.00011874869222271071, 0.00011874230781124438, 0.00011873588015520442, 0.00011872941236519678, 0.00011872293141768348, 0.00011871640093287645, 0.00011870985278554824, 0.00011870331175328422, 0.00011869680227400593, 0.00011869028592994208, 0.0001186836814699198, 0.00011867706565797347, 0.00011867040783496967, 0.00011866369427133726, 0.00011865693501397596, 0.0001186501194439206, 0.00011864330746715143, 0.00011863645919997941, 0.00011862963832044146, 0.00011862280101189847, 0.0001186159433414004, 0.0001186090149493094, 0.00011860200044348909, 0.00011859501130519669, 0.00011858804587186692, 0.00011858103220626776, 0.00011857399950161489, 0.00011856699009075262, 0.00011855995811905863, 0.0001185529107731053, 0.00011854587615560855, 0.00011853880825105263, 0.00011853172726049923, 0.00011852467578494834, 0.00011851761563903028, 0.00011851047653020119, 0.00011850333291635659, 0.00011849617639528496, 0.00011848908485727458, 0.00011848194362107706, 0.00011847481318261516, 0.0001184676708022867, 0.00011846055269898636, 0.00011845337776455763, 0.0001184461887251398, 0.00011843895025569053, 0.00011843174720066886, 0.00011842455476461234, 0.00011841737770281517, 0.00011841012035520559, 0.00011840292514820699, 0.00011839572314793102, 0.00011838849957006085, 0.00011838126020675927, 0.00011837406694835867, 0.00011836685373917502, 0.00011835957685195445, 0.00011835229538821018, 0.00011834496111567281, 0.00011833771504847909, 0.00011833045149753465, 0.00011832314134113198, 0.00011831578679602474, 0.00011830846787986966, 0.00011830113258834067, 0.00011829379836943445, 0.00011828649488116983, 0.00011827914872039704, 0.00011827176864686851, 0.00011826444170391988, 0.00011825708320798554, 0.00011824967991217459, 0.00011824225389463851, 0.00011823485885802267, 0.00011822745686723606, 0.00011822004463290224, 0.0001182126295382411, 0.00011820523798764918, 0.00011819785510742446, 0.00011819043976248468, 0.00011818299450924739, 0.00011817555342136176, 0.00011816813800491378, 0.00011816070508683806, 0.00011815327168608208, 0.00011814585921934666, 0.00011813839090472036, 0.00011813091533201351, 0.00011812348968989538, 0.00011811602563000598, 0.00011810853881263734, 0.00011810105469470262, 0.00011809362935649428, 0.0001180861374799217, 0.00011807866260441963, 0.00011807117162169934, 0.00011806365922227827, 0.00011805618574118576, 0.00011804871470924851, 0.00011804120443719588, 0.00011803371738742565, 0.00011802625527613423, 0.00011801873610131282, 0.0001180111560193966, 0.00011800360668599903, 0.00011799604002974415, 0.00011798853950068144, 0.00011798103064091542, 0.00011797342842721657, 0.00011796581827610941, 0.00011795824859863381, 0.00011795067917143683, 0.00011794309583589828, 0.00011793550130932911, 0.00011792792746650183], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012002245095294539, 0.00012001792248698704, 0.00012001337581928455, 0.00012000881313025521, 0.00012000424743130202, 0.0001199996431958437, 0.00011999505701992836, 0.00011999045726380551, 0.00011998585196183873, 0.00011998127109476544, 0.00011997666432338704, 0.00011997200316007792, 0.00011996729805662087, 0.00011996258363899007, 0.000119957848602196, 0.00011995307083396357, 0.00011994830934776005, 0.00011994352954131148, 0.00011993874644053681, 0.00011993393454403414, 0.00011992910463538882, 0.00011992425654869957, 0.00011991938021138663, 0.00011991445621299627, 0.00011990955314187163, 0.00011990462459304525, 0.00011989964625012457, 0.00011989467110672923, 0.00011988969025158866, 0.00011988470811663796, 0.00011987968635497348, 0.00011987464295503765, 0.00011986957130447813, 0.00011986446583375079, 0.00011985934133177277, 0.00011985420839252793, 0.00011984898361505588, 0.00011984377165938537, 0.0001198385634483445, 0.00011983331937891968, 0.00011982809182852568, 0.00011982280851159008, 0.00011981750746691412, 0.00011981216489950935, 0.00011980676784537314, 0.00011980140823753345, 0.00011979599307645385, 0.00011979051548907995, 0.00011978508335866598, 0.00011977960942112097, 0.00011977411258919468, 0.00011976860696449876, 0.00011976307666790748, 0.00011975749055737423, 0.00011975186548373246, 0.00011974617134774383, 0.0001197404923798753, 0.00011973480706035648, 0.00011972915150827337, 0.0001197234355207117, 0.00011971767035526063, 0.00011971187753169053, 0.00011970601953260437, 0.00011970016956788181, 0.00011969426961946347, 0.00011968831907114445, 0.00011968234420643288, 0.00011967627572598002, 0.00011967019089239767, 0.00011966411909391855, 0.00011965796448698371, 0.00011965180608801884, 0.00011964562706989069, 0.00011963931300809322, 0.00011963293730210761, 0.0001196265085788022, 0.00011962004376011097, 0.00011961354952948684, 0.00011960700761408511, 0.00011960050653410675, 0.00011959394471054738, 0.00011958739905051603, 0.00011958083227361743, 0.00011957424077742311, 0.00011956759922643243, 0.0001195609804750223, 0.00011955431538974526, 0.00011954771827660647, 0.00011954107433189684, 0.0001195343979179301, 0.00011952768744679369, 0.00011952091881539673, 0.00011951414193633447, 0.00011950735991433147, 0.00011950061536232519, 0.00011949383504673571, 0.00011948696097320383, 0.00011948012242625325, 0.00011947324863712361, 0.0001194663332778648, 0.00011945938848297291, 0.0001194523773986561, 0.00011944536482122747, 0.00011943833932719658, 0.00011943134004557323, 0.00011942434085875064, 0.00011941731090908446, 0.0001194102620703687, 0.00011940312760287489, 0.00011939602306871811, 0.00011938893486399064, 0.00011938178124674519, 0.00011937460061128582, 0.0001193674488663552, 0.00011936026524467218, 0.00011935306358714634, 0.00011934586764136573, 0.00011933865446554866, 0.00011933141178299797, 0.00011932419993439143, 0.0001193169757616873, 0.00011930963955819607, 0.00011930231127056751, 0.00011929495605952576, 0.00011928767045593521, 0.00011928034430132353, 0.00011927302352665447, 0.00011926567798528929, 0.00011925835950953843, 0.0001192509833238306, 0.00011924361140415655, 0.00011923620007107041, 0.0001192288217049453, 0.00011922145153908514, 0.00011921410040447567, 0.00011920665930395382, 0.00011919927088894915, 0.00011919187884781576, 0.0001191844529391142, 0.00011917700897086963, 0.00011916960218821952, 0.00011916217438350295, 0.00011915467621292919, 0.00011914717662034417, 0.00011913961744548734, 0.00011913214254849788, 0.00011912464748264869, 0.00011911709648435663, 0.00011910949194734061, 0.0001191019237190121, 0.00011909434743261978, 0.00011908676507897942, 0.0001190792128245774, 0.00011907162203367023, 0.00011906399803880013, 0.00011905643495341235, 0.00011904883529863499, 0.00011904118928629055, 0.00011903350011590437, 0.00011902583424280267, 0.00011901816339266156, 0.00011901048460295758, 0.00011900280531554965, 0.00011899514667100518, 0.0001189874997817538, 0.00011897982902641345, 0.00011897212236528876, 0.00011896441639146953, 0.00011895673354903345, 0.0001189490366286859, 0.00011894133411509407, 0.00011893365617859685, 0.00011892591517590022, 0.00011891817851033796, 0.00011891047836676489, 0.00011890273997108891, 0.00011889497721162003, 0.00011888723035497707, 0.00011887953594684941, 0.00011887177783261729, 0.00011886403294308991, 0.00011885626366606941, 0.00011884847149466762, 0.00011884071707766479, 0.00011883297069502557, 0.00011882520201050976, 0.00011881745702618163, 0.00011880972156932893, 0.0001188019410110191, 0.00011879410790939321, 0.00011878631102165408, 0.00011877850545964626, 0.00011877077066639881, 0.00011876302937929995, 0.00011875519281744666, 0.00011874734952474006, 0.0001187395553862227, 0.00011873177079888097, 0.0001187239707116165, 0.00011871615502962855], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-14 23:49:39,685 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:92]: done!
[2017-12-14 23:49:39,685 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:152]: >> Executing classifier part ... 
[2017-12-14 23:49:39,685 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:97]: =======================================
[2017-12-14 23:49:39,686 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f607108e400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-14 23:49:40,213 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:110]: training ... 
[2017-12-15 05:46:01,927 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:122]: trained!
[2017-12-15 05:46:01,929 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:125]: Training history: 
{'val_loss': [0.0001191955853471846, 0.00011919123419939199, 0.00011918684332880377, 0.00011918242925381018, 0.00011917798664705159, 0.00011917355941455235, 0.00011916912376196455, 0.00011916468312168101, 0.00011916026545340122, 0.00011915584140301612, 0.00011915138335049001, 0.00011914687626122747, 0.00011914235773065565, 0.00011913780863033562, 0.00011913320134023172, 0.00011912858498646564, 0.00011912394549980238, 0.00011911928198638965, 0.00011911455913906212, 0.00011910981757446771, 0.00011910506612386701, 0.000119100292273328, 0.00011909547764524767, 0.00011909069594668559, 0.00011908589292080793, 0.00011908105303246199, 0.00011907621870387726, 0.00011907135552174071, 0.00011906650760660122, 0.00011906162617200097, 0.00011905671956652033, 0.00011905177699242368, 0.00011904681677368289, 0.00011904183007903735, 0.00011903680666493989, 0.00011903169309690077, 0.00011902657765177186, 0.00011902146481669162, 0.00011901632795486192, 0.0001190112079332093, 0.0001190060611853733, 0.00011900090363980168, 0.00011899570195580413, 0.0001189904732238364, 0.0001189852914906219, 0.00011898006912288244, 0.00011897479460780058, 0.00011896956276522688, 0.00011896431179421425, 0.00011895904013945971, 0.00011895375589926496, 0.00011894846733282515, 0.00011894314581899005, 0.00011893779185831693, 0.00011893237446988555, 0.00011892695994178149, 0.00011892153601035137, 0.0001189161437212922, 0.00011891072070159142, 0.00011890525434793177, 0.00011889973218213533, 0.00011889414674947404, 0.00011888857701285891, 0.00011888295840040074, 0.00011887730210313016, 0.00011887164752205596, 0.00011886589257924701, 0.00011886010004101087, 0.00011885432114296064, 0.00011884846428311396, 0.00011884254932286863, 0.00011883664367656413, 0.00011883060211108091, 0.00011882453414120114, 0.00011881839727318711, 0.00011881217132385418, 0.00011880591357125688, 0.00011879959934507211, 0.00011879330285291671, 0.00011878696663796567, 0.00011878065641623915, 0.00011877433570068677, 0.00011876797292044578, 0.00011876155114595386, 0.00011875514163511534, 0.00011874869222271071, 0.00011874230781124438, 0.00011873588015520442, 0.00011872941236519678, 0.00011872293141768348, 0.00011871640093287645, 0.00011870985278554824, 0.00011870331175328422, 0.00011869680227400593, 0.00011869028592994208, 0.0001186836814699198, 0.00011867706565797347, 0.00011867040783496967, 0.00011866369427133726, 0.00011865693501397596, 0.0001186501194439206, 0.00011864330746715143, 0.00011863645919997941, 0.00011862963832044146, 0.00011862280101189847, 0.0001186159433414004, 0.0001186090149493094, 0.00011860200044348909, 0.00011859501130519669, 0.00011858804587186692, 0.00011858103220626776, 0.00011857399950161489, 0.00011856699009075262, 0.00011855995811905863, 0.0001185529107731053, 0.00011854587615560855, 0.00011853880825105263, 0.00011853172726049923, 0.00011852467578494834, 0.00011851761563903028, 0.00011851047653020119, 0.00011850333291635659, 0.00011849617639528496, 0.00011848908485727458, 0.00011848194362107706, 0.00011847481318261516, 0.0001184676708022867, 0.00011846055269898636, 0.00011845337776455763, 0.0001184461887251398, 0.00011843895025569053, 0.00011843174720066886, 0.00011842455476461234, 0.00011841737770281517, 0.00011841012035520559, 0.00011840292514820699, 0.00011839572314793102, 0.00011838849957006085, 0.00011838126020675927, 0.00011837406694835867, 0.00011836685373917502, 0.00011835957685195445, 0.00011835229538821018, 0.00011834496111567281, 0.00011833771504847909, 0.00011833045149753465, 0.00011832314134113198, 0.00011831578679602474, 0.00011830846787986966, 0.00011830113258834067, 0.00011829379836943445, 0.00011828649488116983, 0.00011827914872039704, 0.00011827176864686851, 0.00011826444170391988, 0.00011825708320798554, 0.00011824967991217459, 0.00011824225389463851, 0.00011823485885802267, 0.00011822745686723606, 0.00011822004463290224, 0.0001182126295382411, 0.00011820523798764918, 0.00011819785510742446, 0.00011819043976248468, 0.00011818299450924739, 0.00011817555342136176, 0.00011816813800491378, 0.00011816070508683806, 0.00011815327168608208, 0.00011814585921934666, 0.00011813839090472036, 0.00011813091533201351, 0.00011812348968989538, 0.00011811602563000598, 0.00011810853881263734, 0.00011810105469470262, 0.00011809362935649428, 0.0001180861374799217, 0.00011807866260441963, 0.00011807117162169934, 0.00011806365922227827, 0.00011805618574118576, 0.00011804871470924851, 0.00011804120443719588, 0.00011803371738742565, 0.00011802625527613423, 0.00011801873610131282, 0.0001180111560193966, 0.00011800360668599903, 0.00011799604002974415, 0.00011798853950068144, 0.00011798103064091542, 0.00011797342842721657, 0.00011796581827610941, 0.00011795824859863381, 0.00011795067917143683, 0.00011794309583589828, 0.00011793550130932911, 0.00011792792746650183], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012002245095294539, 0.00012001792248698704, 0.00012001337581928455, 0.00012000881313025521, 0.00012000424743130202, 0.0001199996431958437, 0.00011999505701992836, 0.00011999045726380551, 0.00011998585196183873, 0.00011998127109476544, 0.00011997666432338704, 0.00011997200316007792, 0.00011996729805662087, 0.00011996258363899007, 0.000119957848602196, 0.00011995307083396357, 0.00011994830934776005, 0.00011994352954131148, 0.00011993874644053681, 0.00011993393454403414, 0.00011992910463538882, 0.00011992425654869957, 0.00011991938021138663, 0.00011991445621299627, 0.00011990955314187163, 0.00011990462459304525, 0.00011989964625012457, 0.00011989467110672923, 0.00011988969025158866, 0.00011988470811663796, 0.00011987968635497348, 0.00011987464295503765, 0.00011986957130447813, 0.00011986446583375079, 0.00011985934133177277, 0.00011985420839252793, 0.00011984898361505588, 0.00011984377165938537, 0.0001198385634483445, 0.00011983331937891968, 0.00011982809182852568, 0.00011982280851159008, 0.00011981750746691412, 0.00011981216489950935, 0.00011980676784537314, 0.00011980140823753345, 0.00011979599307645385, 0.00011979051548907995, 0.00011978508335866598, 0.00011977960942112097, 0.00011977411258919468, 0.00011976860696449876, 0.00011976307666790748, 0.00011975749055737423, 0.00011975186548373246, 0.00011974617134774383, 0.0001197404923798753, 0.00011973480706035648, 0.00011972915150827337, 0.0001197234355207117, 0.00011971767035526063, 0.00011971187753169053, 0.00011970601953260437, 0.00011970016956788181, 0.00011969426961946347, 0.00011968831907114445, 0.00011968234420643288, 0.00011967627572598002, 0.00011967019089239767, 0.00011966411909391855, 0.00011965796448698371, 0.00011965180608801884, 0.00011964562706989069, 0.00011963931300809322, 0.00011963293730210761, 0.0001196265085788022, 0.00011962004376011097, 0.00011961354952948684, 0.00011960700761408511, 0.00011960050653410675, 0.00011959394471054738, 0.00011958739905051603, 0.00011958083227361743, 0.00011957424077742311, 0.00011956759922643243, 0.0001195609804750223, 0.00011955431538974526, 0.00011954771827660647, 0.00011954107433189684, 0.0001195343979179301, 0.00011952768744679369, 0.00011952091881539673, 0.00011951414193633447, 0.00011950735991433147, 0.00011950061536232519, 0.00011949383504673571, 0.00011948696097320383, 0.00011948012242625325, 0.00011947324863712361, 0.0001194663332778648, 0.00011945938848297291, 0.0001194523773986561, 0.00011944536482122747, 0.00011943833932719658, 0.00011943134004557323, 0.00011942434085875064, 0.00011941731090908446, 0.0001194102620703687, 0.00011940312760287489, 0.00011939602306871811, 0.00011938893486399064, 0.00011938178124674519, 0.00011937460061128582, 0.0001193674488663552, 0.00011936026524467218, 0.00011935306358714634, 0.00011934586764136573, 0.00011933865446554866, 0.00011933141178299797, 0.00011932419993439143, 0.0001193169757616873, 0.00011930963955819607, 0.00011930231127056751, 0.00011929495605952576, 0.00011928767045593521, 0.00011928034430132353, 0.00011927302352665447, 0.00011926567798528929, 0.00011925835950953843, 0.0001192509833238306, 0.00011924361140415655, 0.00011923620007107041, 0.0001192288217049453, 0.00011922145153908514, 0.00011921410040447567, 0.00011920665930395382, 0.00011919927088894915, 0.00011919187884781576, 0.0001191844529391142, 0.00011917700897086963, 0.00011916960218821952, 0.00011916217438350295, 0.00011915467621292919, 0.00011914717662034417, 0.00011913961744548734, 0.00011913214254849788, 0.00011912464748264869, 0.00011911709648435663, 0.00011910949194734061, 0.0001191019237190121, 0.00011909434743261978, 0.00011908676507897942, 0.0001190792128245774, 0.00011907162203367023, 0.00011906399803880013, 0.00011905643495341235, 0.00011904883529863499, 0.00011904118928629055, 0.00011903350011590437, 0.00011902583424280267, 0.00011901816339266156, 0.00011901048460295758, 0.00011900280531554965, 0.00011899514667100518, 0.0001189874997817538, 0.00011897982902641345, 0.00011897212236528876, 0.00011896441639146953, 0.00011895673354903345, 0.0001189490366286859, 0.00011894133411509407, 0.00011893365617859685, 0.00011892591517590022, 0.00011891817851033796, 0.00011891047836676489, 0.00011890273997108891, 0.00011889497721162003, 0.00011888723035497707, 0.00011887953594684941, 0.00011887177783261729, 0.00011886403294308991, 0.00011885626366606941, 0.00011884847149466762, 0.00011884071707766479, 0.00011883297069502557, 0.00011882520201050976, 0.00011881745702618163, 0.00011880972156932893, 0.0001188019410110191, 0.00011879410790939321, 0.00011878631102165408, 0.00011877850545964626, 0.00011877077066639881, 0.00011876302937929995, 0.00011875519281744666, 0.00011874734952474006, 0.0001187395553862227, 0.00011873177079888097, 0.0001187239707116165, 0.00011871615502962855], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 05:46:01,929 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:129]: evaluating model ... 
[2017-12-15 05:46:06,748 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:133]: evaluated! 
[2017-12-15 05:46:06,749 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:135]: generating reports ... 
[2017-12-15 05:46:08,566 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:138]: done!
[2017-12-15 05:46:08,568 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_2 finished!
[2018-04-29 11:38:17,567 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_2 was already executed!
[2018-04-29 13:12:02,818 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_2
[2018-04-29 13:12:02,818 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:146]: >> Printing header log
[2018-04-29 13:12:02,818 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_2
	layers = 9216,11059
	using GLOBAL obj = 
		{'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'store_history': True, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'autoencoder_configs': {'discard_decoder_function': True, 'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f732ad67828>, 'output_layer_activation': 'relu'}, 'epochs': 200, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'batch': 32, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'numpy_seed': 666, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'mlp_configs': {'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f732ad67898>, 'use_last_dim_as_classifier': False, 'activation': 'sigmoid'}, 'shuffle_batches': True, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:12:02,818 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,993 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,993 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,993 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:57]: =======================================
[2018-04-29 13:12:20,993 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f732ad67828>, 'output_layer_activation': 'relu'}
[2018-04-29 13:12:21,041 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,871 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_2
[2018-04-29 13:14:12,871 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:146]: >> Printing header log
[2018-04-29 13:14:12,871 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_2
	layers = 9216,11059
	using GLOBAL obj = 
		{'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'batch': 32, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'epochs': 200, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'store_history': True, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'shuffle_batches': True, 'data_dir': '/home/dhiego/malware_dataset/', 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'loss_function': 'mse', 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7ff4327e0828>}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'mlp_configs': {'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False, 'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7ff4327e0898>}, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'numpy_seed': 666}
	=======================================
	
[2018-04-29 13:14:12,871 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:148]: >> Loading dataset... 
[2018-04-29 13:14:30,832 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:30,832 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:30,832 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:57]: =======================================
[2018-04-29 13:14:30,832 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'loss_function': 'mse', 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7ff4327e0828>}
[2018-04-29 13:14:30,879 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,672 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_2
[2018-04-29 13:16:33,672 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:146]: >> Printing header log
[2018-04-29 13:16:33,672 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_2
	layers = 9216,11059
	using GLOBAL obj = 
		{'epochs': 200, 'store_history': True, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'autoencoder_configs': {'optimizer': <keras.optimizers.SGD object at 0x7fa5dac99828>, 'loss_function': 'mse', 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu'}, 'shuffle_batches': True, 'numpy_seed': 666, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'mlp_configs': {'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7fa5dac99898>, 'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False, 'classifier_dim': 9}, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'data_dir': '/home/dhiego/malware_dataset/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'batch': 32}
	=======================================
	
[2018-04-29 13:16:33,673 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:148]: >> Loading dataset... 
[2018-04-29 13:16:57,570 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:57,570 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:57,570 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:57]: =======================================
[2018-04-29 13:16:57,571 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:62]: setting configurations for autoencoder: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7fa5dac99828>, 'loss_function': 'mse', 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'hidden_layer_activation': 'relu'}
[2018-04-29 13:16:57,620 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:21,142 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_2
[2018-04-29 14:30:21,142 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:146]: >> Printing header log
[2018-04-29 14:30:21,142 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_2
	layers = 9216,11059
	using GLOBAL obj = 
		{'data_dir': '/home/dhiego/malware_dataset/', 'shuffle_batches': True, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'epochs': 200, 'mlp_configs': {'optimizer': <keras.optimizers.SGD object at 0x7f19ebd4db38>, 'activation': 'sigmoid', 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False}, 'numpy_seed': 666, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f19ebd4dac8>, 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'loss_function': 'mse'}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'store_history': True, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'batch': 32}
	=======================================
	
[2018-04-29 14:30:21,142 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:148]: >> Loading dataset... 
[2018-04-29 14:30:39,421 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:39,422 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:39,422 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:57]: =======================================
[2018-04-29 14:30:39,422 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f19ebd4dac8>, 'discard_decoder_function': True, 'output_layer_activation': 'relu', 'loss_function': 'mse'}
[2018-04-29 14:30:39,536 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:73]: training and evaluate autoencoder
[2018-04-29 21:19:38,589 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:85]: trained and evaluated!
[2018-04-29 21:19:38,590 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:88]: Training history: 
{'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.00011968047071562356, 0.00011967630770586029, 0.00011967209179791866, 0.00011966790359939795, 0.0001196637211394089, 0.00011965954348834515, 0.00011965538954224405, 0.00011965118434265283, 0.00011964699181788704, 0.00011964276993901215, 0.00011963853897859802, 0.00011963431799357541, 0.00011963007786223683, 0.00011962580823377278, 0.0001196215749850968, 0.00011961730487395252, 0.00011961301277404198, 0.00011960872214004919, 0.00011960437503246892, 0.00011960006282088194, 0.00011959571742949806, 0.00011959135796887919, 0.00011958697340888811, 0.00011958258477293061, 0.00011957819531462898, 0.00011957380503398327, 0.00011956937855231995, 0.00011956499408171409, 0.00011956056506151026, 0.0001195561391519124, 0.00011955168379882021, 0.0001195472559942555, 0.00011954276810494007, 0.00011953825520563765, 0.00011953374777671123, 0.00011952921762605968, 0.00011952467202964062, 0.00011952009732939111, 0.00011951552990509921, 0.00011951092930101041, 0.0001195063145383014, 0.00011950168311418576, 0.00011949704064205587, 0.00011949241386597212, 0.00011948778070778305, 0.00011948311665805894, 0.00011947844483181995, 0.00011947375064139674, 0.00011946904504541834, 0.000119464305465176, 0.00011945955630283715, 0.00011945477812605308, 0.00011945000592020228, 0.00011944523870204724, 0.00011944043354880116, 0.00011943564965136243, 0.00011943084589252592, 0.00011942601589018625, 0.00011942120371126119, 0.0001194163413335917, 0.00011941147927770903, 0.00011940661509445793, 0.00011940173308779223, 0.00011939684419846391, 0.0001193919104377508, 0.00011938699671720596, 0.00011938204970960197, 0.00011937710704612007, 0.00011937212839614512, 0.0001193671504970061, 0.00011936212150527036, 0.00011935705311252583, 0.00011935195415003307, 0.00011934682150718616, 0.00011934165747224694, 0.00011933650357359265, 0.00011933132441467274, 0.00011932613374293538, 0.00011932092630252911, 0.00011931565387906159, 0.00011931034595178124, 0.00011930504261890165, 0.0001192996609309306, 0.00011929421511799651, 0.00011928871036444262, 0.00011928322221866422, 0.00011927768330207594, 0.00011927211422691149, 0.00011926651392054815, 0.00011926085492384367, 0.00011925517998081437, 0.00011924948215516655, 0.00011924377746473316, 0.0001192379638852142, 0.0001192321462297288, 0.0001192262913899883, 0.0001192204121659574, 0.00011921451897995377, 0.0001192085723415834, 0.00011920256847878964, 0.00011919650615805632], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012051417163940274, 0.000120509852635703, 0.00012050552225591317, 0.00012050114260343319, 0.00012049678041799152, 0.00012049243759560317, 0.00012048808946437279, 0.00012048375733076908, 0.00012047937184804293, 0.00012047499579799145, 0.00012047058789488778, 0.0001204661617189394, 0.00012046173170356061, 0.00012045727630528085, 0.00012045280730309334, 0.00012044839707737128, 0.00012044394345660558, 0.00012043948376859184, 0.00012043503225714283, 0.00012043053934146599, 0.00012042607597992316, 0.00012042159548314464, 0.00012041709012486926, 0.00012041255661077094, 0.00012040802850031539, 0.00012040349441741256, 0.00012039896369993637, 0.00012039441267139937, 0.00012038990169617952, 0.00012038534531140009, 0.00012038079949690436, 0.00012037620611613198, 0.00012037163356782456, 0.00012036699892502545, 0.00012036233600790245, 0.00012035767612440348, 0.0001203529978021585, 0.00012034830734541743, 0.0001203435997060403, 0.00012033890851459342, 0.00012033420255792961, 0.00012032946944085074, 0.00012032473267394297, 0.00012031998728016689, 0.00012031524036957879, 0.00012031047997358391, 0.00012030567796005949, 0.00012030086971338571, 0.00012029603380859293, 0.00012029117410881172, 0.00012028627935645295, 0.00012028138853832533, 0.00012027647231359653, 0.00012027157708723401, 0.00012026667726303509, 0.00012026173385419103, 0.00012025681227321982, 0.00012025188483830226, 0.00012024693363209647, 0.00012024199140826179, 0.00012023700095454522, 0.00012023200192136072, 0.00012022698264821593, 0.00012022194242410525, 0.00012021687710149583, 0.00012021174795428104, 0.00012020663800421823, 0.00012020150264755426, 0.0001201963697320096, 0.0001201911917624082, 0.00012018601850914415, 0.00012018079631499255, 0.00012017553883126152, 0.00012017025783694431, 0.00012016493987033435, 0.00012015961915450261, 0.00012015428997770389, 0.00012014892885305217, 0.00012014355509620044, 0.00012013815344422783, 0.00012013268559763104, 0.00012012718547137866, 0.00012012168980076157, 0.00012011614272443741, 0.000120110576166559, 0.00012010496969756454, 0.00012009937019642528, 0.00012009373391259474, 0.00012008806935444031, 0.0001200823828736123, 0.00012007662925015261, 0.00012007087709610456, 0.0001200651097976366, 0.00012005932650154196, 0.00012005345499334881, 0.00012004760680614033, 0.00012004171922921995, 0.00012003580778621059, 0.00012002988845103872, 0.00012002392008017858, 0.00012001790238922789]}
[2018-04-29 21:19:38,590 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:92]: done!
[2018-04-29 21:19:38,591 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:152]: >> Executing classifier part ... 
[2018-04-29 21:19:38,591 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:97]: =======================================
[2018-04-29 21:19:38,591 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:101]: setting configurations for classifier: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7f19ebd4db38>, 'activation': 'sigmoid', 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False}
[2018-04-29 21:19:38,680 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:110]: training ... 
[2018-04-30 02:14:46,391 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:122]: trained!
[2018-04-30 02:14:46,393 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:125]: Training history: 
{'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.00011968047071562356, 0.00011967630770586029, 0.00011967209179791866, 0.00011966790359939795, 0.0001196637211394089, 0.00011965954348834515, 0.00011965538954224405, 0.00011965118434265283, 0.00011964699181788704, 0.00011964276993901215, 0.00011963853897859802, 0.00011963431799357541, 0.00011963007786223683, 0.00011962580823377278, 0.0001196215749850968, 0.00011961730487395252, 0.00011961301277404198, 0.00011960872214004919, 0.00011960437503246892, 0.00011960006282088194, 0.00011959571742949806, 0.00011959135796887919, 0.00011958697340888811, 0.00011958258477293061, 0.00011957819531462898, 0.00011957380503398327, 0.00011956937855231995, 0.00011956499408171409, 0.00011956056506151026, 0.0001195561391519124, 0.00011955168379882021, 0.0001195472559942555, 0.00011954276810494007, 0.00011953825520563765, 0.00011953374777671123, 0.00011952921762605968, 0.00011952467202964062, 0.00011952009732939111, 0.00011951552990509921, 0.00011951092930101041, 0.0001195063145383014, 0.00011950168311418576, 0.00011949704064205587, 0.00011949241386597212, 0.00011948778070778305, 0.00011948311665805894, 0.00011947844483181995, 0.00011947375064139674, 0.00011946904504541834, 0.000119464305465176, 0.00011945955630283715, 0.00011945477812605308, 0.00011945000592020228, 0.00011944523870204724, 0.00011944043354880116, 0.00011943564965136243, 0.00011943084589252592, 0.00011942601589018625, 0.00011942120371126119, 0.0001194163413335917, 0.00011941147927770903, 0.00011940661509445793, 0.00011940173308779223, 0.00011939684419846391, 0.0001193919104377508, 0.00011938699671720596, 0.00011938204970960197, 0.00011937710704612007, 0.00011937212839614512, 0.0001193671504970061, 0.00011936212150527036, 0.00011935705311252583, 0.00011935195415003307, 0.00011934682150718616, 0.00011934165747224694, 0.00011933650357359265, 0.00011933132441467274, 0.00011932613374293538, 0.00011932092630252911, 0.00011931565387906159, 0.00011931034595178124, 0.00011930504261890165, 0.0001192996609309306, 0.00011929421511799651, 0.00011928871036444262, 0.00011928322221866422, 0.00011927768330207594, 0.00011927211422691149, 0.00011926651392054815, 0.00011926085492384367, 0.00011925517998081437, 0.00011924948215516655, 0.00011924377746473316, 0.0001192379638852142, 0.0001192321462297288, 0.0001192262913899883, 0.0001192204121659574, 0.00011921451897995377, 0.0001192085723415834, 0.00011920256847878964, 0.00011919650615805632], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012051417163940274, 0.000120509852635703, 0.00012050552225591317, 0.00012050114260343319, 0.00012049678041799152, 0.00012049243759560317, 0.00012048808946437279, 0.00012048375733076908, 0.00012047937184804293, 0.00012047499579799145, 0.00012047058789488778, 0.0001204661617189394, 0.00012046173170356061, 0.00012045727630528085, 0.00012045280730309334, 0.00012044839707737128, 0.00012044394345660558, 0.00012043948376859184, 0.00012043503225714283, 0.00012043053934146599, 0.00012042607597992316, 0.00012042159548314464, 0.00012041709012486926, 0.00012041255661077094, 0.00012040802850031539, 0.00012040349441741256, 0.00012039896369993637, 0.00012039441267139937, 0.00012038990169617952, 0.00012038534531140009, 0.00012038079949690436, 0.00012037620611613198, 0.00012037163356782456, 0.00012036699892502545, 0.00012036233600790245, 0.00012035767612440348, 0.0001203529978021585, 0.00012034830734541743, 0.0001203435997060403, 0.00012033890851459342, 0.00012033420255792961, 0.00012032946944085074, 0.00012032473267394297, 0.00012031998728016689, 0.00012031524036957879, 0.00012031047997358391, 0.00012030567796005949, 0.00012030086971338571, 0.00012029603380859293, 0.00012029117410881172, 0.00012028627935645295, 0.00012028138853832533, 0.00012027647231359653, 0.00012027157708723401, 0.00012026667726303509, 0.00012026173385419103, 0.00012025681227321982, 0.00012025188483830226, 0.00012024693363209647, 0.00012024199140826179, 0.00012023700095454522, 0.00012023200192136072, 0.00012022698264821593, 0.00012022194242410525, 0.00012021687710149583, 0.00012021174795428104, 0.00012020663800421823, 0.00012020150264755426, 0.0001201963697320096, 0.0001201911917624082, 0.00012018601850914415, 0.00012018079631499255, 0.00012017553883126152, 0.00012017025783694431, 0.00012016493987033435, 0.00012015961915450261, 0.00012015428997770389, 0.00012014892885305217, 0.00012014355509620044, 0.00012013815344422783, 0.00012013268559763104, 0.00012012718547137866, 0.00012012168980076157, 0.00012011614272443741, 0.000120110576166559, 0.00012010496969756454, 0.00012009937019642528, 0.00012009373391259474, 0.00012008806935444031, 0.0001200823828736123, 0.00012007662925015261, 0.00012007087709610456, 0.0001200651097976366, 0.00012005932650154196, 0.00012005345499334881, 0.00012004760680614033, 0.00012004171922921995, 0.00012003580778621059, 0.00012002988845103872, 0.00012002392008017858, 0.00012001790238922789]}
[2018-04-30 02:14:46,393 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:129]: evaluating model ... 
[2018-04-30 02:14:52,188 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:133]: evaluated! 
[2018-04-30 02:14:52,189 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:135]: generating reports ... 
[2018-04-30 02:14:54,082 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:138]: done!
[2018-04-30 02:14:54,082 AE_BIGRAMA_1L_MINIDS_OVER_F1_2.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_2 finished!
