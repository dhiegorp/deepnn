[2017-12-14 09:31:57,807 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F2_0
[2017-12-14 09:31:57,807 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:146]: >> Printing header log
[2017-12-14 09:31:57,807 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F2_0
	layers = 9216,18432
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f19e110cef0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f19e10f0438>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,807 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:148]: >> Loading dataset... 
[2017-12-14 09:32:19,909 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:19,909 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:19,909 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:57]: =======================================
[2017-12-14 09:32:19,909 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f19e110cef0>, 'discard_decoder_function': True}
[2017-12-14 09:32:19,954 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:55,113 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F2_0
[2017-12-14 10:18:55,113 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:146]: >> Printing header log
[2017-12-14 10:18:55,113 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F2_0
	layers = 9216,18432
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f47ac38beb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f47ac36e400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:55,114 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:148]: >> Loading dataset... 
[2017-12-14 10:19:17,624 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:17,625 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:17,625 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:57]: =======================================
[2017-12-14 10:19:17,625 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f47ac38beb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:17,673 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:73]: training and evaluate autoencoder
[2017-12-15 05:37:18,944 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:85]: trained and evaluated!
[2017-12-15 05:37:18,947 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:88]: Training history: 
{'val_loss': [0.00011678678727982204, 0.00011677762212964572, 0.00011676844986440517, 0.00011675930759684739, 0.00011675012240650279, 0.00011674093558934703, 0.00011673172547840067, 0.0001167225429159818, 0.00011671338438031239, 0.00011670419708047638, 0.00011669504076156065, 0.00011668583783718669, 0.00011667671675392809, 0.00011666752650437955, 0.00011665838963568956, 0.00011664917691469454, 0.00011663999892879937, 0.00011663077524917529, 0.00011662162245203757, 0.00011661239820034804, 0.00011660315939674328, 0.00011659396294385986, 0.00011658474866756186, 0.00011657554779904815, 0.00011656634921879629, 0.00011655716413571396, 0.00011654789640704921, 0.00011653872022673566, 0.00011652956326424628, 0.00011652038504594951, 0.00011651121639187222, 0.00011650206711651249, 0.00011649294301203318, 0.00011648370651456731, 0.00011647448530197555, 0.00011646525575868049, 0.00011645607795155577, 0.00011644682902954312, 0.00011643758452316077, 0.00011642835186925975, 0.00011641913294492986, 0.00011640992138594279, 0.00011640070737780047, 0.00011639140680900269, 0.00011638215749369504, 0.0001163728911773169, 0.00011636364701059842, 0.00011635441700250016, 0.00011634510204268056, 0.00011633583556540901, 0.00011632652968712866, 0.00011631720230276227, 0.00011630792871042651, 0.00011629860786905887, 0.00011628936493585656, 0.00011628008555135797, 0.00011627084715892527, 0.00011626157401351565, 0.00011625229244801749, 0.00011624301047134728, 0.0001162337375762163, 0.00011622451813345208, 0.0001162152024764277, 0.00011620594584940835, 0.0001161966359130386, 0.00011618738590052617, 0.00011617815886001751, 0.0001161688714129713, 0.00011615963881270141, 0.0001161504021364651, 0.0001161411150112057, 0.00011613188503886154, 0.00011612259530355348, 0.00011611335068990887, 0.00011610409978354414, 0.00011609483073197799, 0.00011608557794852347, 0.00011607629418414869, 0.00011606704671017674, 0.000116057756313418, 0.00011604850597911874, 0.00011603928834193612, 0.00011603007454831835, 0.00011602087114126415, 0.00011601160657683649, 0.00011600228602150157, 0.00011599301443139493, 0.00011598379059087746, 0.00011597452874376074, 0.00011596527259942162, 0.00011595606241696709, 0.00011594682818988603, 0.00011593755668916463, 0.00011592829727332613, 0.00011591906828421948, 0.00011590982703145946, 0.00011590054482238766, 0.00011589132400309091, 0.00011588211087092382, 0.00011587284918470052, 0.00011586360187162198, 0.00011585438244673481, 0.00011584510252592486, 0.00011583585285307629, 0.00011582662688519036, 0.00011581742299545593, 0.00011580817046228005, 0.00011579893273129803, 0.00011578968257576923, 0.00011578051865910905, 0.00011577124267124917, 0.00011576199970229275, 0.00011575270083181432, 0.00011574344200591834, 0.00011573418997329973, 0.00011572490499328584, 0.00011571573494479897, 0.00011570652599586059, 0.00011569732080110182, 0.00011568810130470645, 0.00011567883087660779, 0.00011566961955002232, 0.00011566040062569242, 0.00011565116689916865, 0.00011564192246429448, 0.00011563269397574511, 0.00011562350088374629, 0.00011561425276620071, 0.00011560505068204789, 0.0001155958516191158, 0.00011558665738298606, 0.000115577431754764, 0.00011556826211744919, 0.00011555906559305759, 0.00011554989162949772, 0.00011554067940905997, 0.00011553145435290338, 0.00011552221204539766, 0.00011551298600600354, 0.00011550377035317301, 0.00011549456172602145, 0.00011548538245297899, 0.00011547614268401376, 0.00011546700878291339, 0.00011545783122606731, 0.00011544866028372816, 0.00011543946956937643, 0.00011543028155445861, 0.00011542110743000531, 0.00011541187960290664, 0.00011540270580024017, 0.00011539353780761356, 0.00011538437063733107, 0.00011537517861795499, 0.00011536604332244504, 0.00011535686462146805, 0.00011534772680529464, 0.00011533858603940869, 0.00011532943275959073, 0.00011532028496802581, 0.00011531114150270595, 0.00011530195831459048, 0.00011529279768730672, 0.0001152836391873914, 0.00011527448959024486, 0.00011526533001770679, 0.00011525619588420482, 0.00011524706591605449, 0.00011523794084621469, 0.00011522882419646344, 0.00011521969315569038, 0.00011521054234290472, 0.00011520143084174265, 0.00011519232187912106, 0.00011518319738134673, 0.00011517407125676124, 0.00011516499400801878, 0.00011515588577835608, 0.00011514675130519023, 0.00011513764282524887, 0.0001151285447318711, 0.00011511943004859487, 0.00011511033424347895, 0.00011510120558035297, 0.00011509206799658116, 0.00011508302190752943, 0.0001150739485023518, 0.00011506483945034499, 0.00011505574348433566, 0.00011504668218191801, 0.00011503761254879703, 0.00011502854609779021, 0.00011501949632606704, 0.00011501039692766493, 0.00011500136015255403, 0.0001149922517619979, 0.00011498326280798434, 0.00011497418246651297, 0.00011496506721117129, 0.00011495601826179224], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0073710073710073713, 0.0073710073710073713, 0.0098280098280098278, 0.014742014742014743, 0.017199017199017199, 0.019656019656019656, 0.019656019656019656, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.024570024661555043, 0.024570024661555043, 0.027027027118557503, 0.027027027118557503, 0.027027027118557503, 0.027027027118557503, 0.029484029575559959, 0.031941032032562412], 'loss': [0.00011754792267725885, 0.00011753860658378999, 0.00011752920891427517, 0.00011751980814003576, 0.00011751043585342195, 0.00011750101332241026, 0.00011749158489005185, 0.00011748213903805551, 0.00011747272430440557, 0.00011746333225183523, 0.00011745391142723706, 0.00011744452463610838, 0.00011743508504096158, 0.0001174257324729039, 0.00011741629913460664, 0.00011740692419357179, 0.00011739747322233491, 0.00011738806415302982, 0.00011737860291961168, 0.00011736921508567474, 0.00011735973958474363, 0.00011735024659307401, 0.00011734079860806077, 0.00011733132642515593, 0.00011732187557241997, 0.00011731242187566151, 0.00011730299164208883, 0.00011729348035387434, 0.00011728405559504502, 0.00011727464872985738, 0.00011726522404212862, 0.00011725580350193272, 0.00011724641595239802, 0.00011723704278887726, 0.00011722757842703435, 0.00011721811776242071, 0.00011720865738220932, 0.00011719923662871172, 0.00011718974728687099, 0.00011718025901153871, 0.00011717077860466875, 0.00011716132540561422, 0.00011715187474247979, 0.00011714242061911794, 0.0001171329163224588, 0.00011712345821746543, 0.00011711399200700787, 0.0001171045533361684, 0.0001170951300467507, 0.00011708562539458876, 0.00011707616890120814, 0.00011706668169238431, 0.00011705716492942647, 0.00011704770137338993, 0.00011703818913716792, 0.00011702876494714311, 0.00011701930212581239, 0.00011700988931187766, 0.00011700043947824978, 0.00011699098988162378, 0.000116981534881355, 0.00011697209476480407, 0.00011696269614728174, 0.00011695320422212057, 0.00011694377922628936, 0.00011693429474298711, 0.00011692486654763059, 0.00011691545804713, 0.00011690600205145334, 0.00011689659013812573, 0.00011688718708866831, 0.00011687772801196724, 0.00011686832529431245, 0.00011685885583692918, 0.00011684943863845973, 0.00011684001461433623, 0.00011683056802763406, 0.00011682114170459235, 0.00011681167672654456, 0.00011680225324752538, 0.00011679278663416463, 0.0001167833609273278, 0.00011677396475092481, 0.0001167645678161158, 0.00011675517344092707, 0.00011674572197198624, 0.00011673620110889594, 0.0001167267290918924, 0.00011671731243852727, 0.00011670784772118153, 0.00011669839701064672, 0.00011668898163709171, 0.00011667954941270327, 0.00011667008121142995, 0.00011666061933810674, 0.00011665119078724739, 0.00011664173104694106, 0.00011663224236870561, 0.00011662280362676557, 0.00011661338002924543, 0.00011660390289300137, 0.00011659444118557947, 0.00011658500853458765, 0.00011657550606284297, 0.00011656603748236665, 0.00011655659449809301, 0.00011654718151825696, 0.00011653771620840653, 0.00011652826594817529, 0.00011651880895709072, 0.00011650943195413956, 0.00011649994877434763, 0.00011649049481688711, 0.00011648096641713711, 0.00011647149037110168, 0.0001164620180459957, 0.00011645251403373882, 0.00011644312480149081, 0.00011643369300370575, 0.00011642426914548355, 0.00011641482604270898, 0.00011640533459155156, 0.00011639590388397513, 0.0001163864599279938, 0.00011637700108829462, 0.0001163675342142318, 0.00011635808824373452, 0.00011634867633040691, 0.00011633921779881015, 0.00011632980195125141, 0.00011632037643401608, 0.00011631096971102956, 0.00011630152355093078, 0.00011629213832401449, 0.0001162827256285807, 0.00011627334099416909, 0.00011626392256328988, 0.00011625448379764965, 0.00011624502858407916, 0.00011623558924963443, 0.00011622616541511243, 0.00011621674935425198, 0.00011620736860667115, 0.00011619792453218888, 0.00011618858270031621, 0.00011617920112322883, 0.00011616982651399661, 0.00011616042192402699, 0.0001161510219081936, 0.00011614164412313623, 0.00011613220580779956, 0.00011612281792646225, 0.00011611343765288518, 0.00011610405967822632, 0.00011609465999419556, 0.00011608531318528348, 0.00011607592954627975, 0.00011606657918233952, 0.00011605723588105521, 0.00011604786944838775, 0.0001160385145103113, 0.00011602915364718793, 0.00011601975884539582, 0.00011601038174764389, 0.00011600101057493889, 0.00011599164774469995, 0.00011598227408347524, 0.00011597292694276052, 0.00011596357994424695, 0.0001159542413355998, 0.00011594490881790089, 0.0001159355666305254, 0.00011592620920392925, 0.00011591688201877255, 0.00011590756284427929, 0.00011589822532584077, 0.00011588889164683266, 0.00011587960022525916, 0.00011587028389478843, 0.00011586093748877954, 0.00011585161217593767, 0.0001158423055151435, 0.00011583297664727348, 0.0001158236679008628, 0.00011581432696959726, 0.00011580498139309494, 0.00011579572424199281, 0.00011578643471643432, 0.00011577711625294669, 0.00011576780527871836, 0.00011575853047097642, 0.00011574924625425996, 0.00011573997047481033, 0.00011573070476794045, 0.00011572139322490762, 0.00011571214541167943, 0.00011570283066912127, 0.0001156936268197412, 0.0001156843377207861, 0.00011567500702800163], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0032573289902280132, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0065146579804560263, 0.0065146579804560263, 0.0065146579804560263, 0.0073289902280130291, 0.0081433224755700327, 0.0089576547231270363, 0.0097719870192220226, 0.010586319218241042, 0.011400651465798045, 0.01221498373762404, 0.013029315960912053, 0.013843648208469055, 0.013843648208469055, 0.013843648208469055, 0.014657980456026058, 0.016286644951140065, 0.016286644951140065, 0.016286644951140065, 0.016286644951140065, 0.016286644975409059, 0.017100977198697069, 0.019543973989906063, 0.020358306237463066, 0.021172638436482084, 0.021986970684039087, 0.021986970684039087, 0.023615635179153095, 0.024429967426710098, 0.026058631946093095, 0.026058631921824105, 0.026058631946093095, 0.026058631921824105, 0.026058631946093095, 0.026872964169381109, 0.030130293183878113]}
[2017-12-15 05:37:18,947 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:92]: done!
[2017-12-15 05:37:18,947 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:152]: >> Executing classifier part ... 
[2017-12-15 05:37:18,947 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:97]: =======================================
[2017-12-15 05:37:18,948 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f47ac36e400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-15 05:37:19,031 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:110]: training ... 
[2017-12-15 10:21:26,566 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:122]: trained!
[2017-12-15 10:21:26,567 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:125]: Training history: 
{'val_loss': [0.00011678678727982204, 0.00011677762212964572, 0.00011676844986440517, 0.00011675930759684739, 0.00011675012240650279, 0.00011674093558934703, 0.00011673172547840067, 0.0001167225429159818, 0.00011671338438031239, 0.00011670419708047638, 0.00011669504076156065, 0.00011668583783718669, 0.00011667671675392809, 0.00011666752650437955, 0.00011665838963568956, 0.00011664917691469454, 0.00011663999892879937, 0.00011663077524917529, 0.00011662162245203757, 0.00011661239820034804, 0.00011660315939674328, 0.00011659396294385986, 0.00011658474866756186, 0.00011657554779904815, 0.00011656634921879629, 0.00011655716413571396, 0.00011654789640704921, 0.00011653872022673566, 0.00011652956326424628, 0.00011652038504594951, 0.00011651121639187222, 0.00011650206711651249, 0.00011649294301203318, 0.00011648370651456731, 0.00011647448530197555, 0.00011646525575868049, 0.00011645607795155577, 0.00011644682902954312, 0.00011643758452316077, 0.00011642835186925975, 0.00011641913294492986, 0.00011640992138594279, 0.00011640070737780047, 0.00011639140680900269, 0.00011638215749369504, 0.0001163728911773169, 0.00011636364701059842, 0.00011635441700250016, 0.00011634510204268056, 0.00011633583556540901, 0.00011632652968712866, 0.00011631720230276227, 0.00011630792871042651, 0.00011629860786905887, 0.00011628936493585656, 0.00011628008555135797, 0.00011627084715892527, 0.00011626157401351565, 0.00011625229244801749, 0.00011624301047134728, 0.0001162337375762163, 0.00011622451813345208, 0.0001162152024764277, 0.00011620594584940835, 0.0001161966359130386, 0.00011618738590052617, 0.00011617815886001751, 0.0001161688714129713, 0.00011615963881270141, 0.0001161504021364651, 0.0001161411150112057, 0.00011613188503886154, 0.00011612259530355348, 0.00011611335068990887, 0.00011610409978354414, 0.00011609483073197799, 0.00011608557794852347, 0.00011607629418414869, 0.00011606704671017674, 0.000116057756313418, 0.00011604850597911874, 0.00011603928834193612, 0.00011603007454831835, 0.00011602087114126415, 0.00011601160657683649, 0.00011600228602150157, 0.00011599301443139493, 0.00011598379059087746, 0.00011597452874376074, 0.00011596527259942162, 0.00011595606241696709, 0.00011594682818988603, 0.00011593755668916463, 0.00011592829727332613, 0.00011591906828421948, 0.00011590982703145946, 0.00011590054482238766, 0.00011589132400309091, 0.00011588211087092382, 0.00011587284918470052, 0.00011586360187162198, 0.00011585438244673481, 0.00011584510252592486, 0.00011583585285307629, 0.00011582662688519036, 0.00011581742299545593, 0.00011580817046228005, 0.00011579893273129803, 0.00011578968257576923, 0.00011578051865910905, 0.00011577124267124917, 0.00011576199970229275, 0.00011575270083181432, 0.00011574344200591834, 0.00011573418997329973, 0.00011572490499328584, 0.00011571573494479897, 0.00011570652599586059, 0.00011569732080110182, 0.00011568810130470645, 0.00011567883087660779, 0.00011566961955002232, 0.00011566040062569242, 0.00011565116689916865, 0.00011564192246429448, 0.00011563269397574511, 0.00011562350088374629, 0.00011561425276620071, 0.00011560505068204789, 0.0001155958516191158, 0.00011558665738298606, 0.000115577431754764, 0.00011556826211744919, 0.00011555906559305759, 0.00011554989162949772, 0.00011554067940905997, 0.00011553145435290338, 0.00011552221204539766, 0.00011551298600600354, 0.00011550377035317301, 0.00011549456172602145, 0.00011548538245297899, 0.00011547614268401376, 0.00011546700878291339, 0.00011545783122606731, 0.00011544866028372816, 0.00011543946956937643, 0.00011543028155445861, 0.00011542110743000531, 0.00011541187960290664, 0.00011540270580024017, 0.00011539353780761356, 0.00011538437063733107, 0.00011537517861795499, 0.00011536604332244504, 0.00011535686462146805, 0.00011534772680529464, 0.00011533858603940869, 0.00011532943275959073, 0.00011532028496802581, 0.00011531114150270595, 0.00011530195831459048, 0.00011529279768730672, 0.0001152836391873914, 0.00011527448959024486, 0.00011526533001770679, 0.00011525619588420482, 0.00011524706591605449, 0.00011523794084621469, 0.00011522882419646344, 0.00011521969315569038, 0.00011521054234290472, 0.00011520143084174265, 0.00011519232187912106, 0.00011518319738134673, 0.00011517407125676124, 0.00011516499400801878, 0.00011515588577835608, 0.00011514675130519023, 0.00011513764282524887, 0.0001151285447318711, 0.00011511943004859487, 0.00011511033424347895, 0.00011510120558035297, 0.00011509206799658116, 0.00011508302190752943, 0.0001150739485023518, 0.00011506483945034499, 0.00011505574348433566, 0.00011504668218191801, 0.00011503761254879703, 0.00011502854609779021, 0.00011501949632606704, 0.00011501039692766493, 0.00011500136015255403, 0.0001149922517619979, 0.00011498326280798434, 0.00011497418246651297, 0.00011496506721117129, 0.00011495601826179224], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0024570024570024569, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0049140049140049139, 0.0073710073710073713, 0.0073710073710073713, 0.0098280098280098278, 0.014742014742014743, 0.017199017199017199, 0.019656019656019656, 0.019656019656019656, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.022113022158787351, 0.024570024661555043, 0.024570024661555043, 0.027027027118557503, 0.027027027118557503, 0.027027027118557503, 0.027027027118557503, 0.029484029575559959, 0.031941032032562412], 'loss': [0.00011754792267725885, 0.00011753860658378999, 0.00011752920891427517, 0.00011751980814003576, 0.00011751043585342195, 0.00011750101332241026, 0.00011749158489005185, 0.00011748213903805551, 0.00011747272430440557, 0.00011746333225183523, 0.00011745391142723706, 0.00011744452463610838, 0.00011743508504096158, 0.0001174257324729039, 0.00011741629913460664, 0.00011740692419357179, 0.00011739747322233491, 0.00011738806415302982, 0.00011737860291961168, 0.00011736921508567474, 0.00011735973958474363, 0.00011735024659307401, 0.00011734079860806077, 0.00011733132642515593, 0.00011732187557241997, 0.00011731242187566151, 0.00011730299164208883, 0.00011729348035387434, 0.00011728405559504502, 0.00011727464872985738, 0.00011726522404212862, 0.00011725580350193272, 0.00011724641595239802, 0.00011723704278887726, 0.00011722757842703435, 0.00011721811776242071, 0.00011720865738220932, 0.00011719923662871172, 0.00011718974728687099, 0.00011718025901153871, 0.00011717077860466875, 0.00011716132540561422, 0.00011715187474247979, 0.00011714242061911794, 0.0001171329163224588, 0.00011712345821746543, 0.00011711399200700787, 0.0001171045533361684, 0.0001170951300467507, 0.00011708562539458876, 0.00011707616890120814, 0.00011706668169238431, 0.00011705716492942647, 0.00011704770137338993, 0.00011703818913716792, 0.00011702876494714311, 0.00011701930212581239, 0.00011700988931187766, 0.00011700043947824978, 0.00011699098988162378, 0.000116981534881355, 0.00011697209476480407, 0.00011696269614728174, 0.00011695320422212057, 0.00011694377922628936, 0.00011693429474298711, 0.00011692486654763059, 0.00011691545804713, 0.00011690600205145334, 0.00011689659013812573, 0.00011688718708866831, 0.00011687772801196724, 0.00011686832529431245, 0.00011685885583692918, 0.00011684943863845973, 0.00011684001461433623, 0.00011683056802763406, 0.00011682114170459235, 0.00011681167672654456, 0.00011680225324752538, 0.00011679278663416463, 0.0001167833609273278, 0.00011677396475092481, 0.0001167645678161158, 0.00011675517344092707, 0.00011674572197198624, 0.00011673620110889594, 0.0001167267290918924, 0.00011671731243852727, 0.00011670784772118153, 0.00011669839701064672, 0.00011668898163709171, 0.00011667954941270327, 0.00011667008121142995, 0.00011666061933810674, 0.00011665119078724739, 0.00011664173104694106, 0.00011663224236870561, 0.00011662280362676557, 0.00011661338002924543, 0.00011660390289300137, 0.00011659444118557947, 0.00011658500853458765, 0.00011657550606284297, 0.00011656603748236665, 0.00011655659449809301, 0.00011654718151825696, 0.00011653771620840653, 0.00011652826594817529, 0.00011651880895709072, 0.00011650943195413956, 0.00011649994877434763, 0.00011649049481688711, 0.00011648096641713711, 0.00011647149037110168, 0.0001164620180459957, 0.00011645251403373882, 0.00011644312480149081, 0.00011643369300370575, 0.00011642426914548355, 0.00011641482604270898, 0.00011640533459155156, 0.00011639590388397513, 0.0001163864599279938, 0.00011637700108829462, 0.0001163675342142318, 0.00011635808824373452, 0.00011634867633040691, 0.00011633921779881015, 0.00011632980195125141, 0.00011632037643401608, 0.00011631096971102956, 0.00011630152355093078, 0.00011629213832401449, 0.0001162827256285807, 0.00011627334099416909, 0.00011626392256328988, 0.00011625448379764965, 0.00011624502858407916, 0.00011623558924963443, 0.00011622616541511243, 0.00011621674935425198, 0.00011620736860667115, 0.00011619792453218888, 0.00011618858270031621, 0.00011617920112322883, 0.00011616982651399661, 0.00011616042192402699, 0.0001161510219081936, 0.00011614164412313623, 0.00011613220580779956, 0.00011612281792646225, 0.00011611343765288518, 0.00011610405967822632, 0.00011609465999419556, 0.00011608531318528348, 0.00011607592954627975, 0.00011606657918233952, 0.00011605723588105521, 0.00011604786944838775, 0.0001160385145103113, 0.00011602915364718793, 0.00011601975884539582, 0.00011601038174764389, 0.00011600101057493889, 0.00011599164774469995, 0.00011598227408347524, 0.00011597292694276052, 0.00011596357994424695, 0.0001159542413355998, 0.00011594490881790089, 0.0001159355666305254, 0.00011592620920392925, 0.00011591688201877255, 0.00011590756284427929, 0.00011589822532584077, 0.00011588889164683266, 0.00011587960022525916, 0.00011587028389478843, 0.00011586093748877954, 0.00011585161217593767, 0.0001158423055151435, 0.00011583297664727348, 0.0001158236679008628, 0.00011581432696959726, 0.00011580498139309494, 0.00011579572424199281, 0.00011578643471643432, 0.00011577711625294669, 0.00011576780527871836, 0.00011575853047097642, 0.00011574924625425996, 0.00011573997047481033, 0.00011573070476794045, 0.00011572139322490762, 0.00011571214541167943, 0.00011570283066912127, 0.0001156936268197412, 0.0001156843377207861, 0.00011567500702800163], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0016286644951140066, 0.0024429967426710096, 0.0024429967426710096, 0.0024429967426710096, 0.0032573289902280132, 0.0057003257328990227, 0.0057003257328990227, 0.0057003257328990227, 0.0065146579804560263, 0.0065146579804560263, 0.0065146579804560263, 0.0073289902280130291, 0.0081433224755700327, 0.0089576547231270363, 0.0097719870192220226, 0.010586319218241042, 0.011400651465798045, 0.01221498373762404, 0.013029315960912053, 0.013843648208469055, 0.013843648208469055, 0.013843648208469055, 0.014657980456026058, 0.016286644951140065, 0.016286644951140065, 0.016286644951140065, 0.016286644951140065, 0.016286644975409059, 0.017100977198697069, 0.019543973989906063, 0.020358306237463066, 0.021172638436482084, 0.021986970684039087, 0.021986970684039087, 0.023615635179153095, 0.024429967426710098, 0.026058631946093095, 0.026058631921824105, 0.026058631946093095, 0.026058631921824105, 0.026058631946093095, 0.026872964169381109, 0.030130293183878113]}
[2017-12-15 10:21:26,567 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:129]: evaluating model ... 
[2017-12-15 10:21:28,277 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:133]: evaluated! 
[2017-12-15 10:21:28,278 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:135]: generating reports ... 
[2017-12-15 10:21:28,910 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:138]: done!
[2017-12-15 10:21:28,910 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F2_0 finished!
[2018-04-29 11:38:17,228 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_OVER_F2_0 was already executed!
[2018-04-29 13:12:02,683 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F2_0
[2018-04-29 13:12:02,683 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:146]: >> Printing header log
[2018-04-29 13:12:02,683 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F2_0
	layers = 9216,18432
	using GLOBAL obj = 
		{'batch': 32, 'numpy_seed': 666, 'autoencoder_configs': {'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fa0f9cdb828>}, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'epochs': 200, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'shuffle_batches': True, 'mlp_configs': {'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7fa0f9cdb898>}, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'store_history': True}
	=======================================
	
[2018-04-29 13:12:02,683 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,676 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,676 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,676 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:57]: =======================================
[2018-04-29 13:12:20,676 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fa0f9cdb828>}
[2018-04-29 13:12:20,722 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,904 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F2_0
[2018-04-29 13:14:12,904 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:146]: >> Printing header log
[2018-04-29 13:14:12,904 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F2_0
	layers = 9216,18432
	using GLOBAL obj = 
		{'store_history': True, 'autoencoder_configs': {'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f354d5ca828>, 'discard_decoder_function': True, 'hidden_layer_activation': 'relu'}, 'batch': 32, 'epochs': 200, 'data_dir': '/home/dhiego/malware_dataset/', 'mlp_configs': {'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid', 'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'optimizer': <keras.optimizers.SGD object at 0x7f354d5ca898>}, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'shuffle_batches': True, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'numpy_seed': 666}
	=======================================
	
[2018-04-29 13:14:12,904 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:148]: >> Loading dataset... 
[2018-04-29 13:14:30,933 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:30,933 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:30,933 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:57]: =======================================
[2018-04-29 13:14:30,934 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:62]: setting configurations for autoencoder: 
	 {'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f354d5ca828>, 'discard_decoder_function': True, 'hidden_layer_activation': 'relu'}
[2018-04-29 13:14:30,981 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,707 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F2_0
[2018-04-29 13:16:33,707 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:146]: >> Printing header log
[2018-04-29 13:16:33,707 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F2_0
	layers = 9216,18432
	using GLOBAL obj = 
		{'numpy_seed': 666, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'store_history': True, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'data_dir': '/home/dhiego/malware_dataset/', 'epochs': 200, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'batch': 32, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'mlp_configs': {'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7f9497330898>, 'use_last_dim_as_classifier': False, 'loss_function': 'categorical_crossentropy', 'classifier_dim': 9}, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'autoencoder_configs': {'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f9497330828>, 'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'output_layer_activation': 'relu'}, 'shuffle_batches': True, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:16:33,707 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:148]: >> Loading dataset... 
[2018-04-29 13:16:58,029 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:58,029 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:58,029 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:57]: =======================================
[2018-04-29 13:16:58,030 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:62]: setting configurations for autoencoder: 
	 {'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f9497330828>, 'hidden_layer_activation': 'relu', 'discard_decoder_function': True, 'output_layer_activation': 'relu'}
[2018-04-29 13:16:58,079 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:20,916 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F2_0
[2018-04-29 14:30:20,916 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:146]: >> Printing header log
[2018-04-29 14:30:20,916 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F2_0
	layers = 9216,18432
	using GLOBAL obj = 
		{'epochs': 200, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'store_history': True, 'numpy_seed': 666, 'mlp_configs': {'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7fb0bdecf898>, 'use_last_dim_as_classifier': False}, 'autoencoder_configs': {'discard_decoder_function': True, 'loss_function': 'mse', 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fb0bdecf828>, 'hidden_layer_activation': 'relu'}, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'batch': 32, 'shuffle_batches': True}
	=======================================
	
[2018-04-29 14:30:20,916 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:148]: >> Loading dataset... 
[2018-04-29 14:30:37,324 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:37,324 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:37,325 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:57]: =======================================
[2018-04-29 14:30:37,325 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'loss_function': 'mse', 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fb0bdecf828>, 'hidden_layer_activation': 'relu'}
[2018-04-29 14:30:37,368 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:73]: training and evaluate autoencoder
[2018-04-30 00:10:10,870 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:85]: trained and evaluated!
[2018-04-30 00:10:10,872 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:88]: Training history: 
{'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.00011768853830683322, 0.00011768386735656947, 0.00011767913689362046, 0.00011767442597028243, 0.00011766968926824446, 0.00011766491749144275, 0.00011766018186202753, 0.00011765542774774701, 0.00011765067502787607, 0.00011764591020524516, 0.00011764114888651521, 0.00011763638603035933, 0.00011763157263579516, 0.00011762678189144792, 0.00011762202017942297, 0.00011761723099037872, 0.00011761242676673901, 0.00011760758165829571, 0.00011760275081573489, 0.0001175978892961636, 0.00011759306406699514, 0.0001175882219976496, 0.00011758338319980345, 0.00011757856762423969, 0.00011757369425218708, 0.00011756884328007277, 0.00011756399475711373, 0.00011755914312354873, 0.00011755424881747555, 0.00011754934821868228, 0.00011754443297858855, 0.00011753949730503156, 0.00011753451870868774, 0.00011752954290116306, 0.00011752457951818517, 0.00011751959634531765, 0.00011751462666961967, 0.0001175096413693837, 0.0001175046478099526, 0.00011749966774769104, 0.00011749467263295698, 0.00011748968307798412, 0.00011748466736889337, 0.00011747962802634815, 0.00011747456392409459, 0.00011746954535467651, 0.00011746449857528026, 0.00011745937715921807, 0.00011745431353964475, 0.00011744924271562198, 0.00011744412301575619, 0.00011743903594149885, 0.0001174339325991299, 0.00011742884552487257, 0.000117423716922238, 0.00011741855749957657, 0.00011741336881219129, 0.00011740817342091384, 0.0001174030098507778, 0.00011739784520801903, 0.00011739264860110248, 0.00011738743824673775, 0.00011738218904555264, 0.00011737694311586692, 0.0001173716321137347, 0.00011736637563659198, 0.00011736108744557013, 0.00011735574928820545, 0.00011735041327608625, 0.00011734502443729692, 0.00011733964067558856, 0.00011733425169378285, 0.00011732880164398891, 0.0001173233571539562, 0.00011731783475575817, 0.00011731232388825464, 0.00011730680517272804, 0.00011730126128632104, 0.0001172957127340051, 0.00011729013851025148, 0.00011728457549540555, 0.00011727898500354032, 0.00011727339435078167, 0.00011726781195721815, 0.00011726222423629501, 0.00011725656817142602, 0.00011725094635897666, 0.0001172453184862088, 0.00011723965834537338, 0.00011723398919450691, 0.00011722828425379488, 0.00011722255044165399, 0.00011721678974243632, 0.00011721103680185648, 0.0001172052635887068, 0.000117199503622448, 0.00011719368160496344, 0.00011718786180423257, 0.00011718204125266577, 0.0001171762216128283, 0.00011717031205145079], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011844590520247337, 0.00011844123588629971, 0.00011843660991776929, 0.00011843193318343687, 0.00011842727358434016, 0.0001184225913515642, 0.00011841786586594577, 0.00011841316493372176, 0.00011840845371561629, 0.00011840373683316597, 0.000118399007365916, 0.00011839428391851369, 0.00011838956928758121, 0.00011838478090166469, 0.00011838002356299402, 0.00011837528281445472, 0.00011837051966923808, 0.00011836574228020865, 0.00011836092047702751, 0.00011835611071354173, 0.00011835127893258159, 0.0001183464785780703, 0.0001183416605906194, 0.0001183368473669062, 0.0001183320555207623, 0.00011832720015811543, 0.00011832235659816202, 0.00011831750744496433, 0.00011831264582546792, 0.00011830774014732264, 0.00011830282039126587, 0.00011829789205574117, 0.00011829294404906073, 0.0001182879683131607, 0.00011828300065902465, 0.00011827802340631261, 0.00011827302728825118, 0.00011826802135831206, 0.00011826300208516729, 0.00011825796844970878, 0.00011825293644956322, 0.000118247896912758, 0.00011824285638054487, 0.00011823779411525966, 0.00011823272092418794, 0.0001182276208334032, 0.00011822255534489248, 0.00011821745921203909, 0.0001182122956284516, 0.0001182071731883899, 0.00011820204181335745, 0.00011819686171073916, 0.00011819172173253858, 0.00011818657355407307, 0.00011818142909653703, 0.00011817625660167898, 0.00011817106697158524, 0.0001181658629317774, 0.00011816066292100147, 0.00011815548014026196, 0.00011815029067606953, 0.0001181450568214256, 0.00011813979921919364, 0.00011813450746499123, 0.00011812923438653672, 0.00011812388485127677, 0.00011811856233423079, 0.0001181132109740564, 0.00011810781451242487, 0.00011810241857219748, 0.00011809695745645401, 0.00011809152734055599, 0.0001180860856589664, 0.00011808058159848287, 0.00011807507820160459, 0.00011806951247323274, 0.00011806395553763052, 0.00011805838886125117, 0.00011805279900608828, 0.00011804719976565106, 0.00011804157042597551, 0.0001180359431245161, 0.00011803028769093392, 0.00011802462090496187, 0.00011801896011513728, 0.00011801327299440419, 0.00011800750595663829, 0.00011800176785680152, 0.00011799602684184168, 0.00011799025141420934, 0.00011798447378245955, 0.00011797864173507888, 0.00011797277755024373, 0.00011796688682119839, 0.00011796098343625282, 0.00011795505912404155, 0.00011794916332315604, 0.00011794318191719269, 0.00011793719373297564, 0.00011793119149454733, 0.000117925206178053]}
[2018-04-30 00:10:10,872 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:92]: done!
[2018-04-30 00:10:10,872 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:152]: >> Executing classifier part ... 
[2018-04-30 00:10:10,872 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:97]: =======================================
[2018-04-30 00:10:10,872 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:101]: setting configurations for classifier: 
	 {'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7fb0bdecf898>, 'use_last_dim_as_classifier': False}
[2018-04-30 00:10:11,082 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:110]: training ... 
[2018-04-30 04:23:20,777 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:122]: trained!
[2018-04-30 04:23:20,778 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:125]: Training history: 
{'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_loss': [0.00011768853830683322, 0.00011768386735656947, 0.00011767913689362046, 0.00011767442597028243, 0.00011766968926824446, 0.00011766491749144275, 0.00011766018186202753, 0.00011765542774774701, 0.00011765067502787607, 0.00011764591020524516, 0.00011764114888651521, 0.00011763638603035933, 0.00011763157263579516, 0.00011762678189144792, 0.00011762202017942297, 0.00011761723099037872, 0.00011761242676673901, 0.00011760758165829571, 0.00011760275081573489, 0.0001175978892961636, 0.00011759306406699514, 0.0001175882219976496, 0.00011758338319980345, 0.00011757856762423969, 0.00011757369425218708, 0.00011756884328007277, 0.00011756399475711373, 0.00011755914312354873, 0.00011755424881747555, 0.00011754934821868228, 0.00011754443297858855, 0.00011753949730503156, 0.00011753451870868774, 0.00011752954290116306, 0.00011752457951818517, 0.00011751959634531765, 0.00011751462666961967, 0.0001175096413693837, 0.0001175046478099526, 0.00011749966774769104, 0.00011749467263295698, 0.00011748968307798412, 0.00011748466736889337, 0.00011747962802634815, 0.00011747456392409459, 0.00011746954535467651, 0.00011746449857528026, 0.00011745937715921807, 0.00011745431353964475, 0.00011744924271562198, 0.00011744412301575619, 0.00011743903594149885, 0.0001174339325991299, 0.00011742884552487257, 0.000117423716922238, 0.00011741855749957657, 0.00011741336881219129, 0.00011740817342091384, 0.0001174030098507778, 0.00011739784520801903, 0.00011739264860110248, 0.00011738743824673775, 0.00011738218904555264, 0.00011737694311586692, 0.0001173716321137347, 0.00011736637563659198, 0.00011736108744557013, 0.00011735574928820545, 0.00011735041327608625, 0.00011734502443729692, 0.00011733964067558856, 0.00011733425169378285, 0.00011732880164398891, 0.0001173233571539562, 0.00011731783475575817, 0.00011731232388825464, 0.00011730680517272804, 0.00011730126128632104, 0.0001172957127340051, 0.00011729013851025148, 0.00011728457549540555, 0.00011727898500354032, 0.00011727339435078167, 0.00011726781195721815, 0.00011726222423629501, 0.00011725656817142602, 0.00011725094635897666, 0.0001172453184862088, 0.00011723965834537338, 0.00011723398919450691, 0.00011722828425379488, 0.00011722255044165399, 0.00011721678974243632, 0.00011721103680185648, 0.0001172052635887068, 0.000117199503622448, 0.00011719368160496344, 0.00011718786180423257, 0.00011718204125266577, 0.0001171762216128283, 0.00011717031205145079], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011844590520247337, 0.00011844123588629971, 0.00011843660991776929, 0.00011843193318343687, 0.00011842727358434016, 0.0001184225913515642, 0.00011841786586594577, 0.00011841316493372176, 0.00011840845371561629, 0.00011840373683316597, 0.000118399007365916, 0.00011839428391851369, 0.00011838956928758121, 0.00011838478090166469, 0.00011838002356299402, 0.00011837528281445472, 0.00011837051966923808, 0.00011836574228020865, 0.00011836092047702751, 0.00011835611071354173, 0.00011835127893258159, 0.0001183464785780703, 0.0001183416605906194, 0.0001183368473669062, 0.0001183320555207623, 0.00011832720015811543, 0.00011832235659816202, 0.00011831750744496433, 0.00011831264582546792, 0.00011830774014732264, 0.00011830282039126587, 0.00011829789205574117, 0.00011829294404906073, 0.0001182879683131607, 0.00011828300065902465, 0.00011827802340631261, 0.00011827302728825118, 0.00011826802135831206, 0.00011826300208516729, 0.00011825796844970878, 0.00011825293644956322, 0.000118247896912758, 0.00011824285638054487, 0.00011823779411525966, 0.00011823272092418794, 0.0001182276208334032, 0.00011822255534489248, 0.00011821745921203909, 0.0001182122956284516, 0.0001182071731883899, 0.00011820204181335745, 0.00011819686171073916, 0.00011819172173253858, 0.00011818657355407307, 0.00011818142909653703, 0.00011817625660167898, 0.00011817106697158524, 0.0001181658629317774, 0.00011816066292100147, 0.00011815548014026196, 0.00011815029067606953, 0.0001181450568214256, 0.00011813979921919364, 0.00011813450746499123, 0.00011812923438653672, 0.00011812388485127677, 0.00011811856233423079, 0.0001181132109740564, 0.00011810781451242487, 0.00011810241857219748, 0.00011809695745645401, 0.00011809152734055599, 0.0001180860856589664, 0.00011808058159848287, 0.00011807507820160459, 0.00011806951247323274, 0.00011806395553763052, 0.00011805838886125117, 0.00011805279900608828, 0.00011804719976565106, 0.00011804157042597551, 0.0001180359431245161, 0.00011803028769093392, 0.00011802462090496187, 0.00011801896011513728, 0.00011801327299440419, 0.00011800750595663829, 0.00011800176785680152, 0.00011799602684184168, 0.00011799025141420934, 0.00011798447378245955, 0.00011797864173507888, 0.00011797277755024373, 0.00011796688682119839, 0.00011796098343625282, 0.00011795505912404155, 0.00011794916332315604, 0.00011794318191719269, 0.00011793719373297564, 0.00011793119149454733, 0.000117925206178053]}
[2018-04-30 04:23:20,778 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:129]: evaluating model ... 
[2018-04-30 04:23:21,562 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:133]: evaluated! 
[2018-04-30 04:23:21,562 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:135]: generating reports ... 
[2018-04-30 04:23:22,165 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:138]: done!
[2018-04-30 04:23:22,165 AE_BIGRAMA_1L_MINIDS_OVER_F2_0.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F2_0 finished!
