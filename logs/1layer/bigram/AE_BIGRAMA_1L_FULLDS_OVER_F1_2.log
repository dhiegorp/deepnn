[2018-01-18 00:19:28,525 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_FULLDS_OVER_F1_2
[2018-01-18 00:19:28,525 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:146]: >> Printing header log
[2018-01-18 00:19:28,525 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_FULLDS_OVER_F1_2
	layers = 9216,11059
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fd6aa166be0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fd6aa166470>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2018-01-18 00:19:28,525 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:148]: >> Loading dataset... 
[2018-01-18 00:22:01,144 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (8147, 9216)
	trainy shape = (8147, 9)
	valx shape = (2721, 9216)
	valy shape = (2721, 9)
	=======================================
	
[2018-01-18 00:22:01,145 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:150]: >> Executing autoencoder part ... 
[2018-01-18 00:22:01,145 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:57]: =======================================
[2018-01-18 00:22:01,145 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fd6aa166be0>, 'discard_decoder_function': True}
[2018-01-18 00:22:01,186 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:73]: training and evaluate autoencoder
[2018-01-20 04:53:18,801 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:85]: trained and evaluated!
[2018-01-20 04:53:18,803 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:88]: Training history: 
{'val_loss': [0.00012026888780363105, 0.00012024751513673061, 0.00012022615883471831, 0.00012020481990836664, 0.00012018350888254481, 0.0001201622209868345, 0.00012014094895864804, 0.00012011968702214261, 0.0001200984372042112, 0.00012007720485820447, 0.00012005600412424143, 0.00012003481686189709, 0.00012001363589415187, 0.00011999245114536878, 0.00011997082292416141, 0.00011994838610099637, 0.00011992588448143116, 0.00011990339170211433, 0.00011988089859122133, 0.00011985842611292259, 0.0001198359374622638, 0.00011981347812935559, 0.00011979103082945336, 0.00011976860892186773, 0.00011974620697302802, 0.00011972383207438574, 0.00011970147205388686, 0.00011967913307777843, 0.00011965682817379493, 0.00011963454548006646, 0.00011961226821456067, 0.00011959001665155566, 0.00011956778136252274, 0.00011954558790556659, 0.00011952340780792109, 0.00011950125399570863, 0.00011947913227686012, 0.00011945704702069371, 0.00011943498163235718, 0.00011941294643349565, 0.00011939094849951662, 0.00011936898161069302, 0.00011934704433910813, 0.00011932514420940169, 0.0001193032669317106, 0.00011928142963568736, 0.00011925961878018927, 0.00011923784327498873, 0.0001192160939589572, 0.00011919438598833422, 0.00011917271507936962, 0.0001191510788949863, 0.00011912945994186751, 0.00011910788514185868, 0.00011908633685189901, 0.0001190648303619281, 0.00011904334654214045, 0.00011902190380036121, 0.00011900048510854991, 0.00011897909365411613, 0.00011895773008951622, 0.0001189363986985003, 0.00011891508871019099, 0.0001188938306937715, 0.00011887260269034293, 0.00011885140490847736, 0.0001188302298823631, 0.0001188090843932675, 0.0001187879728960768, 0.00011876690135916195, 0.00011874584667915146, 0.00011872482838160306, 0.00011870384578197238, 0.00011868288704512956, 0.00011866195341715919, 0.00011864104943851557, 0.00011862017323204974, 0.00011859933456856256, 0.0001185785331753059, 0.00011855774970855413, 0.00011853699130254282, 0.00011851627263219124, 0.00011849558761682038, 0.00011847493129348378, 0.00011845429766707044, 0.00011843369608054102, 0.00011841312679594765, 0.00011839259344993213, 0.00011837208287036392, 0.0001183516067051928, 0.0001183311515041921, 0.00011831073326056376, 0.0001182903514288115, 0.00011827000005928236, 0.0001182496771732154, 0.00011822938330006283, 0.00011820910872250384, 0.00011818887209437171, 0.00011816866373845562, 0.00011814847720506437, 0.00011812833020410876], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012040327956270527, 0.00012038188502775074, 0.00012036052701905011, 0.0001203391845848462, 0.00012031786372755846, 0.0001202965784712893, 0.00012027531564840413, 0.00012025406540639717, 0.00012023283117560516, 0.00012021161409292437, 0.00012019040921781258, 0.00012016923514607063, 0.00012014807209945409, 0.00012012690718450525, 0.00012010554274649089, 0.00012008339397060551, 0.00012006082469134627, 0.00012003825114403725, 0.00012001566737180635, 0.0001199930837772992, 0.00011997051017640514, 0.00011994790697959117, 0.00011992532983047326, 0.00011990277429756723, 0.00011988025503281373, 0.000119857754924718, 0.00011983526606859109, 0.00011981279747118793, 0.00011979034590043655, 0.00011976792484726803, 0.00011974551903190347, 0.00011972313327163947, 0.00011970077615616059, 0.00011967844112397666, 0.00011965615739769834, 0.00011963388656577084, 0.00011961164310955571, 0.00011958942914834195, 0.00011956724944405493, 0.00011954509035483232, 0.00011952296612804757, 0.00011950088071649172, 0.00011947882332366895, 0.0001194567986713158, 0.00011943481467841058, 0.00011941285520245497, 0.0001193909346640808, 0.00011936904197118135, 0.00011934718593046066, 0.00011932535268303692, 0.00011930355627444658, 0.00011928179733163478, 0.0001192600731699902, 0.00011923836132040548, 0.00011921669005524961, 0.00011919504684187092, 0.00011917344697344459, 0.00011915187101020512, 0.00011913033283782689, 0.00011910883103541281, 0.00011908735504759978, 0.00011906591125190264, 0.00011904449694942071, 0.00011902311180881971, 0.0001190017833147683, 0.0001189804799565739, 0.00011895920465640827, 0.00011893795491363543, 0.0001189167297726552, 0.00011889554006836605, 0.00011887438652238006, 0.00011885325820691801, 0.00011883216643557155, 0.00011881111079394956, 0.00011879008227083167, 0.00011876907416629964, 0.00011874809697677298, 0.00011872715313412017, 0.00011870624074768181, 0.00011868536843214885, 0.00011866450995227752, 0.00011864367821402874, 0.00011862288595367738, 0.00011860212730365976, 0.00011858139468169033, 0.00011856069030886971, 0.00011854001274375986, 0.0001185193735518022, 0.00011849875938317292, 0.00011847817545794964, 0.00011845762661486298, 0.00011843709887803706, 0.00011841660815866138, 0.00011839615226778639, 0.00011837573049898241, 0.0001183553307902532, 0.00011833496275118556, 0.00011831461578265529, 0.00011829430719263571, 0.00011827402439667626, 0.00011825376475928523], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-01-20 04:53:18,804 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:92]: done!
[2018-01-20 04:53:18,804 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:152]: >> Executing classifier part ... 
[2018-01-20 04:53:18,804 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:97]: =======================================
[2018-01-20 04:53:18,804 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fd6aa166470>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2018-01-20 04:53:19,318 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:110]: training ... 
[2018-01-23 10:47:31,351 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:122]: trained!
[2018-01-23 10:47:31,353 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:125]: Training history: 
{'val_loss': [0.00012026888780363105, 0.00012024751513673061, 0.00012022615883471831, 0.00012020481990836664, 0.00012018350888254481, 0.0001201622209868345, 0.00012014094895864804, 0.00012011968702214261, 0.0001200984372042112, 0.00012007720485820447, 0.00012005600412424143, 0.00012003481686189709, 0.00012001363589415187, 0.00011999245114536878, 0.00011997082292416141, 0.00011994838610099637, 0.00011992588448143116, 0.00011990339170211433, 0.00011988089859122133, 0.00011985842611292259, 0.0001198359374622638, 0.00011981347812935559, 0.00011979103082945336, 0.00011976860892186773, 0.00011974620697302802, 0.00011972383207438574, 0.00011970147205388686, 0.00011967913307777843, 0.00011965682817379493, 0.00011963454548006646, 0.00011961226821456067, 0.00011959001665155566, 0.00011956778136252274, 0.00011954558790556659, 0.00011952340780792109, 0.00011950125399570863, 0.00011947913227686012, 0.00011945704702069371, 0.00011943498163235718, 0.00011941294643349565, 0.00011939094849951662, 0.00011936898161069302, 0.00011934704433910813, 0.00011932514420940169, 0.0001193032669317106, 0.00011928142963568736, 0.00011925961878018927, 0.00011923784327498873, 0.0001192160939589572, 0.00011919438598833422, 0.00011917271507936962, 0.0001191510788949863, 0.00011912945994186751, 0.00011910788514185868, 0.00011908633685189901, 0.0001190648303619281, 0.00011904334654214045, 0.00011902190380036121, 0.00011900048510854991, 0.00011897909365411613, 0.00011895773008951622, 0.0001189363986985003, 0.00011891508871019099, 0.0001188938306937715, 0.00011887260269034293, 0.00011885140490847736, 0.0001188302298823631, 0.0001188090843932675, 0.0001187879728960768, 0.00011876690135916195, 0.00011874584667915146, 0.00011872482838160306, 0.00011870384578197238, 0.00011868288704512956, 0.00011866195341715919, 0.00011864104943851557, 0.00011862017323204974, 0.00011859933456856256, 0.0001185785331753059, 0.00011855774970855413, 0.00011853699130254282, 0.00011851627263219124, 0.00011849558761682038, 0.00011847493129348378, 0.00011845429766707044, 0.00011843369608054102, 0.00011841312679594765, 0.00011839259344993213, 0.00011837208287036392, 0.0001183516067051928, 0.0001183311515041921, 0.00011831073326056376, 0.0001182903514288115, 0.00011827000005928236, 0.0001182496771732154, 0.00011822938330006283, 0.00011820910872250384, 0.00011818887209437171, 0.00011816866373845562, 0.00011814847720506437, 0.00011812833020410876], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012040327956270527, 0.00012038188502775074, 0.00012036052701905011, 0.0001203391845848462, 0.00012031786372755846, 0.0001202965784712893, 0.00012027531564840413, 0.00012025406540639717, 0.00012023283117560516, 0.00012021161409292437, 0.00012019040921781258, 0.00012016923514607063, 0.00012014807209945409, 0.00012012690718450525, 0.00012010554274649089, 0.00012008339397060551, 0.00012006082469134627, 0.00012003825114403725, 0.00012001566737180635, 0.0001199930837772992, 0.00011997051017640514, 0.00011994790697959117, 0.00011992532983047326, 0.00011990277429756723, 0.00011988025503281373, 0.000119857754924718, 0.00011983526606859109, 0.00011981279747118793, 0.00011979034590043655, 0.00011976792484726803, 0.00011974551903190347, 0.00011972313327163947, 0.00011970077615616059, 0.00011967844112397666, 0.00011965615739769834, 0.00011963388656577084, 0.00011961164310955571, 0.00011958942914834195, 0.00011956724944405493, 0.00011954509035483232, 0.00011952296612804757, 0.00011950088071649172, 0.00011947882332366895, 0.0001194567986713158, 0.00011943481467841058, 0.00011941285520245497, 0.0001193909346640808, 0.00011936904197118135, 0.00011934718593046066, 0.00011932535268303692, 0.00011930355627444658, 0.00011928179733163478, 0.0001192600731699902, 0.00011923836132040548, 0.00011921669005524961, 0.00011919504684187092, 0.00011917344697344459, 0.00011915187101020512, 0.00011913033283782689, 0.00011910883103541281, 0.00011908735504759978, 0.00011906591125190264, 0.00011904449694942071, 0.00011902311180881971, 0.0001190017833147683, 0.0001189804799565739, 0.00011895920465640827, 0.00011893795491363543, 0.0001189167297726552, 0.00011889554006836605, 0.00011887438652238006, 0.00011885325820691801, 0.00011883216643557155, 0.00011881111079394956, 0.00011879008227083167, 0.00011876907416629964, 0.00011874809697677298, 0.00011872715313412017, 0.00011870624074768181, 0.00011868536843214885, 0.00011866450995227752, 0.00011864367821402874, 0.00011862288595367738, 0.00011860212730365976, 0.00011858139468169033, 0.00011856069030886971, 0.00011854001274375986, 0.0001185193735518022, 0.00011849875938317292, 0.00011847817545794964, 0.00011845762661486298, 0.00011843709887803706, 0.00011841660815866138, 0.00011839615226778639, 0.00011837573049898241, 0.0001183553307902532, 0.00011833496275118556, 0.00011831461578265529, 0.00011829430719263571, 0.00011827402439667626, 0.00011825376475928523], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-01-23 10:47:31,353 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:129]: evaluating model ... 
[2018-01-23 10:48:11,036 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:133]: evaluated! 
[2018-01-23 10:48:11,037 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:135]: generating reports ... 
[2018-01-23 10:48:14,589 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:138]: done!
[2018-01-23 10:48:14,590 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:154]: >> experiment AE_BIGRAMA_1L_FULLDS_OVER_F1_2 finished!
