[2018-01-18 00:19:28,525 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_FULLDS_OVER_F1_2
[2018-01-18 00:19:28,525 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:146]: >> Printing header log
[2018-01-18 00:19:28,525 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_FULLDS_OVER_F1_2
	layers = 9216,11059
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 1000, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fd6aa166be0>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fd6aa166470>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2018-01-18 00:19:28,525 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:148]: >> Loading dataset... 
[2018-01-18 00:22:01,144 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (8147, 9216)
	trainy shape = (8147, 9)
	valx shape = (2721, 9216)
	valy shape = (2721, 9)
	=======================================
	
[2018-01-18 00:22:01,145 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:150]: >> Executing autoencoder part ... 
[2018-01-18 00:22:01,145 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:57]: =======================================
[2018-01-18 00:22:01,145 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fd6aa166be0>, 'discard_decoder_function': True}
[2018-01-18 00:22:01,186 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:73]: training and evaluate autoencoder
[2018-01-20 04:53:18,801 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:85]: trained and evaluated!
[2018-01-20 04:53:18,803 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:88]: Training history: 
{'val_loss': [0.00012026888780363105, 0.00012024751513673061, 0.00012022615883471831, 0.00012020481990836664, 0.00012018350888254481, 0.0001201622209868345, 0.00012014094895864804, 0.00012011968702214261, 0.0001200984372042112, 0.00012007720485820447, 0.00012005600412424143, 0.00012003481686189709, 0.00012001363589415187, 0.00011999245114536878, 0.00011997082292416141, 0.00011994838610099637, 0.00011992588448143116, 0.00011990339170211433, 0.00011988089859122133, 0.00011985842611292259, 0.0001198359374622638, 0.00011981347812935559, 0.00011979103082945336, 0.00011976860892186773, 0.00011974620697302802, 0.00011972383207438574, 0.00011970147205388686, 0.00011967913307777843, 0.00011965682817379493, 0.00011963454548006646, 0.00011961226821456067, 0.00011959001665155566, 0.00011956778136252274, 0.00011954558790556659, 0.00011952340780792109, 0.00011950125399570863, 0.00011947913227686012, 0.00011945704702069371, 0.00011943498163235718, 0.00011941294643349565, 0.00011939094849951662, 0.00011936898161069302, 0.00011934704433910813, 0.00011932514420940169, 0.0001193032669317106, 0.00011928142963568736, 0.00011925961878018927, 0.00011923784327498873, 0.0001192160939589572, 0.00011919438598833422, 0.00011917271507936962, 0.0001191510788949863, 0.00011912945994186751, 0.00011910788514185868, 0.00011908633685189901, 0.0001190648303619281, 0.00011904334654214045, 0.00011902190380036121, 0.00011900048510854991, 0.00011897909365411613, 0.00011895773008951622, 0.0001189363986985003, 0.00011891508871019099, 0.0001188938306937715, 0.00011887260269034293, 0.00011885140490847736, 0.0001188302298823631, 0.0001188090843932675, 0.0001187879728960768, 0.00011876690135916195, 0.00011874584667915146, 0.00011872482838160306, 0.00011870384578197238, 0.00011868288704512956, 0.00011866195341715919, 0.00011864104943851557, 0.00011862017323204974, 0.00011859933456856256, 0.0001185785331753059, 0.00011855774970855413, 0.00011853699130254282, 0.00011851627263219124, 0.00011849558761682038, 0.00011847493129348378, 0.00011845429766707044, 0.00011843369608054102, 0.00011841312679594765, 0.00011839259344993213, 0.00011837208287036392, 0.0001183516067051928, 0.0001183311515041921, 0.00011831073326056376, 0.0001182903514288115, 0.00011827000005928236, 0.0001182496771732154, 0.00011822938330006283, 0.00011820910872250384, 0.00011818887209437171, 0.00011816866373845562, 0.00011814847720506437, 0.00011812833020410876], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012040327956270527, 0.00012038188502775074, 0.00012036052701905011, 0.0001203391845848462, 0.00012031786372755846, 0.0001202965784712893, 0.00012027531564840413, 0.00012025406540639717, 0.00012023283117560516, 0.00012021161409292437, 0.00012019040921781258, 0.00012016923514607063, 0.00012014807209945409, 0.00012012690718450525, 0.00012010554274649089, 0.00012008339397060551, 0.00012006082469134627, 0.00012003825114403725, 0.00012001566737180635, 0.0001199930837772992, 0.00011997051017640514, 0.00011994790697959117, 0.00011992532983047326, 0.00011990277429756723, 0.00011988025503281373, 0.000119857754924718, 0.00011983526606859109, 0.00011981279747118793, 0.00011979034590043655, 0.00011976792484726803, 0.00011974551903190347, 0.00011972313327163947, 0.00011970077615616059, 0.00011967844112397666, 0.00011965615739769834, 0.00011963388656577084, 0.00011961164310955571, 0.00011958942914834195, 0.00011956724944405493, 0.00011954509035483232, 0.00011952296612804757, 0.00011950088071649172, 0.00011947882332366895, 0.0001194567986713158, 0.00011943481467841058, 0.00011941285520245497, 0.0001193909346640808, 0.00011936904197118135, 0.00011934718593046066, 0.00011932535268303692, 0.00011930355627444658, 0.00011928179733163478, 0.0001192600731699902, 0.00011923836132040548, 0.00011921669005524961, 0.00011919504684187092, 0.00011917344697344459, 0.00011915187101020512, 0.00011913033283782689, 0.00011910883103541281, 0.00011908735504759978, 0.00011906591125190264, 0.00011904449694942071, 0.00011902311180881971, 0.0001190017833147683, 0.0001189804799565739, 0.00011895920465640827, 0.00011893795491363543, 0.0001189167297726552, 0.00011889554006836605, 0.00011887438652238006, 0.00011885325820691801, 0.00011883216643557155, 0.00011881111079394956, 0.00011879008227083167, 0.00011876907416629964, 0.00011874809697677298, 0.00011872715313412017, 0.00011870624074768181, 0.00011868536843214885, 0.00011866450995227752, 0.00011864367821402874, 0.00011862288595367738, 0.00011860212730365976, 0.00011858139468169033, 0.00011856069030886971, 0.00011854001274375986, 0.0001185193735518022, 0.00011849875938317292, 0.00011847817545794964, 0.00011845762661486298, 0.00011843709887803706, 0.00011841660815866138, 0.00011839615226778639, 0.00011837573049898241, 0.0001183553307902532, 0.00011833496275118556, 0.00011831461578265529, 0.00011829430719263571, 0.00011827402439667626, 0.00011825376475928523], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-01-20 04:53:18,804 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:92]: done!
[2018-01-20 04:53:18,804 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:152]: >> Executing classifier part ... 
[2018-01-20 04:53:18,804 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:97]: =======================================
[2018-01-20 04:53:18,804 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fd6aa166470>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2018-01-20 04:53:19,318 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:110]: training ... 
[2018-01-23 10:47:31,351 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:122]: trained!
[2018-01-23 10:47:31,353 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:125]: Training history: 
{'val_loss': [0.00012026888780363105, 0.00012024751513673061, 0.00012022615883471831, 0.00012020481990836664, 0.00012018350888254481, 0.0001201622209868345, 0.00012014094895864804, 0.00012011968702214261, 0.0001200984372042112, 0.00012007720485820447, 0.00012005600412424143, 0.00012003481686189709, 0.00012001363589415187, 0.00011999245114536878, 0.00011997082292416141, 0.00011994838610099637, 0.00011992588448143116, 0.00011990339170211433, 0.00011988089859122133, 0.00011985842611292259, 0.0001198359374622638, 0.00011981347812935559, 0.00011979103082945336, 0.00011976860892186773, 0.00011974620697302802, 0.00011972383207438574, 0.00011970147205388686, 0.00011967913307777843, 0.00011965682817379493, 0.00011963454548006646, 0.00011961226821456067, 0.00011959001665155566, 0.00011956778136252274, 0.00011954558790556659, 0.00011952340780792109, 0.00011950125399570863, 0.00011947913227686012, 0.00011945704702069371, 0.00011943498163235718, 0.00011941294643349565, 0.00011939094849951662, 0.00011936898161069302, 0.00011934704433910813, 0.00011932514420940169, 0.0001193032669317106, 0.00011928142963568736, 0.00011925961878018927, 0.00011923784327498873, 0.0001192160939589572, 0.00011919438598833422, 0.00011917271507936962, 0.0001191510788949863, 0.00011912945994186751, 0.00011910788514185868, 0.00011908633685189901, 0.0001190648303619281, 0.00011904334654214045, 0.00011902190380036121, 0.00011900048510854991, 0.00011897909365411613, 0.00011895773008951622, 0.0001189363986985003, 0.00011891508871019099, 0.0001188938306937715, 0.00011887260269034293, 0.00011885140490847736, 0.0001188302298823631, 0.0001188090843932675, 0.0001187879728960768, 0.00011876690135916195, 0.00011874584667915146, 0.00011872482838160306, 0.00011870384578197238, 0.00011868288704512956, 0.00011866195341715919, 0.00011864104943851557, 0.00011862017323204974, 0.00011859933456856256, 0.0001185785331753059, 0.00011855774970855413, 0.00011853699130254282, 0.00011851627263219124, 0.00011849558761682038, 0.00011847493129348378, 0.00011845429766707044, 0.00011843369608054102, 0.00011841312679594765, 0.00011839259344993213, 0.00011837208287036392, 0.0001183516067051928, 0.0001183311515041921, 0.00011831073326056376, 0.0001182903514288115, 0.00011827000005928236, 0.0001182496771732154, 0.00011822938330006283, 0.00011820910872250384, 0.00011818887209437171, 0.00011816866373845562, 0.00011814847720506437, 0.00011812833020410876], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012040327956270527, 0.00012038188502775074, 0.00012036052701905011, 0.0001203391845848462, 0.00012031786372755846, 0.0001202965784712893, 0.00012027531564840413, 0.00012025406540639717, 0.00012023283117560516, 0.00012021161409292437, 0.00012019040921781258, 0.00012016923514607063, 0.00012014807209945409, 0.00012012690718450525, 0.00012010554274649089, 0.00012008339397060551, 0.00012006082469134627, 0.00012003825114403725, 0.00012001566737180635, 0.0001199930837772992, 0.00011997051017640514, 0.00011994790697959117, 0.00011992532983047326, 0.00011990277429756723, 0.00011988025503281373, 0.000119857754924718, 0.00011983526606859109, 0.00011981279747118793, 0.00011979034590043655, 0.00011976792484726803, 0.00011974551903190347, 0.00011972313327163947, 0.00011970077615616059, 0.00011967844112397666, 0.00011965615739769834, 0.00011963388656577084, 0.00011961164310955571, 0.00011958942914834195, 0.00011956724944405493, 0.00011954509035483232, 0.00011952296612804757, 0.00011950088071649172, 0.00011947882332366895, 0.0001194567986713158, 0.00011943481467841058, 0.00011941285520245497, 0.0001193909346640808, 0.00011936904197118135, 0.00011934718593046066, 0.00011932535268303692, 0.00011930355627444658, 0.00011928179733163478, 0.0001192600731699902, 0.00011923836132040548, 0.00011921669005524961, 0.00011919504684187092, 0.00011917344697344459, 0.00011915187101020512, 0.00011913033283782689, 0.00011910883103541281, 0.00011908735504759978, 0.00011906591125190264, 0.00011904449694942071, 0.00011902311180881971, 0.0001190017833147683, 0.0001189804799565739, 0.00011895920465640827, 0.00011893795491363543, 0.0001189167297726552, 0.00011889554006836605, 0.00011887438652238006, 0.00011885325820691801, 0.00011883216643557155, 0.00011881111079394956, 0.00011879008227083167, 0.00011876907416629964, 0.00011874809697677298, 0.00011872715313412017, 0.00011870624074768181, 0.00011868536843214885, 0.00011866450995227752, 0.00011864367821402874, 0.00011862288595367738, 0.00011860212730365976, 0.00011858139468169033, 0.00011856069030886971, 0.00011854001274375986, 0.0001185193735518022, 0.00011849875938317292, 0.00011847817545794964, 0.00011845762661486298, 0.00011843709887803706, 0.00011841660815866138, 0.00011839615226778639, 0.00011837573049898241, 0.0001183553307902532, 0.00011833496275118556, 0.00011831461578265529, 0.00011829430719263571, 0.00011827402439667626, 0.00011825376475928523], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-01-23 10:47:31,353 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:129]: evaluating model ... 
[2018-01-23 10:48:11,036 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:133]: evaluated! 
[2018-01-23 10:48:11,037 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:135]: generating reports ... 
[2018-01-23 10:48:14,589 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:138]: done!
[2018-01-23 10:48:14,590 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:154]: >> experiment AE_BIGRAMA_1L_FULLDS_OVER_F1_2 finished!
[2018-05-03 21:43:55,293 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_FULLDS_OVER_F1_2
[2018-05-03 21:43:55,293 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:146]: >> Printing header log
[2018-05-03 21:43:55,293 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_FULLDS_OVER_F1_2
	layers = 9216,11059
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fd087a0d630>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fd087a0de10>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2018-05-03 21:43:55,293 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:148]: >> Loading dataset... 
[2018-05-03 21:46:22,108 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (8147, 9216)
	trainy shape = (8147, 9)
	valx shape = (2721, 9216)
	valy shape = (2721, 9)
	=======================================
	
[2018-05-03 21:46:22,108 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:150]: >> Executing autoencoder part ... 
[2018-05-03 21:46:22,109 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:57]: =======================================
[2018-05-03 21:46:22,109 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fd087a0d630>, 'discard_decoder_function': True}
[2018-05-03 21:46:22,319 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:73]: training and evaluate autoencoder
[2018-05-05 16:22:41,419 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:85]: trained and evaluated!
[2018-05-05 16:22:41,420 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:88]: Training history: 
{'val_loss': [0.00011908114922514616, 0.00011905818563976346, 0.00011903520871646215, 0.00011901223161667674, 0.00011898926105749858, 0.00011896630862804944, 0.00011894336595870513, 0.00011892042320379279, 0.00011889749253001845, 0.00011887458882087349, 0.00011885169068969532, 0.00011882880007780888, 0.00011880592928027227, 0.00011878306812518445, 0.00011876022393930908, 0.00011873738635286897, 0.000118714567163558, 0.00011869174763732287, 0.0001186689415506184, 0.00011864613230859239, 0.00011862332804555683, 0.00011860054211012638, 0.00011857775530831951, 0.00011855498992526327, 0.00011853223889624615, 0.00011850949287295954, 0.00011848676217704853, 0.00011846404723100534, 0.00011844133467551934, 0.00011841861211392037, 0.0001183958778669356, 0.0001183731677233988, 0.00011835043689913573, 0.00011832771718802218, 0.00011830497337880865, 0.00011828224597726727, 0.00011825951141475035, 0.00011823676945059774, 0.00011821401844832065, 0.00011819126118353244, 0.00011816851998414421, 0.00011814576567145347, 0.0001181230028393945, 0.00011810023138635525, 0.00011807742483437456, 0.00011805457277089125, 0.00011803170273811903, 0.00011800878120408124, 0.00011798585946147133, 0.0001179629068769301, 0.00011793991315020443, 0.00011791686263838656, 0.00011789375445905605, 0.0001178705894411533, 0.00011784736336510421, 0.00011782408172330754, 0.00011780071180737901, 0.00011777723536458556, 0.00011775365292437948, 0.0001177299515766823, 0.0001177061374182171, 0.00011768222372272648, 0.00011765817096847076, 0.00011763397999508639, 0.00011760957717662078, 0.00011758485399140405, 0.00011755976566595793, 0.00011753422157302942, 0.00011750813591461191, 0.0001174813784721887, 0.0001174540380493439, 0.00011742611868916753, 0.00011739747977127772, 0.00011736813271900818, 0.00011733814198113889, 0.0001173074327391127, 0.00011727593333504202, 0.00011724363253277325, 0.00011721056137746183, 0.00011717661230473032, 0.00011714184863493017, 0.00011710623038639354, 0.00011706985846523453, 0.00011703250578991235, 0.00011699419393426972, 0.00011695510859700287, 0.000116915058897201, 0.00011687430619175392, 0.00011683266513493359, 0.00011679032837249166, 0.00011674715338015737, 0.00011670298587875006, 0.00011665793311711502, 0.0001166119871534683, 0.00011656509610683613, 0.00011651732893436477, 0.00011646883586144882, 0.00011641965589641968, 0.00011636991638056859, 0.0001163196804203269, 0.00011626895089292005], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.000119240980192705, 0.00011921784236884961, 0.00011919471715445114, 0.00011917158796582762, 0.00011914847200020993, 0.0001191253562248192, 0.00011910225204255547, 0.00011907916687316294, 0.00011905608090178074, 0.00011903302158628498, 0.00011900997520800807, 0.00011898694366654028, 0.00011896390720060578, 0.00011894089988940753, 0.00011891789906825275, 0.00011889491842901648, 0.00011887194421998701, 0.000118848979976885, 0.00011882601413873446, 0.0001188030558971588, 0.00011878010614613538, 0.00011875716396042887, 0.00011873422914802616, 0.0001187112943499128, 0.00011868838375974188, 0.00011866548150294036, 0.00011864258176820885, 0.00011861969354354751, 0.00011859681812303548, 0.00011857393887476422, 0.00011855105233088749, 0.00011852815126367424, 0.00011850527831075405, 0.00011848237471075378, 0.00011845948025950885, 0.0001184365604518151, 0.00011841364690464215, 0.00011839072150802699, 0.00011836778230790121, 0.00011834483338208765, 0.0001183218739705716, 0.00011829892402306962, 0.00011827597449978268, 0.00011825301430771096, 0.0001182300596170384, 0.0001182070782115358, 0.00011818406132558103, 0.0001181610317167476, 0.00011813795476819586, 0.00011811486329720067, 0.00011809175639460223, 0.00011806860621403276, 0.00011804540005583382, 0.00011802214632035613, 0.00011799883758696242, 0.0001179754769341142, 0.00011795204574725586, 0.00011792847977611826, 0.00011790478023440293, 0.00011788100055623532, 0.00011785714054781615, 0.00011783318108106633, 0.00011780909798733915, 0.0001177848246505852, 0.00011776034399235383, 0.0001177356490564116, 0.00011771067387776756, 0.00011768536051936533, 0.00011765959651420868, 0.00011763328488138276, 0.00011760638702625047, 0.00011757897966385825, 0.000117550979544462, 0.00011752225566867365, 0.0001174928999759473, 0.00011746290314426924, 0.00011743213343299145, 0.00011740060030166258, 0.00011736830119516852, 0.00011733516347985802, 0.00011730113466291645, 0.00011726619091417603, 0.00011723044394061825, 0.00011719401737815134, 0.00011715664514481782, 0.00011711842098052966, 0.00011707934092981661, 0.0001170395088655249, 0.0001169988097583995, 0.00011695727497445332, 0.0001169150479412353, 0.00011687201680083266, 0.00011682813290247936, 0.00011678330484435802, 0.00011673754832778336, 0.00011669086027518693, 0.00011664333253046282, 0.00011659515187861548, 0.0001165463564132617, 0.00011649690625550921, 0.00011644691940501137], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00012274456855284154, 0.00012274456855284154, 0.00012274456855284154, 0.00012274456855284154, 0.00012274456855284154, 0.00012274456855284154, 0.00012274456855284154, 0.00012274456855284154, 0.00012274456855284154, 0.00012274456855284154, 0.0002454891371056831, 0.0002454891371056831, 0.0002454891371056831]}
[2018-05-05 16:22:41,420 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:92]: done!
[2018-05-05 16:22:41,425 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:152]: >> Executing classifier part ... 
[2018-05-05 16:22:41,425 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:97]: =======================================
[2018-05-05 16:22:41,425 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fd087a0de10>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2018-05-05 16:22:41,607 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:110]: training ... 
[2018-05-06 02:23:43,522 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_FULLDS_OVER_F1_2
[2018-05-06 02:23:43,522 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:146]: >> Printing header log
[2018-05-06 02:23:43,522 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_FULLDS_OVER_F1_2
	layers = 9216,11059
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7efe03844630>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7efe03844e10>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2018-05-06 02:23:43,522 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:148]: >> Loading dataset... 
[2018-05-06 02:27:42,879 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (8147, 9216)
	trainy shape = (8147, 9)
	valx shape = (2721, 9216)
	valy shape = (2721, 9)
	=======================================
	
[2018-05-06 02:27:42,882 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:150]: >> Executing autoencoder part ... 
[2018-05-06 02:27:42,882 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:57]: =======================================
[2018-05-06 02:27:42,882 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7efe03844630>, 'discard_decoder_function': True}
[2018-05-06 02:27:42,986 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:73]: training and evaluate autoencoder
[2018-05-07 20:08:36,819 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:122]: trained!
[2018-05-07 20:08:36,820 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:125]: Training history: 
{'val_loss': [0.00011908114922514616, 0.00011905818563976346, 0.00011903520871646215, 0.00011901223161667674, 0.00011898926105749858, 0.00011896630862804944, 0.00011894336595870513, 0.00011892042320379279, 0.00011889749253001845, 0.00011887458882087349, 0.00011885169068969532, 0.00011882880007780888, 0.00011880592928027227, 0.00011878306812518445, 0.00011876022393930908, 0.00011873738635286897, 0.000118714567163558, 0.00011869174763732287, 0.0001186689415506184, 0.00011864613230859239, 0.00011862332804555683, 0.00011860054211012638, 0.00011857775530831951, 0.00011855498992526327, 0.00011853223889624615, 0.00011850949287295954, 0.00011848676217704853, 0.00011846404723100534, 0.00011844133467551934, 0.00011841861211392037, 0.0001183958778669356, 0.0001183731677233988, 0.00011835043689913573, 0.00011832771718802218, 0.00011830497337880865, 0.00011828224597726727, 0.00011825951141475035, 0.00011823676945059774, 0.00011821401844832065, 0.00011819126118353244, 0.00011816851998414421, 0.00011814576567145347, 0.0001181230028393945, 0.00011810023138635525, 0.00011807742483437456, 0.00011805457277089125, 0.00011803170273811903, 0.00011800878120408124, 0.00011798585946147133, 0.0001179629068769301, 0.00011793991315020443, 0.00011791686263838656, 0.00011789375445905605, 0.0001178705894411533, 0.00011784736336510421, 0.00011782408172330754, 0.00011780071180737901, 0.00011777723536458556, 0.00011775365292437948, 0.0001177299515766823, 0.0001177061374182171, 0.00011768222372272648, 0.00011765817096847076, 0.00011763397999508639, 0.00011760957717662078, 0.00011758485399140405, 0.00011755976566595793, 0.00011753422157302942, 0.00011750813591461191, 0.0001174813784721887, 0.0001174540380493439, 0.00011742611868916753, 0.00011739747977127772, 0.00011736813271900818, 0.00011733814198113889, 0.0001173074327391127, 0.00011727593333504202, 0.00011724363253277325, 0.00011721056137746183, 0.00011717661230473032, 0.00011714184863493017, 0.00011710623038639354, 0.00011706985846523453, 0.00011703250578991235, 0.00011699419393426972, 0.00011695510859700287, 0.000116915058897201, 0.00011687430619175392, 0.00011683266513493359, 0.00011679032837249166, 0.00011674715338015737, 0.00011670298587875006, 0.00011665793311711502, 0.0001166119871534683, 0.00011656509610683613, 0.00011651732893436477, 0.00011646883586144882, 0.00011641965589641968, 0.00011636991638056859, 0.0001163196804203269, 0.00011626895089292005], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.000119240980192705, 0.00011921784236884961, 0.00011919471715445114, 0.00011917158796582762, 0.00011914847200020993, 0.0001191253562248192, 0.00011910225204255547, 0.00011907916687316294, 0.00011905608090178074, 0.00011903302158628498, 0.00011900997520800807, 0.00011898694366654028, 0.00011896390720060578, 0.00011894089988940753, 0.00011891789906825275, 0.00011889491842901648, 0.00011887194421998701, 0.000118848979976885, 0.00011882601413873446, 0.0001188030558971588, 0.00011878010614613538, 0.00011875716396042887, 0.00011873422914802616, 0.0001187112943499128, 0.00011868838375974188, 0.00011866548150294036, 0.00011864258176820885, 0.00011861969354354751, 0.00011859681812303548, 0.00011857393887476422, 0.00011855105233088749, 0.00011852815126367424, 0.00011850527831075405, 0.00011848237471075378, 0.00011845948025950885, 0.0001184365604518151, 0.00011841364690464215, 0.00011839072150802699, 0.00011836778230790121, 0.00011834483338208765, 0.0001183218739705716, 0.00011829892402306962, 0.00011827597449978268, 0.00011825301430771096, 0.0001182300596170384, 0.0001182070782115358, 0.00011818406132558103, 0.0001181610317167476, 0.00011813795476819586, 0.00011811486329720067, 0.00011809175639460223, 0.00011806860621403276, 0.00011804540005583382, 0.00011802214632035613, 0.00011799883758696242, 0.0001179754769341142, 0.00011795204574725586, 0.00011792847977611826, 0.00011790478023440293, 0.00011788100055623532, 0.00011785714054781615, 0.00011783318108106633, 0.00011780909798733915, 0.0001177848246505852, 0.00011776034399235383, 0.0001177356490564116, 0.00011771067387776756, 0.00011768536051936533, 0.00011765959651420868, 0.00011763328488138276, 0.00011760638702625047, 0.00011757897966385825, 0.000117550979544462, 0.00011752225566867365, 0.0001174928999759473, 0.00011746290314426924, 0.00011743213343299145, 0.00011740060030166258, 0.00011736830119516852, 0.00011733516347985802, 0.00011730113466291645, 0.00011726619091417603, 0.00011723044394061825, 0.00011719401737815134, 0.00011715664514481782, 0.00011711842098052966, 0.00011707934092981661, 0.0001170395088655249, 0.0001169988097583995, 0.00011695727497445332, 0.0001169150479412353, 0.00011687201680083266, 0.00011682813290247936, 0.00011678330484435802, 0.00011673754832778336, 0.00011669086027518693, 0.00011664333253046282, 0.00011659515187861548, 0.0001165463564132617, 0.00011649690625550921, 0.00011644691940501137], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00012274456855284154, 0.00012274456855284154, 0.00012274456855284154, 0.00012274456855284154, 0.00012274456855284154, 0.00012274456855284154, 0.00012274456855284154, 0.00012274456855284154, 0.00012274456855284154, 0.00012274456855284154, 0.0002454891371056831, 0.0002454891371056831, 0.0002454891371056831]}
[2018-05-07 20:08:36,820 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:129]: evaluating model ... 
[2018-05-07 20:09:50,905 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:133]: evaluated! 
[2018-05-07 20:09:50,905 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:135]: generating reports ... 
[2018-05-07 20:09:58,116 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:138]: done!
[2018-05-07 20:09:58,117 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:154]: >> experiment AE_BIGRAMA_1L_FULLDS_OVER_F1_2 finished!
[2018-05-08 17:29:35,957 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:85]: trained and evaluated!
[2018-05-08 17:29:35,958 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:88]: Training history: 
{'val_loss': [0.00012030013520981657, 0.00012027122713230107, 0.00012024181676502806, 0.00012021172591254906, 0.00012018092714495839, 0.00012014950690737104, 0.0001201174133295092, 0.00012008477538380497, 0.00012005158022435595, 0.0001200176950869962, 0.00011998305254945622, 0.00011994760561348869, 0.00011991123787446775, 0.00011987389545126667, 0.00011983546418007271, 0.0001197958830669156, 0.00011975542246937363, 0.00011971393044534331, 0.00011967103969400221, 0.00011962701004805937, 0.00011958176056549462, 0.00011953524101251898, 0.00011948763242437039, 0.0001194387555520438, 0.00011938869554108952, 0.00011933760990622977, 0.00011928555729366169, 0.0001192326510436056, 0.0001191787617825292, 0.00011912422238616187, 0.00011906897835300853, 0.00011901308272455951, 0.00011895660611049391, 0.00011889959482451472, 0.00011884222213656, 0.00011878442498833045, 0.00011872640413849777, 0.00011866818516655869, 0.00011860982002835901, 0.00011855139100826758, 0.00011849303037308612, 0.00011843467852467304, 0.0001183763803220746, 0.00011831817973656863, 0.00011826001510568455, 0.00011820191493962447, 0.00011814388427085893, 0.00011808590454181868, 0.00011802796894984435, 0.00011797015745827168, 0.00011791246288473307, 0.00011785486399231, 0.00011779733561864996, 0.000117739902059729, 0.00011768254703087907, 0.00011762530534759743, 0.0001175682023968527, 0.00011751115217741418, 0.00011745420484819875, 0.00011739737698801467, 0.00011734064891621216, 0.00011728404730328046, 0.00011722756741623723, 0.0001171711557910999, 0.00011711485359679294, 0.00011705865166149178, 0.00011700255080878883, 0.00011694657003478951, 0.00011689068640725847, 0.00011683491068102901, 0.00011677923618179384, 0.00011672368246987268, 0.00011666821125624048, 0.000116612852206268, 0.00011655760856084484, 0.00011650244390007293, 0.00011644738280337245, 0.00011639241695994729, 0.00011633756556645299, 0.00011628281232217743, 0.00011622815633667818, 0.00011617363773334518, 0.00011611919373464923, 0.00011606487787142293, 0.00011601065338417002, 0.00011595650690020986, 0.00011590246823465727, 0.00011584850622202673, 0.00011579467415249075, 0.0001157409462219365, 0.00011568734239713189, 0.00011563381633495991, 0.00011558039855379773, 0.00011552706892911131, 0.00011547384526440155, 0.00011542071742520338, 0.00011536772327270161, 0.00011531479261168738, 0.00011526197440312502, 0.000115209237414679, 0.00011515664931730094], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0003675119441381845, 0.0003675119441381845, 0.0003675119441381845, 0.0011025358324145535, 0.001470047776552738, 0.002205071664829107, 0.002205071664829107, 0.002205071664829107, 0.003675119441381845, 0.006247703050349137, 0.006615214994487321, 0.00808526277104006, 0.009555310547592797, 0.013597941933112825, 0.020948180815876516, 0.027563395810363836, 0.03822124219037119, 0.05145167217934583, 0.06284454244762955, 0.08269018743109151, 0.10253583241455347, 0.12238147739801543, 0.14332965821389196, 0.16280779125321573, 0.17934582873943403, 0.19735391400220506, 0.2175670709298052, 0.23520764424843807, 0.25725836089672915, 0.27673649393605293, 0.29694965086365305, 0.31900036751194416, 0.33774347666299154, 0.35575156192576257, 0.3700845277471518, 0.38294744579198825, 0.3976479235575156, 0.40977581771407573, 0.41969864020580666, 0.42704887908857037, 0.43476662991547227, 0.43991179713340683, 0.44689452407203234, 0.4524072032341051, 0.45608232267548693, 0.4597574421168688, 0.46306504961411243, 0.4667401690554943, 0.47115031238515254, 0.4726203601617053], 'loss': [0.00012042747127334236, 0.00012039841549171288, 0.00012036896875172361, 0.00012033897546559014, 0.00012030826449246531, 0.00012027696452579362, 0.00012024501309343672, 0.00012021244325380224, 0.00012017924074344119, 0.00012014539557499207, 0.00012011073849558792, 0.00012007518995642592, 0.00012003877205509036, 0.00012000149599085806, 0.00011996332470073152, 0.00011992402170135234, 0.0001198837385366194, 0.0001198426237389513, 0.00011980045339247166, 0.00011975703709339854, 0.00011971259734742827, 0.00011966711420395115, 0.00011962040116017737, 0.00011957253610645224, 0.00011952362254320586, 0.0001194736710141912, 0.00011942278410085463, 0.00011937096495131824, 0.00011931813997811674, 0.00011926449710761629, 0.00011921017638346947, 0.0001191552185213885, 0.00011909963509128023, 0.00011904341382931133, 0.00011898658588179602, 0.00011892928734316818, 0.00011887163009634693, 0.000118813835379742, 0.00011875592278315703, 0.00011869795190835748, 0.00011863993852096013, 0.00011858196089712269, 0.00011852399015896504, 0.00011846609019773643, 0.00011840826594862058, 0.00011835049760032897, 0.00011829278811164979, 0.00011823515817713194, 0.0001181775937842831, 0.00011812008905214328, 0.00011806269830792222, 0.00011800542102827254, 0.00011794823885316765, 0.00011789113407453246, 0.00011783413467626977, 0.00011777721052750218, 0.00011772039759184049, 0.00011766372972432353, 0.0001176071138200222, 0.00011755059762947363, 0.0001174941946436089, 0.00011743789813035872, 0.00011738171372061948, 0.00011732565672006953, 0.00011726966378581336, 0.00011721377928985024, 0.000117157994943465, 0.00011710231295346886, 0.00011704675282035707, 0.00011699128364698592, 0.00011693592646281092, 0.00011688066463413738, 0.00011682552581799922, 0.00011677046631310347, 0.00011671552642880903, 0.0001166606933377464, 0.00011660594688121726, 0.00011655129507197375, 0.00011649673967117807, 0.00011644228942033913, 0.00011638794622797803, 0.00011633369534659409, 0.00011627958365596724, 0.00011622554482226272, 0.00011617162876889167, 0.00011611780955532817, 0.00011606406929220101, 0.00011601043350832431, 0.0001159568684245714, 0.000115903436463961, 0.00011585011016504463, 0.00011579690052347594, 0.0001157437740691354, 0.00011569075237625995, 0.00011563782255764345, 0.00011558499496859811, 0.0001155322570675414, 0.00011547965815361856, 0.00011542712075712696, 0.00011537469854618014, 0.00011532235179192399], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00012274456855284154, 0.0002454891371056831, 0.0002454891371056831, 0.0002454891371056831, 0.0002454891371056831, 0.0002454891371056831, 0.0003682337056585246, 0.0003682337056585246, 0.0004909782742113662, 0.0007364674113170492, 0.0008592119798698908, 0.0009819565484227323, 0.001350190254081257, 0.001841168528292623, 0.0030686142138210385, 0.003805081626052606, 0.005032527311581022, 0.00638271756474776, 0.007487418681723334, 0.010065054621333006, 0.014606603657788143, 0.020375598379771695, 0.026021848533202406, 0.03412299005768995, 0.045047256658892844, 0.057321713514176996, 0.07880201301092427, 0.099177611392525, 0.1193077206342765, 0.13747391678009704, 0.15686755859589918, 0.17515649932856295, 0.195532097675412, 0.21345280468412683, 0.23591506076221952, 0.25567693626630433, 0.27482508896054764, 0.29888302442708364, 0.3182766662428858, 0.33889775377805353, 0.3549772922401854, 0.370565852431764, 0.38271756477336644, 0.39437829878588637, 0.4030931631165574, 0.41131704922788814, 0.41855897875421544, 0.42543267464804563, 0.4330428378983218, 0.43954829995846095, 0.4454400392672877, 0.4519455014005883, 0.45710077327980764, 0.4617650669396867, 0.4658156376470594, 0.46925248558482935, 0.4723210997949923]}
[2018-05-08 17:29:35,959 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:92]: done!
[2018-05-08 17:29:35,959 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:152]: >> Executing classifier part ... 
[2018-05-08 17:29:35,959 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:97]: =======================================
[2018-05-08 17:29:35,959 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7efe03844e10>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2018-05-08 17:29:36,266 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:110]: training ... 
[2018-05-10 04:12:41,473 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:122]: trained!
[2018-05-10 04:12:41,475 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:125]: Training history: 
{'val_loss': [0.00012030013520981657, 0.00012027122713230107, 0.00012024181676502806, 0.00012021172591254906, 0.00012018092714495839, 0.00012014950690737104, 0.0001201174133295092, 0.00012008477538380497, 0.00012005158022435595, 0.0001200176950869962, 0.00011998305254945622, 0.00011994760561348869, 0.00011991123787446775, 0.00011987389545126667, 0.00011983546418007271, 0.0001197958830669156, 0.00011975542246937363, 0.00011971393044534331, 0.00011967103969400221, 0.00011962701004805937, 0.00011958176056549462, 0.00011953524101251898, 0.00011948763242437039, 0.0001194387555520438, 0.00011938869554108952, 0.00011933760990622977, 0.00011928555729366169, 0.0001192326510436056, 0.0001191787617825292, 0.00011912422238616187, 0.00011906897835300853, 0.00011901308272455951, 0.00011895660611049391, 0.00011889959482451472, 0.00011884222213656, 0.00011878442498833045, 0.00011872640413849777, 0.00011866818516655869, 0.00011860982002835901, 0.00011855139100826758, 0.00011849303037308612, 0.00011843467852467304, 0.0001183763803220746, 0.00011831817973656863, 0.00011826001510568455, 0.00011820191493962447, 0.00011814388427085893, 0.00011808590454181868, 0.00011802796894984435, 0.00011797015745827168, 0.00011791246288473307, 0.00011785486399231, 0.00011779733561864996, 0.000117739902059729, 0.00011768254703087907, 0.00011762530534759743, 0.0001175682023968527, 0.00011751115217741418, 0.00011745420484819875, 0.00011739737698801467, 0.00011734064891621216, 0.00011728404730328046, 0.00011722756741623723, 0.0001171711557910999, 0.00011711485359679294, 0.00011705865166149178, 0.00011700255080878883, 0.00011694657003478951, 0.00011689068640725847, 0.00011683491068102901, 0.00011677923618179384, 0.00011672368246987268, 0.00011666821125624048, 0.000116612852206268, 0.00011655760856084484, 0.00011650244390007293, 0.00011644738280337245, 0.00011639241695994729, 0.00011633756556645299, 0.00011628281232217743, 0.00011622815633667818, 0.00011617363773334518, 0.00011611919373464923, 0.00011606487787142293, 0.00011601065338417002, 0.00011595650690020986, 0.00011590246823465727, 0.00011584850622202673, 0.00011579467415249075, 0.0001157409462219365, 0.00011568734239713189, 0.00011563381633495991, 0.00011558039855379773, 0.00011552706892911131, 0.00011547384526440155, 0.00011542071742520338, 0.00011536772327270161, 0.00011531479261168738, 0.00011526197440312502, 0.000115209237414679, 0.00011515664931730094], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0003675119441381845, 0.0003675119441381845, 0.0003675119441381845, 0.0011025358324145535, 0.001470047776552738, 0.002205071664829107, 0.002205071664829107, 0.002205071664829107, 0.003675119441381845, 0.006247703050349137, 0.006615214994487321, 0.00808526277104006, 0.009555310547592797, 0.013597941933112825, 0.020948180815876516, 0.027563395810363836, 0.03822124219037119, 0.05145167217934583, 0.06284454244762955, 0.08269018743109151, 0.10253583241455347, 0.12238147739801543, 0.14332965821389196, 0.16280779125321573, 0.17934582873943403, 0.19735391400220506, 0.2175670709298052, 0.23520764424843807, 0.25725836089672915, 0.27673649393605293, 0.29694965086365305, 0.31900036751194416, 0.33774347666299154, 0.35575156192576257, 0.3700845277471518, 0.38294744579198825, 0.3976479235575156, 0.40977581771407573, 0.41969864020580666, 0.42704887908857037, 0.43476662991547227, 0.43991179713340683, 0.44689452407203234, 0.4524072032341051, 0.45608232267548693, 0.4597574421168688, 0.46306504961411243, 0.4667401690554943, 0.47115031238515254, 0.4726203601617053], 'loss': [0.00012042747127334236, 0.00012039841549171288, 0.00012036896875172361, 0.00012033897546559014, 0.00012030826449246531, 0.00012027696452579362, 0.00012024501309343672, 0.00012021244325380224, 0.00012017924074344119, 0.00012014539557499207, 0.00012011073849558792, 0.00012007518995642592, 0.00012003877205509036, 0.00012000149599085806, 0.00011996332470073152, 0.00011992402170135234, 0.0001198837385366194, 0.0001198426237389513, 0.00011980045339247166, 0.00011975703709339854, 0.00011971259734742827, 0.00011966711420395115, 0.00011962040116017737, 0.00011957253610645224, 0.00011952362254320586, 0.0001194736710141912, 0.00011942278410085463, 0.00011937096495131824, 0.00011931813997811674, 0.00011926449710761629, 0.00011921017638346947, 0.0001191552185213885, 0.00011909963509128023, 0.00011904341382931133, 0.00011898658588179602, 0.00011892928734316818, 0.00011887163009634693, 0.000118813835379742, 0.00011875592278315703, 0.00011869795190835748, 0.00011863993852096013, 0.00011858196089712269, 0.00011852399015896504, 0.00011846609019773643, 0.00011840826594862058, 0.00011835049760032897, 0.00011829278811164979, 0.00011823515817713194, 0.0001181775937842831, 0.00011812008905214328, 0.00011806269830792222, 0.00011800542102827254, 0.00011794823885316765, 0.00011789113407453246, 0.00011783413467626977, 0.00011777721052750218, 0.00011772039759184049, 0.00011766372972432353, 0.0001176071138200222, 0.00011755059762947363, 0.0001174941946436089, 0.00011743789813035872, 0.00011738171372061948, 0.00011732565672006953, 0.00011726966378581336, 0.00011721377928985024, 0.000117157994943465, 0.00011710231295346886, 0.00011704675282035707, 0.00011699128364698592, 0.00011693592646281092, 0.00011688066463413738, 0.00011682552581799922, 0.00011677046631310347, 0.00011671552642880903, 0.0001166606933377464, 0.00011660594688121726, 0.00011655129507197375, 0.00011649673967117807, 0.00011644228942033913, 0.00011638794622797803, 0.00011633369534659409, 0.00011627958365596724, 0.00011622554482226272, 0.00011617162876889167, 0.00011611780955532817, 0.00011606406929220101, 0.00011601043350832431, 0.0001159568684245714, 0.000115903436463961, 0.00011585011016504463, 0.00011579690052347594, 0.0001157437740691354, 0.00011569075237625995, 0.00011563782255764345, 0.00011558499496859811, 0.0001155322570675414, 0.00011547965815361856, 0.00011542712075712696, 0.00011537469854618014, 0.00011532235179192399], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00012274456855284154, 0.0002454891371056831, 0.0002454891371056831, 0.0002454891371056831, 0.0002454891371056831, 0.0002454891371056831, 0.0003682337056585246, 0.0003682337056585246, 0.0004909782742113662, 0.0007364674113170492, 0.0008592119798698908, 0.0009819565484227323, 0.001350190254081257, 0.001841168528292623, 0.0030686142138210385, 0.003805081626052606, 0.005032527311581022, 0.00638271756474776, 0.007487418681723334, 0.010065054621333006, 0.014606603657788143, 0.020375598379771695, 0.026021848533202406, 0.03412299005768995, 0.045047256658892844, 0.057321713514176996, 0.07880201301092427, 0.099177611392525, 0.1193077206342765, 0.13747391678009704, 0.15686755859589918, 0.17515649932856295, 0.195532097675412, 0.21345280468412683, 0.23591506076221952, 0.25567693626630433, 0.27482508896054764, 0.29888302442708364, 0.3182766662428858, 0.33889775377805353, 0.3549772922401854, 0.370565852431764, 0.38271756477336644, 0.39437829878588637, 0.4030931631165574, 0.41131704922788814, 0.41855897875421544, 0.42543267464804563, 0.4330428378983218, 0.43954829995846095, 0.4454400392672877, 0.4519455014005883, 0.45710077327980764, 0.4617650669396867, 0.4658156376470594, 0.46925248558482935, 0.4723210997949923]}
[2018-05-10 04:12:41,475 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:129]: evaluating model ... 
[2018-05-10 04:13:21,418 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:133]: evaluated! 
[2018-05-10 04:13:21,419 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:135]: generating reports ... 
[2018-05-10 04:13:25,800 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:138]: done!
[2018-05-10 04:13:25,800 AE_BIGRAMA_1L_FULLDS_OVER_F1_2.py:154]: >> experiment AE_BIGRAMA_1L_FULLDS_OVER_F1_2 finished!
