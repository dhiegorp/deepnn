[2017-12-14 09:31:58,009 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_2
[2017-12-14 09:31:58,009 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:146]: >> Printing header log
[2017-12-14 09:31:58,009 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_2
	layers = 9216,1843
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7ff8e6b2beb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7ff8e6b0e400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:58,009 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:148]: >> Loading dataset... 
[2017-12-14 09:32:19,909 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:19,909 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:19,909 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:57]: =======================================
[2017-12-14 09:32:19,910 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7ff8e6b2beb8>, 'discard_decoder_function': True}
[2017-12-14 09:32:19,961 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:54,911 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_2
[2017-12-14 10:18:54,911 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:146]: >> Printing header log
[2017-12-14 10:18:54,911 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_2
	layers = 9216,1843
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fcef588aeb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fcef586d400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:54,911 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:148]: >> Loading dataset... 
[2017-12-14 10:19:18,505 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:18,505 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:18,505 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:57]: =======================================
[2017-12-14 10:19:18,505 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fcef588aeb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:18,549 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:73]: training and evaluate autoencoder
[2017-12-14 14:11:27,980 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:85]: trained and evaluated!
[2017-12-14 14:11:27,992 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:88]: Training history: 
{'val_loss': [0.0001114186469630821, 0.00011141480363065066, 0.00011141090668495905, 0.00011140694631150917, 0.00011140297531909413, 0.00011139898494796149, 0.00011139497087186623, 0.00011139098132307772, 0.00011138690137006891, 0.00011138281798466733, 0.00011137866055254231, 0.00011137445424457427, 0.00011137028234991924, 0.0001113660275772802, 0.00011136169328754173, 0.00011135732270740042, 0.00011135294844458768, 0.00011134852834502968, 0.00011134407086456904, 0.00011133956475854401, 0.000111335052020135, 0.00011133049923714356, 0.00011132590991347063, 0.0001113212767731586, 0.00011131666783836649, 0.0001113120035562408, 0.00011130735745507061, 0.00011130266569592562, 0.00011129795359270258, 0.00011129318549857231, 0.00011128839500450371, 0.00011128358219988205, 0.00011127879186670688, 0.00011127398624865759, 0.00011126913886982948, 0.00011126423852131485, 0.00011125933647448087, 0.0001112544128142986, 0.00011124941918335933, 0.00011124443756579479, 0.00011123942391256428, 0.00011123443886260697, 0.00011122942864176924, 0.00011122440549582746, 0.00011121933756788611, 0.00011121429922645546, 0.00011120927200454725, 0.00011120421380171878, 0.00011119916671841275, 0.0001111940447302851, 0.00011118892790862366, 0.00011118374998321991, 0.00011117858028125718, 0.00011117337886541533, 0.00011116820753664144, 0.0001111629847040988, 0.00011115777944469212, 0.00011115256544341005, 0.00011114731073609487, 0.00011114206543210574, 0.00011113676617519257, 0.00011113149462770029, 0.00011112620820650597, 0.00011112088696082656, 0.00011111559342456803, 0.00011111027030179881, 0.00011110491407074089, 0.00011109957795135942, 0.00011109423300071736, 0.00011108884387589529, 0.0001110834984068189, 0.00011107815016680042, 0.00011107275271127503, 0.00011106734610270224, 0.00011106194473210384, 0.00011105655194248739, 0.00011105116233498509, 0.00011104578769057006, 0.00011104040144395235, 0.00011103499705213321, 0.00011102960066923057, 0.0001110241853187824, 0.00011101877422307112, 0.00011101336443238417, 0.00011100794230653567, 0.00011100253054937371, 0.00011099715443904092, 0.00011099173680033091, 0.00011098633428560158, 0.00011098092490608669, 0.00011097551168300696, 0.0001109700934007233, 0.00011096464274310981, 0.0001109592097480175, 0.00011095377494734357, 0.00011094834563492268, 0.00011094292799621267, 0.00011093748486483543, 0.00011093204802617829, 0.0001109266082378086, 0.0001109211485880411, 0.00011091567422546497, 0.00011091021899132776, 0.00011090477593145871, 0.00011089930941690565, 0.00011089383930906639, 0.00011088837434981631, 0.00011088292467544034, 0.00011087744214305429, 0.00011087194464758099, 0.0001108664716794144, 0.00011086101015255711, 0.0001108555413497422, 0.00011085010123958568, 0.00011084465353168474, 0.00011083919273778632, 0.00011083373979191097, 0.00011082824108079854, 0.00011082275731489634, 0.00011081731050084768, 0.00011081182739639618, 0.00011080638172647845, 0.00011080090900859052, 0.00011079544633760229, 0.00011078998824313777, 0.0001107845385687618, 0.00011077907999161704, 0.00011077361919771862, 0.00011076813298266116, 0.00011076263271624575, 0.00011075718974576192, 0.00011075172094294701, 0.00011074623358375862, 0.00011074077459544181, 0.00011073532206073851, 0.00011072986593274903, 0.00011072442451756819, 0.00011071897999178138, 0.0001107135265632258, 0.00011070804133140585, 0.00011070257138446001, 0.00011069714264410459, 0.00011069168340550915, 0.00011068623601939502, 0.00011068078716736314, 0.00011067533030641478, 0.00011066988717503754, 0.00011066447461340851, 0.00011065900933237161, 0.00011065356652278119, 0.00011064810410207161, 0.00011064262868474977, 0.00011063717468412873, 0.00011063175043091178, 0.00011062630084592103, 0.00011062085362070032, 0.00011061544458084929, 0.00011061002483264787, 0.00011060458366774567, 0.0001105991545162182, 0.00011059371415578304, 0.00011058830136175243, 0.00011058289575429418, 0.00011057745033465509, 0.00011057198333742179, 0.00011056656301715491, 0.00011056117326663624, 0.00011055573725032321, 0.00011055032649427579, 0.00011054491548794973, 0.00011053951486818726, 0.00011053409470881379, 0.00011052867022319526, 0.00011052323567279997, 0.00011051778919841518, 0.00011051240100319948, 0.00011050697528406479, 0.0001105015763804987, 0.00011049616210267328, 0.00011049073221818693, 0.0001104852999560535, 0.00011047989425921003, 0.00011047447279481223, 0.00011046912759389152, 0.00011046373081769387, 0.00011045833722361036, 0.00011045295868199936, 0.00011044755797285166, 0.00011044217394298762, 0.00011043678533659987, 0.00011043139084866408, 0.00011042598082557554, 0.0001104205985119079, 0.00011041519445975263, 0.00011040981100195406, 0.00011040441724697713, 0.00011039904671428263, 0.00011039367192685123, 0.0001103882861807908, 0.00011038293141565064], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011203407230980373, 0.00011202993674555654, 0.00011202578880981139, 0.00011202160063114758, 0.00011201734059351476, 0.0001120130798448763, 0.0001120087736392779, 0.00011200444899493348, 0.00011200012136436544, 0.00011199568241401591, 0.00011199126522043866, 0.00011198678218774006, 0.00011198226322555696, 0.00011197779026543812, 0.00011197326805632931, 0.00011196868557764326, 0.00011196408034677706, 0.00011195947627722003, 0.00011195486590341311, 0.00011195023446013933, 0.00011194556919669775, 0.00011194088904953832, 0.00011193616707154766, 0.00011193139875969009, 0.00011192657953982943, 0.00011192175811585129, 0.00011191689047650722, 0.00011191202748239992, 0.00011190713163983252, 0.00011190223020402083, 0.00011189725088939246, 0.00011189225022089501, 0.00011188721798374757, 0.00011188220283443546, 0.00011187716877237357, 0.00011187207564944401, 0.00011186692220953685, 0.00011186176903033174, 0.00011185661250940017, 0.00011185138977014426, 0.00011184618219900846, 0.00011184094981377617, 0.00011183573894831428, 0.0001118304813460823, 0.00011182523037990288, 0.0001118199255194967, 0.00011181464445407894, 0.00011180937997879254, 0.00011180406134857733, 0.00011179877032908075, 0.00011179341444217052, 0.00011178808069123556, 0.00011178269925552303, 0.00011177733180292121, 0.0001117719403183291, 0.00011176657857747251, 0.00011176116332159217, 0.00011175575666887995, 0.00011175033416074219, 0.00011174488238287264, 0.00011173945380748684, 0.00011173397446629904, 0.00011172851093313641, 0.0001117230043367328, 0.00011171746854169798, 0.0001117119645760152, 0.00011170644624801868, 0.00011170089319924724, 0.00011169535813891823, 0.00011168982675211831, 0.00011168426227985641, 0.0001116787554938513, 0.00011167325226287434, 0.00011166770959478508, 0.00011166215829982753, 0.0001116566132854197, 0.00011165107651867718, 0.00011164554589028327, 0.0001116400277044879, 0.00011163451487493492, 0.00011162898597665471, 0.00011162347115628597, 0.00011161793555085265, 0.00011161240105932815, 0.00011160686860601979, 0.0001116013143011384, 0.00011159576845722401, 0.00011159026316433071, 0.00011158471241447748, 0.00011157916469824825, 0.00011157360728864228, 0.0001115680546664742, 0.00011156250484092828, 0.00011155693548642773, 0.00011155138016243826, 0.00011154582761137076, 0.00011154027570020832, 0.000111534741232384, 0.00011152917820583355, 0.00011152361487118066, 0.00011151805248453527, 0.00011151246959722756, 0.00011150686367333742, 0.00011150127391297528, 0.00011149569875192874, 0.00011149010389602625, 0.00011148452062951552, 0.00011147893812141081, 0.00011147337374394966, 0.00011146777606772484, 0.00011146215800933862, 0.00011145656758537121, 0.00011145098801608977, 0.00011144540401487323, 0.00011143984869088377, 0.00011143427919418209, 0.00011142870012260458, 0.00011142312882468862, 0.0001114175069505722, 0.0001114119019272892, 0.00011140633198028394, 0.00011140072887671614, 0.00011139515819500507, 0.00011138956011587705, 0.00011138397331803836, 0.00011137838813181245, 0.00011137281557778654, 0.00011136723048636138, 0.00011136164852336098, 0.00011135603954214664, 0.00011135041821313452, 0.00011134485288766586, 0.0001113392618000932, 0.0001113336517997708, 0.00011132807196978728, 0.00011132249910765895, 0.00011131692209799777, 0.00011131135575342104, 0.00011130579118635839, 0.00011130021443739928, 0.0001112946078262037, 0.00011128901493741679, 0.00011128346219674778, 0.00011127787881173612, 0.00011127231054744419, 0.00011126673704541079, 0.00011126115681252408, 0.00011125558902223591, 0.00011125005583422173, 0.00011124446320613688, 0.0001112388971696626, 0.00011123331238633987, 0.00011122771212679458, 0.00011122213672874616, 0.00011121658441468054, 0.00011121101050974394, 0.00011120543601230267, 0.00011119990640301681, 0.00011119435970589568, 0.00011118879283991482, 0.00011118323817953061, 0.00011117767458417565, 0.00011117213549481475, 0.00011116659941537767, 0.00011116103003717692, 0.00011115543359336186, 0.0001111498892188591, 0.00011114437555979956, 0.00011113881279395117, 0.00011113327673821428, 0.00011112774243629127, 0.00011112221709303922, 0.00011111667316884003, 0.00011111111708644456, 0.0001111055610040491, 0.0001110999863170063, 0.00011109447460136216, 0.00011108892174219221, 0.00011108339696774465, 0.00011107785562686592, 0.00011107230357350235, 0.00011106674820211251, 0.00011106121330768483, 0.00011105566744007024, 0.00011105019518523857, 0.00011104467199870358, 0.00011103915085038473, 0.00011103364567599239, 0.00011102811931363225, 0.00011102260728988566, 0.00011101709483953569, 0.00011101157020728926, 0.00011100603317984468, 0.00011100052693894388, 0.00011099499749555935, 0.0001109894861591182, 0.00011098396666981249, 0.00011097847045409108, 0.00011097296544560003, 0.0001109674563843769], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433227182599538, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329]}
[2017-12-14 14:11:27,992 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:92]: done!
[2017-12-14 14:11:28,035 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:152]: >> Executing classifier part ... 
[2017-12-14 14:11:28,035 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:97]: =======================================
[2017-12-14 14:11:28,035 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fcef586d400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-14 14:11:28,418 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:110]: training ... 
[2017-12-14 16:00:21,130 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:122]: trained!
[2017-12-14 16:00:21,141 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:125]: Training history: 
{'val_loss': [0.0001114186469630821, 0.00011141480363065066, 0.00011141090668495905, 0.00011140694631150917, 0.00011140297531909413, 0.00011139898494796149, 0.00011139497087186623, 0.00011139098132307772, 0.00011138690137006891, 0.00011138281798466733, 0.00011137866055254231, 0.00011137445424457427, 0.00011137028234991924, 0.0001113660275772802, 0.00011136169328754173, 0.00011135732270740042, 0.00011135294844458768, 0.00011134852834502968, 0.00011134407086456904, 0.00011133956475854401, 0.000111335052020135, 0.00011133049923714356, 0.00011132590991347063, 0.0001113212767731586, 0.00011131666783836649, 0.0001113120035562408, 0.00011130735745507061, 0.00011130266569592562, 0.00011129795359270258, 0.00011129318549857231, 0.00011128839500450371, 0.00011128358219988205, 0.00011127879186670688, 0.00011127398624865759, 0.00011126913886982948, 0.00011126423852131485, 0.00011125933647448087, 0.0001112544128142986, 0.00011124941918335933, 0.00011124443756579479, 0.00011123942391256428, 0.00011123443886260697, 0.00011122942864176924, 0.00011122440549582746, 0.00011121933756788611, 0.00011121429922645546, 0.00011120927200454725, 0.00011120421380171878, 0.00011119916671841275, 0.0001111940447302851, 0.00011118892790862366, 0.00011118374998321991, 0.00011117858028125718, 0.00011117337886541533, 0.00011116820753664144, 0.0001111629847040988, 0.00011115777944469212, 0.00011115256544341005, 0.00011114731073609487, 0.00011114206543210574, 0.00011113676617519257, 0.00011113149462770029, 0.00011112620820650597, 0.00011112088696082656, 0.00011111559342456803, 0.00011111027030179881, 0.00011110491407074089, 0.00011109957795135942, 0.00011109423300071736, 0.00011108884387589529, 0.0001110834984068189, 0.00011107815016680042, 0.00011107275271127503, 0.00011106734610270224, 0.00011106194473210384, 0.00011105655194248739, 0.00011105116233498509, 0.00011104578769057006, 0.00011104040144395235, 0.00011103499705213321, 0.00011102960066923057, 0.0001110241853187824, 0.00011101877422307112, 0.00011101336443238417, 0.00011100794230653567, 0.00011100253054937371, 0.00011099715443904092, 0.00011099173680033091, 0.00011098633428560158, 0.00011098092490608669, 0.00011097551168300696, 0.0001109700934007233, 0.00011096464274310981, 0.0001109592097480175, 0.00011095377494734357, 0.00011094834563492268, 0.00011094292799621267, 0.00011093748486483543, 0.00011093204802617829, 0.0001109266082378086, 0.0001109211485880411, 0.00011091567422546497, 0.00011091021899132776, 0.00011090477593145871, 0.00011089930941690565, 0.00011089383930906639, 0.00011088837434981631, 0.00011088292467544034, 0.00011087744214305429, 0.00011087194464758099, 0.0001108664716794144, 0.00011086101015255711, 0.0001108555413497422, 0.00011085010123958568, 0.00011084465353168474, 0.00011083919273778632, 0.00011083373979191097, 0.00011082824108079854, 0.00011082275731489634, 0.00011081731050084768, 0.00011081182739639618, 0.00011080638172647845, 0.00011080090900859052, 0.00011079544633760229, 0.00011078998824313777, 0.0001107845385687618, 0.00011077907999161704, 0.00011077361919771862, 0.00011076813298266116, 0.00011076263271624575, 0.00011075718974576192, 0.00011075172094294701, 0.00011074623358375862, 0.00011074077459544181, 0.00011073532206073851, 0.00011072986593274903, 0.00011072442451756819, 0.00011071897999178138, 0.0001107135265632258, 0.00011070804133140585, 0.00011070257138446001, 0.00011069714264410459, 0.00011069168340550915, 0.00011068623601939502, 0.00011068078716736314, 0.00011067533030641478, 0.00011066988717503754, 0.00011066447461340851, 0.00011065900933237161, 0.00011065356652278119, 0.00011064810410207161, 0.00011064262868474977, 0.00011063717468412873, 0.00011063175043091178, 0.00011062630084592103, 0.00011062085362070032, 0.00011061544458084929, 0.00011061002483264787, 0.00011060458366774567, 0.0001105991545162182, 0.00011059371415578304, 0.00011058830136175243, 0.00011058289575429418, 0.00011057745033465509, 0.00011057198333742179, 0.00011056656301715491, 0.00011056117326663624, 0.00011055573725032321, 0.00011055032649427579, 0.00011054491548794973, 0.00011053951486818726, 0.00011053409470881379, 0.00011052867022319526, 0.00011052323567279997, 0.00011051778919841518, 0.00011051240100319948, 0.00011050697528406479, 0.0001105015763804987, 0.00011049616210267328, 0.00011049073221818693, 0.0001104852999560535, 0.00011047989425921003, 0.00011047447279481223, 0.00011046912759389152, 0.00011046373081769387, 0.00011045833722361036, 0.00011045295868199936, 0.00011044755797285166, 0.00011044217394298762, 0.00011043678533659987, 0.00011043139084866408, 0.00011042598082557554, 0.0001104205985119079, 0.00011041519445975263, 0.00011040981100195406, 0.00011040441724697713, 0.00011039904671428263, 0.00011039367192685123, 0.0001103882861807908, 0.00011038293141565064], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011203407230980373, 0.00011202993674555654, 0.00011202578880981139, 0.00011202160063114758, 0.00011201734059351476, 0.0001120130798448763, 0.0001120087736392779, 0.00011200444899493348, 0.00011200012136436544, 0.00011199568241401591, 0.00011199126522043866, 0.00011198678218774006, 0.00011198226322555696, 0.00011197779026543812, 0.00011197326805632931, 0.00011196868557764326, 0.00011196408034677706, 0.00011195947627722003, 0.00011195486590341311, 0.00011195023446013933, 0.00011194556919669775, 0.00011194088904953832, 0.00011193616707154766, 0.00011193139875969009, 0.00011192657953982943, 0.00011192175811585129, 0.00011191689047650722, 0.00011191202748239992, 0.00011190713163983252, 0.00011190223020402083, 0.00011189725088939246, 0.00011189225022089501, 0.00011188721798374757, 0.00011188220283443546, 0.00011187716877237357, 0.00011187207564944401, 0.00011186692220953685, 0.00011186176903033174, 0.00011185661250940017, 0.00011185138977014426, 0.00011184618219900846, 0.00011184094981377617, 0.00011183573894831428, 0.0001118304813460823, 0.00011182523037990288, 0.0001118199255194967, 0.00011181464445407894, 0.00011180937997879254, 0.00011180406134857733, 0.00011179877032908075, 0.00011179341444217052, 0.00011178808069123556, 0.00011178269925552303, 0.00011177733180292121, 0.0001117719403183291, 0.00011176657857747251, 0.00011176116332159217, 0.00011175575666887995, 0.00011175033416074219, 0.00011174488238287264, 0.00011173945380748684, 0.00011173397446629904, 0.00011172851093313641, 0.0001117230043367328, 0.00011171746854169798, 0.0001117119645760152, 0.00011170644624801868, 0.00011170089319924724, 0.00011169535813891823, 0.00011168982675211831, 0.00011168426227985641, 0.0001116787554938513, 0.00011167325226287434, 0.00011166770959478508, 0.00011166215829982753, 0.0001116566132854197, 0.00011165107651867718, 0.00011164554589028327, 0.0001116400277044879, 0.00011163451487493492, 0.00011162898597665471, 0.00011162347115628597, 0.00011161793555085265, 0.00011161240105932815, 0.00011160686860601979, 0.0001116013143011384, 0.00011159576845722401, 0.00011159026316433071, 0.00011158471241447748, 0.00011157916469824825, 0.00011157360728864228, 0.0001115680546664742, 0.00011156250484092828, 0.00011155693548642773, 0.00011155138016243826, 0.00011154582761137076, 0.00011154027570020832, 0.000111534741232384, 0.00011152917820583355, 0.00011152361487118066, 0.00011151805248453527, 0.00011151246959722756, 0.00011150686367333742, 0.00011150127391297528, 0.00011149569875192874, 0.00011149010389602625, 0.00011148452062951552, 0.00011147893812141081, 0.00011147337374394966, 0.00011146777606772484, 0.00011146215800933862, 0.00011145656758537121, 0.00011145098801608977, 0.00011144540401487323, 0.00011143984869088377, 0.00011143427919418209, 0.00011142870012260458, 0.00011142312882468862, 0.0001114175069505722, 0.0001114119019272892, 0.00011140633198028394, 0.00011140072887671614, 0.00011139515819500507, 0.00011138956011587705, 0.00011138397331803836, 0.00011137838813181245, 0.00011137281557778654, 0.00011136723048636138, 0.00011136164852336098, 0.00011135603954214664, 0.00011135041821313452, 0.00011134485288766586, 0.0001113392618000932, 0.0001113336517997708, 0.00011132807196978728, 0.00011132249910765895, 0.00011131692209799777, 0.00011131135575342104, 0.00011130579118635839, 0.00011130021443739928, 0.0001112946078262037, 0.00011128901493741679, 0.00011128346219674778, 0.00011127787881173612, 0.00011127231054744419, 0.00011126673704541079, 0.00011126115681252408, 0.00011125558902223591, 0.00011125005583422173, 0.00011124446320613688, 0.0001112388971696626, 0.00011123331238633987, 0.00011122771212679458, 0.00011122213672874616, 0.00011121658441468054, 0.00011121101050974394, 0.00011120543601230267, 0.00011119990640301681, 0.00011119435970589568, 0.00011118879283991482, 0.00011118323817953061, 0.00011117767458417565, 0.00011117213549481475, 0.00011116659941537767, 0.00011116103003717692, 0.00011115543359336186, 0.0001111498892188591, 0.00011114437555979956, 0.00011113881279395117, 0.00011113327673821428, 0.00011112774243629127, 0.00011112221709303922, 0.00011111667316884003, 0.00011111111708644456, 0.0001111055610040491, 0.0001110999863170063, 0.00011109447460136216, 0.00011108892174219221, 0.00011108339696774465, 0.00011107785562686592, 0.00011107230357350235, 0.00011106674820211251, 0.00011106121330768483, 0.00011105566744007024, 0.00011105019518523857, 0.00011104467199870358, 0.00011103915085038473, 0.00011103364567599239, 0.00011102811931363225, 0.00011102260728988566, 0.00011101709483953569, 0.00011101157020728926, 0.00011100603317984468, 0.00011100052693894388, 0.00011099499749555935, 0.0001109894861591182, 0.00011098396666981249, 0.00011097847045409108, 0.00011097296544560003, 0.0001109674563843769], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433227182599538, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329, 0.00081433224755700329]}
[2017-12-14 16:00:21,142 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:129]: evaluating model ... 
[2017-12-14 16:00:23,419 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:133]: evaluated! 
[2017-12-14 16:00:23,419 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:135]: generating reports ... 
[2017-12-14 16:00:27,964 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:138]: done!
[2017-12-14 16:00:27,964 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_2 finished!
[2018-04-29 11:38:17,486 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_2 was already executed!
[2018-04-29 11:42:07,301 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_2 was already executed!
[2018-04-29 13:12:02,684 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_2
[2018-04-29 13:12:02,684 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:146]: >> Printing header log
[2018-04-29 13:12:02,684 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_2
	layers = 9216,1843
	using GLOBAL obj = 
		{'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'batch': 32, 'store_history': True, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'data_dir': '/home/dhiego/malware_dataset/', 'autoencoder_configs': {'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f721cae1860>}, 'shuffle_batches': True, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'epochs': 200, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'numpy_seed': 666, 'mlp_configs': {'activation': 'sigmoid', 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f721cae18d0>, 'use_last_dim_as_classifier': False}, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:12:02,684 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,677 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,677 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,677 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:57]: =======================================
[2018-04-29 13:12:20,677 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f721cae1860>}
[2018-04-29 13:12:20,723 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,863 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_2
[2018-04-29 13:14:12,863 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:146]: >> Printing header log
[2018-04-29 13:14:12,863 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_2
	layers = 9216,1843
	using GLOBAL obj = 
		{'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'epochs': 200, 'mlp_configs': {'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7ff470df9898>}, 'batch': 32, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'data_dir': '/home/dhiego/malware_dataset/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'numpy_seed': 666, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'shuffle_batches': True, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'store_history': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7ff470df9828>, 'discard_decoder_function': True, 'output_layer_activation': 'relu'}, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:14:12,863 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:148]: >> Loading dataset... 
[2018-04-29 13:14:30,825 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:30,825 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:30,825 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:57]: =======================================
[2018-04-29 13:14:30,825 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7ff470df9828>, 'discard_decoder_function': True, 'output_layer_activation': 'relu'}
[2018-04-29 13:14:30,873 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,587 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_2
[2018-04-29 13:16:33,588 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:146]: >> Printing header log
[2018-04-29 13:16:33,588 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_2
	layers = 9216,1843
	using GLOBAL obj = 
		{'mlp_configs': {'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7f877e0a3898>, 'loss_function': 'categorical_crossentropy', 'classifier_dim': 9, 'use_last_dim_as_classifier': False}, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'data_dir': '/home/dhiego/malware_dataset/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'shuffle_batches': True, 'autoencoder_configs': {'discard_decoder_function': True, 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f877e0a3828>, 'hidden_layer_activation': 'relu'}, 'epochs': 200, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'numpy_seed': 666, 'batch': 32, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'store_history': True, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:16:33,588 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:148]: >> Loading dataset... 
[2018-04-29 13:16:58,381 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:58,381 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:58,381 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:57]: =======================================
[2018-04-29 13:16:58,381 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f877e0a3828>, 'hidden_layer_activation': 'relu'}
[2018-04-29 13:16:58,457 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:21,096 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_2
[2018-04-29 14:30:21,096 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:146]: >> Printing header log
[2018-04-29 14:30:21,096 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_UNDER_F0_2
	layers = 9216,1843
	using GLOBAL obj = 
		{'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'batch': 32, 'data_dir': '/home/dhiego/malware_dataset/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'shuffle_batches': True, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fe6c5f2e8d0>, 'loss_function': 'mse', 'output_layer_activation': 'relu', 'discard_decoder_function': True}, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'store_history': True, 'epochs': 200, 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'numpy_seed': 666, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'mlp_configs': {'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7fe6c5f2e940>, 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False}, 'fullds_data_dir': '/home/dhiego/malware_dataset/'}
	=======================================
	
[2018-04-29 14:30:21,096 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:148]: >> Loading dataset... 
[2018-04-29 14:30:39,113 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:39,113 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:39,113 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:57]: =======================================
[2018-04-29 14:30:39,113 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7fe6c5f2e8d0>, 'loss_function': 'mse', 'output_layer_activation': 'relu', 'discard_decoder_function': True}
[2018-04-29 14:30:39,159 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:73]: training and evaluate autoencoder
[2018-04-29 16:18:01,602 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:85]: trained and evaluated!
[2018-04-29 16:18:01,603 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:88]: Training history: 
{'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011240381936422103, 0.00011239962838893508, 0.00011239540821501791, 0.00011239117578810372, 0.00011238691293014857, 0.00011238265052249698, 0.00011237836697427799, 0.00011237409702996672, 0.00011236978679533641, 0.00011236548658588549, 0.00011236117533214712, 0.0001123568795309311, 0.00011235257586125278, 0.00011234825901427012, 0.00011234392657256397, 0.00011233963939821627, 0.00011233533091739985, 0.00011233096979846662, 0.00011232664017708281, 0.00011232229635928657, 0.0001123179237931627, 0.00011231351823637759, 0.00011230907945192938, 0.00011230466517347521, 0.00011230023034695834, 0.00011229580187209175, 0.00011229135604868779, 0.00011228687123847512, 0.00011228241226146701, 0.00011227795316595797, 0.00011227346051098317, 0.00011226896586519263, 0.00011226447932486625, 0.00011225997079076572, 0.00011225542409936307, 0.00011225088731463883, 0.00011224636098169737, 0.00011224183133072964, 0.00011223728790995285, 0.0001122326829634889, 0.00011222807420129472, 0.00011222346809352157, 0.00011221883883066506, 0.00011221423260439097, 0.00011220959912290105, 0.00011220498720858192, 0.00011220037519946202, 0.00011219574534410083, 0.0001121911186171644, 0.0001121864904919169, 0.00011218185715262812, 0.00011217720651220232, 0.0001121725363428219, 0.00011216791907226035, 0.0001121632889561971, 0.00011215862592057316, 0.00011215393999056794, 0.00011214923839473867, 0.00011214453490289437, 0.00011213982778492138, 0.00011213513059732702, 0.00011213045571160925, 0.00011212577632285581, 0.00011212109060615228, 0.00011211636706394923, 0.00011211163735969738, 0.00011210694602604937, 0.00011210220316819337, 0.00011209747336914077, 0.00011209271406335453, 0.00011208797196390453, 0.00011208320718337494, 0.00011207846375671443, 0.00011207367665060804, 0.0001120689286972117, 0.00011206413933958749, 0.0001120593255470698, 0.00011205453526513827, 0.00011204976828049122, 0.00011204495335036451, 0.00011204012185380663, 0.00011203533173777643, 0.00011203053581520023, 0.00011202574733448297, 0.00011202095852196307, 0.00011201615136549793, 0.00011201130344470999, 0.00011200646817982227, 0.00011200165881923967, 0.0001119968084573324, 0.00011199195901973246, 0.0001119871134452631, 0.0001119822792942842, 0.00011197748232889975, 0.00011197261658557069, 0.00011196777629624318, 0.00011196292256890927, 0.00011195805500066575, 0.00011195318873593257, 0.00011194830730307926, 0.0001119434148496387], 'val_loss': [0.00011179325096374972, 0.00011178917646323988, 0.00011178510400071323, 0.00011178101646783704, 0.00011177692071151981, 0.00011177279536869189, 0.00011176868000125549, 0.00011176451665182833, 0.00011176036172248972, 0.000111756196978653, 0.00011175202456556366, 0.00011174783792234591, 0.00011174363681659817, 0.00011173942795221256, 0.00011173525954358146, 0.00011173105836632554, 0.00011172680756239065, 0.0001117225831628523, 0.00011171833539801519, 0.00011171406392821543, 0.00011170978076682776, 0.0001117054795496239, 0.00011170120480832477, 0.00011169690489614525, 0.00011169260457279367, 0.00011168828782043706, 0.00011168393150617824, 0.00011167959644772679, 0.00011167528566630347, 0.00011167093475091246, 0.00011166660369691926, 0.00011166227550325339, 0.00011165792769846834, 0.00011165353657760147, 0.00011164914652935735, 0.00011164476391796425, 0.0001116403898875531, 0.00011163600784822547, 0.00011163157022916265, 0.00011162712762240408, 0.0001116226996390689, 0.00011161824460776353, 0.00011161382030709978, 0.00011160936829701514, 0.00011160492111373285, 0.00011160046118411695, 0.00011159598702437265, 0.00011159151835288138, 0.0001115870533461845, 0.00011158257984789088, 0.00011157809980659853, 0.0001115735958100649, 0.00011156914258434115, 0.00011156467938322588, 0.00011156019346038548, 0.0001115556971509815, 0.00011155118539581002, 0.00011154667053003257, 0.00011154215820279563, 0.0001115376558509502, 0.00011153317212698643, 0.00011152866877402644, 0.00011152416452721416, 0.00011151961989615557, 0.00011151506840031141, 0.0001115105611322784, 0.00011150601560736753, 0.00011150147170926781, 0.00011149690019113243, 0.0001114923376830281, 0.00011148775579620619, 0.00011148319319871664, 0.00011147859879796272, 0.00011147403784516137, 0.0001114694329863357, 0.00011146480728287469, 0.00011146020578493362, 0.00011145562184225145, 0.00011145097838688404, 0.00011144632005781456, 0.00011144170415097464, 0.00011143707950225933, 0.00011143245959096114, 0.00011142783926849092, 0.00011142320333935975, 0.00011141852032209012, 0.00011141384450926993, 0.00011140918430311066, 0.00011140448609035215, 0.00011139978371224199, 0.00011139508142351706, 0.00011139037820518575, 0.00011138571708729715, 0.00011138099196958481, 0.00011137629014566307, 0.00011137157233966244, 0.00011136683628119812, 0.00011136211264728058, 0.00011135736743576885, 0.0001113526088164728, 0.00011134788714903029], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-04-29 16:18:01,603 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:92]: done!
[2018-04-29 16:18:01,603 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:152]: >> Executing classifier part ... 
[2018-04-29 16:18:01,604 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:97]: =======================================
[2018-04-29 16:18:01,604 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x7fe6c5f2e940>, 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False}
[2018-04-29 16:18:01,686 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:110]: training ... 
[2018-04-29 18:10:12,379 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:122]: trained!
[2018-04-29 18:10:12,380 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:125]: Training history: 
{'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00011240381936422103, 0.00011239962838893508, 0.00011239540821501791, 0.00011239117578810372, 0.00011238691293014857, 0.00011238265052249698, 0.00011237836697427799, 0.00011237409702996672, 0.00011236978679533641, 0.00011236548658588549, 0.00011236117533214712, 0.0001123568795309311, 0.00011235257586125278, 0.00011234825901427012, 0.00011234392657256397, 0.00011233963939821627, 0.00011233533091739985, 0.00011233096979846662, 0.00011232664017708281, 0.00011232229635928657, 0.0001123179237931627, 0.00011231351823637759, 0.00011230907945192938, 0.00011230466517347521, 0.00011230023034695834, 0.00011229580187209175, 0.00011229135604868779, 0.00011228687123847512, 0.00011228241226146701, 0.00011227795316595797, 0.00011227346051098317, 0.00011226896586519263, 0.00011226447932486625, 0.00011225997079076572, 0.00011225542409936307, 0.00011225088731463883, 0.00011224636098169737, 0.00011224183133072964, 0.00011223728790995285, 0.0001122326829634889, 0.00011222807420129472, 0.00011222346809352157, 0.00011221883883066506, 0.00011221423260439097, 0.00011220959912290105, 0.00011220498720858192, 0.00011220037519946202, 0.00011219574534410083, 0.0001121911186171644, 0.0001121864904919169, 0.00011218185715262812, 0.00011217720651220232, 0.0001121725363428219, 0.00011216791907226035, 0.0001121632889561971, 0.00011215862592057316, 0.00011215393999056794, 0.00011214923839473867, 0.00011214453490289437, 0.00011213982778492138, 0.00011213513059732702, 0.00011213045571160925, 0.00011212577632285581, 0.00011212109060615228, 0.00011211636706394923, 0.00011211163735969738, 0.00011210694602604937, 0.00011210220316819337, 0.00011209747336914077, 0.00011209271406335453, 0.00011208797196390453, 0.00011208320718337494, 0.00011207846375671443, 0.00011207367665060804, 0.0001120689286972117, 0.00011206413933958749, 0.0001120593255470698, 0.00011205453526513827, 0.00011204976828049122, 0.00011204495335036451, 0.00011204012185380663, 0.00011203533173777643, 0.00011203053581520023, 0.00011202574733448297, 0.00011202095852196307, 0.00011201615136549793, 0.00011201130344470999, 0.00011200646817982227, 0.00011200165881923967, 0.0001119968084573324, 0.00011199195901973246, 0.0001119871134452631, 0.0001119822792942842, 0.00011197748232889975, 0.00011197261658557069, 0.00011196777629624318, 0.00011196292256890927, 0.00011195805500066575, 0.00011195318873593257, 0.00011194830730307926, 0.0001119434148496387], 'val_loss': [0.00011179325096374972, 0.00011178917646323988, 0.00011178510400071323, 0.00011178101646783704, 0.00011177692071151981, 0.00011177279536869189, 0.00011176868000125549, 0.00011176451665182833, 0.00011176036172248972, 0.000111756196978653, 0.00011175202456556366, 0.00011174783792234591, 0.00011174363681659817, 0.00011173942795221256, 0.00011173525954358146, 0.00011173105836632554, 0.00011172680756239065, 0.0001117225831628523, 0.00011171833539801519, 0.00011171406392821543, 0.00011170978076682776, 0.0001117054795496239, 0.00011170120480832477, 0.00011169690489614525, 0.00011169260457279367, 0.00011168828782043706, 0.00011168393150617824, 0.00011167959644772679, 0.00011167528566630347, 0.00011167093475091246, 0.00011166660369691926, 0.00011166227550325339, 0.00011165792769846834, 0.00011165353657760147, 0.00011164914652935735, 0.00011164476391796425, 0.0001116403898875531, 0.00011163600784822547, 0.00011163157022916265, 0.00011162712762240408, 0.0001116226996390689, 0.00011161824460776353, 0.00011161382030709978, 0.00011160936829701514, 0.00011160492111373285, 0.00011160046118411695, 0.00011159598702437265, 0.00011159151835288138, 0.0001115870533461845, 0.00011158257984789088, 0.00011157809980659853, 0.0001115735958100649, 0.00011156914258434115, 0.00011156467938322588, 0.00011156019346038548, 0.0001115556971509815, 0.00011155118539581002, 0.00011154667053003257, 0.00011154215820279563, 0.0001115376558509502, 0.00011153317212698643, 0.00011152866877402644, 0.00011152416452721416, 0.00011151961989615557, 0.00011151506840031141, 0.0001115105611322784, 0.00011150601560736753, 0.00011150147170926781, 0.00011149690019113243, 0.0001114923376830281, 0.00011148775579620619, 0.00011148319319871664, 0.00011147859879796272, 0.00011147403784516137, 0.0001114694329863357, 0.00011146480728287469, 0.00011146020578493362, 0.00011145562184225145, 0.00011145097838688404, 0.00011144632005781456, 0.00011144170415097464, 0.00011143707950225933, 0.00011143245959096114, 0.00011142783926849092, 0.00011142320333935975, 0.00011141852032209012, 0.00011141384450926993, 0.00011140918430311066, 0.00011140448609035215, 0.00011139978371224199, 0.00011139508142351706, 0.00011139037820518575, 0.00011138571708729715, 0.00011138099196958481, 0.00011137629014566307, 0.00011137157233966244, 0.00011136683628119812, 0.00011136211264728058, 0.00011135736743576885, 0.0001113526088164728, 0.00011134788714903029], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-04-29 18:10:12,380 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:129]: evaluating model ... 
[2018-04-29 18:10:16,530 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:133]: evaluated! 
[2018-04-29 18:10:16,531 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:135]: generating reports ... 
[2018-04-29 18:10:21,150 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:138]: done!
[2018-04-29 18:10:21,187 AE_BIGRAMA_1L_MINIDS_UNDER_F0_2.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_UNDER_F0_2 finished!
