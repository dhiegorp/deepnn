[2017-12-14 09:31:57,743 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_6
[2017-12-14 09:31:57,743 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:146]: >> Printing header log
[2017-12-14 09:31:57,743 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_6
	layers = 9216,14746
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fb5470f9e48>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fb54715b390>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,743 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:148]: >> Loading dataset... 
[2017-12-14 09:32:21,448 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:21,448 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:21,448 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:57]: =======================================
[2017-12-14 09:32:21,448 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fb5470f9e48>, 'discard_decoder_function': True}
[2017-12-14 09:32:21,490 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:54,983 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_6
[2017-12-14 10:18:54,984 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:146]: >> Printing header log
[2017-12-14 10:18:54,984 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_6
	layers = 9216,14746
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7efc9cc1eeb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7efc9cc01400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:54,984 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:148]: >> Loading dataset... 
[2017-12-14 10:19:17,696 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:17,696 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:17,696 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:57]: =======================================
[2017-12-14 10:19:17,696 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7efc9cc1eeb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:17,740 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:73]: training and evaluate autoencoder
[2017-12-15 02:59:48,284 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:85]: trained and evaluated!
[2017-12-15 02:59:48,286 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:88]: Training history: 
{'val_loss': [0.00011916941900137487, 0.00011916625546148454, 0.00011916309773163408, 0.00011915992503869633, 0.00011915676746973929, 0.00011915360973988882, 0.00011915043361455829, 0.00011914725471828568, 0.00011914407998736472, 0.00011914090999386088, 0.00011913776059471372, 0.00011913460588608399, 0.00011913144962215127, 0.00011912828502751525, 0.00011912511372898708, 0.00011912195043937538, 0.00011911879042126308, 0.00011911563612380539, 0.00011911247152916936, 0.00011910930382392738, 0.00011910614838233877, 0.00011910298452066161, 0.0001190998173874851, 0.00011909665720847937, 0.00011909348680380348, 0.00011909033365047671, 0.00011908716619551337, 0.00011908400903772837, 0.00011908083586211039, 0.00011907767559371943, 0.00011907451385941073, 0.00011907134910388129, 0.00011906818597516301, 0.00011906501852019967, 0.0001190618580909153, 0.00011905869619571318, 0.00011905552464690635, 0.0001190523714935796, 0.0001190492206285147, 0.00011904604475346282, 0.00011904289569397953, 0.00011903972823901619, 0.00011903656282203606, 0.00011903340648659517, 0.00011903024548524534, 0.00011902708947159125, 0.00011902394270036982, 0.00011902077671132424, 0.00011901762191330927, 0.00011901446867059728, 0.00011901131240666457, 0.00011900814657851239, 0.0001190049906363665, 0.00011900184197017822, 0.00011899867442582965, 0.00011899553720095064, 0.00011899238878504101, 0.00011898924265739322, 0.00011898607641806899, 0.00011898291370052277, 0.00011897977386559509, 0.00011897662863179959, 0.00011897348021588995, 0.00011897031553186869, 0.00011896716000089486, 0.00011896400945761679, 0.00011896085916461734, 0.00011895771001574882, 0.00011895455325125884, 0.00011895138627897573, 0.00011894822225640516, 0.00011894506011092441, 0.00011894189812633705, 0.00011893873843001156, 0.00011893558838729077, 0.00011893242770772775, 0.00011892926311309172, 0.00011892609941230798, 0.00011892294273720321, 0.00011891976530684836, 0.00011891661901830716, 0.00011891344763039376, 0.00011891030060889369, 0.00011890716191809694, 0.00011890400377707441, 0.00011890084915782991, 0.00011889770303018212, 0.00011889455461427249, 0.00011889138773137461, 0.00011888824070987454, 0.00011888507439904213, 0.00011888191298652024, 0.00011887874880305626, 0.0001188755929502956, 0.00011887244078020636, 0.00011886931149273566, 0.00011886615385227042, 0.000118862993011814, 0.00011885983258252963, 0.00011885667714094102, 0.00011885351394071457, 0.0001188503625035842, 0.00011884722545747566, 0.00011884407049856727, 0.00011884090231064506, 0.00011883773984337747, 0.00011883458334704316, 0.00011883143076578188, 0.00011882826944264522, 0.00011882509226256899, 0.00011882193093943233, 0.00011881877835817105, 0.00011881561408532184, 0.00011881244761359601, 0.00011880928098097677, 0.00011880610739418674, 0.00011880295506320409, 0.00011879981122381815, 0.00011879666257550693, 0.00011879351367691705, 0.00011879035006551854, 0.00011878718874238188, 0.00011878402471981133, 0.00011878087549943463, 0.0001187777163751746, 0.00011877456674362585, 0.00011877140141603095, 0.00011876824752974532, 0.00011876508465130569, 0.00011876192815497137, 0.00011875877639605419, 0.00011875561662822051, 0.0001187524683016961, 0.00011874930068583934, 0.00011874613920180928, 0.0001187429804887213, 0.00011873983257336895, 0.00011873667950942741, 0.00011873351859746281, 0.00011873035425310543, 0.0001187272013500573, 0.00011872404517550983, 0.00011872087266134254, 0.0001187177150208773, 0.00011871456612228744, 0.00011871139360812015, 0.00011870822926376276, 0.00011870505960992279, 0.00011870188577285411, 0.00011869871783521054, 0.00011869555806737686, 0.00011869237500575259, 0.00011868920068600368, 0.00011868603094277849, 0.00011868285245767792, 0.00011867967177370073, 0.00011867650628521242, 0.0001186733324660208, 0.00011867018201212794, 0.00011866700753148563, 0.00011866382185981268, 0.00011866063078927192, 0.00011865745516449867, 0.00011865426875986685, 0.00011865109885574824, 0.00011864791032374797, 0.00011864472424090298, 0.00011864154486195013, 0.00011863835321934391, 0.0001186351813666273, 0.00011863200459772313, 0.0001186288172098538, 0.00011862562508456734, 0.00011862243090342061, 0.00011861924980827138, 0.00011861604844055227, 0.00011861285173874211, 0.00011860967897429618, 0.00011860649199759889, 0.00011860330747005688, 0.00011860009834369992, 0.00011859690940052761, 0.00011859371285961085, 0.00011859051876784936, 0.00011858732353195694, 0.00011858412829606453, 0.00011858092864454182, 0.00011857774068460702, 0.00011857458116705198, 0.00011857139949983728, 0.00011856821881586008, 0.0001185650293006223, 0.00011856185049373492, 0.00011855867136506071, 0.00011855549672352498, 0.00011855230320382896, 0.00011854912734665412, 0.00011854595613751116, 0.00011854277504236193, 0.00011853961078738977], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.0001200340157676206, 0.00012003076715919671, 0.00012002751473504262, 0.0001200242650364101, 0.00012002100922312915, 0.0001200177632217259, 0.00012001451513470615, 0.00012001125214026834, 0.00012000798675211158, 0.00012000472501378373, 0.00012000146664088252, 0.00011999822917154684, 0.00011999498530316049, 0.00011999173911215574, 0.00011998848427058249, 0.00011998522731969254, 0.00011998197046360335, 0.00011997871993546425, 0.00011997547774979123, 0.0001199722190213872, 0.0001199689658151269, 0.00011996571651939757, 0.00011996246456924722, 0.00011995920631484696, 0.00011995595837002833, 0.00011995269587329446, 0.00011994945186270699, 0.00011994619429561216, 0.00011994295014282355, 0.00011993968629517899, 0.00011993643406062639, 0.000119933177180837, 0.00011992992710300148, 0.00011992667107641884, 0.00011992341369892552, 0.00011992015561042657, 0.00011991690207236365, 0.00011991364113984217, 0.00011991040279359954, 0.00011990714925553662, 0.00011990387967244664, 0.00011990063919318712, 0.0001198973766253527, 0.00011989411486332465, 0.0001198908613963623, 0.00011988760558308136, 0.00011988435365663119, 0.00011988111222936418, 0.00011987785205524869, 0.00011987460297282106, 0.00011987135500430223, 0.00011986810016272899, 0.00011986483449016997, 0.00011986158706675548, 0.00011985833928783816, 0.00011985507605639848, 0.00011985183439212957, 0.00011984858192057509, 0.00011984534186791896, 0.00011984207150272279, 0.00011983881168411013, 0.000119835571631454, 0.00011983232783416821, 0.00011982907804073493, 0.00011982581767701796, 0.0001198225662956721, 0.00011981931344491464, 0.00011981606557119656, 0.00011981282066000195, 0.00011980955885057353, 0.00011980629372311884, 0.00011980302968587277, 0.00011979977498650066, 0.00011979651194466247, 0.00011979325648688435, 0.00011979001411160982, 0.00011978675471960054, 0.00011978349129855936, 0.00011978023866110357, 0.00011977698512304065, 0.00011977371350173454, 0.00011977047183746562, 0.00011976720848752501, 0.00011976396419253527, 0.00011976072679430016, 0.0001197574746545483, 0.00011975422078468275, 0.00011975098319684613, 0.000119747735441629, 0.00011974448074225689, 0.00011974123798777935, 0.0001197379783587682, 0.00011973471820835291, 0.00011973146241877216, 0.00011972821513755879, 0.00011972496446721857, 0.00011972173538774931, 0.00011971848035657457, 0.00011971522975733492, 0.00011971196967802019, 0.00011970872161470062, 0.00011970546236489247, 0.00011970221332986521, 0.00011969897863345148, 0.00011969572831861408, 0.00011969246290675713, 0.00011968920906059177, 0.00011968595165939826, 0.00011968269850053833, 0.00011967944685849043, 0.00011967616947803871, 0.00011967291240864782, 0.00011966966344472111, 0.00011966638995110018, 0.0001196631311278954, 0.00011965986126040318, 0.00011965659172471357, 0.00011965333226160374, 0.00011965009130834047, 0.00011964684288951808, 0.00011964359539500301, 0.00011964033187916107, 0.00011963707175244598, 0.00011963381063032301, 0.00011963056185599781, 0.00011962729627823954, 0.00011962405425846783, 0.00011962078320596622, 0.00011961753509524627, 0.00011961426906718445, 0.00011961100808726259, 0.0001196077546914008, 0.00011960449546529283, 0.00011960124794707758, 0.00011959798101840863, 0.0001195947190193787, 0.00011959146026727449, 0.0001195882174416964, 0.00011958495745718243, 0.00011958169232972773, 0.00011957842983299386, 0.0001195751687108709, 0.00011957191507800722, 0.00011956864070747933, 0.00011956537953795598, 0.0001195621284173122, 0.00011955885833651829, 0.00011955559273505984, 0.00011955232125595485, 0.00011954904657732452, 0.00011954576850956737, 0.00011954251053956935, 0.00011953921946040916, 0.00011953594639339159, 0.00011953267512758829, 0.00011952939798413846, 0.00011952611740416141, 0.00011952285279811084, 0.00011951958451853118, 0.00011951633569680561, 0.00011951306836523346, 0.00011950979427910783, 0.00011950651803626512, 0.00011950325407011962, 0.00011949998057649868, 0.00011949672255910029, 0.00011949344721686471, 0.00011949017168502763, 0.00011948690461415755, 0.00011948362583539476, 0.0001194803654479776, 0.00011947710221653792, 0.00011947382514418864, 0.00011947054847474256, 0.00011946727062028708, 0.00011946399788507215, 0.00011946070607120614, 0.00011945741906847823, 0.00011945415327741828, 0.00011945087599176731, 0.00011944759982002516, 0.00011944429359644505, 0.0001194410165003956, 0.00011943773317119677, 0.00011943444915469251, 0.00011943116784000963, 0.00011942787571804118, 0.00011942459065872866, 0.00011942131159556362, 0.00011941805812860126, 0.00011941479110513155, 0.00011941151751670986, 0.00011940823954375345, 0.000119404965102125, 0.00011940169888446169, 0.00011939842477463586, 0.0001193951395968224, 0.00011939187757409229, 0.00011938861064542334, 0.00011938533919001853], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 02:59:48,287 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:92]: done!
[2017-12-15 02:59:48,287 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:152]: >> Executing classifier part ... 
[2017-12-15 02:59:48,287 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:97]: =======================================
[2017-12-15 02:59:48,287 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7efc9cc01400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-15 02:59:48,560 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:110]: training ... 
[2017-12-15 08:44:50,070 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:122]: trained!
[2017-12-15 08:44:50,074 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:125]: Training history: 
{'val_loss': [0.00011916941900137487, 0.00011916625546148454, 0.00011916309773163408, 0.00011915992503869633, 0.00011915676746973929, 0.00011915360973988882, 0.00011915043361455829, 0.00011914725471828568, 0.00011914407998736472, 0.00011914090999386088, 0.00011913776059471372, 0.00011913460588608399, 0.00011913144962215127, 0.00011912828502751525, 0.00011912511372898708, 0.00011912195043937538, 0.00011911879042126308, 0.00011911563612380539, 0.00011911247152916936, 0.00011910930382392738, 0.00011910614838233877, 0.00011910298452066161, 0.0001190998173874851, 0.00011909665720847937, 0.00011909348680380348, 0.00011909033365047671, 0.00011908716619551337, 0.00011908400903772837, 0.00011908083586211039, 0.00011907767559371943, 0.00011907451385941073, 0.00011907134910388129, 0.00011906818597516301, 0.00011906501852019967, 0.0001190618580909153, 0.00011905869619571318, 0.00011905552464690635, 0.0001190523714935796, 0.0001190492206285147, 0.00011904604475346282, 0.00011904289569397953, 0.00011903972823901619, 0.00011903656282203606, 0.00011903340648659517, 0.00011903024548524534, 0.00011902708947159125, 0.00011902394270036982, 0.00011902077671132424, 0.00011901762191330927, 0.00011901446867059728, 0.00011901131240666457, 0.00011900814657851239, 0.0001190049906363665, 0.00011900184197017822, 0.00011899867442582965, 0.00011899553720095064, 0.00011899238878504101, 0.00011898924265739322, 0.00011898607641806899, 0.00011898291370052277, 0.00011897977386559509, 0.00011897662863179959, 0.00011897348021588995, 0.00011897031553186869, 0.00011896716000089486, 0.00011896400945761679, 0.00011896085916461734, 0.00011895771001574882, 0.00011895455325125884, 0.00011895138627897573, 0.00011894822225640516, 0.00011894506011092441, 0.00011894189812633705, 0.00011893873843001156, 0.00011893558838729077, 0.00011893242770772775, 0.00011892926311309172, 0.00011892609941230798, 0.00011892294273720321, 0.00011891976530684836, 0.00011891661901830716, 0.00011891344763039376, 0.00011891030060889369, 0.00011890716191809694, 0.00011890400377707441, 0.00011890084915782991, 0.00011889770303018212, 0.00011889455461427249, 0.00011889138773137461, 0.00011888824070987454, 0.00011888507439904213, 0.00011888191298652024, 0.00011887874880305626, 0.0001188755929502956, 0.00011887244078020636, 0.00011886931149273566, 0.00011886615385227042, 0.000118862993011814, 0.00011885983258252963, 0.00011885667714094102, 0.00011885351394071457, 0.0001188503625035842, 0.00011884722545747566, 0.00011884407049856727, 0.00011884090231064506, 0.00011883773984337747, 0.00011883458334704316, 0.00011883143076578188, 0.00011882826944264522, 0.00011882509226256899, 0.00011882193093943233, 0.00011881877835817105, 0.00011881561408532184, 0.00011881244761359601, 0.00011880928098097677, 0.00011880610739418674, 0.00011880295506320409, 0.00011879981122381815, 0.00011879666257550693, 0.00011879351367691705, 0.00011879035006551854, 0.00011878718874238188, 0.00011878402471981133, 0.00011878087549943463, 0.0001187777163751746, 0.00011877456674362585, 0.00011877140141603095, 0.00011876824752974532, 0.00011876508465130569, 0.00011876192815497137, 0.00011875877639605419, 0.00011875561662822051, 0.0001187524683016961, 0.00011874930068583934, 0.00011874613920180928, 0.0001187429804887213, 0.00011873983257336895, 0.00011873667950942741, 0.00011873351859746281, 0.00011873035425310543, 0.0001187272013500573, 0.00011872404517550983, 0.00011872087266134254, 0.0001187177150208773, 0.00011871456612228744, 0.00011871139360812015, 0.00011870822926376276, 0.00011870505960992279, 0.00011870188577285411, 0.00011869871783521054, 0.00011869555806737686, 0.00011869237500575259, 0.00011868920068600368, 0.00011868603094277849, 0.00011868285245767792, 0.00011867967177370073, 0.00011867650628521242, 0.0001186733324660208, 0.00011867018201212794, 0.00011866700753148563, 0.00011866382185981268, 0.00011866063078927192, 0.00011865745516449867, 0.00011865426875986685, 0.00011865109885574824, 0.00011864791032374797, 0.00011864472424090298, 0.00011864154486195013, 0.00011863835321934391, 0.0001186351813666273, 0.00011863200459772313, 0.0001186288172098538, 0.00011862562508456734, 0.00011862243090342061, 0.00011861924980827138, 0.00011861604844055227, 0.00011861285173874211, 0.00011860967897429618, 0.00011860649199759889, 0.00011860330747005688, 0.00011860009834369992, 0.00011859690940052761, 0.00011859371285961085, 0.00011859051876784936, 0.00011858732353195694, 0.00011858412829606453, 0.00011858092864454182, 0.00011857774068460702, 0.00011857458116705198, 0.00011857139949983728, 0.00011856821881586008, 0.0001185650293006223, 0.00011856185049373492, 0.00011855867136506071, 0.00011855549672352498, 0.00011855230320382896, 0.00011854912734665412, 0.00011854595613751116, 0.00011854277504236193, 0.00011853961078738977], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.0001200340157676206, 0.00012003076715919671, 0.00012002751473504262, 0.0001200242650364101, 0.00012002100922312915, 0.0001200177632217259, 0.00012001451513470615, 0.00012001125214026834, 0.00012000798675211158, 0.00012000472501378373, 0.00012000146664088252, 0.00011999822917154684, 0.00011999498530316049, 0.00011999173911215574, 0.00011998848427058249, 0.00011998522731969254, 0.00011998197046360335, 0.00011997871993546425, 0.00011997547774979123, 0.0001199722190213872, 0.0001199689658151269, 0.00011996571651939757, 0.00011996246456924722, 0.00011995920631484696, 0.00011995595837002833, 0.00011995269587329446, 0.00011994945186270699, 0.00011994619429561216, 0.00011994295014282355, 0.00011993968629517899, 0.00011993643406062639, 0.000119933177180837, 0.00011992992710300148, 0.00011992667107641884, 0.00011992341369892552, 0.00011992015561042657, 0.00011991690207236365, 0.00011991364113984217, 0.00011991040279359954, 0.00011990714925553662, 0.00011990387967244664, 0.00011990063919318712, 0.0001198973766253527, 0.00011989411486332465, 0.0001198908613963623, 0.00011988760558308136, 0.00011988435365663119, 0.00011988111222936418, 0.00011987785205524869, 0.00011987460297282106, 0.00011987135500430223, 0.00011986810016272899, 0.00011986483449016997, 0.00011986158706675548, 0.00011985833928783816, 0.00011985507605639848, 0.00011985183439212957, 0.00011984858192057509, 0.00011984534186791896, 0.00011984207150272279, 0.00011983881168411013, 0.000119835571631454, 0.00011983232783416821, 0.00011982907804073493, 0.00011982581767701796, 0.0001198225662956721, 0.00011981931344491464, 0.00011981606557119656, 0.00011981282066000195, 0.00011980955885057353, 0.00011980629372311884, 0.00011980302968587277, 0.00011979977498650066, 0.00011979651194466247, 0.00011979325648688435, 0.00011979001411160982, 0.00011978675471960054, 0.00011978349129855936, 0.00011978023866110357, 0.00011977698512304065, 0.00011977371350173454, 0.00011977047183746562, 0.00011976720848752501, 0.00011976396419253527, 0.00011976072679430016, 0.0001197574746545483, 0.00011975422078468275, 0.00011975098319684613, 0.000119747735441629, 0.00011974448074225689, 0.00011974123798777935, 0.0001197379783587682, 0.00011973471820835291, 0.00011973146241877216, 0.00011972821513755879, 0.00011972496446721857, 0.00011972173538774931, 0.00011971848035657457, 0.00011971522975733492, 0.00011971196967802019, 0.00011970872161470062, 0.00011970546236489247, 0.00011970221332986521, 0.00011969897863345148, 0.00011969572831861408, 0.00011969246290675713, 0.00011968920906059177, 0.00011968595165939826, 0.00011968269850053833, 0.00011967944685849043, 0.00011967616947803871, 0.00011967291240864782, 0.00011966966344472111, 0.00011966638995110018, 0.0001196631311278954, 0.00011965986126040318, 0.00011965659172471357, 0.00011965333226160374, 0.00011965009130834047, 0.00011964684288951808, 0.00011964359539500301, 0.00011964033187916107, 0.00011963707175244598, 0.00011963381063032301, 0.00011963056185599781, 0.00011962729627823954, 0.00011962405425846783, 0.00011962078320596622, 0.00011961753509524627, 0.00011961426906718445, 0.00011961100808726259, 0.0001196077546914008, 0.00011960449546529283, 0.00011960124794707758, 0.00011959798101840863, 0.0001195947190193787, 0.00011959146026727449, 0.0001195882174416964, 0.00011958495745718243, 0.00011958169232972773, 0.00011957842983299386, 0.0001195751687108709, 0.00011957191507800722, 0.00011956864070747933, 0.00011956537953795598, 0.0001195621284173122, 0.00011955885833651829, 0.00011955559273505984, 0.00011955232125595485, 0.00011954904657732452, 0.00011954576850956737, 0.00011954251053956935, 0.00011953921946040916, 0.00011953594639339159, 0.00011953267512758829, 0.00011952939798413846, 0.00011952611740416141, 0.00011952285279811084, 0.00011951958451853118, 0.00011951633569680561, 0.00011951306836523346, 0.00011950979427910783, 0.00011950651803626512, 0.00011950325407011962, 0.00011949998057649868, 0.00011949672255910029, 0.00011949344721686471, 0.00011949017168502763, 0.00011948690461415755, 0.00011948362583539476, 0.0001194803654479776, 0.00011947710221653792, 0.00011947382514418864, 0.00011947054847474256, 0.00011946727062028708, 0.00011946399788507215, 0.00011946070607120614, 0.00011945741906847823, 0.00011945415327741828, 0.00011945087599176731, 0.00011944759982002516, 0.00011944429359644505, 0.0001194410165003956, 0.00011943773317119677, 0.00011943444915469251, 0.00011943116784000963, 0.00011942787571804118, 0.00011942459065872866, 0.00011942131159556362, 0.00011941805812860126, 0.00011941479110513155, 0.00011941151751670986, 0.00011940823954375345, 0.000119404965102125, 0.00011940169888446169, 0.00011939842477463586, 0.0001193951395968224, 0.00011939187757409229, 0.00011938861064542334, 0.00011938533919001853], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 08:44:50,075 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:129]: evaluating model ... 
[2017-12-15 08:44:53,878 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:133]: evaluated! 
[2017-12-15 08:44:53,879 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:135]: generating reports ... 
[2017-12-15 08:44:55,529 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:138]: done!
[2017-12-15 08:44:55,530 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_6 finished!
[2018-04-29 11:38:17,507 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:143]: The experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_6 was already executed!
[2018-04-29 13:12:02,718 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_6
[2018-04-29 13:12:02,718 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:146]: >> Printing header log
[2018-04-29 13:12:02,718 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_6
	layers = 9216,14746
	using GLOBAL obj = 
		{'autoencoder_configs': {'optimizer': <keras.optimizers.SGD object at 0x7feaee318828>, 'discard_decoder_function': True, 'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu'}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'shuffle_batches': True, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'numpy_seed': 666, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'store_history': True, 'mlp_configs': {'classifier_dim': 9, 'optimizer': <keras.optimizers.SGD object at 0x7feaee318898>, 'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'use_last_dim_as_classifier': False}, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'batch': 32, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'epochs': 200, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/'}
	=======================================
	
[2018-04-29 13:12:02,719 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:148]: >> Loading dataset... 
[2018-04-29 13:12:20,717 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:12:20,718 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:12:20,718 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:57]: =======================================
[2018-04-29 13:12:20,718 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:62]: setting configurations for autoencoder: 
	 {'optimizer': <keras.optimizers.SGD object at 0x7feaee318828>, 'discard_decoder_function': True, 'loss_function': 'mse', 'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu'}
[2018-04-29 13:12:20,763 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:73]: training and evaluate autoencoder
[2018-04-29 13:14:12,977 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_6
[2018-04-29 13:14:12,977 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:146]: >> Printing header log
[2018-04-29 13:14:12,977 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_6
	layers = 9216,14746
	using GLOBAL obj = 
		{'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'shuffle_batches': True, 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'numpy_seed': 666, 'autoencoder_configs': {'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f1ec3aeb828>}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'store_history': True, 'data_dir': '/home/dhiego/malware_dataset/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'epochs': 200, 'batch': 32, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'mlp_configs': {'activation': 'sigmoid', 'classifier_dim': 9, 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f1ec3aeb898>, 'use_last_dim_as_classifier': False}, 'fullds_data_dir': '/home/dhiego/malware_dataset/'}
	=======================================
	
[2018-04-29 13:14:12,977 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:148]: >> Loading dataset... 
[2018-04-29 13:14:31,102 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:14:31,103 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:14:31,103 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:57]: =======================================
[2018-04-29 13:14:31,103 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:62]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f1ec3aeb828>}
[2018-04-29 13:14:31,151 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:73]: training and evaluate autoencoder
[2018-04-29 13:16:33,698 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_6
[2018-04-29 13:16:33,698 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:146]: >> Printing header log
[2018-04-29 13:16:33,698 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_6
	layers = 9216,14746
	using GLOBAL obj = 
		{'mlp_configs': {'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f53d30ee898>, 'use_last_dim_as_classifier': False, 'activation': 'sigmoid', 'classifier_dim': 9}, 'shuffle_batches': True, 'batch': 32, 'store_history': True, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'epochs': 200, 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'numpy_seed': 666, 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'autoencoder_configs': {'loss_function': 'mse', 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f53d30ee828>, 'discard_decoder_function': True, 'hidden_layer_activation': 'relu'}, 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9]}
	=======================================
	
[2018-04-29 13:16:33,698 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:148]: >> Loading dataset... 
[2018-04-29 13:16:58,349 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 13:16:58,349 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:150]: >> Executing autoencoder part ... 
[2018-04-29 13:16:58,349 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:57]: =======================================
[2018-04-29 13:16:58,350 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:62]: setting configurations for autoencoder: 
	 {'loss_function': 'mse', 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x7f53d30ee828>, 'discard_decoder_function': True, 'hidden_layer_activation': 'relu'}
[2018-04-29 13:16:58,430 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:73]: training and evaluate autoencoder
[2018-04-29 14:30:21,121 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_6
[2018-04-29 14:30:21,121 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:146]: >> Printing header log
[2018-04-29 14:30:21,121 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_6
	layers = 9216,14746
	using GLOBAL obj = 
		{'epochs': 200, 'fullds_data_dir': '/home/dhiego/malware_dataset/', 'reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/', 'tensorflow_dir': '/home/dhiego/deepnn/tensorflow/1layer/bigram/', 'data_dir': '/home/dhiego/malware_dataset/', 'log_dir': '/home/dhiego/deepnn/logs/1layer/bigram/', 'fullds_reports_dir': '/home/dhiego/deepnn/reports/1layer/bigram/fullds/', 'mlp_configs': {'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f12f09bb908>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'activation': 'sigmoid'}, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f12f09bb898>, 'discard_decoder_function': True, 'output_layer_activation': 'relu'}, 'executed_path': '/home/dhiego/deepnn/executed/1layer/bigram/', 'store_history': True, 'batch': 32, 'numpy_seed': 666, 'checkpoints_dir': '/home/dhiego/deepnn/checkpoints/1layer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'shuffle_batches': True}
	=======================================
	
[2018-04-29 14:30:21,121 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:148]: >> Loading dataset... 
[2018-04-29 14:30:39,543 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiego/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2018-04-29 14:30:39,543 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:150]: >> Executing autoencoder part ... 
[2018-04-29 14:30:39,543 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:57]: =======================================
[2018-04-29 14:30:39,544 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7f12f09bb898>, 'discard_decoder_function': True, 'output_layer_activation': 'relu'}
[2018-04-29 14:30:39,646 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:73]: training and evaluate autoencoder
[2018-04-29 22:34:56,759 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:85]: trained and evaluated!
[2018-04-29 22:34:56,764 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:88]: Training history: 
{'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012020240401309166, 0.00012019851455159322, 0.0001201946221749717, 0.00012019072970354942, 0.00012018685235284688, 0.00012018295675299983, 0.00012017905911493664, 0.00012017516875283107, 0.00012017125675245416, 0.00012016739333746197, 0.00012016349133856424, 0.00012015957673116669, 0.000120155687364469, 0.00012015178780669062, 0.00012014787900583904, 0.00012014400366965244, 0.00012014008128859333, 0.00012013618037990425, 0.0001201322801585206, 0.000120128367186436, 0.00012012444741239753, 0.00012012052725915606, 0.00012011660630010821, 0.0001201126948448356, 0.0001201087780333206, 0.00012010485368514592, 0.00012010092800976072, 0.00012009697650117096, 0.00012009306258107885, 0.00012008912332548612, 0.00012008517705463784, 0.00012008122071120978, 0.00012007727723698365, 0.00012007334776956843, 0.00012006939576327474, 0.00012006542261641364, 0.000120061444350312, 0.00012005748603976839, 0.0001200535126796056, 0.00012004956358843498, 0.0001200455892565645, 0.0001200416276279946, 0.0001200376596951748, 0.00012003368515000263, 0.00012002973065518922, 0.00012002575321859416, 0.00012002179566645654, 0.0001200178320233707, 0.00012001384190717525, 0.00012000989035118511, 0.00012000593310714994, 0.00012000195213922691, 0.00011999797946636957, 0.00011999400975603568, 0.00011999005277270257, 0.00011998607799052852, 0.0001199821108635151, 0.00011997811140944568, 0.00011997414525413995, 0.00011997017262868299, 0.00011996618694442261, 0.0001199621832717198, 0.00011995817777410253, 0.00011995419190024065, 0.0001199501877061337, 0.00011994618604794684, 0.00011994217256336633, 0.00011993815294043723, 0.0001199341208275092, 0.00011993011838721615, 0.00011992610177421087, 0.00011992204757270795, 0.00011991801787719907, 0.00011991398190114046, 0.00011990992170349007, 0.00011990586079483402, 0.0001199017750957817, 0.00011989769847390124, 0.00011989363588253188, 0.00011988956208097376, 0.00011988546550353528, 0.00011988134785662996, 0.00011987725599552884, 0.00011987316332862133, 0.00011986906452336522, 0.00011986496294518715, 0.00011986085096262668, 0.00011985674682482834, 0.00011985262562289488, 0.00011984852551412845, 0.0001198443956379263, 0.00011984027611870616, 0.00011983614804371827, 0.00011983202752909025, 0.00011982789435856203, 0.00011982374985934409, 0.00011981960673473702, 0.00011981547003288083, 0.00011981133700455372, 0.00011980717394808884, 0.00011980299003545881], 'val_loss': [0.00011943549499703979, 0.00011943171838174302, 0.00011942794732620748, 0.00011942418256339203, 0.00011942040496485774, 0.00011941662475627479, 0.00011941284658567504, 0.00011940904978719364, 0.00011940530104221116, 0.00011940151682916996, 0.00011939771920834446, 0.00011939394561426842, 0.00011939015625263805, 0.00011938636206420532, 0.00011938259631815236, 0.00011937878798897646, 0.00011937499347875692, 0.0001193712065662818, 0.000119367404529826, 0.0001193635881917336, 0.00011935978002345111, 0.00011935596899484131, 0.00011935216753045097, 0.0001193483678537652, 0.00011934455895252383, 0.00011934074686916834, 0.00011933691744507847, 0.000119333116713647, 0.00011932929644260455, 0.00011932546685762127, 0.00011932162999668037, 0.00011931781398037481, 0.00011931399860764288, 0.00011931017401035537, 0.00011930632374163017, 0.00011930247216788063, 0.00011929863743430819, 0.00011929478128403494, 0.00011929095815266516, 0.00011928711115543934, 0.00011928328157045607, 0.00011927942697548579, 0.00011927556894812276, 0.00011927172652742064, 0.00011926786277940296, 0.0001192640156927919, 0.00011926016519166511, 0.00011925628304816737, 0.00011925244316600574, 0.00011924859502464899, 0.00011924472628893555, 0.00011924086066382807, 0.00011923699478844195, 0.00011923313904934076, 0.00011922926761419341, 0.00011922540753097011, 0.00011922151248024534, 0.00011921763695125452, 0.0001192137664993447, 0.00011920987936815119, 0.00011920598079564841, 0.0001192020827952111, 0.00011919819779138605, 0.00011919429324795, 0.00011919039124305445, 0.00011918647608065323, 0.0001191825528199503, 0.00011917861379169303, 0.00011917470762144582, 0.00011917078477191495, 0.00011916683274704542, 0.00011916291038019477, 0.00011915896709720062, 0.00011915501408909359, 0.00011915105920389675, 0.00011914707521486945, 0.00011914309809062772, 0.00011913913723449761, 0.00011913516999626216, 0.00011913117284972921, 0.00011912715257029907, 0.00011912316228855167, 0.00011911916530291213, 0.00011911517108821468, 0.00011911117304782943, 0.00011910715881084076, 0.00011910315594365317, 0.00011909913909661581, 0.00011909514464951677, 0.00011909111407290828, 0.00011908708480132415, 0.0001190830580682805, 0.0001190790399162188, 0.0001190750056569389, 0.00011907096135075929, 0.00011906692210378363, 0.00011906289397633041, 0.00011905886225559101, 0.00011905480110923432, 0.00011905072222884827, 0.0001190466360725046], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-04-29 22:34:56,765 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:92]: done!
[2018-04-29 22:34:56,765 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:152]: >> Executing classifier part ... 
[2018-04-29 22:34:56,765 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:97]: =======================================
[2018-04-29 22:34:56,765 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:101]: setting configurations for classifier: 
	 {'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7f12f09bb908>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9, 'activation': 'sigmoid'}
[2018-04-29 22:34:57,045 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:110]: training ... 
[2018-04-30 03:38:06,946 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:122]: trained!
[2018-04-30 03:38:06,948 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:125]: Training history: 
{'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.00012020240401309166, 0.00012019851455159322, 0.0001201946221749717, 0.00012019072970354942, 0.00012018685235284688, 0.00012018295675299983, 0.00012017905911493664, 0.00012017516875283107, 0.00012017125675245416, 0.00012016739333746197, 0.00012016349133856424, 0.00012015957673116669, 0.000120155687364469, 0.00012015178780669062, 0.00012014787900583904, 0.00012014400366965244, 0.00012014008128859333, 0.00012013618037990425, 0.0001201322801585206, 0.000120128367186436, 0.00012012444741239753, 0.00012012052725915606, 0.00012011660630010821, 0.0001201126948448356, 0.0001201087780333206, 0.00012010485368514592, 0.00012010092800976072, 0.00012009697650117096, 0.00012009306258107885, 0.00012008912332548612, 0.00012008517705463784, 0.00012008122071120978, 0.00012007727723698365, 0.00012007334776956843, 0.00012006939576327474, 0.00012006542261641364, 0.000120061444350312, 0.00012005748603976839, 0.0001200535126796056, 0.00012004956358843498, 0.0001200455892565645, 0.0001200416276279946, 0.0001200376596951748, 0.00012003368515000263, 0.00012002973065518922, 0.00012002575321859416, 0.00012002179566645654, 0.0001200178320233707, 0.00012001384190717525, 0.00012000989035118511, 0.00012000593310714994, 0.00012000195213922691, 0.00011999797946636957, 0.00011999400975603568, 0.00011999005277270257, 0.00011998607799052852, 0.0001199821108635151, 0.00011997811140944568, 0.00011997414525413995, 0.00011997017262868299, 0.00011996618694442261, 0.0001199621832717198, 0.00011995817777410253, 0.00011995419190024065, 0.0001199501877061337, 0.00011994618604794684, 0.00011994217256336633, 0.00011993815294043723, 0.0001199341208275092, 0.00011993011838721615, 0.00011992610177421087, 0.00011992204757270795, 0.00011991801787719907, 0.00011991398190114046, 0.00011990992170349007, 0.00011990586079483402, 0.0001199017750957817, 0.00011989769847390124, 0.00011989363588253188, 0.00011988956208097376, 0.00011988546550353528, 0.00011988134785662996, 0.00011987725599552884, 0.00011987316332862133, 0.00011986906452336522, 0.00011986496294518715, 0.00011986085096262668, 0.00011985674682482834, 0.00011985262562289488, 0.00011984852551412845, 0.0001198443956379263, 0.00011984027611870616, 0.00011983614804371827, 0.00011983202752909025, 0.00011982789435856203, 0.00011982374985934409, 0.00011981960673473702, 0.00011981547003288083, 0.00011981133700455372, 0.00011980717394808884, 0.00011980299003545881], 'val_loss': [0.00011943549499703979, 0.00011943171838174302, 0.00011942794732620748, 0.00011942418256339203, 0.00011942040496485774, 0.00011941662475627479, 0.00011941284658567504, 0.00011940904978719364, 0.00011940530104221116, 0.00011940151682916996, 0.00011939771920834446, 0.00011939394561426842, 0.00011939015625263805, 0.00011938636206420532, 0.00011938259631815236, 0.00011937878798897646, 0.00011937499347875692, 0.0001193712065662818, 0.000119367404529826, 0.0001193635881917336, 0.00011935978002345111, 0.00011935596899484131, 0.00011935216753045097, 0.0001193483678537652, 0.00011934455895252383, 0.00011934074686916834, 0.00011933691744507847, 0.000119333116713647, 0.00011932929644260455, 0.00011932546685762127, 0.00011932162999668037, 0.00011931781398037481, 0.00011931399860764288, 0.00011931017401035537, 0.00011930632374163017, 0.00011930247216788063, 0.00011929863743430819, 0.00011929478128403494, 0.00011929095815266516, 0.00011928711115543934, 0.00011928328157045607, 0.00011927942697548579, 0.00011927556894812276, 0.00011927172652742064, 0.00011926786277940296, 0.0001192640156927919, 0.00011926016519166511, 0.00011925628304816737, 0.00011925244316600574, 0.00011924859502464899, 0.00011924472628893555, 0.00011924086066382807, 0.00011923699478844195, 0.00011923313904934076, 0.00011922926761419341, 0.00011922540753097011, 0.00011922151248024534, 0.00011921763695125452, 0.0001192137664993447, 0.00011920987936815119, 0.00011920598079564841, 0.0001192020827952111, 0.00011919819779138605, 0.00011919429324795, 0.00011919039124305445, 0.00011918647608065323, 0.0001191825528199503, 0.00011917861379169303, 0.00011917470762144582, 0.00011917078477191495, 0.00011916683274704542, 0.00011916291038019477, 0.00011915896709720062, 0.00011915501408909359, 0.00011915105920389675, 0.00011914707521486945, 0.00011914309809062772, 0.00011913913723449761, 0.00011913516999626216, 0.00011913117284972921, 0.00011912715257029907, 0.00011912316228855167, 0.00011911916530291213, 0.00011911517108821468, 0.00011911117304782943, 0.00011910715881084076, 0.00011910315594365317, 0.00011909913909661581, 0.00011909514464951677, 0.00011909111407290828, 0.00011908708480132415, 0.0001190830580682805, 0.0001190790399162188, 0.0001190750056569389, 0.00011907096135075929, 0.00011906692210378363, 0.00011906289397633041, 0.00011905886225559101, 0.00011905480110923432, 0.00011905072222884827, 0.0001190466360725046], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2018-04-30 03:38:06,948 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:129]: evaluating model ... 
[2018-04-30 03:38:10,230 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:133]: evaluated! 
[2018-04-30 03:38:10,231 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:135]: generating reports ... 
[2018-04-30 03:38:11,651 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:138]: done!
[2018-04-30 03:38:11,651 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_6 finished!
