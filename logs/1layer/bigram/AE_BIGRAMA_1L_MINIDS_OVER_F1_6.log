[2017-12-14 09:31:57,743 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_6
[2017-12-14 09:31:57,743 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:146]: >> Printing header log
[2017-12-14 09:31:57,743 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_6
	layers = 9216,14746
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fb5470f9e48>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7fb54715b390>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 09:31:57,743 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:148]: >> Loading dataset... 
[2017-12-14 09:32:21,448 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 09:32:21,448 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:150]: >> Executing autoencoder part ... 
[2017-12-14 09:32:21,448 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:57]: =======================================
[2017-12-14 09:32:21,448 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7fb5470f9e48>, 'discard_decoder_function': True}
[2017-12-14 09:32:21,490 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:73]: training and evaluate autoencoder
[2017-12-14 10:18:54,983 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:145]: >> Initializing execution of experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_6
[2017-12-14 10:18:54,984 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:146]: >> Printing header log
[2017-12-14 10:18:54,984 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:35]: 
	=======================================
	network_name = AE_BIGRAMA_1L_MINIDS_OVER_F1_6
	layers = 9216,14746
	using GLOBAL obj = 
		{'numpy_seed': 666, 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'log_dir': '/home/dhiegorp/deepnn/logs/1layer/bigram/', 'reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/', 'fullds_reports_dir': '/home/dhiegorp/deepnn/reports/1layer/bigram/fullds/', 'tensorflow_dir': '/home/dhiegorp/deepnn/tensorflow/1layer/bigram/', 'checkpoints_dir': '/home/dhiegorp/deepnn/checkpoints/1layer/bigram/', 'executed_path': '/home/dhiegorp/deepnn/executed/1layer/bigram/', 'data_dir': '/home/dhiegorp/malware_dataset/', 'fullds_data_dir': '/home/dhiegorp/malware_dataset/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'epochs': 200, 'batch': 32, 'store_history': True, 'shuffle_batches': True, 'autoencoder_configs': {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7efc9cc1eeb8>, 'discard_decoder_function': True}, 'mlp_configs': {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7efc9cc01400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}}
	=======================================
	
[2017-12-14 10:18:54,984 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:148]: >> Loading dataset... 
[2017-12-14 10:19:17,696 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:52]: 
	=======================================
	loading malware dataset on = /home/dhiegorp/malware_dataset/	
	trainx shape = (1228, 9216)
	trainy shape = (1228, 9)
	valx shape = (407, 9216)
	valy shape = (407, 9)
	=======================================
	
[2017-12-14 10:19:17,696 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:150]: >> Executing autoencoder part ... 
[2017-12-14 10:19:17,696 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:57]: =======================================
[2017-12-14 10:19:17,696 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:62]: setting configurations for autoencoder: 
	 {'hidden_layer_activation': 'relu', 'output_layer_activation': 'relu', 'loss_function': 'mse', 'optimizer': <keras.optimizers.SGD object at 0x7efc9cc1eeb8>, 'discard_decoder_function': True}
[2017-12-14 10:19:17,740 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:73]: training and evaluate autoencoder
[2017-12-15 02:59:48,284 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:85]: trained and evaluated!
[2017-12-15 02:59:48,286 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:88]: Training history: 
{'val_loss': [0.00011916941900137487, 0.00011916625546148454, 0.00011916309773163408, 0.00011915992503869633, 0.00011915676746973929, 0.00011915360973988882, 0.00011915043361455829, 0.00011914725471828568, 0.00011914407998736472, 0.00011914090999386088, 0.00011913776059471372, 0.00011913460588608399, 0.00011913144962215127, 0.00011912828502751525, 0.00011912511372898708, 0.00011912195043937538, 0.00011911879042126308, 0.00011911563612380539, 0.00011911247152916936, 0.00011910930382392738, 0.00011910614838233877, 0.00011910298452066161, 0.0001190998173874851, 0.00011909665720847937, 0.00011909348680380348, 0.00011909033365047671, 0.00011908716619551337, 0.00011908400903772837, 0.00011908083586211039, 0.00011907767559371943, 0.00011907451385941073, 0.00011907134910388129, 0.00011906818597516301, 0.00011906501852019967, 0.0001190618580909153, 0.00011905869619571318, 0.00011905552464690635, 0.0001190523714935796, 0.0001190492206285147, 0.00011904604475346282, 0.00011904289569397953, 0.00011903972823901619, 0.00011903656282203606, 0.00011903340648659517, 0.00011903024548524534, 0.00011902708947159125, 0.00011902394270036982, 0.00011902077671132424, 0.00011901762191330927, 0.00011901446867059728, 0.00011901131240666457, 0.00011900814657851239, 0.0001190049906363665, 0.00011900184197017822, 0.00011899867442582965, 0.00011899553720095064, 0.00011899238878504101, 0.00011898924265739322, 0.00011898607641806899, 0.00011898291370052277, 0.00011897977386559509, 0.00011897662863179959, 0.00011897348021588995, 0.00011897031553186869, 0.00011896716000089486, 0.00011896400945761679, 0.00011896085916461734, 0.00011895771001574882, 0.00011895455325125884, 0.00011895138627897573, 0.00011894822225640516, 0.00011894506011092441, 0.00011894189812633705, 0.00011893873843001156, 0.00011893558838729077, 0.00011893242770772775, 0.00011892926311309172, 0.00011892609941230798, 0.00011892294273720321, 0.00011891976530684836, 0.00011891661901830716, 0.00011891344763039376, 0.00011891030060889369, 0.00011890716191809694, 0.00011890400377707441, 0.00011890084915782991, 0.00011889770303018212, 0.00011889455461427249, 0.00011889138773137461, 0.00011888824070987454, 0.00011888507439904213, 0.00011888191298652024, 0.00011887874880305626, 0.0001188755929502956, 0.00011887244078020636, 0.00011886931149273566, 0.00011886615385227042, 0.000118862993011814, 0.00011885983258252963, 0.00011885667714094102, 0.00011885351394071457, 0.0001188503625035842, 0.00011884722545747566, 0.00011884407049856727, 0.00011884090231064506, 0.00011883773984337747, 0.00011883458334704316, 0.00011883143076578188, 0.00011882826944264522, 0.00011882509226256899, 0.00011882193093943233, 0.00011881877835817105, 0.00011881561408532184, 0.00011881244761359601, 0.00011880928098097677, 0.00011880610739418674, 0.00011880295506320409, 0.00011879981122381815, 0.00011879666257550693, 0.00011879351367691705, 0.00011879035006551854, 0.00011878718874238188, 0.00011878402471981133, 0.00011878087549943463, 0.0001187777163751746, 0.00011877456674362585, 0.00011877140141603095, 0.00011876824752974532, 0.00011876508465130569, 0.00011876192815497137, 0.00011875877639605419, 0.00011875561662822051, 0.0001187524683016961, 0.00011874930068583934, 0.00011874613920180928, 0.0001187429804887213, 0.00011873983257336895, 0.00011873667950942741, 0.00011873351859746281, 0.00011873035425310543, 0.0001187272013500573, 0.00011872404517550983, 0.00011872087266134254, 0.0001187177150208773, 0.00011871456612228744, 0.00011871139360812015, 0.00011870822926376276, 0.00011870505960992279, 0.00011870188577285411, 0.00011869871783521054, 0.00011869555806737686, 0.00011869237500575259, 0.00011868920068600368, 0.00011868603094277849, 0.00011868285245767792, 0.00011867967177370073, 0.00011867650628521242, 0.0001186733324660208, 0.00011867018201212794, 0.00011866700753148563, 0.00011866382185981268, 0.00011866063078927192, 0.00011865745516449867, 0.00011865426875986685, 0.00011865109885574824, 0.00011864791032374797, 0.00011864472424090298, 0.00011864154486195013, 0.00011863835321934391, 0.0001186351813666273, 0.00011863200459772313, 0.0001186288172098538, 0.00011862562508456734, 0.00011862243090342061, 0.00011861924980827138, 0.00011861604844055227, 0.00011861285173874211, 0.00011860967897429618, 0.00011860649199759889, 0.00011860330747005688, 0.00011860009834369992, 0.00011859690940052761, 0.00011859371285961085, 0.00011859051876784936, 0.00011858732353195694, 0.00011858412829606453, 0.00011858092864454182, 0.00011857774068460702, 0.00011857458116705198, 0.00011857139949983728, 0.00011856821881586008, 0.0001185650293006223, 0.00011856185049373492, 0.00011855867136506071, 0.00011855549672352498, 0.00011855230320382896, 0.00011854912734665412, 0.00011854595613751116, 0.00011854277504236193, 0.00011853961078738977], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.0001200340157676206, 0.00012003076715919671, 0.00012002751473504262, 0.0001200242650364101, 0.00012002100922312915, 0.0001200177632217259, 0.00012001451513470615, 0.00012001125214026834, 0.00012000798675211158, 0.00012000472501378373, 0.00012000146664088252, 0.00011999822917154684, 0.00011999498530316049, 0.00011999173911215574, 0.00011998848427058249, 0.00011998522731969254, 0.00011998197046360335, 0.00011997871993546425, 0.00011997547774979123, 0.0001199722190213872, 0.0001199689658151269, 0.00011996571651939757, 0.00011996246456924722, 0.00011995920631484696, 0.00011995595837002833, 0.00011995269587329446, 0.00011994945186270699, 0.00011994619429561216, 0.00011994295014282355, 0.00011993968629517899, 0.00011993643406062639, 0.000119933177180837, 0.00011992992710300148, 0.00011992667107641884, 0.00011992341369892552, 0.00011992015561042657, 0.00011991690207236365, 0.00011991364113984217, 0.00011991040279359954, 0.00011990714925553662, 0.00011990387967244664, 0.00011990063919318712, 0.0001198973766253527, 0.00011989411486332465, 0.0001198908613963623, 0.00011988760558308136, 0.00011988435365663119, 0.00011988111222936418, 0.00011987785205524869, 0.00011987460297282106, 0.00011987135500430223, 0.00011986810016272899, 0.00011986483449016997, 0.00011986158706675548, 0.00011985833928783816, 0.00011985507605639848, 0.00011985183439212957, 0.00011984858192057509, 0.00011984534186791896, 0.00011984207150272279, 0.00011983881168411013, 0.000119835571631454, 0.00011983232783416821, 0.00011982907804073493, 0.00011982581767701796, 0.0001198225662956721, 0.00011981931344491464, 0.00011981606557119656, 0.00011981282066000195, 0.00011980955885057353, 0.00011980629372311884, 0.00011980302968587277, 0.00011979977498650066, 0.00011979651194466247, 0.00011979325648688435, 0.00011979001411160982, 0.00011978675471960054, 0.00011978349129855936, 0.00011978023866110357, 0.00011977698512304065, 0.00011977371350173454, 0.00011977047183746562, 0.00011976720848752501, 0.00011976396419253527, 0.00011976072679430016, 0.0001197574746545483, 0.00011975422078468275, 0.00011975098319684613, 0.000119747735441629, 0.00011974448074225689, 0.00011974123798777935, 0.0001197379783587682, 0.00011973471820835291, 0.00011973146241877216, 0.00011972821513755879, 0.00011972496446721857, 0.00011972173538774931, 0.00011971848035657457, 0.00011971522975733492, 0.00011971196967802019, 0.00011970872161470062, 0.00011970546236489247, 0.00011970221332986521, 0.00011969897863345148, 0.00011969572831861408, 0.00011969246290675713, 0.00011968920906059177, 0.00011968595165939826, 0.00011968269850053833, 0.00011967944685849043, 0.00011967616947803871, 0.00011967291240864782, 0.00011966966344472111, 0.00011966638995110018, 0.0001196631311278954, 0.00011965986126040318, 0.00011965659172471357, 0.00011965333226160374, 0.00011965009130834047, 0.00011964684288951808, 0.00011964359539500301, 0.00011964033187916107, 0.00011963707175244598, 0.00011963381063032301, 0.00011963056185599781, 0.00011962729627823954, 0.00011962405425846783, 0.00011962078320596622, 0.00011961753509524627, 0.00011961426906718445, 0.00011961100808726259, 0.0001196077546914008, 0.00011960449546529283, 0.00011960124794707758, 0.00011959798101840863, 0.0001195947190193787, 0.00011959146026727449, 0.0001195882174416964, 0.00011958495745718243, 0.00011958169232972773, 0.00011957842983299386, 0.0001195751687108709, 0.00011957191507800722, 0.00011956864070747933, 0.00011956537953795598, 0.0001195621284173122, 0.00011955885833651829, 0.00011955559273505984, 0.00011955232125595485, 0.00011954904657732452, 0.00011954576850956737, 0.00011954251053956935, 0.00011953921946040916, 0.00011953594639339159, 0.00011953267512758829, 0.00011952939798413846, 0.00011952611740416141, 0.00011952285279811084, 0.00011951958451853118, 0.00011951633569680561, 0.00011951306836523346, 0.00011950979427910783, 0.00011950651803626512, 0.00011950325407011962, 0.00011949998057649868, 0.00011949672255910029, 0.00011949344721686471, 0.00011949017168502763, 0.00011948690461415755, 0.00011948362583539476, 0.0001194803654479776, 0.00011947710221653792, 0.00011947382514418864, 0.00011947054847474256, 0.00011946727062028708, 0.00011946399788507215, 0.00011946070607120614, 0.00011945741906847823, 0.00011945415327741828, 0.00011945087599176731, 0.00011944759982002516, 0.00011944429359644505, 0.0001194410165003956, 0.00011943773317119677, 0.00011943444915469251, 0.00011943116784000963, 0.00011942787571804118, 0.00011942459065872866, 0.00011942131159556362, 0.00011941805812860126, 0.00011941479110513155, 0.00011941151751670986, 0.00011940823954375345, 0.000119404965102125, 0.00011940169888446169, 0.00011939842477463586, 0.0001193951395968224, 0.00011939187757409229, 0.00011938861064542334, 0.00011938533919001853], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 02:59:48,287 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:92]: done!
[2017-12-15 02:59:48,287 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:152]: >> Executing classifier part ... 
[2017-12-15 02:59:48,287 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:97]: =======================================
[2017-12-15 02:59:48,287 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:101]: setting configurations for classifier: 
	 {'activation': 'sigmoid', 'loss_function': 'categorical_crossentropy', 'optimizer': <keras.optimizers.SGD object at 0x7efc9cc01400>, 'use_last_dim_as_classifier': False, 'classifier_dim': 9}
[2017-12-15 02:59:48,560 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:110]: training ... 
[2017-12-15 08:44:50,070 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:122]: trained!
[2017-12-15 08:44:50,074 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:125]: Training history: 
{'val_loss': [0.00011916941900137487, 0.00011916625546148454, 0.00011916309773163408, 0.00011915992503869633, 0.00011915676746973929, 0.00011915360973988882, 0.00011915043361455829, 0.00011914725471828568, 0.00011914407998736472, 0.00011914090999386088, 0.00011913776059471372, 0.00011913460588608399, 0.00011913144962215127, 0.00011912828502751525, 0.00011912511372898708, 0.00011912195043937538, 0.00011911879042126308, 0.00011911563612380539, 0.00011911247152916936, 0.00011910930382392738, 0.00011910614838233877, 0.00011910298452066161, 0.0001190998173874851, 0.00011909665720847937, 0.00011909348680380348, 0.00011909033365047671, 0.00011908716619551337, 0.00011908400903772837, 0.00011908083586211039, 0.00011907767559371943, 0.00011907451385941073, 0.00011907134910388129, 0.00011906818597516301, 0.00011906501852019967, 0.0001190618580909153, 0.00011905869619571318, 0.00011905552464690635, 0.0001190523714935796, 0.0001190492206285147, 0.00011904604475346282, 0.00011904289569397953, 0.00011903972823901619, 0.00011903656282203606, 0.00011903340648659517, 0.00011903024548524534, 0.00011902708947159125, 0.00011902394270036982, 0.00011902077671132424, 0.00011901762191330927, 0.00011901446867059728, 0.00011901131240666457, 0.00011900814657851239, 0.0001190049906363665, 0.00011900184197017822, 0.00011899867442582965, 0.00011899553720095064, 0.00011899238878504101, 0.00011898924265739322, 0.00011898607641806899, 0.00011898291370052277, 0.00011897977386559509, 0.00011897662863179959, 0.00011897348021588995, 0.00011897031553186869, 0.00011896716000089486, 0.00011896400945761679, 0.00011896085916461734, 0.00011895771001574882, 0.00011895455325125884, 0.00011895138627897573, 0.00011894822225640516, 0.00011894506011092441, 0.00011894189812633705, 0.00011893873843001156, 0.00011893558838729077, 0.00011893242770772775, 0.00011892926311309172, 0.00011892609941230798, 0.00011892294273720321, 0.00011891976530684836, 0.00011891661901830716, 0.00011891344763039376, 0.00011891030060889369, 0.00011890716191809694, 0.00011890400377707441, 0.00011890084915782991, 0.00011889770303018212, 0.00011889455461427249, 0.00011889138773137461, 0.00011888824070987454, 0.00011888507439904213, 0.00011888191298652024, 0.00011887874880305626, 0.0001188755929502956, 0.00011887244078020636, 0.00011886931149273566, 0.00011886615385227042, 0.000118862993011814, 0.00011885983258252963, 0.00011885667714094102, 0.00011885351394071457, 0.0001188503625035842, 0.00011884722545747566, 0.00011884407049856727, 0.00011884090231064506, 0.00011883773984337747, 0.00011883458334704316, 0.00011883143076578188, 0.00011882826944264522, 0.00011882509226256899, 0.00011882193093943233, 0.00011881877835817105, 0.00011881561408532184, 0.00011881244761359601, 0.00011880928098097677, 0.00011880610739418674, 0.00011880295506320409, 0.00011879981122381815, 0.00011879666257550693, 0.00011879351367691705, 0.00011879035006551854, 0.00011878718874238188, 0.00011878402471981133, 0.00011878087549943463, 0.0001187777163751746, 0.00011877456674362585, 0.00011877140141603095, 0.00011876824752974532, 0.00011876508465130569, 0.00011876192815497137, 0.00011875877639605419, 0.00011875561662822051, 0.0001187524683016961, 0.00011874930068583934, 0.00011874613920180928, 0.0001187429804887213, 0.00011873983257336895, 0.00011873667950942741, 0.00011873351859746281, 0.00011873035425310543, 0.0001187272013500573, 0.00011872404517550983, 0.00011872087266134254, 0.0001187177150208773, 0.00011871456612228744, 0.00011871139360812015, 0.00011870822926376276, 0.00011870505960992279, 0.00011870188577285411, 0.00011869871783521054, 0.00011869555806737686, 0.00011869237500575259, 0.00011868920068600368, 0.00011868603094277849, 0.00011868285245767792, 0.00011867967177370073, 0.00011867650628521242, 0.0001186733324660208, 0.00011867018201212794, 0.00011866700753148563, 0.00011866382185981268, 0.00011866063078927192, 0.00011865745516449867, 0.00011865426875986685, 0.00011865109885574824, 0.00011864791032374797, 0.00011864472424090298, 0.00011864154486195013, 0.00011863835321934391, 0.0001186351813666273, 0.00011863200459772313, 0.0001186288172098538, 0.00011862562508456734, 0.00011862243090342061, 0.00011861924980827138, 0.00011861604844055227, 0.00011861285173874211, 0.00011860967897429618, 0.00011860649199759889, 0.00011860330747005688, 0.00011860009834369992, 0.00011859690940052761, 0.00011859371285961085, 0.00011859051876784936, 0.00011858732353195694, 0.00011858412829606453, 0.00011858092864454182, 0.00011857774068460702, 0.00011857458116705198, 0.00011857139949983728, 0.00011856821881586008, 0.0001185650293006223, 0.00011856185049373492, 0.00011855867136506071, 0.00011855549672352498, 0.00011855230320382896, 0.00011854912734665412, 0.00011854595613751116, 0.00011854277504236193, 0.00011853961078738977], 'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'loss': [0.0001200340157676206, 0.00012003076715919671, 0.00012002751473504262, 0.0001200242650364101, 0.00012002100922312915, 0.0001200177632217259, 0.00012001451513470615, 0.00012001125214026834, 0.00012000798675211158, 0.00012000472501378373, 0.00012000146664088252, 0.00011999822917154684, 0.00011999498530316049, 0.00011999173911215574, 0.00011998848427058249, 0.00011998522731969254, 0.00011998197046360335, 0.00011997871993546425, 0.00011997547774979123, 0.0001199722190213872, 0.0001199689658151269, 0.00011996571651939757, 0.00011996246456924722, 0.00011995920631484696, 0.00011995595837002833, 0.00011995269587329446, 0.00011994945186270699, 0.00011994619429561216, 0.00011994295014282355, 0.00011993968629517899, 0.00011993643406062639, 0.000119933177180837, 0.00011992992710300148, 0.00011992667107641884, 0.00011992341369892552, 0.00011992015561042657, 0.00011991690207236365, 0.00011991364113984217, 0.00011991040279359954, 0.00011990714925553662, 0.00011990387967244664, 0.00011990063919318712, 0.0001198973766253527, 0.00011989411486332465, 0.0001198908613963623, 0.00011988760558308136, 0.00011988435365663119, 0.00011988111222936418, 0.00011987785205524869, 0.00011987460297282106, 0.00011987135500430223, 0.00011986810016272899, 0.00011986483449016997, 0.00011986158706675548, 0.00011985833928783816, 0.00011985507605639848, 0.00011985183439212957, 0.00011984858192057509, 0.00011984534186791896, 0.00011984207150272279, 0.00011983881168411013, 0.000119835571631454, 0.00011983232783416821, 0.00011982907804073493, 0.00011982581767701796, 0.0001198225662956721, 0.00011981931344491464, 0.00011981606557119656, 0.00011981282066000195, 0.00011980955885057353, 0.00011980629372311884, 0.00011980302968587277, 0.00011979977498650066, 0.00011979651194466247, 0.00011979325648688435, 0.00011979001411160982, 0.00011978675471960054, 0.00011978349129855936, 0.00011978023866110357, 0.00011977698512304065, 0.00011977371350173454, 0.00011977047183746562, 0.00011976720848752501, 0.00011976396419253527, 0.00011976072679430016, 0.0001197574746545483, 0.00011975422078468275, 0.00011975098319684613, 0.000119747735441629, 0.00011974448074225689, 0.00011974123798777935, 0.0001197379783587682, 0.00011973471820835291, 0.00011973146241877216, 0.00011972821513755879, 0.00011972496446721857, 0.00011972173538774931, 0.00011971848035657457, 0.00011971522975733492, 0.00011971196967802019, 0.00011970872161470062, 0.00011970546236489247, 0.00011970221332986521, 0.00011969897863345148, 0.00011969572831861408, 0.00011969246290675713, 0.00011968920906059177, 0.00011968595165939826, 0.00011968269850053833, 0.00011967944685849043, 0.00011967616947803871, 0.00011967291240864782, 0.00011966966344472111, 0.00011966638995110018, 0.0001196631311278954, 0.00011965986126040318, 0.00011965659172471357, 0.00011965333226160374, 0.00011965009130834047, 0.00011964684288951808, 0.00011964359539500301, 0.00011964033187916107, 0.00011963707175244598, 0.00011963381063032301, 0.00011963056185599781, 0.00011962729627823954, 0.00011962405425846783, 0.00011962078320596622, 0.00011961753509524627, 0.00011961426906718445, 0.00011961100808726259, 0.0001196077546914008, 0.00011960449546529283, 0.00011960124794707758, 0.00011959798101840863, 0.0001195947190193787, 0.00011959146026727449, 0.0001195882174416964, 0.00011958495745718243, 0.00011958169232972773, 0.00011957842983299386, 0.0001195751687108709, 0.00011957191507800722, 0.00011956864070747933, 0.00011956537953795598, 0.0001195621284173122, 0.00011955885833651829, 0.00011955559273505984, 0.00011955232125595485, 0.00011954904657732452, 0.00011954576850956737, 0.00011954251053956935, 0.00011953921946040916, 0.00011953594639339159, 0.00011953267512758829, 0.00011952939798413846, 0.00011952611740416141, 0.00011952285279811084, 0.00011951958451853118, 0.00011951633569680561, 0.00011951306836523346, 0.00011950979427910783, 0.00011950651803626512, 0.00011950325407011962, 0.00011949998057649868, 0.00011949672255910029, 0.00011949344721686471, 0.00011949017168502763, 0.00011948690461415755, 0.00011948362583539476, 0.0001194803654479776, 0.00011947710221653792, 0.00011947382514418864, 0.00011947054847474256, 0.00011946727062028708, 0.00011946399788507215, 0.00011946070607120614, 0.00011945741906847823, 0.00011945415327741828, 0.00011945087599176731, 0.00011944759982002516, 0.00011944429359644505, 0.0001194410165003956, 0.00011943773317119677, 0.00011943444915469251, 0.00011943116784000963, 0.00011942787571804118, 0.00011942459065872866, 0.00011942131159556362, 0.00011941805812860126, 0.00011941479110513155, 0.00011941151751670986, 0.00011940823954375345, 0.000119404965102125, 0.00011940169888446169, 0.00011939842477463586, 0.0001193951395968224, 0.00011939187757409229, 0.00011938861064542334, 0.00011938533919001853], 'acc': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}
[2017-12-15 08:44:50,075 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:129]: evaluating model ... 
[2017-12-15 08:44:53,878 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:133]: evaluated! 
[2017-12-15 08:44:53,879 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:135]: generating reports ... 
[2017-12-15 08:44:55,529 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:138]: done!
[2017-12-15 08:44:55,530 AE_BIGRAMA_1L_MINIDS_OVER_F1_6.py:154]: >> experiment AE_BIGRAMA_1L_MINIDS_OVER_F1_6 finished!
